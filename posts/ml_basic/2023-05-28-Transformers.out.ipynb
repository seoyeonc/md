{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers\n",
        "\n",
        "SEOYEON CHOI  \n",
        "5/28/23\n",
        "\n",
        "> Attention is all you need\n",
        "\n",
        "ref: [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379),\n",
        "[Attention is all you\n",
        "need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "\n",
        "$\\star$ seq2seq 구조인 인코더-디코더를 따르면서 어텐션만으로 구현한\n",
        "모델, RNN을 사용하지 않고 인코더-디코더 구조로 설계하였지만 RNN보다\n",
        "우수한 성능을 보임"
      ],
      "id": "1e8fba7a-863e-4f8e-ae97-76c7d41dc037"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://github.com/huggingface/transformers"
      ],
      "id": "6ea559a3-3fa5-464f-b66b-08fc38aa2667"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://github.com/huggingface/transformers/blob/main/README_ko.md"
      ],
      "id": "12f5f23a-9a0f-42f4-aced-935d4defdb1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# seq2seq 의 한계\n",
        "\n",
        "-   인코더, 디코더로 구성되어 있는 seq2seq\n",
        "\n",
        "-   인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축\n",
        "\n",
        "    -   $\\to$ 입력 시퀀스의 정보가 일부 손실된다는 단점 존재\n",
        "    -   $\\to$ 이를 위해 어텐션 메카니즘 등장\n",
        "\n",
        "-   디코더는 이 벡터 표현을 통해 출력 시퀀스를 만듦\n",
        "\n",
        "-   $\\star$ 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아니라\n",
        "    어텐션만으로 인코더와 디코더를 만든다면??\n",
        "\n",
        "# 트랜스포머의 주요 하이퍼파라미터\n",
        "\n",
        "각 값은 논문의 설정으로서, 바뀔 수 있음\n",
        "\n",
        "$d_{model} = 512$\n",
        "\n",
        "-   인코더와 디코더에서의 정해진 입력 및 출력의 크기\n",
        "-   임베딩 벡터의 차원도 이와 같음\n",
        "-   각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때도 유지\n",
        "\n",
        "$num_layers = 6$\n",
        "\n",
        "-   하나의 인코더와 디코더를 층으로 생각하였을때, 모델에서 인코더와\n",
        "    디코더가 몇 층으로 구성되어 있는지를 의미\n",
        "\n",
        "$num_heads = 8$\n",
        "\n",
        "-   어텐션을 병렬로 수행하고 결과값을 다시 합치는 방식을 수행하기 위함,\n",
        "    즉 병렬의 개수\n",
        "\n",
        "$d_{ff} = 2048$\n",
        "\n",
        "-   트랜스포머 내부에 피드 포워드 신경망이 존재, 그 신경망의 은닉층의\n",
        "    크기를 의미\n",
        "-   단, 피드 포워드 신경망의 입력층과 출력층의 크기는 $d_{model}$\n",
        "\n",
        "# Transformer"
      ],
      "id": "613a075f-bbb4-440a-8d94-1d79b2d6d899"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "4599bf53-7394-4ebc-a98c-414ebb4d370b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   인코더-디코더 구조를 가진 트랜스포머\n",
        "-   인코더와 디코더라는 단위가 N개로 구성되는 구조 $\\to$ Encoders,\n",
        "    Decoders로 표현\n",
        "    -   seq2seq에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의\n",
        "        시점time step을 가지는 구조"
      ],
      "id": "eda217aa-787c-453f-a9a4-8377884850d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "c681a9c4-0e8a-417b-b01a-79f8f2dd8862"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   symbol인 sos를 입력받아 eos symbol 나올때까지 연산을 진행하는, RNN을\n",
        "    사용하지 않지만 인코더 디코더 구조 유지되는 모습을 보임\n",
        "\n",
        "## Positional Encoding\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> RNN이 자연어 처리에서 유용했던 이유\n",
        ">\n",
        "> -   단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의\n",
        ">     특성으로 인해 각 단어의 위치 정보 position information을 가질 수\n",
        ">     있어서\n",
        "\n",
        "**포지셔널 인코딩**\n",
        "\n",
        "-   트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에\n",
        "    위치 정보들을 더하여 모델의 입력으로 사용"
      ],
      "id": "e50d0421-ad76-499e-a41b-8fe2efc4b379"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "0605fa6c-cec3-40df-92e6-07dfcf4780e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기\n",
        "    전에 포지셔널 인코딩 값이 더해지는 과정\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n",
        "\n",
        "-   트랜스포머가 위치 정보를 가진 값을 만들기 위해 사용하는 두 개의 함수"
      ],
      "id": "fa49dc6f-66bc-4d1f-b722-09ef940316ce"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "0c03865b-884e-48cf-8218-7c6d264ae98f"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_model = pd.DataFrame(np.empty((4, 4), dtype=str), index=['I', 'am', 'a', 'student'])\n",
        "\n",
        "values = ['pos1,i1', 'pos1,i2', 'pos1,i3', 'pos1,i4',\n",
        "          'pos2,i1', 'pos2,i2', 'pos2,i3', 'pos2,i4',\n",
        "          'pos3,i1', 'pos3,i2', 'pos3,i3', 'pos3,i4',\n",
        "          'pos4,i1', 'pos4,i2', 'pos4,i3', 'pos4,i4']\n",
        "\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        d_model.iloc[i, j] = values[i * 4 + j]"
      ],
      "id": "69131fd1-0113-4458-9ece-d9ce3b3d3fe7"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>pos1,i1</td>\n",
              "      <td>pos1,i2</td>\n",
              "      <td>pos1,i3</td>\n",
              "      <td>pos1,i4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>am</th>\n",
              "      <td>pos2,i1</td>\n",
              "      <td>pos2,i2</td>\n",
              "      <td>pos2,i3</td>\n",
              "      <td>pos2,i4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>pos3,i1</td>\n",
              "      <td>pos3,i2</td>\n",
              "      <td>pos3,i3</td>\n",
              "      <td>pos3,i4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student</th>\n",
              "      <td>pos4,i1</td>\n",
              "      <td>pos4,i2</td>\n",
              "      <td>pos4,i3</td>\n",
              "      <td>pos4,i4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "d_model"
      ],
      "id": "875fc836-5fc2-4c1a-9d44-7bf83fd92081"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   pos는 입력 문장에서 임베딩 벡터의 위치\n",
        "-   i는 임베딩 벡터 내의 차원의 인덱스를 의미\n",
        "-   위 식에 따르면\n",
        "    -   **짝수**면 *sin*함수 사용 $\\to$ (pos,2i)\n",
        "    -   **홀수**면 *cos*함수 사용 $\\to$ (pos,2i+1)\n",
        "-   여기서 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는\n",
        "    하이퍼파라미터\n",
        "-   임베딩 벡터도 같은 차원임\n",
        "-   이와 깉은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨.\n",
        "    -   **각 임베딩 멕터에 포지셔널 인코딩의 값을 더하면(차원 같음!)\n",
        "        같은 단어라도 문장 내의 위치에 따라 트랜스포머의 입력으로\n",
        "        들어가는 임베딩 벡터의 값이 달라짐**"
      ],
      "id": "eecc4abb-bc80-4d98-b158-65e4b15bc8d0"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "id": "a9a49668-4b28-449a-987a-bd98fe8f46a6"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "\n",
        "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        angle_rads = np.zeros(angle_rads.shape)\n",
        "        angle_rads[:, 0::2] = sines\n",
        "        angle_rads[:, 1::2] = cosines\n",
        "        pos_encoding = tf.constant(angle_rads)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        print(pos_encoding.shape)\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "id": "86de7698-c8eb-4c1b-bc12-e0eacbc328df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "포지셔널 인코딩 행렬 시각화"
      ],
      "id": "9427c6c4-aa2c-4023-9fcd-b697f3d86036"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 50, 128)"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nZGKz4QAAAABJRU5ErkJggg==\n"
          }
        }
      ],
      "source": [
        "# 문장의 길이 50, 임베딩 벡터의 차원 128\n",
        "sample_pos_encoding = PositionalEncoding(50, 128)\n",
        "\n",
        "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 128))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "12eb9870-18b4-4aff-b4a4-e21ab0e92432"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention\n",
        "\n",
        "**Encoder Self-Attention**\n",
        "\n",
        "-   인코더에서 이루어짐\n",
        "-   Query = Key = Value(값이 같다는 말이 아님)"
      ],
      "id": "e1b6bd1e-c1dc-44bb-a0bc-c9b20ae47104"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "e8cb0a61-4b3d-4932-a644-bbbba7e5918a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Masked Decoder Self-Attention**\n",
        "\n",
        "-   디코더에서 이루어짐\n",
        "-   Query = Key = Value(값이 같다는 말이 아님)"
      ],
      "id": "9c66a263-7bf2-460a-9f4d-8fcf44f3f4bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "a5bfa1ec-8f43-429d-aca5-97d9912cc82e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Encoder-Decoder Attention**\n",
        "\n",
        "-   디코더에서 이루어짐\n",
        "-   Query = 디코더 벡터, Key = Value = 인코더 벡터"
      ],
      "id": "9daff1ad-056f-4327-afc5-68549f012324"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "4ebe8265-8978-430f-9f80-cdc8b47b421d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Important**\n",
        ">\n",
        "> 셀프 어텐션은 Query, Key, Value가 동일한 경우를 말함\n",
        ">\n",
        "> Encoder-Decoder Attention의 경우 Query가 디코더의 벡터인 반면에 Key와\n",
        "> Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않음\n",
        ">\n",
        "> **여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아닌\n",
        "> 벡터의 출처가 같다는 의미**"
      ],
      "id": "e222d4c9-d101-4e98-be6a-d8f96dcd2038"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</p>"
            ]
          }
        }
      ],
      "source": [],
      "id": "47d281b8-8468-4cf1-9bcb-9a8605547443"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위(각각 하나의 층으로 봄)의 인코더, 디코더가 num_layer지정한 수만큼 있음\n",
        "\n",
        "# Encoder\n",
        "\n",
        "-   하나의 인코더 층은 크게 두 개의 서브층sublayer로 나뉨\n",
        "    -   셀프 어텐션 Self Attention\n",
        "    -   피드 포워드 신경망 Feed Forward Neural Network\n",
        "\n",
        "> **Tip**\n",
        ">\n",
        "> **Multi-head Self Attention**\n",
        ">\n",
        "> -   셀프 어텐션을 병렬적으로 사용하였다는 의미\n",
        ">\n",
        "> **Position-wise FFNN**\n",
        ">\n",
        "> -   순방향신경망\n",
        "\n",
        "# Self Attention of Encoder\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> Remind\n",
        ">\n",
        "> -   어텐션 함수는 주어진 Query에 대해 모든 Key와의 유사도를 구함\n",
        "> -   유사도를 가중치로 하여 Key와 맴핑되어 있는 각각의 Value에 반영해\n",
        ">     좀\n",
        "> -   이 유사도가 반영된 Value를 모두 가중합하여 Return\n",
        "\n",
        "-   seq2seq에서의 Q,K,V의 정의\n",
        "    -   Q = Query, t 시점의 디코더 셀에서의 은닉상태\n",
        "    -   K = Key, 모든 시점의 인코더 셀의 은닉상태들\n",
        "    -   V = Value, 모든 시점의 인코더 셀의 은닉상태들\n",
        "\n",
        "$\\to$ t 시점의\\~의 의미는 변하면서 반복적으로 쿼리 수행하니까 결국은\n",
        "전체 시점에 대해서 일반화 가능\n",
        "\n",
        "-   seq2seq에서의 Q,K,V의 정의\n",
        "    -   Q = Query, 모든 시점의 디코더 셀에서의 은닉상태들\n",
        "    -   K = Key, 모든 시점의 인코더 셀의 은닉상태들\n",
        "    -   V = Value, 모든 시점의 인코더 셀의 은닉상태들\n",
        "\n",
        "## 1. Self Attention\n",
        "\n",
        "-   *셀프 어텐션*\n",
        "    -   Q = 입력 문장의 모든 단어 벡터들\n",
        "    -   K = 입력 문장의 모든 단어 벡터들\n",
        "    -   V = 입력 문장의 모든 단어 벡터들\n",
        "-   **연속된 문장들에 대하여 지칭하는 단어가 다르지만 의미는 같을 수\n",
        "    있는데, 셀프 어텐션은 이 유사도를 구하여서 연관 가능성을 찾아낸다.**\n",
        "\n",
        "## 2. Q, K, V\n",
        "\n",
        "-   셀프 어텐션은 일단 문장의 각 단어 벡터로부터 Q벡터, K벡터, V벡터를\n",
        "    얻음.\n",
        "    -   Q벡터, K벡터, V벡터는 $d_{model}$ 차원을 가지는 단어 벡터들보다\n",
        "        더 작은 차원을 가짐\n",
        "    -   논문을 예로 들면 $d_{model}$의 차원은 512, Q벡터, K벡터, V벡터의\n",
        "        차원은 각각 64\n",
        "    -   이 64는 또 다른 하이퍼파라미터인 num_heads로 결정되는데,\n",
        "        트랜스포머는 $d_{model}$을 num_heads로 나눈 값을 Q벡터, K벡터,\n",
        "        V벡터의 차원으로 결정.\n",
        "    -   논문의 num_heads = 8이었으니까 $512/8 = 64$로 결정된 것\n",
        "    -   이 Q벡터, K벡터, V벡터는 단어마다, 벡터마다 서로 다른 가중치\n",
        "        행렬을 곱하여 얻음\n",
        "    -   각 단어마다 Q벡터, K벡터, V벡터 각각의 가중치, Q벡터, K벡터,\n",
        "        V벡터 각각이 존재하는 것\n",
        "\n",
        "## 3. Scaled dot-product Attention\n",
        "\n",
        "각 단어 별로 Q벡터, K벡터, V벡터를 구한 후 각 Q벡터는 모든 K벡터에\n",
        "대해서 어텐션 스코어를 구하고, 어텐션 분포를 구한 뒤 이를 사용하여 모든\n",
        "V벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함-\\> 모든 Q벡터에\n",
        "대해 반복\n",
        "\n",
        "-   내적한 후 특정값을 나눔으로써 값을 조정하는 과정 추가한 스케일드\n",
        "    갓-프로덕트 어텐션\n",
        "\n",
        "$$score(q,k) = \\frac{q k}{\\sqrt{n}}$$\n",
        "\n",
        "$\\sqrt{n}$이 결정되는 과정\n",
        "\n",
        "-   논문을 예로 들면 $d_{model}$의 차원이 512, num_heads가 8, Q,K,V의\n",
        "    차원 $d_k$가 64(512/8) 이었음, 여기서 64에 root 취한 8으로 결정되는\n",
        "    것\n",
        "\n",
        "## 4. 행렬 연산으로 일괄 처리\n",
        "\n",
        "Q 벡터마다 3을 연산하는 것을 피하기 위함\n",
        "\n",
        "1.  문장 행렬에 가중치 행렬을 곱하여 Q행렬, K행렬, V행렬을 구한다(단지\n",
        "    벡터를 행렬화한 것 뿐).\n",
        "2.  각 단어의 Q벡터와 전치한 K벡터의 내적이 각 행렬의 원소가 되는 행렬을\n",
        "    결과로 추출.\n",
        "3.  2번의 결과에 $\\sqrt{d_k}$를 나누어 softmax취한 후 V행렬을 곱하여 각\n",
        "    행과 열이 어텐션 스코어 값을 가지는 행렬을 구함.\n",
        "\n",
        "$$Attention(Q,K,V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$$\n",
        "\n",
        "입력 문장의 길이가 seq_len이라면, 문장 행렬의 크기는\n",
        "(seq_len,$d_{model}$)\n",
        "\n",
        "*차원 정리*\n",
        "\n",
        "-   $Q$ = (seq_len, $d_k)$\n",
        "    -   $W^Q = (d_{model},d_k)$\n",
        "-   $K^\\top$ = ($d_k$, seq_len)\n",
        "    -   $W^K = (d_{model},d_k)$\n",
        "-   $V$ = (seq_len, $d_v)$\n",
        "    -   $W^V = (d_{model},d_v)$\n",
        "-   논문에서는 $d_k,d_v$의 차원이 $d_{model}$/num_heads로 같게 설정함\n",
        "-   attention score matrix = (seq_len, $d_v$)\n",
        "\n",
        "## 5. Scaled dot-product attention 구현"
      ],
      "id": "bf20b406-7336-4f9f-80a1-231dc202bdfa"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "    # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
        "\n",
        "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # 스케일링\n",
        "    # dk의 루트값으로 나눠준다.\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
        "    # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
        "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "id": "e401490f-c341-440c-aa52-f11b492ed56c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "scaled_dot_product_attention 실행"
      ],
      "id": "462719a1-94f3-438e-ae02-bd505aa2730d"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 임의의 Query, Key, Value인 Q, K, V 행렬 생성\n",
        "np.set_printoptions(suppress=True)\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)"
      ],
      "id": "3d847da3-9365-47ba-ba0a-813fe8cf8a4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "query에 해당하는 \\[0,10,0\\]은 key에 해당하는 두 번째 값과 일치해야 함."
      ],
      "id": "e6df62db-4210-4eb3-9a47-ae3b2625e875"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)"
          ]
        }
      ],
      "source": [
        "# 함수 실행\n",
        "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
        "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
        "print(temp_out) # 어텐션 값"
      ],
      "id": "74f8d590-e3e3-4a62-801c-c657c5501681"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "두 번째 값과 일치했어서 두 번째가 1인 값을 반환, 결과적으로 \\[10,0\\]의\n",
        "어텐션 값 반환"
      ],
      "id": "700e2a71-7667-4da2-972f-af32522cf700"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)"
          ]
        }
      ],
      "source": [
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
        "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
        "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
        "print(temp_out) # 어텐션 값"
      ],
      "id": "ef2375ae-a2c8-43c0-98f9-e75e73c3b3a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "세 번째 값과 네 번째 값이 같이 일치하다면? 합이 1이되게 나눠서 0.5,0.5씩\n",
        "나눠짐\n",
        "\n",
        "-   \\[100,5\\] \\* 0.5 + \\[1000,6\\] \\* 0.5 = \\[550,5.5\\]"
      ],
      "id": "abfcfe23-5f71-4679-b0e6-49a99d1e4875"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "[550.0, 5.5]"
            ]
          }
        }
      ],
      "source": [
        "[100*0.5 + 1000*0.5,5*0.5 + 6*0.5]"
      ],
      "id": "a54fd84b-74a0-452a-ad8d-9888a68e1f99"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)"
          ]
        }
      ],
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
        "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
        "print(temp_out) # 어텐션 값"
      ],
      "id": "60914313-2d43-4391-bec3-30f29716c19a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Multi-head Attention\n",
        "\n",
        "왜 스케일링하여 어텐션 스코어를 구했을까?\n",
        "\n",
        "-   논문에서는 한 번의 어텐션보다 여러번의 어텐션을 병렬로 사용하는 것이\n",
        "    더 효과적이라고 판단\n",
        "-   그래서 $d_{model}$의 차원을 num_heads로 나누어\n",
        "    $d_{model}$/num_heads의 차원을 가지는 Q,K,V에 대해서 num_heads 개의\n",
        "    병렬 어텐션 수행\n",
        "-   num_heads만큼 병렬이 이뤄지는데, 이 때 나오는 각각의 **어텐션 값\n",
        "    행렬을 어텐션 헤드**라고 함.\n",
        "    -   이 때 가중치 행렬의 값$W^Q,W^K,W^V$은 num_heads의 어텐션\n",
        "        해드마다 전부 다름\n",
        "\n",
        "병렬로 수행한 효과?\n",
        "\n",
        "-   어텐션을 병렬로 수행하여 다른 시각으로 정보를 수집할 수 있음\n",
        "\n",
        "## 7. Multi-head Attention 구현\n",
        "\n",
        "가중치 행렬\n",
        "\n",
        "-   Q, K, V 행렬을 만들기 위한 가중치 행렬 $W^Q,W^K,W^V$\n",
        "-   어텐션 헤드들을 연결concatenation 후에 곱해주는 행렬 $W^O$\n",
        "\n",
        "가중치 행렬을 곱하는 것은 Dense layer 지나게 하여 구현\n",
        "\n",
        "1.  $W^Q,W^K,W^V$에 해당하는 $d_{model}$의 크기의 밀집층(Dense layer)을\n",
        "    지나게 한다.\n",
        "2.  지정된 헤드수 num_heads 만큼 나눈다(split).\n",
        "3.  scaled dot-product attention\n",
        "4.  나눠졌던 헤드들을 연결concatenatetion한다.\n",
        "5.  $W^O$에 해당하는 밀집층을 지나게 한다."
      ],
      "id": "9ed947a4-73c3-4101-8539-95e817e81724"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        # d_model을 num_heads로 나눈 값.\n",
        "        # 논문 기준 : 64\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # WO에 해당하는 밀집층 정의\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    # num_heads 개수만큼 q, k, v를 split하는 함수\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
        "        # q : (batch_size, query의 문장 길이, d_model)\n",
        "        # k : (batch_size, key의 문장 길이, d_model)\n",
        "        # v : (batch_size, value의 문장 길이, d_model)\n",
        "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # 2. 헤드 나누기\n",
        "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
        "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # 4. 헤드 연결(concatenate)하기\n",
        "        # (batch_size, query의 문장 길이, d_model)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # 5. WO에 해당하는 밀집층 지나기\n",
        "        # (batch_size, query의 문장 길이, d_model)\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "id": "24cc9ea5-eec0-4499-b042-3110c871ee55"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Padding Mask\n",
        "\n",
        "어텐션에서 제외하기 위해 값을 가리는 역할\n",
        "\n",
        "-   방법: 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주기\n",
        "    -   소프트맥스 함수를 지나면 값이 0이 되어 유사도 구할때 반영되지\n",
        "        않름."
      ],
      "id": "e50dad8e-235e-4d0c-ba2d-78e74caab897"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "    # (batch_size, 1, 1, key의 문장 길이)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "id": "7d93050b-58ca-4b03-a7be-69d60fa22df7"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)"
          ]
        }
      ],
      "source": [
        "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))"
      ],
      "id": "1ce48078-dcab-4d14-88c4-cbe53dc8a17c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Position-wise Feed Forward Neural Network\n",
        "\n",
        "인코더와 디코더에서 공통적으로 가지고 있는 서브층\n",
        "\n",
        "= FFNN(Fully Connected FFNN)\n",
        "\n",
        "$$FFNN(x) = MAX(0,xW-1 + b_1)W_2 + b_2$$\n",
        "\n",
        "$x$ -\\> $F_1 = xW_1 + b_1$ -\\> 활성화 함수:ReLU $F_2 = max(0,F_1)$ -\\>\n",
        "$F_3 = F_2W_2 + b_2$\n",
        "\n",
        "-   여기서 $x$는 멀티 헤드 어텐션의 결과로 나온 (seq_len, $d_{model}$)의\n",
        "    차원을 가지는 행렬\n",
        "-   가중치 행렬 $W_1$ = ($d_{model}, d_{ff}$)\n",
        "-   가중치 행렬 $W_2$ = ($d_{ff},d_{model}$)\n",
        "-   논문은 $d_{ff}$를 2048로 정의\n",
        "-   매개변수 $W_1,W_2,b_1,b_2$는 각 인코더 층마다 동일하게 계산되지만\n",
        "    값은 층마다 다 다르다.\n",
        "\n",
        "``` python\n",
        "# 다음의 코드는 인코더와 디코더 내부에서 사용할 예정입니다.\n",
        "outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
        "outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "```\n",
        "\n",
        "# Residual connection and Layer Normalization\n",
        "\n",
        "## 1. 잔차 연결\n",
        "\n",
        "$$H(x) = x + F(x)$$\n",
        "\n",
        "-   $F(x)$는 트랜스포머에서 서브층에 해당\n",
        "-   즉, 장차 연결은 서브층의 입력과 출력을 더하는 것\n",
        "-   서브층의 입력과 출력은 동일한 차원을 갖고 있어서 가능\n",
        "-   그래서 재귀하는 것처럼 다이어그램 그려보면 화살표가 출력층에서 나와\n",
        "    입력층으로 들어가는 모습\n",
        "-   *잔차 연결은 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는\n",
        "    기법*\n",
        "\n",
        "식으로 표현 -\\> $x + Sublayer(x)$\n",
        "\n",
        "서브층이 멀티 헤드 어텐션이었다면 \\$H(x) - x + Multi - head Attention(x)\n",
        "\n",
        "참고 : [잔차연결 관련 논문](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "## 2. 층 정규화\n",
        "\n",
        "잔차연결과 층 정규화 모두 수행한 함수\n",
        "\n",
        "$$LN = LayerNorm(x+Sublayer(x))$$\n",
        "\n",
        "텐서의 마지막 차원에 대하여 평균과 분산을 구하고, 이를 가지고 어떤\n",
        "수식을 통해 값을 정규화하여 학습을 도움\n",
        "\n",
        "-   텐서의 마지막 차원 = 트랜스포머에서는 $d_{model}$ 차원을 의미\n",
        "\n",
        "1.  평균과 분산을 통한 벡터 $x_i$ 정규화\n",
        "\n",
        "    -   스칼라인 평균과 분산 도출\n",
        "    -   $\\epsilon$은 분모가 0이 되는 것을 방지\n",
        "\n",
        "$$\\hat{x}_{i,k} = \\frac{x_{i,k} - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}}$$\n",
        "\n",
        "1.  감마와 베타 도입\n",
        "\n",
        "-   LayerNormalization(케라스에 내장되어 있음)\n",
        "\n",
        "$$ln_i = \\gamma \\hat{x}_i + \\beta = LayerNorm(x_i)$$\n",
        "\n",
        "참고: [층 정규화 관련 논문](https://arxiv.org/pdf/1607.06450.pdf)\n",
        "\n",
        "# Encoder 구현\n",
        "\n",
        "인코더 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로 어텐션 시 패딩\n",
        "토큰을 제외하도록 패딩 마스크를 사용\n",
        "\n",
        "-   multiheadattention 함수의 mask 인자값으로 padding_mask가 사용되는\n",
        "    이유\n",
        "-   인코더는 두 개의 서브층으로 이루어짐\n",
        "    -   멀티 헤드 어텐션\n",
        "    -   피드 포워드 신경망\n",
        "-   서브층 이후 드롭 아웃, 잔차 연결, 층 정규화 수행"
      ],
      "id": "23c6ffeb-a4dc-4e01-8b48-3ac6d7540ea7"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "    # 인코더는 패딩 마스크 사용\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "          'mask': padding_mask # 패딩 마스크 사용\n",
        "      })\n",
        "\n",
        "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
        "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "id": "39bcce0f-da28-4c2f-9a81-7250356f934c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder 쌓기"
      ],
      "id": "e6c4f14b-c38a-4b53-ab96-bf7d692e7f20"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encoder(vocab_size, num_layers, dff,\n",
        "            d_model, num_heads, dropout,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "    # 인코더는 패딩 마스크 사용\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    # 포지셔널 인코딩 + 드롭아웃\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    # 인코더를 num_layers개 쌓기\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "            dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "id": "6692068c-707c-4dd4-9b54-fc9fdbc2f93f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "인코더 층을 num_layers 만큼 쌓는 클래스\n",
        "\n",
        "# Encoder에서 Decoder로\n",
        "\n",
        "인코더에서 num_layers만큼 총 연산을 순차적으로 한 후 마지막 층의\n",
        "인코더의 출력을 디코더로 전달\n",
        "\n",
        "# Decoder: Self-Attention and Look-ahead Mask\n",
        "\n",
        "트랜스포머는 문장 행렬로 입력을 한 번에 받기 때문에 현재 시점의 단어를\n",
        "예측하고자 할 때 입력 문장 행렬로부터 미래 시점의 단어까지 참고하느\n",
        "현상이 발생\n",
        "\n",
        "이를 위해 디코더에서 현재 시점의 예측에서 현재 시점보다 미래에 있는\n",
        "단어들을 참고하지 못하도록 룩-어헤드 마스크 도입\n",
        "\n",
        "**디코더의 첫번때 서브층에서 이루어짐**\n",
        "\n",
        "**디코더의 셀프 어텐션은 인코더의 멀티 헤드 셀프 어텐션과 동일한 연산을\n",
        "수행하나, 어텐션 스코어 행렬에서 마스킹을 적용하는 점이 다름**\n",
        "\n",
        "-\\> 미리보기 방지를 위함\n",
        "\n",
        "트랜스포머 마스킹의 종류\n",
        "\n",
        "1.  인코더의 셀프 어텐션 = 패딩 마스크를 전달\n",
        "2.  디코더의 첫번째 서브층인 마스크드 셀프 어텐션 = 룩-어헤드 마스크를\n",
        "    전달\n",
        "3.  디코더의 두번째 서브층인 인코더-디코더 어텐션 = 패딩 마스크를 전달"
      ],
      "id": "e2a79fa9-cf80-4755-a55c-a8dea03a555a"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "id": "aa6e4d36-05ec-4fb6-b9a9-169a070bb960"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "마스킹을 하고자 하는 위치에 1, 마스킹을 하지 않고자 하는 위이에 0을 리턴"
      ],
      "id": "095255e4-63e5-49fd-9ac8-baae62e851d7"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 1.]\n",
            "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)"
          ]
        }
      ],
      "source": [
        "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
      ],
      "id": "d528dd6d-6280-4c80-b640-0ccc11d2eb1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2nd Decoder sublayer : Encoder-Decoder Attention\n",
        "\n",
        "디코더 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의\n",
        "어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이건\n",
        "**셀프 어텐션이 아님!**\n",
        "\n",
        "-   **셀프 어텐션은 Query, Key, Value가 출처가 같은 경우를 말하는데,\n",
        "    인코더-디코더 어텐션은 Query가 디코더인 행렬인 반면, Key, Value가\n",
        "    인코더 행렬이기 때문**\n",
        "\n",
        "-   인코더의 첫번째 서브층 = Query = Key = Value\n",
        "\n",
        "-   디코더의 첫번째 서브층 = Query = Key = Value\n",
        "\n",
        "-   디코더의 두번째 서브층 = Query = 디코더 행렬(의 첫번째 서브층 결과),\n",
        "    Key = Value = 인코더 행렬(의 마지막 층에서 얻은 값)\n",
        "\n",
        "# Decoder 구현\n",
        "\n",
        "첫번째 서브층은 mask 인자값으로 look_ahead_mask가 들어가고, 두 번째\n",
        "서브층은 mask의 인자값으로 padding_mask가 들어가있음\n",
        "\n",
        "세 개의 서브층 모두 서브층 연산 후에는 드롭 아웃, 잔차 연결, 층 정규화가\n",
        "수행"
      ],
      "id": "a1980297-9402-4b82-a3c0-712e2d1654d4"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "\n",
        "    # 룩어헤드 마스크(첫번째 서브층)\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "\n",
        "    # 패딩 마스크(두번째 서브층)\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
        "      })\n",
        "\n",
        "    # 잔차 연결과 층 정규화\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
        "          'mask': padding_mask # 패딩 마스크\n",
        "      })\n",
        "\n",
        "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
        "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "id": "4fe93f19-9f39-4edc-a95b-6736a41f0f1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoder 쌓기\n",
        "\n",
        "num_layers 개수만큼 쌓기"
      ],
      "id": "50744b9d-eda4-4bbe-8adc-00860f20a21d"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decoder(vocab_size, num_layers, dff,\n",
        "            d_model, num_heads, dropout,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "\n",
        "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    # 포지셔널 인코딩 + 드롭아웃\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    # 디코더를 num_layers개 쌓기\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "id": "dda40245-ad1a-4718-83e3-b0aeb246f459"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer 구현\n",
        "\n",
        "vocab_size는 다중 클래스 분류 문제를 풀 수 있도록 추가"
      ],
      "id": "99331b76-3e2b-4640-8408-3d6db1fea7cc"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transformer(vocab_size, num_layers, dff,\n",
        "                d_model, num_heads, dropout,\n",
        "                name=\"transformer\"):\n",
        "\n",
        "    # 인코더의 입력\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "    # 디코더의 입력\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    # 인코더의 패딩 마스크\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "    # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask, output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    # 디코더의 패딩 마스크(두번째 서브층)\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
        "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "    )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
        "\n",
        "    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
        "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    # 다음 단어 예측을 위한 출력층\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "id": "f56a49fe-de5d-4c9c-ae25-d0a7a4c06d76"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer hyperparameter 정하기\n",
        "\n",
        "예제 - num_layers = 4 \\# 인코더,디코더 층의 개수 - $d_{ff}$ = 128 \\#\n",
        "포지션 와이즈 피드 포워드 신경망의 은닉층 - $d_{model}$ = 128 \\#\n",
        "인코더와 디코더의 입, 출력의 차원 - num_heads = 4 \\# 멀티-헤드\n",
        "어텐션에서 병렬적으로 사용할 헤드의 수 - $d_v$ = 128 / 4 = 32"
      ],
      "id": "99c88105-d197-4179-baff-d69d24254b56"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 9000, 128)\n",
            "(1, 9000, 128)\n",
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work."
          ]
        }
      ],
      "source": [
        "small_transformer = transformer(\n",
        "    vocab_size = 9000,\n",
        "    num_layers = 4,\n",
        "    dff = 512,\n",
        "    d_model = 128,\n",
        "    num_heads = 4,\n",
        "    dropout = 0.3,\n",
        "    name=\"small_transformer\")\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    small_transformer, to_file='small_transformer.png', show_shapes=True)"
      ],
      "id": "3986f0b9-4ec0-485f-bb98-2a056739e811"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Function 정의\n",
        "\n",
        "예제가 다중 클래스라 크로스 엔트로피 함수를 손실 함수로 정의함"
      ],
      "id": "b0ec5ac1-ad73-4434-afcb-228122a0886f"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "id": "23372291-b489-42b3-ae35-379b4c9ec198"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 학습률\n",
        "\n",
        "$$lrate = d^{-0.5}_{model} \\times min(step_num^{-0.5} , step_num \\times warmup_steps^{-1/4})$$"
      ],
      "id": "d221b66c-e07b-4296-9c6d-115733010ab5"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "id": "f07cb84d-2522-4534-ad2a-2a8d840d73fa"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          }
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEGCAYAAAC3lehYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nAABJRU5ErkJggg==\n"
          }
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "id": "da96dec1-ab06-4463-b355-b2f87a20720e"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  }
}