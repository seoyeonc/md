{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Principal Component Analysis\n",
        "\n",
        "SEOYEON CHOI  \n",
        "2023-03-28\n",
        "\n",
        "> Principal Component Analysis\n",
        "\n",
        "-   주성분 분석은 최대 분산 순으로 특징을 나열함으로써 차원축소하는\n",
        "    비지도학습이다.\n",
        "-   변수 사이 상관관계 있을때\n",
        "\n",
        "Refernece: [핸즈 온\n",
        "머신러닝](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/),\n",
        "[최규빈교수님 통계전산강의노트](https://guebin.github.io/SC2022/)\n",
        "[사이킷런\n",
        "홈페이지](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "\n",
        "# 차원 축소\n",
        "\n",
        "차원 축소가 필요한 이유\n",
        "\n",
        "-   특성이 너무 많으면 [차원의 저주 curse of\n",
        "    dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
        "    에 빠지게 된다.\n",
        "\n",
        "차원 축소를 위한 접근 방법\n",
        "\n",
        "1.  [투영\n",
        "    Projection](https://en.wikipedia.org/wiki/Projection_(linear_algebra))\n",
        "\n",
        "-   ex) 3d 공간 안에 있는 저차원 부분 공간 subspace이 있는데 여기에\n",
        "    수직[1] 투영하면 평면에 투영된 좌표를 얻음.\n",
        "-   ex) 아래 그림 처럼 부분 공간 subspace가 휘어있기도 함.(swiss roll\n",
        "    예제) 오른쪽은 왼쪽의 스위스 롤을 펼쳐서 2D 데이터 셋을 얻은 것임.\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://www.researchgate.net/publication/200688576/figure/fig1/AS:305995638165506@1449966453759/The-Swiss-roll-data-set-On-the-left-the-data-is-presented-in-its-original-form-On.png\"\n",
        "alt=\"image\" />\n",
        "<figcaption aria-hidden=\"true\">image</figcaption>\n",
        "</figure>\n",
        "\n",
        "1.  [매니폴드](https://en.wikipedia.org/wiki/Manifold)\n",
        "\n",
        "-   위 스위스 롤 예제는 2D 매니폴드의 한 예\n",
        "-   매니폴드를 펼쳐서 보느냐, 어떻게 경계선을 그어서 보느냐 등 저차원의\n",
        "    매니폴드 공간에 가깝게 놓여있다고 가정[2]\n",
        "-   저차원으로 차원축소 할 수 있지만, 오히려 복잡해지는 경우[3]도 있다.\n",
        "\n",
        "$\\star$ 2D 매니폴드는 곡선, 3D 매니폴드는 곡면이라 생각\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://www.researchgate.net/publication/11580034/figure/fig1/AS:281957268246529@1444235259540/A-The-Swiss-roll-data-used-by-Tenenbaum-et-al-1-to-illustrate-their-algorithm.png\"\n",
        "alt=\"image\" />\n",
        "<figcaption aria-hidden=\"true\">image</figcaption>\n",
        "</figure>\n",
        "\n",
        "# PCA\n",
        "\n",
        "[주성분 분석 principal component\n",
        "analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
        "\n",
        "PCA is a statistical technique for reducing the dimensionality of a\n",
        "dataset.\n",
        "\n",
        "## 1. 분산 보존\n",
        "\n",
        "분산이 최대로 보존되는 차원 축소가 정보를 가장 적게 손실되어 합리적으로\n",
        "보임\n",
        "\n",
        "## 2. 주성분\n",
        "\n",
        "분산이 큰 순서대로 차원의 수만큼 찾음\n",
        "\n",
        "i번째 축 = 주성분 PC principal component\n",
        "\n",
        "[특이값 분해 SVD singular value\n",
        "decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)=\n",
        "training set에서 주성분 찾는 방법\n",
        "\n",
        "[교수님 lecture note with\n",
        "julia](https://guebin.github.io/SC2022/0510.html)\n",
        "\n",
        "-   이론 :\n",
        "    $X_{n \\times m} = U_{n \\times n} D_{n \\times m} (V_{m \\times m})^\\top$\n",
        "    -   ver 1 = $U, V$가 모두 직교 행렬\n",
        "    -   ver 2 = $U$ 또는 $V$가 직교 행렬\n",
        "-   왜?\n",
        "    -   데이터 매트릭스 $X$가 존재할때 정보는 유지하면서 비용을 줄이는\n",
        "        $Z$[4]를 찾고 싶다.\n",
        "    -   $Z$ 구하는 법\n",
        "        -   $Z = \\tilde{U} \\tilde{D}$ $\\to$\n",
        "            $\\tilde{U} \\tilde{D} = X\\tilde{V}$ $\\to$ $Z = X\\tilde{V}$\n",
        "        -   $X^\\top X = \\psi \\lambda \\psi^\\top$을 구해서\n",
        "            $Z = X\\tilde{\\psi}$ $\\to$ $\\hat{X} = Z\\tilde{\\psi}^\\top$\n",
        "\n",
        "$\\star$ PCA는 데이터셋의 평균이 0이라고 가정\n",
        "\n",
        "## 3. d차원으로 투영하기\n",
        "\n",
        "$X_{d-proj,n \\times d} = X_{n \\times M} W_{d,n \\times d}$\n",
        "\n",
        "## 4. 설명된 분산의 비율\n",
        "\n",
        "설명된 분산의 비율 explained variance ratio\n",
        "\n",
        "-   공분산 행렬의 고유값\n",
        "-   투영한 분산의 비율\n",
        "-   주성분 선택된 순으로 작아짐\n",
        "\n",
        "## 5. 적절한 차원 수 선택하기\n",
        "\n",
        "차원 수를 임의로 정하는 것보다는 충분한 분산[5]이 될 떄까지 선택\n",
        "\n",
        "## 6. 압축을 위한 PCA\n",
        "\n",
        "[참고](https://guebin.github.io/SC2022/0512.html)\n",
        "\n",
        "중요한 특징만을 살리기 위해 PCA를 시도하여 차원 축소하였다.\n",
        "\n",
        "이후 원본 데이터로 돌아가려 할 때 특징은 살아있지만 완벽히 데이터셋이\n",
        "일치하지 않는데,\n",
        "\n",
        "여기서 이 오류를 재구성 오차 = 재건 오류 reconstruction error 라고 한다.\n",
        "\n",
        "$X_{n \\times M} = X_{d-proj,n \\times d} (W_{d,n \\times d})^\\top$\n",
        "\n",
        "## 7. 커널 PCA\n",
        "\n",
        "차원 축소를 통해 비선형 투영 수행\n",
        "\n",
        "$\\zeta = \\Psi \\alpha$\n",
        "\n",
        "-   이 때, $||\\alpha_j|| = 1$로 정규화한다.\n",
        "-   그러기 위해 $\\alpha_j$를 $||\\zeta_j||$로 나누어 정규화\n",
        "    -   $||\\zeta_j|| = \\sqrt{\\lambda}_j$\n",
        "    -   $\\alpha_j \\to \\frac{1}{\\sqrt{\\lambda}_j} \\alpha_j, j=1, \\dots, m$\n",
        "\n",
        "특징 벡터로 내적하여 나오는 커널$K$ 행렬로 중심화\n",
        "\n",
        "-   $K = HKH$\n",
        "-   $H = I_n - 1_{n \\times 1} /n$\n",
        "\n",
        "$\\alpha$ 정규화 한 후 중심화하면\n",
        "\n",
        "$(z_1, \\dots, z_n) = (\\frac{1}{\\sqrt{\\lambda_1}} \\alpha_1,\\dots , \\frac{1}{\\sqrt{\\lambda_m}}\\alpha_m)^\\top HKH$\n",
        "\n",
        "-   여기서 m개가 주성분!\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://tekworld.org/wp-content/uploads/2018/12/Screen-Shot-2018-12-08-at-1.17.16-PM.png\"\n",
        "alt=\"image\" />\n",
        "<figcaption aria-hidden=\"true\">image</figcaption>\n",
        "</figure>\n",
        "\n",
        "$\\star$ 고유벡터\n",
        "\n",
        "-   선형 $C = \\psi \\psi^\\top$\n",
        "-   비선형 $K = \\psi^\\top \\psi$\n",
        "    -   $\\psi$의 길이에 따라 고유값 문제의 표현을 다르게 하여 계산 시간\n",
        "        줄이기\n",
        "    -   차원 수가 표본 수보다 큰 경우에는 커널 행렬을 사용하는 것이\n",
        "        효율적\n",
        "\n",
        "[1] 샘플과 평면 사이의 가장 짧은 직선을 따라\n",
        "\n",
        "[2] 이를 매니폴드 가설 또는 매니폴드 가정이라 한다.\n",
        "\n",
        "[3] 무조건적인 차원축소는 피할 것\n",
        "\n",
        "[4] X로 복원 가능해야 함\n",
        "\n",
        "[5] 예를 들어 설명된 분산의 비율이 약 95%까지"
      ],
      "id": "fa0f1c67-177c-492b-adaa-fecc38c66fab"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.7",
      "display_name": "Julia 1.7.2",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "version": "1.7.2"
    }
  }
}