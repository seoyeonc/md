{
 "cells": [
  {
   "cell_type": "raw",
   "id": "17d11bd2-6949-4a25-90bf-7ae768cb89a5",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Principal Component Analysis\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2023-03-28\"\n",
    "categories:\n",
    "  - Machine learning basic\n",
    "  - 비지도학습\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5fc2a-7283-46c5-a814-cfc395b365fd",
   "metadata": {},
   "source": [
    "> Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887b078-8ef0-41c4-87b2-a6408ac61f9a",
   "metadata": {},
   "source": [
    "- 주성분 분석은 최대 분산 순으로 특징을 나열함으로써 차원축소하는 비지도학습이다.\n",
    "- 변수 사이 상관관계 있을때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d39b32-a993-489e-b865-70aa3d123117",
   "metadata": {},
   "source": [
    "Refernece: [핸즈 온 머신러닝](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [최규빈교수님 통계전산강의노트](https://guebin.github.io/SC2022/) [사이킷런 홈페이지](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f972d-37f6-4cec-91ab-507db83e84dc",
   "metadata": {},
   "source": [
    "# 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d20cb8-c210-497f-aa2a-db8cbc6c586e",
   "metadata": {},
   "source": [
    "차원 축소가 필요한 이유\n",
    "\n",
    "- 특성이 너무 많으면 [차원의 저주 curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) 에 빠지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb908c3-144e-411a-b062-fdfd3b6ca607",
   "metadata": {},
   "source": [
    "차원 축소를 위한 접근 방법\n",
    "\n",
    "1. [투영 Projection](https://en.wikipedia.org/wiki/Projection_(linear_algebra))\n",
    "\n",
    "- ex) 3d 공간 안에 있는 저차원 부분 공간 subspace이 있는데 여기에 수직^[샘플과 평면 사이의 가장 짧은 직선을 따라] 투영하면 평면에 투영된 좌표를 얻음.\n",
    "- ex) 아래 그림 처럼 부분 공간 subspace가 휘어있기도 함.(swiss roll 예제) 오른쪽은 왼쪽의 스위스 롤을 펼쳐서 2D 데이터 셋을 얻은 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b0e54-775b-4ada-ac30-849ac66f0bbb",
   "metadata": {},
   "source": [
    "![image](https://www.researchgate.net/publication/200688576/figure/fig1/AS:305995638165506@1449966453759/The-Swiss-roll-data-set-On-the-left-the-data-is-presented-in-its-original-form-On.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b417a-5367-462b-b7a0-d4c7a29fd6a1",
   "metadata": {},
   "source": [
    "2. [매니폴드](https://en.wikipedia.org/wiki/Manifold)\n",
    "\n",
    "- 위 스위스 롤 예제는 2D 매니폴드의 한 예\n",
    "- 매니폴드를 펼쳐서 보느냐, 어떻게 경계선을 그어서 보느냐 등 저차원의 매니폴드 공간에 가깝게 놓여있다고 가정^[이를 매니폴드 가설 또는 매니폴드 가정이라 한다.]\n",
    "- 저차원으로 차원축소 할 수 있지만, 오히려 복잡해지는 경우^[무조건적인 차원축소는 피할 것]도 있다.\n",
    "\n",
    "$\\star$ 2D 매니폴드는 곡선, 3D 매니폴드는 곡면이라 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dc342-a8c0-481e-9988-b37e08d9b4eb",
   "metadata": {},
   "source": [
    "![image](https://www.researchgate.net/publication/11580034/figure/fig1/AS:281957268246529@1444235259540/A-The-Swiss-roll-data-used-by-Tenenbaum-et-al-1-to-illustrate-their-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d02c3f-e9ed-45e2-b4e7-436b5f0b1dd2",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf2460-cbaf-4c1e-a296-4266e077468f",
   "metadata": {},
   "source": [
    "[주성분 분석 principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "PCA is a statistical technique for reducing the dimensionality of a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd26b6-3707-48bc-8bff-5ac426010a4e",
   "metadata": {},
   "source": [
    "## 1. 분산 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd4809-8a01-4393-ab4d-eb4839139c11",
   "metadata": {},
   "source": [
    "분산이 최대로 보존되는 차원 축소가 정보를 가장 적게 손실되어 합리적으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8f67b-33b1-4b63-a6d4-8deca95d00b4",
   "metadata": {},
   "source": [
    "## 2. 주성분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682b40a-9bbd-4338-ab05-55fac75e3155",
   "metadata": {},
   "source": [
    "분산이 큰 순서대로 차원의 수만큼 찾음\n",
    "\n",
    "i번째 축 = 주성분 PC principal component\n",
    "\n",
    "[특이값 분해 SVD singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)= training set에서 주성분 찾는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204584dd-792a-422f-83a3-5a42234116a6",
   "metadata": {},
   "source": [
    "[교수님 lecture note with julia](https://guebin.github.io/SC2022/0510.html)\n",
    "\n",
    "- 이론 : $X_{n \\times m} = U_{n \\times n} D_{n \\times m} (V_{m \\times m})^\\top$\n",
    "    - ver 1 = $U, V$가 모두 직교 행렬\n",
    "    - ver 2 = $U$ 또는 $V$가 직교 행렬\n",
    "\n",
    "- 왜?\n",
    "    - 데이터 매트릭스 $X$가 존재할때 정보는 유지하면서 비용을 줄이는 $Z$^[X로 복원 가능해야 함]를 찾고 싶다.\n",
    "    - $Z$ 구하는 법\n",
    "        - $Z = \\tilde{U} \\tilde{D}$ $\\to$ $\\tilde{U} \\tilde{D} = X\\tilde{V}$ $\\to$ $Z = X\\tilde{V}$\n",
    "        - $X^\\top X = \\psi \\lambda \\psi^\\top$을 구해서 $Z = X\\tilde{\\psi}$ $\\to$ $\\hat{X} = Z\\tilde{\\psi}^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20442a-9be2-432b-bce7-39ab6ad1c529",
   "metadata": {},
   "source": [
    "$\\star$ PCA는 데이터셋의 평균이 0이라고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88b6f6-cc17-41c3-9fc9-90658db37e95",
   "metadata": {},
   "source": [
    "## 3. d차원으로 투영하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063ba20-7cc7-410f-863a-a290194b5646",
   "metadata": {},
   "source": [
    "$X_{d-proj,n \\times d} = X_{n \\times M} W_{d,n \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47557e3-8d59-4c70-8741-3244a4744fbb",
   "metadata": {},
   "source": [
    "## 4. 설명된 분산의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2280f5d-9034-4c08-aeb7-72386fdd409a",
   "metadata": {},
   "source": [
    "설명된 분산의 비율 explained variance ratio\n",
    "\n",
    "- 공분산 행렬의 고유값\n",
    "- 투영한 분산의 비율\n",
    "- 주성분 선택된 순으로 작아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3ac78-9bf7-48be-820f-3211e5c7d3ec",
   "metadata": {},
   "source": [
    "## 5. 적절한 차원 수 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1666e6c-fedd-436d-b091-9dba1cb03e8c",
   "metadata": {},
   "source": [
    "차원 수를 임의로 정하는 것보다는 충분한 분산^[예를 들어 설명된 분산의 비율이 약 95%까지]이 될 떄까지 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1868882-ad6d-4bf8-ac3a-4e83103875c4",
   "metadata": {},
   "source": [
    "## 6. 압축을 위한 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d5997-1b2f-4fa7-a337-d750dbcf7fee",
   "metadata": {},
   "source": [
    "[참고](https://guebin.github.io/SC2022/0512.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14b0e8-561d-438c-aa5f-bb5a7130901a",
   "metadata": {},
   "source": [
    "중요한 특징만을 살리기 위해 PCA를 시도하여 차원 축소하였다.\n",
    "\n",
    "이후 원본 데이터로 돌아가려 할 때 특징은 살아있지만 완벽히 데이터셋이 일치하지 않는데,\n",
    "\n",
    "여기서 이 오류를 재구성 오차 = 재건 오류 reconstruction error 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d17e45-92b3-4964-a003-f470bed30ea5",
   "metadata": {},
   "source": [
    "$X_{n \\times M} = X_{d-proj,n \\times d} (W_{d,n \\times d})^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4ef58-b84c-4d5f-bb58-396e7e8b1f1d",
   "metadata": {},
   "source": [
    "## 7. 커널 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a05c0e-aa58-42c3-824d-1d292f152667",
   "metadata": {},
   "source": [
    "차원 축소를 통해 비선형 투영 수행\n",
    "\n",
    "$\\zeta = \\Psi \\alpha$\n",
    "\n",
    "- 이 때, $||\\alpha_j|| = 1$로 정규화한다.\n",
    "- 그러기 위해 $\\alpha_j$를 $||\\zeta_j||$로 나누어 정규화\n",
    "    - $||\\zeta_j|| = \\sqrt{\\lambda}_j$\n",
    "    - $\\alpha_j \\to \\frac{1}{\\sqrt{\\lambda}_j} \\alpha_j, j=1, \\dots, m$\n",
    "\n",
    "특징 벡터로 내적하여 나오는 커널$K$ 행렬로 중심화\n",
    "\n",
    "- $K = HKH$\n",
    "- $H = I_n - 1_{n \\times 1} /n$\n",
    "\n",
    "$\\alpha$ 정규화 한 후 중심화하면\n",
    "\n",
    "$(z_1, \\dots, z_n) = (\\frac{1}{\\sqrt{\\lambda_1}} \\alpha_1,\\dots , \\frac{1}{\\sqrt{\\lambda_m}}\\alpha_m)^\\top HKH$\n",
    "\n",
    "- 여기서 m개가 주성분!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea614fc2-24df-46a9-aab4-28a03f3a29d5",
   "metadata": {},
   "source": [
    "![image](https://tekworld.org/wp-content/uploads/2018/12/Screen-Shot-2018-12-08-at-1.17.16-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb6607-22d7-478e-b2bd-830daf46feb0",
   "metadata": {},
   "source": [
    "$\\star$ 고유벡터\n",
    "\n",
    "- 선형 $C = \\psi \\psi^\\top$\n",
    "- 비선형 $K = \\psi^\\top \\psi$\n",
    "    - $\\psi$의 길이에 따라 고유값 문제의 표현을 다르게 하여 계산 시간 줄이기\n",
    "    - 차원 수가 표본 수보다 큰 경우에는 커널 행렬을 사용하는 것이 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9164db5-fe97-4a82-8686-b75daabdcaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
