{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lasso and Ridge\n",
        "\n",
        "SEOYEON CHOI  \n",
        "2023-03-29\n",
        "\n",
        "> Lasso and Ridge\n",
        "\n",
        "릿지ridge 회귀 = 능형회귀 = 티호노프 tikhonov 규제 = L2 노름\n",
        "\n",
        "$\\star$ **숙지!**\n",
        "\n",
        "릿지 회귀는 L2 규제를 사용하여 모델의 가중치가 커지지 않도록 제한합니다.\n",
        "이 규제는 일반적으로 극단적인 가중치 값을 갖는 것을 방지하고, 모델의\n",
        "일반화 성능을 향상시킵니다.\n",
        "\n",
        "하지만 릿지 회귀는 가중치를 완전히 0으로 만들지는 않습니다. 대신,\n",
        "가중치를 작게 만들어 모델이 더 일반적인 패턴을 학습하도록 유도합니다.\n",
        "이는 희소성을 가진 데이터에서 모델의 성능을 향상시키는 데 도움이 됩니다.\n",
        "\n",
        "반면, 라쏘 회귀는 L1 규제를 사용하여 가중치를 0으로 만들 수 있습니다.\n",
        "이는 희귀 학습에서 유용합니다. 희소성을 가진 데이터에서는 많은 특성 중\n",
        "일부만 중요하다는 것을 알 수 있습니다. 따라서, 라쏘 회귀는 이러한 특성을\n",
        "선택하고 다른 특성을 제거하여 모델을 희소하게 만들 수 있습니다.\n",
        "\n",
        "Refernece: [책_The Elements of Statistical\n",
        "Learning](https://hastie.su.domains/Papers/ESLII.pdf), [그림과 수식으로\n",
        "배우는 통통\n",
        "머신러닝](https://product.kyobobook.co.kr/detail/S000001875111),\n",
        "[2021데이터과학 최규빈교수님 lecture\n",
        "노트](https://seoyeonc.github.io/md/posts/ml_basic/2021-03-31-Ridge%20Regression_note3_0331.html)\n",
        "\n",
        "# 벌점화모형\n",
        "\n",
        "기본 가정 : $\\bf{y= X\\beta + \\epsilon}$ 의 회귀식이 있을때,\n",
        "$\\bf{\\hat{\\beta} = (X'X)^{-1} X' y}$로 정의할 수 있음.\n",
        "\n",
        "-   의문 : $\\bf{(X'X)^{-1}}$을 구할 수 없을 때는?\n",
        "    -   1.  $\\bf{(X'X)^{-1}}$가 full-rank 일 때\n",
        "\n",
        "    -   1.  $\\bf{(X'X)^{-1}}$가 full rank 라면 rank = $p$\n",
        "\n",
        "        -   $\\to$ $\\bf{(X'X)^{-1}}$가 존재하지 않는다는 의미는\n",
        "            $rank(\\bf{X'X})<p$\n",
        "        -   이유 1) 공선성 때문\n",
        "        -   이유 2) $n<p$, 데이터가 부족할 때 이러한 현상이 발생한다.\n",
        "\n",
        "해결 방법\n",
        "\n",
        "1.  리지 Ridge\n",
        "\n",
        "-   idea : $\\bf{(X'X+\\lambda I)^{-1}}$을 대신 계산한다.\n",
        "\n",
        "1.  PCA를 이용하여 $\\bf{(X'X)} = \\Psi \\Lambda \\Psi$로 대신 구함\n",
        "\n",
        "-   idea : $\\bf{(X'X)}$ $\\to$ 대칭이고 실수면 $\\to$ 고유 분해 가능\n",
        "-   $\\bf{(X'X) = \\Psi \\lambda \\Psi^\\top}$는\n",
        "    $\\bf{(X'X)^{-1} = \\Psi \\lambda^{-1} \\Psi^\\top}$을 의미하니까\n",
        "    -   $\\bf{(X'X)(X'X)^{-1}} = I$\n",
        "\n",
        "## 예제)\n",
        "\n",
        "$y = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon, \\epsilon \\sim N(0,\\sigma^2)_{idd}$임\n",
        "모형이 있다고 할 때,\n",
        "\n",
        "원래 모형은 $y = 5 x_{i1} + 600 x_{i2}$이다.\n",
        "\n",
        "이 때, $x_{i3}$이 $x_{i1}$과 거의 $1$의 상관관계를 가지고 있다면? $\\to$\n",
        "다중공선성을 가지고 있다면?\n",
        "\n",
        "$\\beta_1 + \\beta_3 = 5$만 된다면 원래 모형에 근사하다고 나올 터, 심지어\n",
        "음수가 나올 때 조차도.\n",
        "\n",
        "-   다중공선성의 특징\n",
        "    -   추정하는 $\\beta$가 어떤 값인지 거의 예측이 안 된다.\n",
        "        -   $\\beta$들의 분산이 크다. (다양한 값 나오고 그러니..)\n",
        "    -   $\\beta$는 그래도 더하면 원래 값에 근사함.\n",
        "\n",
        "모두 참이라고 생각되는 모형 $\\to$ 합은 일단 원히던 5임\n",
        "\n",
        "-   조건) 다중공선성 없는 $x_{i2}$는 $y$쪽으로 옮긴 상황. 즉 신경 안\n",
        "    써도 됌\n",
        "\n",
        "1.  $\\beta_1=2,\\beta_2=3$\n",
        "2.  $\\beta_1=5,\\beta_2=0$\n",
        "3.  $\\beta_1=10,\\beta_2=-5$\n",
        "4.  $\\beta_1=10000,\\beta_2=-9995$\n",
        "\n",
        "$\\dots$\n",
        "\n",
        "-\\> 음수 있는 해석 불가한 이상한 모형들 다 가능하겠다. 해석 불가능.\n",
        "\n",
        "# Ridge와 Lasso\n",
        "\n",
        "$loss = \\sum(y-X_1\\beta_1 - X_2\\beta_2)^2$\n",
        "\n",
        "$loss\\tilde{} = (y-XB)^\\top (y-XB) + \\lambda B^\\top B$,\n",
        "$\\bf{\\hat{B} = X'X + \\lambda I} \\to \\hat{\\beta}$과 달리 **불편추정량**이\n",
        "아님을 알 수 있음\n",
        "\n",
        "-   하지만, 분산이 더 작다!\n",
        "\n",
        "-   수정된 loss 값 구별 위해 $\\tilde{}$ 기호 추가\n",
        "\n",
        "-   $(X'X)$가 양정치 행렬이라는 조건이 필요하지만, 저절로 만족되어\n",
        "    고려하지 않아도 됌.\n",
        "\n",
        "-   $\\lambda$ 잘 찾으면 항상 ridge regression이 linear regression 보다\n",
        "    좋다고 주장할 수 있다.\n",
        "\n",
        "$\\beta^{QLS} = (X^\\top X)^{-1}X^\\top Y$[1]\n",
        "\n",
        "$\\beta^{R} = (X^\\top X + \\lambda I)^{-1}X^\\top Y$\n",
        "\n",
        "-   $(X^\\top X)$ 가 역행렬 구할 수 없다고 보고 $\\lambda I$를 더해줘서\n",
        "    역행렬 구할 수 있게\n",
        "    -   왜냐하면 역행렬 구할 수 없다는 말은 대각선에 0이 존재한다는\n",
        "        것이니까.\n",
        "\n",
        "벌점화를 주자. = 패널티를 주자 $loss + \\lambda(\\beta^2_1 + \\beta^2_2)$\n",
        "\n",
        "1.  $\\lambda(2^2 + 3^2)$\n",
        "2.  $\\lambda(5^2_)$\n",
        "3.  $\\lambda(10^2+(-5)^2)$\n",
        "4.  $\\lambda((10000)^2 + (-9995)^2)$\n",
        "\n",
        "$\\lambda$ = hyperparameter 하이퍼파라메터 = 조율 모수 tuning parameter =\n",
        "Regularization parameter 규제화항\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://miro.medium.com/v2/resize:fit:761/1*nrWncnoJ4V_BkzEf1pd4MA.png\"\n",
        "alt=\"image\" />\n",
        "<figcaption aria-hidden=\"true\">image</figcaption>\n",
        "</figure>\n",
        "\n",
        "`1`\n",
        "\n",
        "$loss = SSE + L_2-panalty:Ridge$\n",
        "\n",
        "-   [Ridge](https://en.wikipedia.org/wiki/Ridge_regression) = $L_2$ norm\n",
        "    = Regularization 정규화 = $L_2$ 패널티 = 벌점\n",
        "\n",
        "-   $(\\lambda(\\beta_1^2 +\\beta^2_2)$이니까 원 모형이 나오지\n",
        "\n",
        "    -   $(\\lambda(\\beta_1^2 +\\beta^2_2) = k$\n",
        "    -   $\\beta_1^2 +\\beta^2_2 = \\frac{k}{\\lambda}$\n",
        "\n",
        "-   $\\lambda$ 가 0이면 loss와 ridge 함수가 만나는 면,\n",
        "\n",
        "-   $\\lambda$ 가 $\\infty$이면 중심(0,0)에 있을터.\n",
        "\n",
        "-   너무 큰 $\\beta_i$들을 구할 때 패널티를 부여하여 되도록 작은\n",
        "    $\\beta_i$들을 선택하게 제약을 거는 것\n",
        "\n",
        "-   가장 최적의 점 $\\beta_1 = \\beta_2$\n",
        "\n",
        "`2`\n",
        "\n",
        "$loss = SSE + L_1-panalty:Lasso$\n",
        "\n",
        "-   [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) = $L_1$\n",
        "    norm\n",
        "\n",
        "-   $(\\lambda(|\\beta_1| +|\\beta_2|)$이니까 마름모 모형이 나오지\n",
        "\n",
        "    -   $(\\lambda(|\\beta_1| +|\\beta_2|) = k$\n",
        "    -   $|\\beta_1| +|\\beta_2| = \\frac{k}{\\lambda}$\n",
        "\n",
        "-   $\\lambda$ 가 0이면 loss와 lasso 함수가 만나는 면,\n",
        "\n",
        "-   $\\lambda$ 가 $\\infty$이면 중심(0,0)에 있을터.\n",
        "\n",
        "-   가장 최적의 점 $(0,절편), (절편,0)$\n",
        "\n",
        "`3`\n",
        "\n",
        "$loss = SSE + L_1-panalty + L_2-panalty : Elastic-net$\n",
        "\n",
        "-   섞어쓰기도 함\n",
        "\n",
        "$\\lambda \\beta^\\top\\beta$\n",
        "\n",
        "-   패널티항= 벌점항 = L2-패널티, 정규화항이라 부른다.\n",
        "-   Ridge(최소제곱학습은 $L_2$norm노름에 대한 제약조건 사용)\n",
        "    -   $\\lambda ||\\beta||^2_2$로 표현하기도 한다. $||\\dot||_2$는 벡터의\n",
        "        $L_2$-norm노름 이라고 한다.\n",
        "        -   $x = (x_1,x_2) \\to ||x||^2_2 := x^2_1 + x^2_2. ||x||_2 := \\sqrt{x^2_1 + x^2_2}$\n",
        "        -   $\\lambda \\beta^\\top \\beta = \\lambda(\\beta^2_1 + \\beta^2_2)$\n",
        "-   Lasso(희소 학습에서는 $L_1$norm노름에 대한 제약조건 사용)\n",
        "    -   희소 학습이라 부르는 이유\n",
        "        -   $|\\beta_1| +|\\beta_2|$해가 0이 되는 구간이 마름모라 4 꼭짓점\n",
        "            나오는데, 이게 희소한 해를 갖는다고 말함\n",
        "\n",
        "[1] Constrained Least Squares"
      ],
      "id": "06617a60-f67a-4be6-b2c1-1e9547d50b10"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  }
}