{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a686c0fe-395c-4688-b365-b78f67ee0f50",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Extra-3: 딥러닝의 기초 (5)\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2022-12-21\"\n",
    "categories:\n",
    "  - 딥러닝의 기초\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753ccb9-d749-4845-b773-76111a04e94a",
   "metadata": {},
   "source": [
    "> 벡터미분, 역전파와 기울기 소멸 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242c004-448a-4566-8c11-4b68a6028d94",
   "metadata": {},
   "source": [
    "기울기 소멸: loss 를 W로 미분했더니 그 값이 거의 0ㅇ이 나오는 현상 $\\to$ update가 거의 이루어지지 않음\n",
    "\n",
    "이유\n",
    "- W -> loss인 함수는 W에 어떤한 선형변환 $\\to$ 비선형변환 $\\to$ 선형변환 $\\to$ 비선형변환 $\\to \\dots$ $\\to$ loss 의 과정으로 해석 가능\n",
    "- 즉 loss는 W의 합성의 합섭ㅇ의... 합성함수로 해석가능\n",
    "- loss 를 W로 미분한 값은 각 변환단ㅇ계에서 정의되는 함수의 도함수를 모두 곱한 것과 같음(체인룰)\n",
    "- 체인 중에서도 하나라도 0이 나오면 곱한 값은 0 이다. \n",
    "    - 체인이 길수록 하나라도 0이 나오는 경우가 많음\n",
    "    \n",
    "왜 깊은 신경망일수록 기울기 소멸이 빈번한가?\n",
    "- 체인이 길기 때문에.\n",
    "\n",
    "왜 순환신경망일수록 기울기 소멸이 빈번할까>?\n",
    "- 체인이 길기 때문에."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064a093-286b-4c33-925b-a2f474f84e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217616da-3e8c-4391-b402-65b3f42d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a71754-5d08-4e2f-a22f-edacf7050bed",
   "metadata": {},
   "source": [
    "# 벡터미분 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c31496-5718-44cb-aca9-492554de2c1a",
   "metadata": {},
   "source": [
    "`-` 벡터미분에 대한 강의노트: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f58d2-307b-4290-a964-b44aafe0ad14",
   "metadata": {},
   "source": [
    "- <https://github.com/guebin/STML2022/blob/main/posts/II.%20DNN/supp.pdf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fce0d-0317-4bd6-8dc4-88d8ddec2221",
   "metadata": {},
   "source": [
    "`-` 요약: 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f74ea-bb4f-49b8-9430-715440eca1e8",
   "metadata": {},
   "source": [
    "- $loss = ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\bf W} - {\\bf W}^\\top {\\bf X}^\\top {\\bf y} + {\\bf W}^\\top {\\bf X}^\\top {\\bf X} {\\bf W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcf8a5-5f63-41ef-b367-c3b4ad56449a",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial {\\bf W}}loss = -2{\\bf X}^\\top {\\bf y} +2 {\\bf X}^\\top {\\bf X} {\\bf W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27eebe5-7ad1-4b47-9973-568540f922e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336425b-1cfa-4f3c-9c9e-fecf8a49e457",
   "metadata": {},
   "source": [
    "`-` 모형의 매트릭스화 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ea5a0-fddf-46dc-b2d4-f97f2067d684",
   "metadata": {},
   "source": [
    "우리의 모형은 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5fb3b-abd7-43bd-a6d8-4632b25ff0dc",
   "metadata": {},
   "source": [
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575dc6e-0eec-4564-a186-8b02fde6f5aa",
   "metadata": {},
   "source": [
    "풀어서 쓰면 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc76f1-d1ce-4576-811d-c5de1223890e",
   "metadata": {},
   "source": [
    "$\\begin{cases}\n",
    "y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ \n",
    "y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \n",
    "\\dots \\\\ \n",
    "y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883bd58-2440-402b-930d-d80413ac6b69",
   "metadata": {},
   "source": [
    "아래와 같이 쓸 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c268db9-51ea-47cf-b65f-7b010c84194c",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\dots \\\\\n",
    "y_{10} \n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix} \n",
    "1 & x_1 \\\\ \n",
    "1 & x_2 \\\\ \n",
    "\\dots & \\dots \\\\\n",
    "1 & x_{10} \n",
    "\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \n",
    "\\epsilon_1 \\\\ \n",
    "\\epsilon_2 \\\\ \n",
    "\\dots \\\\\n",
    "\\epsilon_{10} \n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8353e1b-ac65-40f9-8630-52d9849cb9b2",
   "metadata": {},
   "source": [
    "벡터와 매트릭스 형태로 정리하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cc946-2e4c-42a4-9ec9-5ec11b416747",
   "metadata": {},
   "source": [
    "${\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5244f-b1f1-453f-b6d0-706f67e952c2",
   "metadata": {},
   "source": [
    "`-` 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958850-9631-4bb1-ba6b-d7d7ab77c0a6",
   "metadata": {},
   "source": [
    "$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6476cc-cd8c-488d-9968-5e9769a4a02c",
   "metadata": {},
   "source": [
    "이것을 벡터표현으로 하면 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3d496-a27e-4204-a23d-2854594fd952",
   "metadata": {},
   "source": [
    "$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3e250-805c-4488-a4a3-51ef2e0c0f13",
   "metadata": {},
   "source": [
    "풀어보면 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e5749-0407-40a8-b522-7bdca9736ea0",
   "metadata": {},
   "source": [
    "$loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956aa8e-f396-4c23-b8ba-0d62f677a1fc",
   "metadata": {},
   "source": [
    "`-` 미분하는 과정의 매트릭스화 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d4e63-0195-4613-8520-3365305c8b0b",
   "metadata": {},
   "source": [
    "loss를 최소화하는 ${\\boldsymbol \\beta}$를 구해야하므로 loss를 ${\\boldsymbol \\beta}$로 미분한 식을 0이라고 놓고 풀면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58cce3-0ec9-42df-b7bd-44bb771f536d",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7a75a-109d-4e24-ac46-fc50a711edee",
   "metadata": {},
   "source": [
    "$= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaa51c-fdce-4e5e-a9f6-6c5a452399c0",
   "metadata": {},
   "source": [
    "따라서 $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0$을 풀면 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429007a-8263-40a6-93f2-80183a237aab",
   "metadata": {},
   "source": [
    "$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fb0ab-a765-436d-95fa-57cf1906d0c5",
   "metadata": {},
   "source": [
    "`-` 공식도 매트릭스로 표현하면: $\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $ <-- 외우세요 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e866df9-3ae7-47ef-9935-bfae168ffc74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad30f4e-9fd3-4a32-a19e-7df5d70c3a64",
   "metadata": {},
   "source": [
    "**벡터미분 / 매트릭스 미분**\n",
    "\n",
    "(1) 정의 1: 벡터로 미분\n",
    "\n",
    "$\\frac{\\partial}{\\partial y} = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix}$ , $y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
    "\n",
    "(2) 정의 2: 매트릭스로 미분\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\bf{X} } := \\begin{bmatrix} \\frac{\\partial}{\\partial x_{11} } & \\dots & \\frac{\\partial}{\\partial x_{1p} } \\\\ \\vdots &  & \\vdots \\\\ \\frac{\\partial}{\\partial x_{n1} }  & \\dots & \\frac{\\partial}{\\partial x_{np} } \\end{bmatrix}, \\bf{X} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np}  \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df394d5d-f2b4-4ebe-a6a5-318a1b923ed7",
   "metadata": {},
   "source": [
    "`1` \n",
    "\n",
    "$\\frac{\\partial}{\\partial x}(x^\\top y) = y$, 단 $x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
    "\n",
    "pf. $\\bf{x}^\\top \\bf{y} = \\begin{bmatrix} x_1 & \\dots x_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 y_1 + x_2 y_2 + \\dots  + x_n y_n$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix}$ 이므로\n",
    "\n",
    "$\\big( \\frac{\\partial}{\\partial x} \\big) x^\\top y = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix} (x_1 y_1 + x_2 y_2 + \\dots + x_n y_n)$\n",
    "\n",
    "$ = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}(x_1y_1 + \\dots + x_n y_n) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}(x_1y_1 + \\dots + x_n y_n) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 y_1 + \\frac{\\partial}{\\partial x_1}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_1}x_n y_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} x_1 y_1 + \\frac{\\partial}{\\partial x_n}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_n}x_n y_n  \\end{bmatrix}$\n",
    "\n",
    "$ = \\begin{bmatrix} y_1 + 0 + \\dots + 0 \\\\ \\vdots \\\\ 0+0+ \\dots + y_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de113db-092e-4d9c-b931-7eda10c74267",
   "metadata": {},
   "source": [
    "`2`  $\\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$ 임을 보이는 다른 풀이\n",
    "\n",
    "pf. $\\frac{\\partial}{\\partial x} \\big(\\bf{x}^\\top \\bf{y} \\big) = \\big( \\frac{\\partial}{\\partial x} x^\\top \\big) y = \\bf{I} \\bf{y} = \\bf{y}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} x^\\top = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}\\end{bmatrix} \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 & \\dots & \\frac{\\partial}{\\partial x_n} x_n \\\\ \\vdots & & \\\\ \\frac{\\partial}{\\partial x_n} x_1 & \\dots &\\frac{\\partial}{\\partial x_n} x_n \\end{bmatrix} = \\bf{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3c3d4-727a-46c5-a16e-127b9f886337",
   "metadata": {},
   "source": [
    "`2` $\\frac{\\partial}{\\partial x}(\\bf{y}^\\top \\bf{x} ) = \\bf{y}$\n",
    "\n",
    "pf. $\\frac{\\partial}{\\partial x}(\\bf{y}^\\top\\bf{x}) = \\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca3d9b-56d8-494e-b264-ddf9ec7d5f05",
   "metadata": {},
   "source": [
    "`3` $\\frac{\\partial}{\\partial B}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{x}^\\top \\bf{y}$\n",
    "\n",
    "단, $\\bf{B} := p \\times 1$ vector, $\\bf{X} := n \\times p$ matrix, $\\bf{y} := n \\times 1$ vector\n",
    "\n",
    "pf. $\\bf{y}^\\top \\bf{X} \\bf{B}$는 스칼라($1 \\times 1$)이므로 $\\bf{y}^\\top\\bf{X}\\bf{B} = (\\bf{y}^\\top \\bf{X} \\bf{B})^\\top = \\bf{B}^\\top \\bf{X}^\\top \\bf{y}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\frac{\\partial}{\\partial \\bf{B}}(\\bf{B}^\\top \\bf{X}^\\top \\bf{y}) = \\big( \\frac{\\partial}{\\partial \\bf{B}} \\bf{B}^\\top \\big) \\bf{X}^\\top \\bf{y} = \\bf{X}^\\top \\bf{y}$\n",
    "\n",
    "`(다른 풀이)` $\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{X}^\\top \\bf{y}$임을 보이는 다른 풀이\n",
    "\n",
    "$\\bf{XB} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} = \\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n",
    "\n",
    "$\\bf{y}^\\top \\bf{XB} = \\begin{bmatrix} y_1 & \\dots y_n \\end{bmatrix}\\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n",
    "\n",
    "$ = y_1(x_{11}B_1 + \\dots + x_{1p}B_p) + y_2(x_{21}B_1 + \\dots + x_{2p}B_p ) + \\dots + y_n(x_{n1}B_1 + \\dots + x_{np} B_p) = A_1 + A_2 + \\dots A_n$\n",
    "\n",
    "* note: $A_1, A_2, \\dots$은 모두 스칼라($1 \\times 1$)\n",
    "\n",
    "$\\big( \\frac{\\partial}{\\partial B} \\big)(\\bf{y}^\\top \\bf{XB}) = \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} \\\\ \\vdots \\\\\\frac{\\partial}{\\partial B_p} \\end{bmatrix}(A_1 + A_2 + \\dots A_n)$\n",
    "\n",
    "$ = \\begin{bmatrix}\\frac{\\partial}{\\partial B_1} A_1 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_1 \\end{bmatrix} + \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_2 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_2 \\end{bmatrix}+ \\dots +\\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_n \\end{bmatrix}$\n",
    "\n",
    "* $\\frac{\\partial}{\\partial B_1}A_1 = \\frac{\\partial}{\\partial B_1}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = y_1 x_{11} + 0 + \\dots + 0$\n",
    "\n",
    "* $\\frac{\\partial}{\\partial B_2}A_2 = \\frac{\\partial}{\\partial B_2}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = 0 + y_1 x_{12} + 0 + \\dots + 0$\n",
    "\n",
    "$ = \\begin{bmatrix} y_1 x_{11} \\\\ y_1 x_{12} \\\\ \\vdots \\\\ y_1 x_{1p}\\end{bmatrix} + \\begin{bmatrix} y_2 x_{21} \\\\ y_2 x_{22} \\\\ \\vdots \\\\ y_2 x_{2p} \\end{bmatrix}  + \\dots + \\begin{bmatrix} y_n x_{n1} \\\\ y_n x_{n2} \\\\ \\vdots \\\\ y_n x_{np} \\end{bmatrix} $\n",
    "\n",
    "$ = \\begin{bmatrix} y_1 x_{11} + y_2 x_{21} + \\dots + y_n x_{n1} \\\\ y_1 x_{12} + y_2 x_{22} + \\dots + y_n x_{n2} \\\\ \\vdots \\\\ y_1 x_{1p} + y_2 x_{2p} + \\dots + y_n x_{np} \\end{bmatrix} = \\begin{bmatrix} x_{11} & x_{21} & \\dots & x_{n1} \\\\ x_{12} & x_{22} & \\dots & x_{n2} \\\\ \\vdots & \\vdots & & \\vdots \\\\ x_{1p} & x_{2p} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{X}^\\top \\bf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b23ad6-ecb5-4ace-8e5d-ff89f9415b5c",
   "metadata": {},
   "source": [
    "`4` $\\frac{\\partial}{\\partial y} \\bf{y}^\\top \\bf{y} = 2\\bf{y}$\n",
    " \n",
    "pf. $\\bf{y}^\\top \\bf{y} = \\begin{bmatrix} y_1 & \\dots & y_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = y_{1}^{2} + \\dots + y_{n}^{2}$\n",
    "\n",
    "$\\therefore \\frac{\\partial}{\\partial y}(\\bf{y}^\\top y) = \\frac{\\partial}{\\partial y}(y_{1}^{2} + \\dots + y_{n}^{2}) = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix} (y_{1}^{2} + \\dots + y_{n}^{2})$\n",
    "\n",
    "$ = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1}(y_{1}^{2} + \\dots + y_{n}^{2}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n}(y_{1}^{2} + \\dots + y_{n}^{2}) \\end{bmatrix} = \\begin{bmatrix}2y_1 + 0 + \\dots + 0 \\\\ 0 + 2y_2 + \\dots + 0 \\\\ \\vdots \\\\ 0 + 0 + \\dots + 2y_n \\end{bmatrix} = 2\\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = 2\\bf{y}$\n",
    "\n",
    "(틀린 풀이) $\\frac{\\partial}{\\partial y}(\\bf{y}^\\top \\bf{y}) {\\color{orange}=} \\big( \\frac{\\partial}{\\partial y} \\bf{y}^\\top \\big) \\bf{y}$ 미분하는 값에 대해 상수일때만 가능\n",
    "\n",
    "그런데 $\\big(\\frac{\\partial}{\\partial y}\\bf{y}^\\top \\big) = \\bf{I}$이므로, $\\frac{\\partial}{\\partial y} (\\bf{y}^\\top \\bf{y}) = \\bf{I} \\bf{y} = \\bf{y}$\n",
    "\n",
    "(틀린 풀이의 스칼라 버전) $\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} y \\big) y = 1 \\times y = \\bf{y}$\n",
    "\n",
    "(올바른 풀이) : $\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} \\big) {\\color{red}y} {\\color{blue}y} = \\big( \\frac{d}{dy} {\\color{red}y} \\big) {\\color{blue}y} + {\\color{red}y} \\big( \\frac{d}{dy} {\\color{blue}y} \\big)$\n",
    "\n",
    "Note: 곱의 미분 : 함수 $f(x), g(x)$ 가 $x$에 대하여 미분 가능하면 $\\{ {\\color{red}f(x)} \\times {\\color{blue}g(x)} \\}' = {\\color{red}f(x)}'g{\\color{blue}(x)} + {\\color{red}f(x)}{\\color{blue}g(x)}'$\n",
    "\n",
    "* 여기서는 ${\\color{red}y}$가 $f(x)$, ${\\color{blue}y}$가 $g(x)$\n",
    "\n",
    "다시 벡터로 돌아오자\n",
    "\n",
    "(올바른 풀이) $\\frac{\\partial}{\\partial y}({\\color{red}y}^\\top {\\color{blue}y}) = A + B = \\bf{I}{\\color{red}y} + \\bf{I}{\\color{blue}y} = 2\\bf{y}$\n",
    "\n",
    "* $A:= $ 빨간 $y$만 변수로 보고 미분\n",
    "* $B :=$ 파란 $y$만 변수로 보고 미분\n",
    "\n",
    "$A = \\big( \\frac{\\partial}{\\partial y} {\\color{red}y}^\\top \\big) {\\color{blue}y} = \\bf{I} {\\color{blue}y} $\n",
    "\n",
    "$B = \\big( \\frac{\\partial}{\\partial y} \\big) ( {\\color{red}y}^\\top {\\color{blue}y} )$ 스칼라라 순서 변경 가능$:= \\big( \\frac{\\partial}{\\partial y} \\big) ({\\color{blue}y}^\\top {\\color{red}y} \\big) = ( \\frac{\\partial}{\\partial y} {\\color{blue}y}^\\top \\big) {\\color{red}y} = \\bf{I} {\\color{red}y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170a7e6-784b-455e-853f-6309e6ba9d40",
   "metadata": {},
   "source": [
    "`5` $\\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{X} \\bf{B} = 2\\bf{X}^\\top \\bf{XB}$\n",
    "\n",
    "pf. $\\frac{\\partial}{\\partial B}({\\color{red}B}^\\top{\\color{red}X}^\\top {\\color{blue}X} {\\color{blue}B}) = A + B = \\bf{I}\\bf{X}^\\top \\bf{XB} + \\bf{I}\\bf{X}^\\top\\bf{XB} = 2\\bf{X}^\\top \\bf{XB}$\n",
    "\n",
    "$A = \\big( \\frac{\\partial}{\\partial B} {\\color{red}B}^\\top\\big) \\bf{X}^\\top \\bf{X} {\\color{blue}B} = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$\n",
    "\n",
    "$B = \\big( \\frac{\\partial}{\\partial B} \\big) ( {\\color{red}B}^\\top \\bf{X}^\\top \\bf{X} {\\color{blue}B}) = \\big(\\frac{\\partial}{\\partial B} \\big) ({\\color{blue}B}^\\top \\bf{X}^\\top \\bf{X}{\\color{red}B}) = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1885c18-485f-44b0-b169-4dccefae584c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd118178-666d-46a6-b93d-5efae0671676",
   "metadata": {},
   "source": [
    "$loss = (\\bf{y} - \\bf{XB})^\\top(\\bf{y} - \\bf{XB}) = \\bf{y}\\bf{y}^\\top - \\bf{y}^\\top\\bf{XB} - \\bf{B}^\\top\\bf{X}^\\top \\bf{y} + \\bf{B}^\\top \\bf{X}^\\top \\bf{XB}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial B} loss = 0 - \\frac{\\partial}{\\partial B} \\bf{y}^\\top \\bf{XB} - \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{y} + \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{XB} = 0 - \\bf{X}^\\top \\bf{y} - \\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB} = - 2\\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB}$\n",
    "\n",
    "$\\therefore \\frac{\\partial}{\\partial B} loss = 0 \\leftrightarrow 2\\bf{X}^\\top \\bf{y} = 2\\bf{X}^\\top \\bf{XB}$\n",
    "\n",
    "$\\therefore \\hat{\\bf{B}} = (\\bf{X}^\\top \\bf{X})^{-1}\\bf{X}^\\top \\bf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0560ef-daa5-4735-8dbd-2e2843ad919a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c1569-cb25-49ab-9adb-03f2180e8015",
   "metadata": {},
   "source": [
    "# 체인룰 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ebd9a-90fd-4226-9be0-eefd7b250a14",
   "metadata": {},
   "source": [
    "`-` 체인룰: 어려운 하나의 미분을 손쉬운 여러개의 미분으로 나누는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325aed8d-2df0-4717-810c-3758d31e5451",
   "metadata": {},
   "source": [
    "`-` 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1eed13-087b-43a9-9c92-1b26ad33fcdd",
   "metadata": {},
   "source": [
    "- ${\\bf X} \\to {\\bf X}{\\bf W} \\to {\\bf y} -{\\bf X}{\\bf W} \\to ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef811c10-d520-4ed5-8cf7-40b0d1e73fc4",
   "metadata": {},
   "source": [
    "`-` 위의 과정을 수식으로 정리해보면 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce83c4-bdfd-44d8-936c-a01ad12c60c9",
   "metadata": {},
   "source": [
    "- ${\\bf u}={\\bf X}{\\bf W}$, $\\quad {\\bf u}: n \\times 1$ \n",
    "\n",
    "- ${\\bf v} = {\\bf y}- {\\bf u},$  $\\quad {\\bf v}: n \\times 1$\n",
    "\n",
    "- $loss={\\bf v}^\\top {\\bf v},$ $\\quad loss: 1 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409f372-580e-4a76-8a85-b491eaccffa0",
   "metadata": {},
   "source": [
    "`-` 손실함수에 대한 미분은 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab10e6-1864-4a1e-bb08-b4d37b8df867",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial }{\\partial {\\bf W}} loss = \\frac{\\partial }{\\partial {\\bf W}} {\\bf v}^\\top {\\bf v}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97564d8-1800-4afc-b788-bb15812b9b8c",
   "metadata": {},
   "source": [
    "(그런데 이걸 어떻게 계산함?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3922387-6ed2-4df8-b3f9-24d8a69ffc00",
   "metadata": {},
   "source": [
    "`-` 계산할 수 있는것들의 모음.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856e109-5965-45a3-8221-9e60bb513e99",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial}{\\partial {\\bf v}} loss = 2{\\bf v}$ $\\quad \\to$ (n,1) 벡터 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771abf8-3d42-484a-9fa0-dd982b7b9586",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top = -{\\bf I}$ $\\quad \\to$ (n,n) 매트릭스 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70bee73-64b4-4910-a534-9bf7d46bf9c6",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top = {\\bf X}^\\top$ $\\quad \\to$ (p,n) 매트릭스 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccae60-baa8-444d-8e24-41d04b72b9b2",
   "metadata": {},
   "source": [
    "`-` 혹시.. 아래와 같이 쓸 수 있을까?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823dc28-0355-411d-8f75-6ff95bb223a8",
   "metadata": {},
   "source": [
    "$$ \\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) = \n",
    "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
    "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
    "\\frac{\\partial loss}{\\partial \\bf v}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475ae41-4a23-4ae6-aef9-5716bdacb4ab",
   "metadata": {},
   "source": [
    "- 가능할것 같다. 뭐 기호야 정의하기 나름이니까!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb23444-959b-4897-a7cf-29e16a7bd264",
   "metadata": {},
   "source": [
    "`-` 그렇다면 혹시 아래와 같이 쓸 수 있을까? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd16996-e8d8-4a4b-b070-c8dd78fc89a7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
    "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
    "\\frac{\\partial loss}{\\partial \\bf v} = \\frac{\\partial loss }{\\partial\\bf W}=\\frac{\\partial }{\\partial \\bf W} loss\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064edd0-c116-48d6-bb10-1c305d7c42a0",
   "metadata": {},
   "source": [
    "- 이건 선을 넘는 것임.\n",
    "- 그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156df62-cf34-4d0d-a531-8cdc88057fff",
   "metadata": {},
   "source": [
    "`-` 결국 정리하면 아래의 꼴이 되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac267e-20db-4908-82f1-407f6d226424",
   "metadata": {},
   "source": [
    "$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f99e49-72fd-4bc8-a3c3-e11b99c13ea4",
   "metadata": {},
   "source": [
    "`-` 그렇다면? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9bdf9-ee86-4db9-adb1-f9684b58e3b6",
   "metadata": {},
   "source": [
    "$$\\left({\\bf X}^\\top  \\right) \n",
    "\\left(-{\\bf I} \\right) \n",
    "\\left(2{\\bf v}\\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3879dbf-dc36-4593-a3c9-fc42465e3c52",
   "metadata": {},
   "source": [
    "그런데, ${\\bf v}={\\bf y}-{\\bf u}={\\bf y} -{\\bf X}{\\bf W}$ 이므로 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffac2c-8fde-417f-80cb-1b51cd3ba90f",
   "metadata": {},
   "source": [
    "$$-2{\\bf X}^\\top\\left({\\bf y}-{\\bf X}{\\bf W}\\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a7535-0f41-4332-bb6c-a335f400bae3",
   "metadata": {},
   "source": [
    "정리하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa24e6-54e7-4a28-858e-6fd081cfa217",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial }{\\partial \\bf W}loss = -2{\\bf X}^\\top{\\bf y}+2{\\bf X}^\\top {\\bf X}{\\bf W}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14241112-fc39-4173-88df-fd3512b28067",
   "metadata": {},
   "source": [
    "## 예시: 2021 빅데이터분석 중간고사 문제 2-(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba9719-39ac-4094-bc1f-d5d01d33cb62",
   "metadata": {},
   "source": [
    "`-` 미분계수를 계산하는 문제였음.. \n",
    "\n",
    "- <https://guebin.github.io/BDA2021/2021/11/09/mid.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48620971-3845-4ca4-b140-fda39f76c851",
   "metadata": {},
   "source": [
    "`-` 체인룰을 이용하여 미분계수를 계산하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dc749e-b596-4353-9d1b-4f376cd239e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones= torch.ones(5)\n",
    "x = torch.tensor([11.0,12.0,13.0,14.0,15.0])\n",
    "X = torch.vstack([ones,x]).T\n",
    "y = torch.tensor([17.7,18.5,21.2,23.6,24.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fda8da-e858-47a7-9c19-68ab7e2f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([3.0,3.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bd4b74-a730-4ec1-a76b-751fe19cd568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642814471/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "u = X@W \n",
    "v = y-u \n",
    "loss = v.T @ v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ff1f54-5059-414e-ac49-68618f3284cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2212.1799)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357230e7-3c00-44eb-a0b3-a89b7b3af781",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial}{\\partial\\bf W}loss $ 의 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724d98c-9c43-4692-b33b-cd7e33dd79c7",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial \\bf W}loss = \\left({\\bf X}^\\top  \\right) \n",
    "\\left(-{\\bf I} \\right) \n",
    "\\left(2{\\bf v}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1700538-b4b0-41f4-909e-e010c4c42fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 209.6000, 2748.6001])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T @ -torch.eye(5) @ (2*v) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22c15b-fa8e-4101-b088-e9963651d3e4",
   "metadata": {},
   "source": [
    "`-` 참고로 중간고사 답은 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6016230-fa9d-49fe-ab24-52a4bbc183f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41.9200, 549.7200])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T @ -torch.eye(5)@ (2*v) / 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98fa2f-a69f-4fc1-a8b7-d78fc4605ef6",
   "metadata": {},
   "source": [
    "입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca276697-20ed-42c9-aa79-e741810c7424",
   "metadata": {},
   "source": [
    "`-` 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6003451-0365-4cb4-8a6d-d05749e2f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_W = torch.tensor([3.0,3.0],requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdcdefd-f53c-43cd-a781-3ebe386eff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss = (y-X@_W).T @ (y-X@_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579ab062-5062-4d4c-bc69-3e65be0c3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3124d0ed-9b38-4521-a286-19e474404013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 209.6000, 2748.6001])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_W.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765a9f6-d7ef-473f-b301-ae4d4f2ef343",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}$ 임을 확인하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c99dfc-d8d9-4667-99d8-85f2dd049260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36416d0c-642c-4a97-9996-bd43823aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7463c3b3-6156-40ea-8668-7d57923d084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss = _v.T @ _v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9af246f-c445-4539-8b81-1cd76f6ef6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a89d62dc-d4c7-4cb6-8ed7-ec72f3076446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-36.6000, -41.0000, -41.6000, -42.8000, -47.6000]),\n",
       " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_v.grad.data, v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432322f-860b-4519-8dea-4f120a96035c",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top$ 의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41878d75-2399-4e1e-9884-73fd35b4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 39., 42., 45., 48.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629d401f-e176-456d-a892-aba0fad18911",
   "metadata": {},
   "outputs": [],
   "source": [
    "_v = y - _u ### 이전의 _v와 또다른 임시 _v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd503f1e-091d-486a-b9b3-b472277fdf91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dfdd06dff626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "(_v.T).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c7139-e2fe-435d-b1dd-348dd385a35c",
   "metadata": {},
   "source": [
    "- 사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b153c4-e8cb-45a7-bd7e-725716e6e8cf",
   "metadata": {},
   "source": [
    "그런데 $\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)$ 이므로\n",
    "\n",
    "조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea9353-0730-4cb2-8ed2-dd7f3454f2fa",
   "metadata": {},
   "source": [
    "(0) $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top$의 결과를 저장할 매트릭스를 만든다. 적당히 `A`라고 만들자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119090a0-f7a0-4c8f-b15f-81fd360105f9",
   "metadata": {},
   "source": [
    "(1) `_u` 하나를 임시로 만든다. 그리고 $v_1$을 `_u`로 미분하고 그 결과를 `A`의 첫번째 칼럼에 기록한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79d878-9a21-4ff0-856e-954b0726246a",
   "metadata": {},
   "source": [
    "(2) `_u`를 또하나 임시로 만들고 $v_2$를 `_u`로 미분한뒤 그 결과를 `A`의 두번째 칼럼에 기록한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc33a5a-ccf8-4a05-8f2a-8bb852c64370",
   "metadata": {},
   "source": [
    "(3) (1)-(2)와 같은 작업을 $v_5$까지 반복한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbde909-ef40-4556-b909-da1dabcb9c3d",
   "metadata": {},
   "source": [
    "***(0)을 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3e7f9c-c22d-4a71-b185-64d3bab3318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.zeros((5,5))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad252f07-48d8-4893-8c34-a2a3e4cd0982",
   "metadata": {},
   "source": [
    "***(1)을 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4afa843-9af7-4d00-b739-f706934933c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36., 39., 42., 45., 48.]),\n",
       " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc1e08db-3c27-4fbf-b32d-f87ce6f2bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "v1 = (y-_u)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cd9ad-f9ba-4dad-836b-772d0b34849b",
   "metadata": {},
   "source": [
    "- 이때 $v_1=g(f({\\bf u}))$와 같이 표현할 수 있다. 여기에서 $f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top$, 그리고 $g((v_1,\\dots,v_n)^\\top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을 수행하는 함수이고, $g$는 프로젝션 함수이다. 즉 $f:\\mathbb{R}^5 \\to \\mathbb{R}^5$인 함수이고, $g:\\mathbb{R}^5 \\to \\mathbb{R}$인 함수이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d96420-805a-418a-ba79-3241aa1bc7bb",
   "metadata": {},
   "source": [
    "여기서 v1은 꼬리표호서 selection 작성되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "181939d0-a5de-453e-a576-d8f25698dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72319636-170a-4730-bee6-9988284f1117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7237e87d-acea-4dd5-ba2d-29101714c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[:,0]= _u.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5093f1-953a-4464-9d49-7e41bcc8e504",
   "metadata": {},
   "source": [
    "A의 첫번째 칼럼에 이것을 넣어주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5cf043b-2eec-40d2-a2f9-c66a89868c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796369ad-9462-4678-8e46-5ef1ae6bf298",
   "metadata": {},
   "source": [
    "***(2)를 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ce63e8-ad56-4bb2-943d-94e8585c3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "v2 = (y-_u)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b69505a2-1fe1-4089-a61c-49da015d75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68042b18-2a87-4c20-9378-992a3ac0c6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0., -1., -0., -0., -0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ec45c0-b578-4618-9965-eedc5a833e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -0.,  0.,  0.,  0.],\n",
       "        [-0., -1.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:,1]= _u.grad.data\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed59da1-6ab0-4af7-8091-935bd4e98aad",
   "metadata": {},
   "source": [
    "***(3)을 수행*** // 그냥 (1)~(2)도 새로 수행하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f2b31cf-b516-4778-913d-ae6b844d4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "    _v = (y-_u)[i]\n",
    "    _v.backward()\n",
    "    A[:,i]= _u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddd53a8d-5749-41ce-bd55-14b845372ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -0., -0., -0., -0.],\n",
       "        [-0., -1., -0., -0., -0.],\n",
       "        [-0., -0., -1., -0., -0.],\n",
       "        [-0., -0., -0., -1., -0.],\n",
       "        [-0., -0., -0., -0., -1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93166864-96cf-444d-99c6-66884e186d9e",
   "metadata": {},
   "source": [
    "- 이론적인 결과인 $-{\\bf I}$와 일치한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a31d0-b565-4c0e-aa80-6f0f1e687912",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top$의 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92843d1e-7445-4ab5-b263-3d7b00440796",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00ab6d57-15e5-4795-9e3d-7096a634c35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.zeros((2,5))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d41741d-71d4-49ce-88d3-0c822829aa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b514297b-a61e-46cb-a69a-c958fecd60ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_W = torch.tensor([3., 3.],requires_grad=True)\n",
    "_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ea85c9-2524-4ea3-9c26-4f3474f5482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    _W = torch.tensor([3., 3.],requires_grad=True)\n",
    "    _u = (X@_W)[i]\n",
    "    _u.backward()\n",
    "    B[:,i]= _W.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eb14f57-0152-4079-9581-c712da151855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
       "        [11., 12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B # X의 트랜스포즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84338235-2a97-47bb-a3fb-abfb857d4aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 11.],\n",
       "        [ 1., 12.],\n",
       "        [ 1., 13.],\n",
       "        [ 1., 14.],\n",
       "        [ 1., 15.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f569721-df19-4efb-bfdd-5a84ccd61f46",
   "metadata": {},
   "source": [
    "- 이론적인 결과와 일치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf823d78-352d-4c63-88e8-a289f4ae031a",
   "metadata": {},
   "source": [
    "## 잠깐 생각해보자.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625d920-dd31-4699-b12c-4f24d7a517d4",
   "metadata": {},
   "source": [
    "`-` 결국 위의 예제에 한정하여 임의의 ${\\bf \\hat{W}}$에 대한 $\\frac{\\partial}{\\partial {\\bf \\hat W}}loss$는 아래와 같이 계산할 수 있다. \n",
    "\n",
    "- (단계1) $2{\\bf v}$를 계산하고 \n",
    "- (단계2) (단계1)의 결과 앞에 $-{\\bf I}$를 곱하고 \n",
    "- (단계3) (단계2)의 결과 앞에 ${\\bf X}^\\top$를 곱한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987eda3-e42e-4346-b79a-1adb399b99ad",
   "metadata": {},
   "source": [
    "`-` step1에서 ${\\bf v}$는 어떻게 알지? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f67a3b-fc42-44fb-bcb2-833b2615a5f8",
   "metadata": {},
   "source": [
    "- X $\\to$ u=X@W $\\to$ v = y-u "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd7ce6-ce54-4077-a522-ea06aceeb411",
   "metadata": {},
   "source": [
    "- 그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것 아니었나? \n",
    "- step1: yhat, step2: loss, step3: derivate, step4: update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f270b8-30e1-4590-b576-b100dc3a2fab",
   "metadata": {},
   "source": [
    "`-` **(중요)** step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을 다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한 방법을 통하여 이용하여 보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51dcf6-fdf5-436c-b262-67efa7fa92e1",
   "metadata": {},
   "source": [
    "### backprogation 알고리즘 모티브"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a8610-1131-48be-92cb-fdd6b5d6adea",
   "metadata": {},
   "source": [
    "`-` 아래와 같이 함수의 변환을 아키텍처로 이해하자. (함수의입력=레이어의입력, 함수의출력=레이어의출력) \n",
    "\n",
    "- ${\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91158c-60bf-4a15-9a37-d7df9caa1705",
   "metadata": {},
   "source": [
    "`-` 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다. (${\\bf X} \\to {\\bf \\hat y} \\to loss$가 아니라 ${\\bf W} \\to loss({\\bf W})$로 생각해보세요)\n",
    "\n",
    "- ${\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dad0ce-dde4-4d76-9bf0-b229fd7aa88f",
   "metadata": {},
   "source": [
    "`-` 그렇다면 아래와 같은 사실을 관찰할 수 있다. \n",
    "\n",
    "- (단계1) $2{\\bf v}$는 function of ${\\bf v}$이고, ${\\bf v}$는 l3의 입력 (혹은 l2의 출력) \n",
    "- (단계2) $-{\\bf I}$는 function of ${\\bf u}$이고, ${\\bf u}$는 l2의 입력 (혹은 l1의 출력) \n",
    "- (단계3) 마찬가지의 논리로 ${\\bf X}^\\top$는 function of ${\\bf W}$로 해석할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21dbed-6035-4108-80df-d745a03f0467",
   "metadata": {},
   "source": [
    "`-` 요약: $2{\\bf v},-{\\bf I}, {\\bf X}^\\top$와 같은 핵심적인 값들이 사실 각 층의 입/출력 값들의 함수꼴로 표현가능하다. $\\to$ 각 층의 입/출력 값들을 모두 기록하면 미분계산을 유리하게 할 수 있다. \n",
    "\n",
    "- 문득의문: 각 층의 입출력값 ${\\bf v}, {\\bf u}, {\\bf W}$로 부터 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 만들어내는 방법을 모른다면 헛수고 아닌가? \n",
    "- 의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, ...) 정도가 전부임. 따라서 변환규칙은 미리 계산할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b33de-b724-4406-9402-c0b9df5a31c6",
   "metadata": {},
   "source": [
    "`-` 결국 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a93bee-2825-419c-8b04-77cacb9b1b79",
   "metadata": {},
   "source": [
    "`(1)` 순전파를 하면서 입출력값을 모두 저장하고 \n",
    "\n",
    "`(2)` 그에 대응하는 층별 미분계수값 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 구하고 \n",
    "\n",
    "`(3)` 층별미분계수값을 다시 곱하면 (그러니까 ${\\bf X}^\\top (-{\\bf I}) 2{\\bf v}$ 를 계산) 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d73ca-fbcf-49c9-8e01-a87c708d8feb",
   "metadata": {},
   "source": [
    "### backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5de5bd-0665-4276-b1b5-96c6e5d63352",
   "metadata": {},
   "source": [
    "`(1)` 순전파를 계산하고 각 층별 입출력 값을 기록 \n",
    "\n",
    "- yhat = net(X) \n",
    "- loss = loss_fn(yhat,y) \n",
    "\n",
    "`(2)` 역전파를 수행하여 손실함수의 미분값을 계산 \n",
    "\n",
    "- loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9867bc-ba60-4886-9fb5-104bcef25698",
   "metadata": {},
   "source": [
    "`-` 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU 메모리.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04675f70-3717-4117-97cf-83389fddfb5b",
   "metadata": {},
   "source": [
    "`-` 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e422d85-f035-4e34-b04a-4dcf47cfdfb4",
   "metadata": {},
   "source": [
    "***gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한 코어숫자)***\n",
    "\n",
    "- 아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 // `net.to(\"cuda:0\")`\n",
    "- 순전파 계산: ***중간 계산결과를 모두 GPU메모리에 저장*** (순전파 계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를 계산하기 위한 대비) // `net(X)` \n",
    "- 오차 및 손실함수 계산: `loss = loss_fn(yhat,y)`\n",
    "- 역전파 계산: ***순전파단계에서 저장된 계산결과를 활용***하여 손실함수의 미분값을 계산 // `loss.backward()`\n",
    "- 다음 순전파 계산: ***이전값은 삭제하고 새로운 중간계산결과를 GPU메모리에 올림*** \n",
    "- 반복. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b847a2-ca7c-4905-b409-0f9756e72cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## some comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863bbef-414e-4627-be53-25bdc024a7e8",
   "metadata": {},
   "source": [
    "`-` 역전파기법은 체인룰 + $\\alpha$ 이다. \n",
    "- 미분 계산을 하기 위함인데 여기서 파라메터 업데이트 필요하지 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245186f-0e59-4c01-bbe8-3a6817b328f1",
   "metadata": {},
   "source": [
    "`-` 오차역전파기법이라는 용어를 쓰는 사람도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36840ad-ab19-483f-9961-c61289e4153e",
   "metadata": {},
   "source": [
    "`-` 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우 순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. \n",
    "- 예) 개/고양이 확인 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ac8e8-a3ca-4bf7-a590-62640737e680",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 기울기소멸 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23be2f-7b46-4fe0-9d46-28f3aa7b725f",
   "metadata": {},
   "source": [
    "## 고요속의 외침 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff2c44-994b-4751-a441-21935bf659e9",
   "metadata": {},
   "source": [
    "`-` <https://www.youtube.com/watch?v=ouitOnaDtFY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d30fc-ab1a-4e1f-8ba2-c0048994112a",
   "metadata": {},
   "source": [
    "`-` 중간에 한명이라도 잘못 말한다면.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6e3a5-7070-4c27-b99b-3d37e4a8a5cd",
   "metadata": {},
   "source": [
    "## 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454d93b-a28e-4a96-95ab-7517da6f21ab",
   "metadata": {},
   "source": [
    "`-` In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fa379-79c8-45f8-8661-1e3f072f414a",
   "metadata": {},
   "source": [
    "## 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c6d5f-76ce-4c2c-8c2d-75ff2cd46b06",
   "metadata": {},
   "source": [
    "`-` 당연한것 아닌가? \n",
    "\n",
    "- 그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의 기울기를 통하여 업데이트 하는 방식 \n",
    "- 역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $\\alpha$). 구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을 각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다. \n",
    "- 0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인 기울기가 나올 것이라고 생각하는것 자체가 이상함) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98b22c-85b0-4ea5-883d-50b3653e6e45",
   "metadata": {},
   "source": [
    "0 전달되면 업데이트 안 되잖아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52fbd89c-dc57-41c2-be75-31ff2d98e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ce5cfc-eed8-4acf-b927-12a929125508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27649548, -1.3435985 , -1.98858349,  1.98266931,  0.74704939,\n",
       "       -0.4831162 ,  1.41216797, -1.20423568,  0.66637324,  1.18085266,\n",
       "        0.23356958, -0.39851281,  0.2357107 , -0.28592461, -0.96001359,\n",
       "       -1.6276445 , -0.75687257,  0.48770524,  0.02553662,  0.18478814,\n",
       "        1.28850714,  1.41398052, -0.33970936,  0.36829707,  1.85991256,\n",
       "        0.02243541, -0.68804507,  0.63659842, -1.13905311,  0.94093391,\n",
       "        1.58809026, -0.25106013, -0.14446307,  1.31747003,  1.52190566,\n",
       "       -0.44264824, -1.95722305, -0.77865942, -0.3350363 , -0.53638126,\n",
       "       -0.77254936, -1.22118632,  0.6137345 ,  0.89975951, -1.70293244,\n",
       "        1.42661365, -0.43175558, -1.30904545, -0.53912915,  1.51725173,\n",
       "       -1.83965849,  1.00143736, -0.67129779,  0.36061957, -1.68850939,\n",
       "       -0.4272909 , -0.34715325,  1.72387253, -0.98340508, -1.60825385,\n",
       "        1.64523373, -0.79036932,  1.82578785, -0.53592773, -0.61384056,\n",
       "        0.9689625 ,  1.27971335,  0.51555469, -0.53425795,  0.38883373,\n",
       "       -0.28595978, -1.93730647, -1.94581503, -0.48984819, -1.21831701,\n",
       "       -1.25965989,  1.79542393,  1.2637913 , -0.93178556, -0.61210568,\n",
       "        1.23775906, -1.80601708,  1.40265496, -0.59602715,  1.44638486,\n",
       "       -1.71721283,  1.58345756, -1.03992841,  1.10167726,  1.13332066,\n",
       "       -0.76344022, -0.7246539 ,  0.27256115, -1.95501872, -0.65922909,\n",
       "        0.78715854, -0.29077574, -0.45110518, -1.64836114,  1.91692815])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-2,high=2,size=100) \n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92b87c0d-6c57-4f92-afed-27b622bf5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.089052412862103e-11"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a9d9a-5ef2-4ce2-bf0b-c0148d947355",
   "metadata": {},
   "source": [
    "- 기울기가 소멸함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b17f42bf-9be7-4b3c-a466-64bf8407bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6059047739389678e+16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-5,high=5,size=100) \n",
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5eef5-c513-4581-8daa-7b99d6209452",
   "metadata": {},
   "source": [
    "- 기울기가 폭발함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11615aac-a8c2-435d-8c91-ad6b297155de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13058.879478242436"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-1,high=3.5,size=100) \n",
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40eaf9-767f-4136-9535-20c852e3c2ff",
   "metadata": {},
   "source": [
    "`-` 도깨비: 기울기가 소멸하기도 하고 터지기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbad05d-18e6-4922-a999-981ab1f70395",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [해결책](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (기울기 소멸에 대한 해결책) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64f70-36c6-40d9-8555-58320f30c207",
   "metadata": {},
   "source": [
    "`-` Multi-level hierarchy\n",
    "\n",
    "- 여러층을 쪼개서 학습하자 $\\to$ 어떻게? 사전학습, 층벼학습 \n",
    "- 기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임. \n",
    "- 결국 입력자료를 바꾼뒤에 학습하는 형태 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40b0d1-1009-4cf6-a37e-d4aee3ebe714",
   "metadata": {},
   "source": [
    "`-` Gradient clipping\n",
    "\n",
    "- 너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be361a7-e05e-49d8-a00c-e4797b9566a3",
   "metadata": {},
   "source": [
    "`-` Faster hardware\n",
    "\n",
    "- GPU를 중심으로 한 테크닉 \n",
    "- 근본적인 문제해결책은 아니라는 힌튼의 비판 \n",
    "- CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73399fec-0511-4d68-9983-8290109fecba",
   "metadata": {},
   "source": [
    "`-` Residual Networks, LSTM \n",
    "\n",
    "- 아키텍처를 변경하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ce99-2d2c-44cb-8056-29d2d0b88327",
   "metadata": {},
   "source": [
    "`-` Other activation functions\n",
    "\n",
    "- 렐루의 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2d474-24f6-44d5-a09e-92998110abc1",
   "metadata": {},
   "source": [
    "- 렐루가 음수는 아예 0, 양수는 기울기 확실하니까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d784652-c6ce-4617-a60a-63f3d9eb1f7f",
   "metadata": {},
   "source": [
    "`-` 배치정규화 \n",
    "\n",
    "- 어쩌다보니 되는것. \n",
    "- 배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기 소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로 빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한 언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n",
    "- 심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에 언급했음) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415d412-42cb-4989-86ae-7c073885969a",
   "metadata": {},
   "source": [
    "`-` **기울기를 안구하면 안되나?**\n",
    "\n",
    "- 베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $\\to$ GPU를 어떻게 쓰지? $\\to$ 느리다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
