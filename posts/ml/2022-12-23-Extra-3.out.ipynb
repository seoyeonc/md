{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra-3: 딥러닝의 기초 (5)\n",
        "\n",
        "SEOYEON CHOI  \n",
        "2022-12-21\n",
        "\n",
        "> 벡터미분, 역전파와 기울기 소멸\n",
        "\n",
        "기울기 소멸: loss 를 W로 미분했더니 그 값이 거의 0ㅇ이 나오는 현상 $\\to$\n",
        "update가 거의 이루어지지 않음\n",
        "\n",
        "이유 - W -\\> loss인 함수는 W에 어떤한 선형변환 $\\to$ 비선형변환 $\\to$\n",
        "선형변환 $\\to$ 비선형변환 $\\to \\dots$ $\\to$ loss 의 과정으로 해석 가능 -\n",
        "즉 loss는 W의 합성의 합섭ㅇ의… 합성함수로 해석가능 - loss 를 W로 미분한\n",
        "값은 각 변환단ㅇ계에서 정의되는 함수의 도함수를 모두 곱한 것과\n",
        "같음(체인룰) - 체인 중에서도 하나라도 0이 나오면 곱한 값은 0 이다. -\n",
        "체인이 길수록 하나라도 0이 나오는 경우가 많음\n",
        "\n",
        "왜 깊은 신경망일수록 기울기 소멸이 빈번한가? - 체인이 길기 때문에.\n",
        "\n",
        "왜 순환신경망일수록 기울기 소멸이 빈번할까\\>? - 체인이 길기 때문에.\n",
        "\n",
        "# import"
      ],
      "id": "bf8bf96f-36eb-45d7-9921-86a5796658eb"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch "
      ],
      "id": "217616da-3e8c-4391-b402-65b3f42d71bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 벡터미분\n",
        "\n",
        "`-` 벡터미분에 대한 강의노트:\n",
        "\n",
        "-   <https://github.com/guebin/STML2022/blob/main/posts/II.%20DNN/supp.pdf>\n",
        "\n",
        "`-` 요약: 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로\n",
        "계산할 수 있다.\n",
        "\n",
        "-   $loss = ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\bf W} - {\\bf W}^\\top {\\bf X}^\\top {\\bf y} + {\\bf W}^\\top {\\bf X}^\\top {\\bf X} {\\bf W}$\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial {\\bf W}}loss = -2{\\bf X}^\\top {\\bf y} +2 {\\bf X}^\\top {\\bf X} {\\bf W}$\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "`-` 모형의 매트릭스화\n",
        "\n",
        "우리의 모형은 아래와 같다.\n",
        "\n",
        "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10$\n",
        "\n",
        "풀어서 쓰면\n",
        "\n",
        "$\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}$\n",
        "\n",
        "아래와 같이 쓸 수 있다.\n",
        "\n",
        "$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_{10} \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{10} \\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_{10} \\end{bmatrix}$\n",
        "\n",
        "벡터와 매트릭스 형태로 정리하면\n",
        "\n",
        "${\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}$\n",
        "\n",
        "`-` 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n",
        "\n",
        "$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$\n",
        "\n",
        "이것을 벡터표현으로 하면 아래와 같다.\n",
        "\n",
        "$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})$\n",
        "\n",
        "풀어보면\n",
        "\n",
        "$loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$\n",
        "\n",
        "`-` 미분하는 과정의 매트릭스화\n",
        "\n",
        "loss를 최소화하는 ${\\boldsymbol \\beta}$를 구해야하므로 loss를\n",
        "${\\boldsymbol \\beta}$로 미분한 식을 0이라고 놓고 풀면 된다.\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$\n",
        "\n",
        "$= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta}$\n",
        "\n",
        "따라서 $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0$을 풀면\n",
        "아래와 같다.\n",
        "\n",
        "$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}$\n",
        "\n",
        "`-` 공식도 매트릭스로 표현하면:\n",
        "$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}$\n",
        "\\<– 외우세요\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**벡터미분 / 매트릭스 미분**\n",
        "\n",
        "1.  정의 1: 벡터로 미분\n",
        "\n",
        "$\\frac{\\partial}{\\partial y} = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix}$\n",
        ", $y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
        "\n",
        "1.  정의 2: 매트릭스로 미분\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\bf{X} } := \\begin{bmatrix} \\frac{\\partial}{\\partial x_{11} } & \\dots & \\frac{\\partial}{\\partial x_{1p} } \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial}{\\partial x_{n1} } & \\dots & \\frac{\\partial}{\\partial x_{np} } \\end{bmatrix}, \\bf{X} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np} \\end{bmatrix}$\n",
        "\n",
        "`1`\n",
        "\n",
        "$\\frac{\\partial}{\\partial x}(x^\\top y) = y$, 단\n",
        "$x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n",
        "\n",
        "pf.\n",
        "$\\bf{x}^\\top \\bf{y} = \\begin{bmatrix} x_1 & \\dots x_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n$\n",
        "\n",
        "$\\frac{\\partial}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix}$\n",
        "이므로\n",
        "\n",
        "$\\big( \\frac{\\partial}{\\partial x} \\big) x^\\top y = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix} (x_1 y_1 + x_2 y_2 + \\dots + x_n y_n)$\n",
        "\n",
        "$= \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}(x_1y_1 + \\dots + x_n y_n) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}(x_1y_1 + \\dots + x_n y_n) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 y_1 + \\frac{\\partial}{\\partial x_1}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_1}x_n y_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} x_1 y_1 + \\frac{\\partial}{\\partial x_n}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_n}x_n y_n \\end{bmatrix}$\n",
        "\n",
        "$= \\begin{bmatrix} y_1 + 0 + \\dots + 0 \\\\ \\vdots \\\\ 0+0+ \\dots + y_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{y}$\n",
        "\n",
        "`2` $\\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$ 임을\n",
        "보이는 다른 풀이\n",
        "\n",
        "pf.\n",
        "$\\frac{\\partial}{\\partial x} \\big(\\bf{x}^\\top \\bf{y} \\big) = \\big( \\frac{\\partial}{\\partial x} x^\\top \\big) y = \\bf{I} \\bf{y} = \\bf{y}$\n",
        "\n",
        "$\\frac{\\partial}{\\partial x} x^\\top = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}\\end{bmatrix} \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 & \\dots & \\frac{\\partial}{\\partial x_n} x_n \\\\ \\vdots & & \\\\ \\frac{\\partial}{\\partial x_n} x_1 & \\dots &\\frac{\\partial}{\\partial x_n} x_n \\end{bmatrix} = \\bf{I}$\n",
        "\n",
        "`2` $\\frac{\\partial}{\\partial x}(\\bf{y}^\\top \\bf{x} ) = \\bf{y}$\n",
        "\n",
        "pf.\n",
        "$\\frac{\\partial}{\\partial x}(\\bf{y}^\\top\\bf{x}) = \\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$\n",
        "\n",
        "`3`\n",
        "$\\frac{\\partial}{\\partial B}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{x}^\\top \\bf{y}$\n",
        "\n",
        "단, $\\bf{B} := p \\times 1$ vector, $\\bf{X} := n \\times p$ matrix,\n",
        "$\\bf{y} := n \\times 1$ vector\n",
        "\n",
        "pf. $\\bf{y}^\\top \\bf{X} \\bf{B}$는 스칼라($1 \\times 1$)이므로\n",
        "$\\bf{y}^\\top\\bf{X}\\bf{B} = (\\bf{y}^\\top \\bf{X} \\bf{B})^\\top = \\bf{B}^\\top \\bf{X}^\\top \\bf{y}$\n",
        "\n",
        "$\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\frac{\\partial}{\\partial \\bf{B}}(\\bf{B}^\\top \\bf{X}^\\top \\bf{y}) = \\big( \\frac{\\partial}{\\partial \\bf{B}} \\bf{B}^\\top \\big) \\bf{X}^\\top \\bf{y} = \\bf{X}^\\top \\bf{y}$\n",
        "\n",
        "`(다른 풀이)`\n",
        "$\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{X}^\\top \\bf{y}$임을\n",
        "보이는 다른 풀이\n",
        "\n",
        "$\\bf{XB} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} = \\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n",
        "\n",
        "$\\bf{y}^\\top \\bf{XB} = \\begin{bmatrix} y_1 & \\dots y_n \\end{bmatrix}\\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n",
        "\n",
        "$= y_1(x_{11}B_1 + \\dots + x_{1p}B_p) + y_2(x_{21}B_1 + \\dots + x_{2p}B_p ) + \\dots + y_n(x_{n1}B_1 + \\dots + x_{np} B_p) = A_1 + A_2 + \\dots A_n$\n",
        "\n",
        "-   note: $A_1, A_2, \\dots$은 모두 스칼라($1 \\times 1$)\n",
        "\n",
        "$\\big( \\frac{\\partial}{\\partial B} \\big)(\\bf{y}^\\top \\bf{XB}) = \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} \\\\ \\vdots \\\\\\frac{\\partial}{\\partial B_p} \\end{bmatrix}(A_1 + A_2 + \\dots A_n)$\n",
        "\n",
        "$= \\begin{bmatrix}\\frac{\\partial}{\\partial B_1} A_1 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_1 \\end{bmatrix} + \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_2 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_2 \\end{bmatrix}+ \\dots +\\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_n \\end{bmatrix}$\n",
        "\n",
        "-   $\\frac{\\partial}{\\partial B_1}A_1 = \\frac{\\partial}{\\partial B_1}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = y_1 x_{11} + 0 + \\dots + 0$\n",
        "\n",
        "-   $\\frac{\\partial}{\\partial B_2}A_2 = \\frac{\\partial}{\\partial B_2}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = 0 + y_1 x_{12} + 0 + \\dots + 0$\n",
        "\n",
        "$= \\begin{bmatrix} y_1 x_{11} \\\\ y_1 x_{12} \\\\ \\vdots \\\\ y_1 x_{1p}\\end{bmatrix} + \\begin{bmatrix} y_2 x_{21} \\\\ y_2 x_{22} \\\\ \\vdots \\\\ y_2 x_{2p} \\end{bmatrix} + \\dots + \\begin{bmatrix} y_n x_{n1} \\\\ y_n x_{n2} \\\\ \\vdots \\\\ y_n x_{np} \\end{bmatrix}$\n",
        "\n",
        "$= \\begin{bmatrix} y_1 x_{11} + y_2 x_{21} + \\dots + y_n x_{n1} \\\\ y_1 x_{12} + y_2 x_{22} + \\dots + y_n x_{n2} \\\\ \\vdots \\\\ y_1 x_{1p} + y_2 x_{2p} + \\dots + y_n x_{np} \\end{bmatrix} = \\begin{bmatrix} x_{11} & x_{21} & \\dots & x_{n1} \\\\ x_{12} & x_{22} & \\dots & x_{n2} \\\\ \\vdots & \\vdots & & \\vdots \\\\ x_{1p} & x_{2p} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{X}^\\top \\bf{y}$\n",
        "\n",
        "`4` $\\frac{\\partial}{\\partial y} \\bf{y}^\\top \\bf{y} = 2\\bf{y}$\n",
        "\n",
        "pf.\n",
        "$\\bf{y}^\\top \\bf{y} = \\begin{bmatrix} y_1 & \\dots & y_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = y_{1}^{2} + \\dots + y_{n}^{2}$\n",
        "\n",
        "$\\therefore \\frac{\\partial}{\\partial y}(\\bf{y}^\\top y) = \\frac{\\partial}{\\partial y}(y_{1}^{2} + \\dots + y_{n}^{2}) = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix} (y_{1}^{2} + \\dots + y_{n}^{2})$\n",
        "\n",
        "$= \\begin{bmatrix} \\frac{\\partial}{\\partial y_1}(y_{1}^{2} + \\dots + y_{n}^{2}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n}(y_{1}^{2} + \\dots + y_{n}^{2}) \\end{bmatrix} = \\begin{bmatrix}2y_1 + 0 + \\dots + 0 \\\\ 0 + 2y_2 + \\dots + 0 \\\\ \\vdots \\\\ 0 + 0 + \\dots + 2y_n \\end{bmatrix} = 2\\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = 2\\bf{y}$\n",
        "\n",
        "(틀린 풀이)\n",
        "$\\frac{\\partial}{\\partial y}(\\bf{y}^\\top \\bf{y}) {\\color{orange}=} \\big( \\frac{\\partial}{\\partial y} \\bf{y}^\\top \\big) \\bf{y}$\n",
        "미분하는 값에 대해 상수일때만 가능\n",
        "\n",
        "그런데\n",
        "$\\big(\\frac{\\partial}{\\partial y}\\bf{y}^\\top \\big) = \\bf{I}$이므로,\n",
        "$\\frac{\\partial}{\\partial y} (\\bf{y}^\\top \\bf{y}) = \\bf{I} \\bf{y} = \\bf{y}$\n",
        "\n",
        "(틀린 풀이의 스칼라 버전)\n",
        "$\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} y \\big) y = 1 \\times y = \\bf{y}$\n",
        "\n",
        "(올바른 풀이) :\n",
        "$\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} \\big) {\\color{red}y} {\\color{blue}y} = \\big( \\frac{d}{dy} {\\color{red}y} \\big) {\\color{blue}y} + {\\color{red}y} \\big( \\frac{d}{dy} {\\color{blue}y} \\big)$\n",
        "\n",
        "Note: 곱의 미분 : 함수 $f(x), g(x)$ 가 $x$에 대하여 미분 가능하면\n",
        "$\\{ {\\color{red}f(x)} \\times {\\color{blue}g(x)} \\}' = {\\color{red}f(x)}'g{\\color{blue}(x)} + {\\color{red}f(x)}{\\color{blue}g(x)}'$\n",
        "\n",
        "-   여기서는 ${\\color{red}y}$가 $f(x)$, ${\\color{blue}y}$가 $g(x)$\n",
        "\n",
        "다시 벡터로 돌아오자\n",
        "\n",
        "(올바른 풀이)\n",
        "$\\frac{\\partial}{\\partial y}({\\color{red}y}^\\top {\\color{blue}y}) = A + B = \\bf{I}{\\color{red}y} + \\bf{I}{\\color{blue}y} = 2\\bf{y}$\n",
        "\n",
        "-   $A :=$ 빨간 $y$만 변수로 보고 미분\n",
        "-   $B :=$ 파란 $y$만 변수로 보고 미분\n",
        "\n",
        "$A = \\big( \\frac{\\partial}{\\partial y} {\\color{red}y}^\\top \\big) {\\color{blue}y} = \\bf{I} {\\color{blue}y}$\n",
        "\n",
        "$B = \\big( \\frac{\\partial}{\\partial y} \\big) ( {\\color{red}y}^\\top {\\color{blue}y} )$\n",
        "스칼라라 순서 변경\n",
        "가능$:= \\big( \\frac{\\partial}{\\partial y} \\big) ({\\color{blue}y}^\\top {\\color{red}y} \\big) = ( \\frac{\\partial}{\\partial y} {\\color{blue}y}^\\top \\big) {\\color{red}y} = \\bf{I} {\\color{red}y}$\n",
        "\n",
        "`5`\n",
        "$\\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{X} \\bf{B} = 2\\bf{X}^\\top \\bf{XB}$\n",
        "\n",
        "pf.\n",
        "$\\frac{\\partial}{\\partial B}({\\color{red}B}^\\top{\\color{red}X}^\\top {\\color{blue}X} {\\color{blue}B}) = A + B = \\bf{I}\\bf{X}^\\top \\bf{XB} + \\bf{I}\\bf{X}^\\top\\bf{XB} = 2\\bf{X}^\\top \\bf{XB}$\n",
        "\n",
        "$A = \\big( \\frac{\\partial}{\\partial B} {\\color{red}B}^\\top\\big) \\bf{X}^\\top \\bf{X} {\\color{blue}B} = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$\n",
        "\n",
        "$B = \\big( \\frac{\\partial}{\\partial B} \\big) ( {\\color{red}B}^\\top \\bf{X}^\\top \\bf{X} {\\color{blue}B}) = \\big(\\frac{\\partial}{\\partial B} \\big) ({\\color{blue}B}^\\top \\bf{X}^\\top \\bf{X}{\\color{red}B}) = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "$loss = (\\bf{y} - \\bf{XB})^\\top(\\bf{y} - \\bf{XB}) = \\bf{y}\\bf{y}^\\top - \\bf{y}^\\top\\bf{XB} - \\bf{B}^\\top\\bf{X}^\\top \\bf{y} + \\bf{B}^\\top \\bf{X}^\\top \\bf{XB}$\n",
        "\n",
        "$\\frac{\\partial}{\\partial B} loss = 0 - \\frac{\\partial}{\\partial B} \\bf{y}^\\top \\bf{XB} - \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{y} + \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{XB} = 0 - \\bf{X}^\\top \\bf{y} - \\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB} = - 2\\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB}$\n",
        "\n",
        "$\\therefore \\frac{\\partial}{\\partial B} loss = 0 \\leftrightarrow 2\\bf{X}^\\top \\bf{y} = 2\\bf{X}^\\top \\bf{XB}$\n",
        "\n",
        "$\\therefore \\hat{\\bf{B}} = (\\bf{X}^\\top \\bf{X})^{-1}\\bf{X}^\\top \\bf{y}$\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# 체인룰\n",
        "\n",
        "`-` 체인룰: 어려운 하나의 미분을 손쉬운 여러개의 미분으로 나누는 기법\n",
        "\n",
        "`-` 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다.\n",
        "\n",
        "-   ${\\bf X} \\to {\\bf X}{\\bf W} \\to {\\bf y} -{\\bf X}{\\bf W} \\to ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n",
        "\n",
        "`-` 위의 과정을 수식으로 정리해보면 아래와 같다.\n",
        "\n",
        "-   ${\\bf u}={\\bf X}{\\bf W}$, $\\quad {\\bf u}: n \\times 1$\n",
        "\n",
        "-   ${\\bf v} = {\\bf y}- {\\bf u},$ $\\quad {\\bf v}: n \\times 1$\n",
        "\n",
        "-   $loss={\\bf v}^\\top {\\bf v},$ $\\quad loss: 1 \\times 1$\n",
        "\n",
        "`-` 손실함수에 대한 미분은 아래와 같다.\n",
        "\n",
        "$$\\frac{\\partial }{\\partial {\\bf W}} loss = \\frac{\\partial }{\\partial {\\bf W}} {\\bf v}^\\top {\\bf v}$$\n",
        "\n",
        "(그런데 이걸 어떻게 계산함?)\n",
        "\n",
        "`-` 계산할 수 있는것들의 모음..\n",
        "\n",
        "-   $\\frac{\\partial}{\\partial {\\bf v}} loss = 2{\\bf v}$ $\\quad \\to$\n",
        "    (n,1) 벡터\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top = -{\\bf I}$\n",
        "    $\\quad \\to$ (n,n) 매트릭스\n",
        "\n",
        "-   $\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top = {\\bf X}^\\top$\n",
        "    $\\quad \\to$ (p,n) 매트릭스\n",
        "\n",
        "`-` 혹시.. 아래와 같이 쓸 수 있을까?\n",
        "\n",
        "$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) = \n",
        "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
        "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
        "\\frac{\\partial loss}{\\partial \\bf v}$$\n",
        "\n",
        "-   가능할것 같다. 뭐 기호야 정의하기 나름이니까!\n",
        "\n",
        "`-` 그렇다면 혹시 아래와 같이 쓸 수 있을까?\n",
        "\n",
        "$$\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
        "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
        "\\frac{\\partial loss}{\\partial \\bf v} = \\frac{\\partial loss }{\\partial\\bf W}=\\frac{\\partial }{\\partial \\bf W} loss$$\n",
        "\n",
        "-   이건 선을 넘는 것임.\n",
        "-   그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다.\n",
        "\n",
        "`-` 결국 정리하면 아래의 꼴이 되었다.\n",
        "\n",
        "$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
        "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) \n",
        "=\n",
        "\\frac{\\partial }{\\partial \\bf W}loss$$\n",
        "\n",
        "`-` 그렇다면?\n",
        "\n",
        "$$\\left({\\bf X}^\\top  \\right) \n",
        "\\left(-{\\bf I} \\right) \n",
        "\\left(2{\\bf v}\\right) \n",
        "= \n",
        "\\frac{\\partial }{\\partial \\bf W}loss$$\n",
        "\n",
        "그런데, ${\\bf v}={\\bf y}-{\\bf u}={\\bf y} -{\\bf X}{\\bf W}$ 이므로\n",
        "\n",
        "$$-2{\\bf X}^\\top\\left({\\bf y}-{\\bf X}{\\bf W}\\right) \n",
        "= \n",
        "\\frac{\\partial }{\\partial \\bf W}loss$$\n",
        "\n",
        "정리하면\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\bf W}loss = -2{\\bf X}^\\top{\\bf y}+2{\\bf X}^\\top {\\bf X}{\\bf W}$$\n",
        "\n",
        "## 예시: 2021 빅데이터분석 중간고사 문제 2-(b)\n",
        "\n",
        "`-` 미분계수를 계산하는 문제였음..\n",
        "\n",
        "-   <https://guebin.github.io/BDA2021/2021/11/09/mid.html>\n",
        "\n",
        "`-` 체인룰을 이용하여 미분계수를 계산하여 보자."
      ],
      "id": "ff4aa027-2b80-41f0-abff-b0dea22db1b8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "ones= torch.ones(5)\n",
        "x = torch.tensor([11.0,12.0,13.0,14.0,15.0])\n",
        "X = torch.vstack([ones,x]).T\n",
        "y = torch.tensor([17.7,18.5,21.2,23.6,24.2])"
      ],
      "id": "69dc749e-b596-4353-9d1b-4f376cd239e2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "W = torch.tensor([3.0,3.0]) "
      ],
      "id": "73fda8da-e858-47a7-9c19-68ab7e2f0cd8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642814471/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until"
          ]
        }
      ],
      "source": [
        "u = X@W \n",
        "v = y-u \n",
        "loss = v.T @ v "
      ],
      "id": "97bd4b74-a730-4ec1-a76b-751fe19cd568"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor(2212.1799)"
            ]
          }
        }
      ],
      "source": [
        "loss"
      ],
      "id": "97ff1f54-5059-414e-ac49-68618f3284cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` $\\frac{\\partial}{\\partial\\bf W}loss$ 의 계산\n",
        "\n",
        "$\\frac{\\partial }{\\partial \\bf W}loss = \\left({\\bf X}^\\top \\right) \\left(-{\\bf I} \\right) \\left(2{\\bf v}\\right)$"
      ],
      "id": "b147ae15-ecca-450f-aa15-d93c26005c39"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([ 209.6000, 2748.6001])"
            ]
          }
        }
      ],
      "source": [
        "X.T @ -torch.eye(5) @ (2*v) "
      ],
      "id": "b1700538-b4b0-41f4-909e-e010c4c42fd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 참고로 중간고사 답은"
      ],
      "id": "08a28216-93b2-43e8-bf05-83bfb2cc6798"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([ 41.9200, 549.7200])"
            ]
          }
        }
      ],
      "source": [
        "X.T @ -torch.eye(5)@ (2*v) / 5 "
      ],
      "id": "e6016230-fa9d-49fe-ab24-52a4bbc183f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "입니다.\n",
        "\n",
        "`-` 확인"
      ],
      "id": "44cdba53-d0b5-4848-b302-3abac9e17e70"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "_W = torch.tensor([3.0,3.0],requires_grad=True) "
      ],
      "id": "f6003451-0365-4cb4-8a6d-d05749e2f6f1"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss = (y-X@_W).T @ (y-X@_W)"
      ],
      "id": "bfdcdefd-f53c-43cd-a781-3ebe386eff3b"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss.backward()"
      ],
      "id": "579ab062-5062-4d4c-bc69-3e65be0c3034"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([ 209.6000, 2748.6001])"
            ]
          }
        }
      ],
      "source": [
        "_W.grad.data"
      ],
      "id": "3124d0ed-9b38-4521-a286-19e474404013"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` $\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}$ 임을 확인하라."
      ],
      "id": "1d203d84-a71c-42f4-8057-b42a73073806"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])"
            ]
          }
        }
      ],
      "source": [
        "v"
      ],
      "id": "f0c99dfc-d8d9-4667-99d8-85f2dd049260"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "_v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True)"
      ],
      "id": "36416d0c-642c-4a97-9996-bd43823aed17"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss = _v.T @ _v "
      ],
      "id": "7463c3b3-6156-40ea-8668-7d57923d084f"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "_loss.backward() "
      ],
      "id": "f9af246f-c445-4539-8b81-1cd76f6ef6bb"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([-36.6000, -41.0000, -41.6000, -42.8000, -47.6000]),\n",
              " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
            ]
          }
        }
      ],
      "source": [
        "_v.grad.data, v "
      ],
      "id": "a89d62dc-d4c7-4cb6-8ed7-ec72f3076446"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` $\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top$ 의 계산"
      ],
      "id": "9952c544-040a-48a2-9e66-c07c27ffbed8"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([36., 39., 42., 45., 48.], requires_grad=True)"
            ]
          }
        }
      ],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "_u"
      ],
      "id": "41878d75-2399-4e1e-9884-73fd35b4c988"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "_v = y - _u ### 이전의 _v와 또다른 임시 _v "
      ],
      "id": "629d401f-e176-456d-a892-aba0fad18911"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "(_v.T).backward()"
      ],
      "id": "fd503f1e-091d-486a-b9b3-b472277fdf91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음\n",
        "\n",
        "그런데\n",
        "$\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)$\n",
        "이므로\n",
        "\n",
        "조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다.\n",
        "\n",
        "1.  $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top$의 결과를 저장할\n",
        "    매트릭스를 만든다. 적당히 `A`라고 만들자.\n",
        "\n",
        "2.  `_u` 하나를 임시로 만든다. 그리고 $v_1$을 `_u`로 미분하고 그 결과를\n",
        "    `A`의 첫번째 칼럼에 기록한다.\n",
        "\n",
        "3.  `_u`를 또하나 임시로 만들고 $v_2$를 `_u`로 미분한뒤 그 결과를 `A`의\n",
        "    두번째 칼럼에 기록한다.\n",
        "\n",
        "4.  (1)-(2)와 같은 작업을 $v_5$까지 반복한다.\n",
        "\n",
        "***(0)을 수행***"
      ],
      "id": "7386d247-0b6c-4b7e-807a-a32c8d128ce3"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          }
        }
      ],
      "source": [
        "A = torch.zeros((5,5))\n",
        "A"
      ],
      "id": "6f3e7f9c-c22d-4a71-b185-64d3bab3318d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(1)을 수행***"
      ],
      "id": "cea9db9a-3a95-4afa-9baf-b42c5641a42f"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([36., 39., 42., 45., 48.]),\n",
              " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
            ]
          }
        }
      ],
      "source": [
        "u,v "
      ],
      "id": "c4afa843-9af7-4d00-b739-f706934933c7"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "v1 = (y-_u)[0]"
      ],
      "id": "bc1e08db-3c27-4fbf-b32d-f87ce6f2bd89"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이때 $v_1=g(f({\\bf u}))$와 같이 표현할 수 있다. 여기에서\n",
        "    $f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top$, 그리고\n",
        "    $g((v_1,\\dots,v_n)^\\top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을\n",
        "    수행하는 함수이고, $g$는 프로젝션 함수이다. 즉\n",
        "    $f:\\mathbb{R}^5 \\to \\mathbb{R}^5$인 함수이고,\n",
        "    $g:\\mathbb{R}^5 \\to \\mathbb{R}$인 함수이다.\n",
        "\n",
        "여기서 $v_1$은 꼬리표로서 selection 작성되어 있음"
      ],
      "id": "c5d66f8f-8a3c-45ba-8aca-d8d63998c6ce"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "v1.backward()"
      ],
      "id": "181939d0-a5de-453e-a576-d8f25698dc82"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([-1., -0., -0., -0., -0.])"
            ]
          }
        }
      ],
      "source": [
        "_u.grad.data"
      ],
      "id": "72319636-170a-4730-bee6-9988284f1117"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "A[:,0]= _u.grad.data"
      ],
      "id": "7237e87d-acea-4dd5-ba2d-29101714c02c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A의 첫번째 칼럼에 이것을 넣어주세요"
      ],
      "id": "9ff840d7-29a9-479e-b659-9164ca04f733"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[-1.,  0.,  0.,  0.,  0.],\n",
              "        [-0.,  0.,  0.,  0.,  0.],\n",
              "        [-0.,  0.,  0.,  0.,  0.],\n",
              "        [-0.,  0.,  0.,  0.,  0.],\n",
              "        [-0.,  0.,  0.,  0.,  0.]])"
            ]
          }
        }
      ],
      "source": [
        "A"
      ],
      "id": "e5cf043b-2eec-40d2-a2f9-c66a89868c53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(2)를 수행***"
      ],
      "id": "964851d3-b357-499d-82f5-dd115a8c6a1e"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "v2 = (y-_u)[1]"
      ],
      "id": "e4ce63e8-ad56-4bb2-943d-94e8585c3e05"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "v2.backward()"
      ],
      "id": "b69505a2-1fe1-4089-a61c-49da015d75cc"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([-0., -1., -0., -0., -0.])"
            ]
          }
        }
      ],
      "source": [
        "_u.grad.data"
      ],
      "id": "68042b18-2a87-4c20-9378-992a3ac0c6b4"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[-1., -0.,  0.,  0.,  0.],\n",
              "        [-0., -1.,  0.,  0.,  0.],\n",
              "        [-0., -0.,  0.,  0.,  0.],\n",
              "        [-0., -0.,  0.,  0.,  0.],\n",
              "        [-0., -0.,  0.,  0.,  0.]])"
            ]
          }
        }
      ],
      "source": [
        "A[:,1]= _u.grad.data\n",
        "A"
      ],
      "id": "06ec45c0-b578-4618-9965-eedc5a833e07"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***(3)을 수행*** // 그냥 (1)~(2)도 새로 수행하자."
      ],
      "id": "cacc6a6c-e751-4240-b3e7-2b4499d4a443"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5): \n",
        "    _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
        "    _v = (y-_u)[i]\n",
        "    _v.backward()\n",
        "    A[:,i]= _u.grad.data"
      ],
      "id": "6f2b31cf-b516-4778-913d-ae6b844d4091"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[-1., -0., -0., -0., -0.],\n",
              "        [-0., -1., -0., -0., -0.],\n",
              "        [-0., -0., -1., -0., -0.],\n",
              "        [-0., -0., -0., -1., -0.],\n",
              "        [-0., -0., -0., -0., -1.]])"
            ]
          }
        }
      ],
      "source": [
        "A"
      ],
      "id": "ddd53a8d-5749-41ce-bd55-14b845372ac7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이론적인 결과인 $-{\\bf I}$와 일치한다.\n",
        "\n",
        "`-` $\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top$의 계산\n",
        "\n",
        "$\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)$"
      ],
      "id": "2ca4f532-999a-479a-8153-6dba762c8062"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          }
        }
      ],
      "source": [
        "B = torch.zeros((2,5))\n",
        "B"
      ],
      "id": "00ab6d57-15e5-4795-9e3d-7096a634c35d"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([3., 3.])"
            ]
          }
        }
      ],
      "source": [
        "W"
      ],
      "id": "7d41741d-71d4-49ce-88d3-0c822829aa82"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([3., 3.], requires_grad=True)"
            ]
          }
        }
      ],
      "source": [
        "_W = torch.tensor([3., 3.],requires_grad=True)\n",
        "_W"
      ],
      "id": "b514297b-a61e-46cb-a69a-c958fecd60ac"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5): \n",
        "    _W = torch.tensor([3., 3.],requires_grad=True)\n",
        "    _u = (X@_W)[i]\n",
        "    _u.backward()\n",
        "    B[:,i]= _W.grad.data"
      ],
      "id": "d8ea85c9-2524-4ea3-9c26-4f3474f5482d"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
              "        [11., 12., 13., 14., 15.]])"
            ]
          }
        }
      ],
      "source": [
        "B # X의 트랜스포즈"
      ],
      "id": "3eb14f57-0152-4079-9581-c712da151855"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[ 1., 11.],\n",
              "        [ 1., 12.],\n",
              "        [ 1., 13.],\n",
              "        [ 1., 14.],\n",
              "        [ 1., 15.]])"
            ]
          }
        }
      ],
      "source": [
        "X"
      ],
      "id": "84338235-2a97-47bb-a3fb-abfb857d4aba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이론적인 결과와 일치\n",
        "\n",
        "## 잠깐 생각해보자..\n",
        "\n",
        "`-` 결국 위의 예제에 한정하여 임의의 ${\\bf \\hat{W}}$에 대한\n",
        "$\\frac{\\partial}{\\partial {\\bf \\hat W}}loss$는 아래와 같이 계산할 수\n",
        "있다.\n",
        "\n",
        "-   (단계1) $2{\\bf v}$를 계산하고\n",
        "-   (단계2) (단계1)의 결과 앞에 $-{\\bf I}$를 곱하고\n",
        "-   (단계3) (단계2)의 결과 앞에 ${\\bf X}^\\top$를 곱한다.\n",
        "\n",
        "`-` step1에서 ${\\bf v}$는 어떻게 알지?\n",
        "\n",
        "-   X $\\to$ u=X@W $\\to$ v = y-u\n",
        "\n",
        "-   그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것\n",
        "    아니었나?\n",
        "\n",
        "-   step1: yhat, step2: loss, step3: derivate, step4: update\n",
        "\n",
        "`-` **(중요)** step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을\n",
        "다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한\n",
        "방법을 통하여 이용하여 보자.\n",
        "\n",
        "### backprogation 알고리즘 모티브\n",
        "\n",
        "`-` 아래와 같이 함수의 변환을 아키텍처로 이해하자.\n",
        "(함수의입력=레이어의입력, 함수의출력=레이어의출력)\n",
        "\n",
        "-   ${\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n",
        "\n",
        "`-` 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다.\n",
        "(${\\bf X} \\to {\\bf \\hat y} \\to loss$가 아니라\n",
        "${\\bf W} \\to loss({\\bf W})$로 생각해보세요)\n",
        "\n",
        "-   ${\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss$\n",
        "\n",
        "`-` 그렇다면 아래와 같은 사실을 관찰할 수 있다.\n",
        "\n",
        "-   (단계1) $2{\\bf v}$는 function of ${\\bf v}$이고, ${\\bf v}$는 l3의\n",
        "    입력 (혹은 l2의 출력)\n",
        "-   (단계2) $-{\\bf I}$는 function of ${\\bf u}$이고, ${\\bf u}$는 l2의\n",
        "    입력 (혹은 l1의 출력)\n",
        "-   (단계3) 마찬가지의 논리로 ${\\bf X}^\\top$는 function of ${\\bf W}$로\n",
        "    해석할 수 있다.\n",
        "\n",
        "`-` 요약: $2{\\bf v},-{\\bf I}, {\\bf X}^\\top$와 같은 핵심적인 값들이 사실\n",
        "각 층의 입/출력 값들의 함수꼴로 표현가능하다. $\\to$ 각 층의 입/출력\n",
        "값들을 모두 기록하면 미분계산을 유리하게 할 수 있다.\n",
        "\n",
        "-   문득의문: 각 층의 입출력값 ${\\bf v}, {\\bf u}, {\\bf W}$로 부터\n",
        "    $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 만들어내는 방법을 모른다면\n",
        "    헛수고 아닌가?\n",
        "-   의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, …) 정도가\n",
        "    전부임. 따라서 변환규칙은 미리 계산할 수 있음.\n",
        "\n",
        "`-` 결국\n",
        "\n",
        "`(1)` 순전파를 하면서 입출력값을 모두 저장하고\n",
        "\n",
        "`(2)` 그에 대응하는 층별 미분계수값 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$\n",
        "를 구하고\n",
        "\n",
        "`(3)` 층별미분계수값을 다시 곱하면 (그러니까\n",
        "${\\bf X}^\\top (-{\\bf I}) 2{\\bf v}$ 를 계산) 된다.\n",
        "\n",
        "### backpropagation\n",
        "\n",
        "`(1)` 순전파를 계산하고 각 층별 입출력 값을 기록\n",
        "\n",
        "-   yhat = net(X)\n",
        "-   loss = loss_fn(yhat,y)\n",
        "\n",
        "`(2)` 역전파를 수행하여 손실함수의 미분값을 계산\n",
        "\n",
        "-   loss.backward()\n",
        "\n",
        "`-` 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU\n",
        "메모리..\n",
        "\n",
        "`-` 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용)\n",
        "\n",
        "***gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한\n",
        "코어숫자)***\n",
        "\n",
        "-   아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 //\n",
        "    `net.to(\"cuda:0\")`\n",
        "-   순전파 계산: ***중간 계산결과를 모두 GPU메모리에 저장*** (순전파\n",
        "    계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를\n",
        "    계산하기 위한 대비) // `net(X)`\n",
        "-   오차 및 손실함수 계산: `loss = loss_fn(yhat,y)`\n",
        "-   역전파 계산: ***순전파단계에서 저장된 계산결과를 활용***하여\n",
        "    손실함수의 미분값을 계산 // `loss.backward()`\n",
        "-   다음 순전파 계산: ***이전값은 삭제하고 새로운 중간계산결과를\n",
        "    GPU메모리에 올림***\n",
        "-   반복.\n",
        "\n",
        "## some comments\n",
        "\n",
        "`-` 역전파기법은 체인룰 + $\\alpha$ 이다. - 미분 계산을 하기 위함인데\n",
        "여기서 파라메터 업데이트 필요하지\n",
        "\n",
        "`-` 오차역전파기법이라는 용어를 쓰는 사람도 있다.\n",
        "\n",
        "`-` 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우\n",
        "순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. - 예)\n",
        "개/고양이 확인 등\n",
        "\n",
        "# 기울기소멸\n",
        "\n",
        "## 고요속의 외침\n",
        "\n",
        "`-` <https://www.youtube.com/watch?v=ouitOnaDtFY>\n",
        "\n",
        "`-` 중간에 한명이라도 잘못 말한다면..\n",
        "\n",
        "## 정의\n",
        "\n",
        "`-` In machine learning, the vanishing gradient problem is encountered\n",
        "when training artificial neural networks with gradient-based learning\n",
        "methods and backpropagation.\n",
        "\n",
        "## 이해\n",
        "\n",
        "`-` 당연한것 아닌가?\n",
        "\n",
        "-   그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의\n",
        "    기울기를 통하여 업데이트 하는 방식\n",
        "-   역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $\\alpha$).\n",
        "    구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을\n",
        "    각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다.\n",
        "-   0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인\n",
        "    기울기가 나올 것이라고 생각하는것 자체가 이상함)\n",
        "\n",
        "0 전달되면 업데이트 안 되잖아"
      ],
      "id": "b23c6ff0-10a5-43e6-afa7-dabe4b8be7bb"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np "
      ],
      "id": "52fbd89c-dc57-41c2-be75-31ff2d98e3c3"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([ 1.27649548, -1.3435985 , -1.98858349,  1.98266931,  0.74704939,\n",
              "       -0.4831162 ,  1.41216797, -1.20423568,  0.66637324,  1.18085266,\n",
              "        0.23356958, -0.39851281,  0.2357107 , -0.28592461, -0.96001359,\n",
              "       -1.6276445 , -0.75687257,  0.48770524,  0.02553662,  0.18478814,\n",
              "        1.28850714,  1.41398052, -0.33970936,  0.36829707,  1.85991256,\n",
              "        0.02243541, -0.68804507,  0.63659842, -1.13905311,  0.94093391,\n",
              "        1.58809026, -0.25106013, -0.14446307,  1.31747003,  1.52190566,\n",
              "       -0.44264824, -1.95722305, -0.77865942, -0.3350363 , -0.53638126,\n",
              "       -0.77254936, -1.22118632,  0.6137345 ,  0.89975951, -1.70293244,\n",
              "        1.42661365, -0.43175558, -1.30904545, -0.53912915,  1.51725173,\n",
              "       -1.83965849,  1.00143736, -0.67129779,  0.36061957, -1.68850939,\n",
              "       -0.4272909 , -0.34715325,  1.72387253, -0.98340508, -1.60825385,\n",
              "        1.64523373, -0.79036932,  1.82578785, -0.53592773, -0.61384056,\n",
              "        0.9689625 ,  1.27971335,  0.51555469, -0.53425795,  0.38883373,\n",
              "       -0.28595978, -1.93730647, -1.94581503, -0.48984819, -1.21831701,\n",
              "       -1.25965989,  1.79542393,  1.2637913 , -0.93178556, -0.61210568,\n",
              "        1.23775906, -1.80601708,  1.40265496, -0.59602715,  1.44638486,\n",
              "       -1.71721283,  1.58345756, -1.03992841,  1.10167726,  1.13332066,\n",
              "       -0.76344022, -0.7246539 ,  0.27256115, -1.95501872, -0.65922909,\n",
              "        0.78715854, -0.29077574, -0.45110518, -1.64836114,  1.91692815])"
            ]
          }
        }
      ],
      "source": [
        "grads = np.random.uniform(low=-2,high=2,size=100) \n",
        "grads"
      ],
      "id": "95ce5cfc-eed8-4acf-b927-12a929125508"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "-4.089052412862103e-11"
            ]
          }
        }
      ],
      "source": [
        "grads.prod()"
      ],
      "id": "92b87c0d-6c57-4f92-afed-27b622bf5c61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   기울기가 소멸함"
      ],
      "id": "2a7d19f3-dc6e-4c5a-bb8d-70d75d133c59"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "-1.6059047739389678e+16"
            ]
          }
        }
      ],
      "source": [
        "grads = np.random.uniform(low=-5,high=5,size=100) \n",
        "grads.prod()"
      ],
      "id": "b17f42bf-9be7-4b3c-a466-64bf8407bd42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   기울기가 폭발함."
      ],
      "id": "2bda7b80-b682-4677-be66-4bad3797e44c"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "13058.879478242436"
            ]
          }
        }
      ],
      "source": [
        "grads = np.random.uniform(low=-1,high=3.5,size=100) \n",
        "grads.prod()"
      ],
      "id": "11615aac-a8c2-435d-8c91-ad6b297155de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 도깨비: 기울기가 소멸하기도 하고 터지기도 한다.\n",
        "\n",
        "## [해결책](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (기울기 소멸에 대한 해결책)\n",
        "\n",
        "`-` Multi-level hierarchy\n",
        "\n",
        "-   여러층을 쪼개서 학습하자 $\\to$ 어떻게? 사전학습, 층벼학습\n",
        "-   기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임.\n",
        "-   결국 입력자료를 바꾼뒤에 학습하는 형태\n",
        "\n",
        "`-` Gradient clipping\n",
        "\n",
        "-   너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)\n",
        "\n",
        "`-` Faster hardware\n",
        "\n",
        "-   GPU를 중심으로 한 테크닉\n",
        "-   근본적인 문제해결책은 아니라는 힌튼의 비판\n",
        "-   CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도?\n",
        "\n",
        "`-` Residual Networks, LSTM\n",
        "\n",
        "-   아키텍처를 변경하는 방법\n",
        "\n",
        "`-` Other activation functions\n",
        "\n",
        "-   렐루의 개발\n",
        "\n",
        "-   렐루가 음수는 아예 0, 양수는 기울기 확실하니까\n",
        "\n",
        "`-` 배치정규화\n",
        "\n",
        "-   어쩌다보니 되는것.\n",
        "-   배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기\n",
        "    소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로\n",
        "    빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한\n",
        "    언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n",
        "-   심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에\n",
        "    언급했음)\n",
        "\n",
        "`-` **기울기를 안구하면 안되나?**\n",
        "\n",
        "-   베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $\\to$\n",
        "    GPU를 어떻게 쓰지? $\\to$ 느리다"
      ],
      "id": "8976ee92-ba34-4624-99b0-dc9b5883a4b1"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  }
}