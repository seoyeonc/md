{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a686c0fe-395c-4688-b365-b78f67ee0f50",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Extra-3: 딥러닝의 기초 (5)\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2022-12-21\"\n",
    "categories:\n",
    "  - 딥러닝의 기초\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753ccb9-d749-4845-b773-76111a04e94a",
   "metadata": {},
   "source": [
    "> 벡터미분, 역전파와 기울기 소멸 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d83e5-421f-4758-9f96-f3002e7ab6f4",
   "metadata": {},
   "source": [
    "# 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f85dc-d48c-47c5-bb35-9af172c46981",
   "metadata": {},
   "source": [
    "> <https://youtube.com/playlist?list=PLQqh36zP38-zwEYE2j1g3IGIRvJkv9QPf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db194c8c-d08c-4e7e-bda4-2fdf68964331",
   "metadata": {},
   "source": [
    "이 강의는 2021년 빅데이터분석의 강의노트 및 강의영상을 편집하여 만들었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064a093-286b-4c33-925b-a2f474f84e06",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217616da-3e8c-4391-b402-65b3f42d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a71754-5d08-4e2f-a22f-edacf7050bed",
   "metadata": {},
   "source": [
    "# 벡터미분 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c31496-5718-44cb-aca9-492554de2c1a",
   "metadata": {},
   "source": [
    "`-` 벡터미분에 대한 강의노트: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f58d2-307b-4290-a964-b44aafe0ad14",
   "metadata": {},
   "source": [
    "- <https://github.com/guebin/STML2022/blob/main/posts/II.%20DNN/supp.pdf>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fce0d-0317-4bd6-8dc4-88d8ddec2221",
   "metadata": {},
   "source": [
    "`-` 요약: 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f74ea-bb4f-49b8-9430-715440eca1e8",
   "metadata": {},
   "source": [
    "- $loss = ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\bf W} - {\\bf W}^\\top {\\bf X}^\\top {\\bf y} + {\\bf W}^\\top {\\bf X}^\\top {\\bf X} {\\bf W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcf8a5-5f63-41ef-b367-c3b4ad56449a",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial {\\bf W}}loss = -2{\\bf X}^\\top {\\bf y} +2 {\\bf X}^\\top {\\bf X} {\\bf W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c1569-cb25-49ab-9adb-03f2180e8015",
   "metadata": {},
   "source": [
    "# 체인룰 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ebd9a-90fd-4226-9be0-eefd7b250a14",
   "metadata": {},
   "source": [
    "`-` 체인룰: 어려운 하나의 미분을 손쉬운 여러개의 미분으로 나누는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325aed8d-2df0-4717-810c-3758d31e5451",
   "metadata": {},
   "source": [
    "`-` 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1eed13-087b-43a9-9c92-1b26ad33fcdd",
   "metadata": {},
   "source": [
    "- ${\\bf X} \\to {\\bf X}{\\bf W} \\to {\\bf y} -{\\bf X}{\\bf W} \\to ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef811c10-d520-4ed5-8cf7-40b0d1e73fc4",
   "metadata": {},
   "source": [
    "`-` 위의 과정을 수식으로 정리해보면 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce83c4-bdfd-44d8-936c-a01ad12c60c9",
   "metadata": {},
   "source": [
    "- ${\\bf u}={\\bf X}{\\bf W}$, $\\quad {\\bf u}: n \\times 1$ \n",
    "\n",
    "- ${\\bf v} = {\\bf y}- {\\bf u},$  $\\quad {\\bf v}: n \\times 1$\n",
    "\n",
    "- $loss={\\bf v}^\\top {\\bf v},$ $\\quad loss: 1 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409f372-580e-4a76-8a85-b491eaccffa0",
   "metadata": {},
   "source": [
    "`-` 손실함수에 대한 미분은 아래와 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab10e6-1864-4a1e-bb08-b4d37b8df867",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial }{\\partial {\\bf W}} loss = \\frac{\\partial }{\\partial {\\bf W}} {\\bf v}^\\top {\\bf v}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97564d8-1800-4afc-b788-bb15812b9b8c",
   "metadata": {},
   "source": [
    "(그런데 이걸 어떻게 계산함?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3922387-6ed2-4df8-b3f9-24d8a69ffc00",
   "metadata": {},
   "source": [
    "`-` 계산할 수 있는것들의 모음.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856e109-5965-45a3-8221-9e60bb513e99",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial}{\\partial {\\bf v}} loss = 2{\\bf v}$ $\\quad \\to$ (n,1) 벡터 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771abf8-3d42-484a-9fa0-dd982b7b9586",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top = -{\\bf I}$ $\\quad \\to$ (n,n) 매트릭스 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70bee73-64b4-4910-a534-9bf7d46bf9c6",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top = {\\bf X}^\\top$ $\\quad \\to$ (p,n) 매트릭스 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccae60-baa8-444d-8e24-41d04b72b9b2",
   "metadata": {},
   "source": [
    "`-` 혹시.. 아래와 같이 쓸 수 있을까?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823dc28-0355-411d-8f75-6ff95bb223a8",
   "metadata": {},
   "source": [
    "$$ \\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) = \n",
    "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
    "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
    "\\frac{\\partial loss}{\\partial \\bf v}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475ae41-4a23-4ae6-aef9-5716bdacb4ab",
   "metadata": {},
   "source": [
    "- 가능할것 같다. 뭐 기호야 정의하기 나름이니까!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb23444-959b-4897-a7cf-29e16a7bd264",
   "metadata": {},
   "source": [
    "`-` 그렇다면 혹시 아래와 같이 쓸 수 있을까? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd16996-e8d8-4a4b-b070-c8dd78fc89a7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n",
    "\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n",
    "\\frac{\\partial loss}{\\partial \\bf v} = \\frac{\\partial loss }{\\partial\\bf W}=\\frac{\\partial }{\\partial \\bf W} loss\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064edd0-c116-48d6-bb10-1c305d7c42a0",
   "metadata": {},
   "source": [
    "- 이건 선을 넘는 것임.\n",
    "- 그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156df62-cf34-4d0d-a531-8cdc88057fff",
   "metadata": {},
   "source": [
    "`-` 결국 정리하면 아래의 꼴이 되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac267e-20db-4908-82f1-407f6d226424",
   "metadata": {},
   "source": [
    "$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n",
    "\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f99e49-72fd-4bc8-a3c3-e11b99c13ea4",
   "metadata": {},
   "source": [
    "`-` 그렇다면? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9bdf9-ee86-4db9-adb1-f9684b58e3b6",
   "metadata": {},
   "source": [
    "$$\\left({\\bf X}^\\top  \\right) \n",
    "\\left(-{\\bf I} \\right) \n",
    "\\left(2{\\bf v}\\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3879dbf-dc36-4593-a3c9-fc42465e3c52",
   "metadata": {},
   "source": [
    "그런데, ${\\bf v}={\\bf y}-{\\bf u}={\\bf y} -{\\bf X}{\\bf W}$ 이므로 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffac2c-8fde-417f-80cb-1b51cd3ba90f",
   "metadata": {},
   "source": [
    "$$-2{\\bf X}^\\top\\left({\\bf y}-{\\bf X}{\\bf W}\\right) \n",
    "= \n",
    "\\frac{\\partial }{\\partial \\bf W}loss $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a7535-0f41-4332-bb6c-a335f400bae3",
   "metadata": {},
   "source": [
    "정리하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa24e6-54e7-4a28-858e-6fd081cfa217",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial }{\\partial \\bf W}loss = -2{\\bf X}^\\top{\\bf y}+2{\\bf X}^\\top {\\bf X}{\\bf W}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14241112-fc39-4173-88df-fd3512b28067",
   "metadata": {},
   "source": [
    "## 예시: 2021 빅데이터분석 중간고사 문제 2-(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba9719-39ac-4094-bc1f-d5d01d33cb62",
   "metadata": {},
   "source": [
    "`-` 미분계수를 계산하는 문제였음.. \n",
    "\n",
    "- <https://guebin.github.io/BDA2021/2021/11/09/mid.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48620971-3845-4ca4-b140-fda39f76c851",
   "metadata": {},
   "source": [
    "`-` 체인룰을 이용하여 미분계수를 계산하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dc749e-b596-4353-9d1b-4f376cd239e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones= torch.ones(5)\n",
    "x = torch.tensor([11.0,12.0,13.0,14.0,15.0])\n",
    "X = torch.vstack([ones,x]).T\n",
    "y = torch.tensor([17.7,18.5,21.2,23.6,24.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fda8da-e858-47a7-9c19-68ab7e2f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([3.0,3.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bd4b74-a730-4ec1-a76b-751fe19cd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = X@W \n",
    "v = y-u \n",
    "loss = v.T @ v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ff1f54-5059-414e-ac49-68618f3284cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2212.1799)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357230e7-3c00-44eb-a0b3-a89b7b3af781",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial}{\\partial\\bf W}loss $ 의 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1700538-b4b0-41f4-909e-e010c4c42fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 209.6000, 2748.5999])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T @ -torch.eye(5) @ (2*v) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22c15b-fa8e-4101-b088-e9963651d3e4",
   "metadata": {},
   "source": [
    "`-` 참고로 중간고사 답은 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6016230-fa9d-49fe-ab24-52a4bbc183f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41.9200, 549.7200])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T @ -torch.eye(5)@ (2*v) / 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98fa2f-a69f-4fc1-a8b7-d78fc4605ef6",
   "metadata": {},
   "source": [
    "입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca276697-20ed-42c9-aa79-e741810c7424",
   "metadata": {},
   "source": [
    "`-` 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6003451-0365-4cb4-8a6d-d05749e2f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_W = torch.tensor([3.0,3.0],requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfdcdefd-f53c-43cd-a781-3ebe386eff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss = (y-X@_W).T @ (y-X@_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "579ab062-5062-4d4c-bc69-3e65be0c3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3124d0ed-9b38-4521-a286-19e474404013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 209.6000, 2748.5999])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_W.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765a9f6-d7ef-473f-b301-ae4d4f2ef343",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}$ 임을 확인하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0c99dfc-d8d9-4667-99d8-85f2dd049260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36416d0c-642c-4a97-9996-bd43823aed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7463c3b3-6156-40ea-8668-7d57923d084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss = _v.T @ _v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9af246f-c445-4539-8b81-1cd76f6ef6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a89d62dc-d4c7-4cb6-8ed7-ec72f3076446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-36.6000, -41.0000, -41.6000, -42.8000, -47.6000]),\n",
       " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_v.grad.data, v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432322f-860b-4519-8dea-4f120a96035c",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top$ 의 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41878d75-2399-4e1e-9884-73fd35b4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 39., 42., 45., 48.], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "629d401f-e176-456d-a892-aba0fad18911",
   "metadata": {},
   "outputs": [],
   "source": [
    "_v = y - _u ### 이전의 _v와 또다른 임시 _v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd503f1e-091d-486a-b9b3-b472277fdf91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2147125/3227738085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38r40/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "(_v.T).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c7139-e2fe-435d-b1dd-348dd385a35c",
   "metadata": {},
   "source": [
    "- 사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b153c4-e8cb-45a7-bd7e-725716e6e8cf",
   "metadata": {},
   "source": [
    "그런데 $\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)$ 이므로\n",
    "\n",
    "조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea9353-0730-4cb2-8ed2-dd7f3454f2fa",
   "metadata": {},
   "source": [
    "(0) $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top$의 결과를 저장할 매트릭스를 만든다. 적당히 `A`라고 만들자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119090a0-f7a0-4c8f-b15f-81fd360105f9",
   "metadata": {},
   "source": [
    "(1) `_u` 하나를 임시로 만든다. 그리고 $v_1$을 `_u`로 미분하고 그 결과를 `A`의 첫번째 칼럼에 기록한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79d878-9a21-4ff0-856e-954b0726246a",
   "metadata": {},
   "source": [
    "(2) `_u`를 또하나 임시로 만들고 $v_2$를 `_u`로 미분한뒤 그 결과를 `A`의 두번째 칼럼에 기록한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc33a5a-ccf8-4a05-8f2a-8bb852c64370",
   "metadata": {},
   "source": [
    "(3) (1)-(2)와 같은 작업을 $v_5$까지 반복한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbde909-ef40-4556-b909-da1dabcb9c3d",
   "metadata": {},
   "source": [
    "***(0)을 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f3e7f9c-c22d-4a71-b185-64d3bab3318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.zeros((5,5))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad252f07-48d8-4893-8c34-a2a3e4cd0982",
   "metadata": {},
   "source": [
    "***(1)을 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4afa843-9af7-4d00-b739-f706934933c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([36., 39., 42., 45., 48.]),\n",
       " tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc1e08db-3c27-4fbf-b32d-f87ce6f2bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "v1 = (y-_u)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cd9ad-f9ba-4dad-836b-772d0b34849b",
   "metadata": {},
   "source": [
    "- 이때 $v_1=g(f({\\bf u}))$와 같이 표현할 수 있다. 여기에서 $f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top$, 그리고 $g((v_1,\\dots,v_n)^\\top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을 수행하는 함수이고, $g$는 프로젝션 함수이다. 즉 $f:\\mathbb{R}^5 \\to \\mathbb{R}^5$인 함수이고, $g:\\mathbb{R}^5 \\to \\mathbb{R}$인 함수이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "181939d0-a5de-453e-a576-d8f25698dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72319636-170a-4730-bee6-9988284f1117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7237e87d-acea-4dd5-ba2d-29101714c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[:,0]= _u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5cf043b-2eec-40d2-a2f9-c66a89868c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.],\n",
       "        [-0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796369ad-9462-4678-8e46-5ef1ae6bf298",
   "metadata": {},
   "source": [
    "***(2)를 수행***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4ce63e8-ad56-4bb2-943d-94e8585c3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "v2 = (y-_u)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b69505a2-1fe1-4089-a61c-49da015d75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68042b18-2a87-4c20-9378-992a3ac0c6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0., -1., -0., -0., -0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06ec45c0-b578-4618-9965-eedc5a833e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -0.,  0.,  0.,  0.],\n",
       "        [-0., -1.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.],\n",
       "        [-0., -0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:,1]= _u.grad.data\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed59da1-6ab0-4af7-8091-935bd4e98aad",
   "metadata": {},
   "source": [
    "***(3)을 수행*** // 그냥 (1)~(2)도 새로 수행하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f2b31cf-b516-4778-913d-ae6b844d4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n",
    "    _v = (y-_u)[i]\n",
    "    _v.backward()\n",
    "    A[:,i]= _u.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddd53a8d-5749-41ce-bd55-14b845372ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -0., -0., -0., -0.],\n",
       "        [-0., -1., -0., -0., -0.],\n",
       "        [-0., -0., -1., -0., -0.],\n",
       "        [-0., -0., -0., -1., -0.],\n",
       "        [-0., -0., -0., -0., -1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93166864-96cf-444d-99c6-66884e186d9e",
   "metadata": {},
   "source": [
    "- 이론적인 결과인 $-{\\bf I}$와 일치한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a31d0-b565-4c0e-aa80-6f0f1e687912",
   "metadata": {},
   "source": [
    "`-` $\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top$의 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92843d1e-7445-4ab5-b263-3d7b00440796",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00ab6d57-15e5-4795-9e3d-7096a634c35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.zeros((2,5))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d41741d-71d4-49ce-88d3-0c822829aa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b514297b-a61e-46cb-a69a-c958fecd60ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_W = torch.tensor([3., 3.],requires_grad=True)\n",
    "_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8ea85c9-2524-4ea3-9c26-4f3474f5482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    _W = torch.tensor([3., 3.],requires_grad=True)\n",
    "    _u = (X@_W)[i]\n",
    "    _u.backward()\n",
    "    B[:,i]= _W.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3eb14f57-0152-4079-9581-c712da151855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
       "        [11., 12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B # X의 트랜스포즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84338235-2a97-47bb-a3fb-abfb857d4aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 11.],\n",
       "        [ 1., 12.],\n",
       "        [ 1., 13.],\n",
       "        [ 1., 14.],\n",
       "        [ 1., 15.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f569721-df19-4efb-bfdd-5a84ccd61f46",
   "metadata": {},
   "source": [
    "- 이론적인 결과와 일치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf823d78-352d-4c63-88e8-a289f4ae031a",
   "metadata": {},
   "source": [
    "## 잠깐 생각해보자.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625d920-dd31-4699-b12c-4f24d7a517d4",
   "metadata": {},
   "source": [
    "`-` 결국 위의 예제에 한정하여 임의의 ${\\bf \\hat{W}}$에 대한 $\\frac{\\partial}{\\partial {\\bf \\hat W}}loss$는 아래와 같이 계산할 수 있다. \n",
    "\n",
    "- (단계1) $2{\\bf v}$를 계산하고 \n",
    "- (단계2) (단계1)의 결과 앞에 $-{\\bf I}$를 곱하고 \n",
    "- (단계3) (단계2)의 결과 앞에 ${\\bf X}^\\top$를 곱한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987eda3-e42e-4346-b79a-1adb399b99ad",
   "metadata": {},
   "source": [
    "`-` step1에서 ${\\bf v}$는 어떻게 알지? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f67a3b-fc42-44fb-bcb2-833b2615a5f8",
   "metadata": {},
   "source": [
    "- X $\\to$ u=X@W $\\to$ v = y-u "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd7ce6-ce54-4077-a522-ea06aceeb411",
   "metadata": {},
   "source": [
    "- 그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것 아니었나? \n",
    "- step1: yhat, step2: loss, step3: derivate, step4: update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f270b8-30e1-4590-b576-b100dc3a2fab",
   "metadata": {},
   "source": [
    "`-` **(중요)** step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을 다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한 방법을 통하여 이용하여 보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51dcf6-fdf5-436c-b262-67efa7fa92e1",
   "metadata": {},
   "source": [
    "### backprogation 알고리즘 모티브"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a8610-1131-48be-92cb-fdd6b5d6adea",
   "metadata": {},
   "source": [
    "`-` 아래와 같이 함수의 변환을 아키텍처로 이해하자. (함수의입력=레이어의입력, 함수의출력=레이어의출력) \n",
    "\n",
    "- ${\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91158c-60bf-4a15-9a37-d7df9caa1705",
   "metadata": {},
   "source": [
    "`-` 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다. (${\\bf X} \\to {\\bf \\hat y} \\to loss$가 아니라 ${\\bf W} \\to loss({\\bf W})$로 생각해보세요)\n",
    "\n",
    "- ${\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dad0ce-dde4-4d76-9bf0-b229fd7aa88f",
   "metadata": {},
   "source": [
    "`-` 그렇다면 아래와 같은 사실을 관찰할 수 있다. \n",
    "\n",
    "- (단계1) $2{\\bf v}$는 function of ${\\bf v}$이고, ${\\bf v}$는 l3의 입력 (혹은 l2의 출력) \n",
    "- (단계2) $-{\\bf I}$는 function of ${\\bf u}$이고, ${\\bf u}$는 l2의 입력 (혹은 l1의 출력) \n",
    "- (단계3) 마찬가지의 논리로 ${\\bf X}^\\top$는 function of ${\\bf W}$로 해석할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21dbed-6035-4108-80df-d745a03f0467",
   "metadata": {},
   "source": [
    "`-` 요약: $2{\\bf v},-{\\bf I}, {\\bf X}^\\top$와 같은 핵심적인 값들이 사실 각 층의 입/출력 값들의 함수꼴로 표현가능하다. $\\to$ 각 층의 입/출력 값들을 모두 기록하면 미분계산을 유리하게 할 수 있다. \n",
    "\n",
    "- 문득의문: 각 층의 입출력값 ${\\bf v}, {\\bf u}, {\\bf W}$로 부터 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 만들어내는 방법을 모른다면 헛수고 아닌가? \n",
    "- 의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, ...) 정도가 전부임. 따라서 변환규칙은 미리 계산할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b33de-b724-4406-9402-c0b9df5a31c6",
   "metadata": {},
   "source": [
    "`-` 결국 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a93bee-2825-419c-8b04-77cacb9b1b79",
   "metadata": {},
   "source": [
    "`(1)` 순전파를 하면서 입출력값을 모두 저장하고 \n",
    "\n",
    "`(2)` 그에 대응하는 층별 미분계수값 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 구하고 \n",
    "\n",
    "`(3)` 층별미분계수값을 다시 곱하면 (그러니까 ${\\bf X}^\\top (-{\\bf I}) 2{\\bf v}$ 를 계산) 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d73ca-fbcf-49c9-8e01-a87c708d8feb",
   "metadata": {},
   "source": [
    "### backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5de5bd-0665-4276-b1b5-96c6e5d63352",
   "metadata": {},
   "source": [
    "`(1)` 순전파를 계산하고 각 층별 입출력 값을 기록 \n",
    "\n",
    "- yhat = net(X) \n",
    "- loss = loss_fn(yhat,y) \n",
    "\n",
    "`(2)` 역전파를 수행하여 손실함수의 미분값을 계산 \n",
    "\n",
    "- loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9867bc-ba60-4886-9fb5-104bcef25698",
   "metadata": {},
   "source": [
    "`-` 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU 메모리.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04675f70-3717-4117-97cf-83389fddfb5b",
   "metadata": {},
   "source": [
    "`-` 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e422d85-f035-4e34-b04a-4dcf47cfdfb4",
   "metadata": {},
   "source": [
    "***gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한 코어숫자)***\n",
    "\n",
    "- 아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 // `net.to(\"cuda:0\")`\n",
    "- 순전파 계산: ***중간 계산결과를 모두 GPU메모리에 저장*** (순전파 계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를 계산하기 위한 대비) // `net(X)` \n",
    "- 오차 및 손실함수 계산: `loss = loss_fn(yhat,y)`\n",
    "- 역전파 계산: ***순전파단계에서 저장된 계산결과를 활용***하여 손실함수의 미분값을 계산 // `loss.backward()`\n",
    "- 다음 순전파 계산: ***이전값은 삭제하고 새로운 중간계산결과를 GPU메모리에 올림*** \n",
    "- 반복. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b847a2-ca7c-4905-b409-0f9756e72cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## some comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863bbef-414e-4627-be53-25bdc024a7e8",
   "metadata": {},
   "source": [
    "`-` 역전파기법은 체인룰 + $\\alpha$ 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245186f-0e59-4c01-bbe8-3a6817b328f1",
   "metadata": {},
   "source": [
    "`-` 오차역전파기법이라는 용어를 쓰는 사람도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36840ad-ab19-483f-9961-c61289e4153e",
   "metadata": {},
   "source": [
    "`-` 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우 순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ac8e8-a3ca-4bf7-a590-62640737e680",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 기울기소멸 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23be2f-7b46-4fe0-9d46-28f3aa7b725f",
   "metadata": {},
   "source": [
    "## 고요속의 외침 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff2c44-994b-4751-a441-21935bf659e9",
   "metadata": {},
   "source": [
    "`-` <https://www.youtube.com/watch?v=ouitOnaDtFY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d30fc-ab1a-4e1f-8ba2-c0048994112a",
   "metadata": {},
   "source": [
    "`-` 중간에 한명이라도 잘못 말한다면.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6e3a5-7070-4c27-b99b-3d37e4a8a5cd",
   "metadata": {},
   "source": [
    "## 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454d93b-a28e-4a96-95ab-7517da6f21ab",
   "metadata": {},
   "source": [
    "`-` In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fa379-79c8-45f8-8661-1e3f072f414a",
   "metadata": {},
   "source": [
    "## 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c6d5f-76ce-4c2c-8c2d-75ff2cd46b06",
   "metadata": {},
   "source": [
    "`-` 당연한것 아닌가? \n",
    "\n",
    "- 그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의 기울기를 통하여 업데이트 하는 방식 \n",
    "- 역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $\\alpha$). 구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을 각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다. \n",
    "- 0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인 기울기가 나올 것이라고 생각하는것 자체가 이상함) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fbd89c-dc57-41c2-be75-31ff2d98e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ce5cfc-eed8-4acf-b927-12a929125508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04807346, -1.29968172,  0.60383275, -1.11097504, -1.44903838,\n",
       "       -1.81770819, -0.82995838, -0.30307517, -1.62486386,  1.14841271,\n",
       "        0.05317044, -1.63595745, -1.27254039, -0.2793212 ,  1.4193291 ,\n",
       "       -1.87957006, -1.50413435, -1.6143583 , -1.97979251, -0.1319077 ,\n",
       "       -0.36277507,  0.24188449,  0.8205258 , -1.15353317, -0.94630341,\n",
       "        0.60335535, -1.4326661 ,  1.27171997,  1.51390194, -0.3285052 ,\n",
       "       -1.56389259, -1.55964141,  0.29636461, -0.74477874, -1.6119124 ,\n",
       "        0.01097634, -0.25255016,  1.80684873,  0.90766818, -0.25062987,\n",
       "       -0.74179267, -0.50494702,  1.89992546, -0.2911778 ,  0.3116119 ,\n",
       "        1.63679034,  0.86030588, -1.10851323, -1.13171181, -1.58949804,\n",
       "        0.82233945, -1.81269013, -0.31257892,  1.00176613, -0.49897359,\n",
       "       -1.05885742,  0.49664799, -1.02531506, -0.81051658, -1.34188376,\n",
       "       -1.98882591,  0.32634551,  1.39557143, -1.13150272, -1.5616825 ,\n",
       "       -0.0478864 , -1.34446087, -0.56475749, -1.90177931, -0.38516623,\n",
       "        0.75889489, -1.86710065, -1.85428312,  1.82248239,  1.04363342,\n",
       "        1.08128997, -0.36466711, -1.99204485,  0.81840524, -1.11993576,\n",
       "       -0.53666968, -0.09132125,  1.57303259,  1.82113986,  0.31948837,\n",
       "        0.34639304, -1.68151764,  1.0845407 , -0.01560537,  0.73393873,\n",
       "        0.85033511,  1.98350062, -0.45107395,  1.45704639,  1.48581033,\n",
       "       -1.57196761,  0.92182647,  0.78667051, -1.01047196, -0.13306075])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-2,high=2,size=100) \n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b87c0d-6c57-4f92-afed-27b622bf5c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.5832605040779705e-14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a9d9a-5ef2-4ce2-bf0b-c0148d947355",
   "metadata": {},
   "source": [
    "- 기울기가 소멸함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17f42bf-9be7-4b3c-a466-64bf8407bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2118230498890502e+26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-5,high=5,size=100) \n",
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5eef5-c513-4581-8daa-7b99d6209452",
   "metadata": {},
   "source": [
    "- 기울기가 폭발함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11615aac-a8c2-435d-8c91-ad6b297155de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.058085058858178e-07"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = np.random.uniform(low=-1,high=3.5,size=100) \n",
    "grads.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40eaf9-767f-4136-9535-20c852e3c2ff",
   "metadata": {},
   "source": [
    "`-` 도깨비: 기울기가 소멸하기도 하고 터지기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbad05d-18e6-4922-a999-981ab1f70395",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [해결책](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (기울기 소멸에 대한 해결책) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64f70-36c6-40d9-8555-58320f30c207",
   "metadata": {},
   "source": [
    "`-` Multi-level hierarchy\n",
    "\n",
    "- 여러층을 쪼개서 학습하자 $\\to$ 어떻게? 사전학습, 층벼학습 \n",
    "- 기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임. \n",
    "- 결국 입력자료를 바꾼뒤에 학습하는 형태 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40b0d1-1009-4cf6-a37e-d4aee3ebe714",
   "metadata": {},
   "source": [
    "`-` Gradient clipping\n",
    "\n",
    "- 너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be361a7-e05e-49d8-a00c-e4797b9566a3",
   "metadata": {},
   "source": [
    "`-` Faster hardware\n",
    "\n",
    "- GPU를 중심으로 한 테크닉 \n",
    "- 근본적인 문제해결책은 아니라는 힌튼의 비판 \n",
    "- CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73399fec-0511-4d68-9983-8290109fecba",
   "metadata": {},
   "source": [
    "`-` Residual Networks, LSTM \n",
    "\n",
    "- 아키텍처를 변경하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ce99-2d2c-44cb-8056-29d2d0b88327",
   "metadata": {},
   "source": [
    "`-` Other activation functions\n",
    "\n",
    "- 렐루의 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d784652-c6ce-4617-a60a-63f3d9eb1f7f",
   "metadata": {},
   "source": [
    "`-` 배치정규화 \n",
    "\n",
    "- 어쩌다보니 되는것. \n",
    "- 배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기 소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로 빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한 언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n",
    "- 심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에 언급했음) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415d412-42cb-4989-86ae-7c073885969a",
   "metadata": {},
   "source": [
    "`-` **기울기를 안구하면 안되나?**\n",
    "\n",
    "- 베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $\\to$ GPU를 어떻게 쓰지? $\\to$ 느리다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
