[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ";)",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 28, 2023\n\n\nSequence-to-Sequence, seq2seq\n\n\nSEOYEON CHOI\n\n\n\n\nMay 28, 2023\n\n\nAttention Mechanism\n\n\nSEOYEON CHOI\n\n\n\n\nMay 25, 2023\n\n\nSurvival Analysis Tutorial with R\n\n\nSEOYEON CHOI\n\n\n\n\nMay 24, 2023\n\n\nTransformers\n\n\nSEOYEON CHOI\n\n\n\n\nMay 20, 2023\n\n\nManifold learning Embedding\n\n\nSEOYEON CHOI\n\n\n\n\nMay 9, 2023\n\n\nExpectation Maximization(EM algorithm)\n\n\nSEOYEON CHOI\n\n\n\n\nMay 9, 2023\n\n\n10wk: 확률변수, 분포 (2)\n\n\nGUEBIN CHOI\n\n\n\n\nMay 4, 2023\n\n\nQuestions of PyTorch Geometric Temporal\n\n\nSEOYEON CHOI\n\n\n\n\nMay 2, 2023\n\n\n9wk: 확률변수, 분포 (1)\n\n\nGUEBIN CHOI\n\n\n\n\nApr 27, 2023\n\n\nClinical Trial Data and Survival Analysis\n\n\nSEOYEON CHOI\n\n\n\n\nApr 25, 2023\n\n\n8wk: 확률공간,분포,확률변수\n\n\nGUEBIN CHOI\n\n\n\n\nApr 20, 2023\n\n\nHazard ratio, Odds ratio\n\n\nSEOYEON CHOI\n\n\n\n\nApr 18, 2023\n\n\n7wk: 측도론 (3)\n\n\nGUEBIN CHOI\n\n\n\n\nApr 17, 2023\n\n\nAnything\n\n\nSEOYEON CHOI\n\n\n\n\nApr 17, 2023\n\n\nSurvival Analysis\n\n\nSEOYEON CHOI\n\n\n\n\nApr 11, 2023\n\n\n6wk: 측도론 (2)\n\n\nGUEBIN CHOI\n\n\n\n\nApr 9, 2023\n\n\nClustering\n\n\nSEOYEON CHOI\n\n\n\n\nApr 9, 2023\n\n\nLogistic Regression\n\n\nSEOYEON CHOI\n\n\n\n\nApr 4, 2023\n\n\n5wk: 측도론 (1)_2\n\n\nGUEBIN CHOI\n\n\n\n\nApr 4, 2023\n\n\n5wk: 측도론 (1)\n\n\nGUEBIN CHOI\n\n\n\n\nApr 2, 2023\n\n\nEnsemble and Random Forest\n\n\nSEOYEON CHOI\n\n\n\n\nMar 29, 2023\n\n\nLasso and Ridge\n\n\nSEOYEON CHOI\n\n\n\n\nMar 28, 2023\n\n\nPrincipal Component Analysis\n\n\nSEOYEON CHOI\n\n\n\n\nMar 28, 2023\n\n\n4wk: 측도론 (1)\n\n\nGUEBIN CHOI\n\n\n\n\nMar 28, 2023\n\n\n4wk: 측도론 intro (4)\n\n\nGUEBIN CHOI\n\n\n\n\nMar 26, 2023\n\n\nMap\n\n\nSEOYEON CHOI\n\n\n\n\nMar 25, 2023\n\n\nSupport Vector Machine\n\n\nSEOYEON CHOI\n\n\n\n\nMar 23, 2023\n\n\nMachine Learning basic\n\n\nSEOYEON CHOI\n\n\n\n\nMar 23, 2023\n\n\nTree\n\n\nSEOYEON CHOI\n\n\n\n\nMar 21, 2023\n\n\n3주차: 측도론\n\n\nGUEBIN CHOI\n\n\n\n\nMar 19, 2023\n\n\nQueue\n\n\nSEOYEON CHOI\n\n\n\n\nMar 14, 2023\n\n\n2주차: 측도론\n\n\nGUEBIN CHOI\n\n\n\n\nMar 7, 2023\n\n\nAdvanced Probability Theory\n\n\nSEOYEON CHOI\n\n\n\n\nMar 7, 2023\n\n\n1주차: 측도론\n\n\nSEOYEON CHOI\n\n\n\n\nMar 5, 2023\n\n\nStack\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nAdvanced Regression Analysis GT\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nTheoritical Statistics GT\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nTheoritical Statistics Final term 6 Explanation\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 23, 2023\n\n\nAdvanced Regression Analysis Final Term\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 22, 2023\n\n\nAdvanced Regression Analysis Mid Term\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 12, 2023\n\n\nArrayList & LinkedList\n\n\nSEOYEON CHOI\n\n\n\n\nJan 30, 2023\n\n\n코딩 테스트 공부(Done)\n\n\nSEOYEON CHOI\n\n\n\n\nJan 26, 2023\n\n\nTheoritical Statistics Final term\n\n\nSEOYEON CHOI\n\n\n\n\nJan 23, 2023\n\n\n두 큐 합 같게 만들기(Done)\n\n\nSEOYEON CHOI\n\n\n\n\nJan 21, 2023\n\n\n성격 유형 검사하기(Done)\n\n\nSEOYEON CHOI\n\n\n\n\nJan 20, 2023\n\n\nTheoritical Statistics HW9\n\n\nSEOYEON CHOI\n\n\n\n\nJan 18, 2023\n\n\nTheoritical Statistics HW8\n\n\nSEOYEON CHOI\n\n\n\n\nJan 18, 2023\n\n\nTheoritical Statistics HW7\n\n\nSEOYEON CHOI\n\n\n\n\nJan 15, 2023\n\n\nAlgorithm\n\n\nSEOYEON CHOI\n\n\n\n\nJan 15, 2023\n\n\n내장함수\n\n\nSEOYEON CHOI\n\n\n\n\nJan 15, 2023\n\n\nTheoritical Statistics Mid term\n\n\nSEOYEON CHOI\n\n\n\n\nJan 14, 2023\n\n\nTheoritical Statistics HW6\n\n\nSEOYEON CHOI\n\n\n\n\nJan 12, 2023\n\n\nTheoritical Statistics HW5\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nTheoritical Statistics HW4\n\n\nSEOYEON CHOI\n\n\n\n\nJan 8, 2023\n\n\nTheoritical Statistics HW3\n\n\nSEOYEON CHOI\n\n\n\n\nJan 6, 2023\n\n\nTheoritical Statistics HW2\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nTheoritical Statistics HW1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 1, 2023\n\n\nChapter 03 Greedy\n\n\nSEOYEON CHOI\n\n\n\n\nDec 31, 2022\n\n\n확률변수와 확률분포\n\n\nSEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nCoding Test\n\n\nSEOYEON CHOI\n\n\n\n\nDec 27, 2022\n\n\nTheoritical Statistics\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nExtra-2: 생성모형(GAN)\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nExtra-1: 추천시스템\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nExtra-3: 딥러닝의 기초 (5)\n\n\nSEOYEON CHOI\n\n\n\n\nDec 13, 2022\n\n\nFinalterm\n\n\nSEOYEON CHOI\n\n\n\n\nDec 11, 2022\n\n\n고급회귀분석 CH10\n\n\nSEOYEON CHOI\n\n\n\n\nDec 8, 2022\n\n\nRegression HW 4\n\n\nSEOYEON CHOI\n\n\n\n\nDec 8, 2022\n\n\nRNN (13주차)\n\n\nSEOYEON CHOI\n\n\n\n\nDec 8, 2022\n\n\nstudy\n\n\nSEOYEON CHOI\n\n\n\n\nDec 7, 2022\n\n\nA1: 깊은복사와 얕은복사 (12주차)\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\n고급회귀분석 실습 CH11\n\n\nSEOYEON CHOI\n\n\n\n\nNov 30, 2022\n\n\nRNN (12주차)\n\n\nSEOYEON CHOI\n\n\n\n\nNov 29, 2022\n\n\nDeep Learning final example\n\n\nSEOYEON CHOI\n\n\n\n\nNov 28, 2022\n\n\n고급회귀분석 실습 CH13\n\n\nSEOYEON CHOI\n\n\n\n\nNov 21, 2022\n\n\nRegression HW 3\n\n\nSEOYEON CHOI\n\n\n\n\nNov 21, 2022\n\n\n고급회귀분석 실습 CH10\n\n\nSEOYEON CHOI\n\n\n\n\nNov 21, 2022\n\n\nRNN (11주차)\n\n\nSEOYEON CHOI\n\n\n\n\nNov 14, 2022\n\n\n고급회귀분석 실습 CH06, CH07\n\n\nSEOYEON CHOI\n\n\n\n\nNov 9, 2022\n\n\nRNN (10주차)\n\n\nSEOYEON CHOI\n\n\n\n\nNov 2, 2022\n\n\nMidterm\n\n\nSEOYEON CHOI\n\n\n\n\nNov 2, 2022\n\n\nRNN (9주차)\n\n\nSEOYEON CHOI\n\n\n\n\nOct 26, 2022\n\n\nCNN (8주차) 2\n\n\nSEOYEON CHOI\n\n\n\n\nOct 26, 2022\n\n\nCNN (8주차) 1\n\n\nSEOYEON CHOI\n\n\n\n\nOct 23, 2022\n\n\nRegression HW 2\n\n\nSEOYEON CHOI\n\n\n\n\nOct 19, 2022\n\n\nCNN (7주차)\n\n\nSEOYEON CHOI\n\n\n\n\nOct 12, 2022\n\n\nDNN (6주차)\n\n\nSEOYEON CHOI\n\n\n\n\nOct 5, 2022\n\n\nHomework\n\n\nSEOYEON CHOI\n\n\n\n\nOct 5, 2022\n\n\nDNN (5주차)\n\n\nSEOYEON CHOI\n\n\n\n\nSep 29, 2022\n\n\nDNN (4주차)\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\nRegression HW 1\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\n고급회귀분석 실습 CH03, CH04\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\nDNN (3주차)\n\n\nSEOYEON CHOI\n\n\n\n\nSep 19, 2022\n\n\nAssignment 1\n\n\nSEOYEON CHOI\n\n\n\n\nSep 14, 2022\n\n\nDNN (2주차)\n\n\nSEOYEON CHOI\n\n\n\n\nSep 7, 2022\n\n\nIntro\n\n\nSEOYEON CHOI\n\n\n\n\nSep 7, 2022\n\n\nDNN (1주차)\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nAdvanced Regression Analysis\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nSpecial Topics in Machine Learning\n\n\nSEOYEON CHOI\n\n\n\n\nMar 31, 2021\n\n\nRidge Regression\n\n\nGUEBINCHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ml_basic/index.html",
    "href": "posts/ml_basic/index.html",
    "title": "Machine Learning basic",
    "section": "",
    "text": "Let’s study Machine Learning from begining."
  },
  {
    "objectID": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html",
    "href": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html",
    "title": "Manifold learning Embedding",
    "section": "",
    "text": "Expectation Maximization\n참고: LocallyLinearEmbedding, t-SNE, 머신러닝 도감, sklean"
  },
  {
    "objectID": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html#algorithm",
    "href": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html#algorithm",
    "title": "Manifold learning Embedding",
    "section": "Algorithm",
    "text": "Algorithm\n\n데이터 포인트 \\(x_i\\) 근처에 있는 점(\\(k\\)개)을 찾는다.\n근처에 있는 \\(K\\)개 점의 선형 결합으로 \\(x_i\\)를 재구성하는 가중치 \\(W_{ij}\\)를 계산한다.\n가중치 \\(W_{ij}\\)로 낮은 차원(\\(d\\)차원)의 \\(y_i\\)를 계산한다.\n\n\nimport sklearn.datasets # 이제 sklearn에서 samples_generator없음\nfrom sklearn.manifold import LocallyLinearEmbedding\n\n\ndata, color = sklearn.datasets.make_swiss_roll(n_samples=1500)\n\n\ndata.shape\n\n(1500, 3)\n\n\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\nfig.add_axes(ax)\nax.scatter(\n    data[:, 0], data[:, 1], data[:, 2], c=color, s=50, alpha=0.8\n)\nax.set_title(\"Swiss Roll in Ambient Space\")\nax.view_init(azim=-66, elev=12)\n_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n\n\n\n\n\nn_neighbors = 12 # 근처에 있는 점 개수\n\n\nn_components = 2 # 차원 축소 후 차원 수\n\n\n# 국소 선형 임베딩 모델 설정\nmodel = LocallyLinearEmbedding(n_neighbors = n_neighbors,\n                               n_components = n_components)\n\n\nmodel.fit(data) # 학습\n\nLocallyLinearEmbedding(n_neighbors=12)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LocallyLinearEmbeddingLocallyLinearEmbedding(n_neighbors=12)\n\n\n\nprint(model.transform(data)) # 차원 축소한 데이터 출력\n\n[[ 0.010313   -0.02643186]\n [-0.01607846 -0.01272895]\n [ 0.00912732  0.02124591]\n ...\n [ 0.03685466 -0.08256278]\n [ 0.00095445  0.00296153]\n [-0.00527724 -0.00157992]]\n\n\n\nlle = model.transform(data)\n\n\nmodel_5 = LocallyLinearEmbedding(n_neighbors = 5,\n                               n_components = n_components)\n\n\nmodel_5.fit(data) # 학습\n\nLocallyLinearEmbedding()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LocallyLinearEmbeddingLocallyLinearEmbedding()\n\n\n\nlle_5 = model_5.transform(data)\n\n\nmodel_50 = LocallyLinearEmbedding(n_neighbors = 50,\n                               n_components = n_components)\n\n\nmodel_50.fit(data) # 학습\n\nLocallyLinearEmbedding(n_neighbors=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LocallyLinearEmbeddingLocallyLinearEmbedding(n_neighbors=50)\n\n\n\nlle_50 = model_50.transform(data)\n\n\nfig, ax = plt.subplots(figsize=(15, 12), nrows=3)\nax[0].scatter(lle[:, 0], lle[:, 1], c=color, label = 'neighbors=12')\nax[0].legend(fontsize=15)\nax[0].set_title(\"LLE Embedding of Swiss Roll\")\nax[1].scatter(lle_5[:, 0], lle_5[:, 1], c=color, label = 'neighbors=5')\nax[1].legend(fontsize=15)\nax[2].scatter(lle_50[:, 0], lle_50[:, 1], c=color, label = 'neighbors=50')\nax[2].legend(fontsize=15)\n\n<matplotlib.legend.Legend at 0x7f8953936e20>\n\n\n\n\n\n\n근처에 있는 점이 5개라면 숨어 있는 낮은 차원의 데이터 구조를 꺼낼 수 없으므로 차우너 축소 후 점들이 좁은 범위에 모여 있다. 따라서 넓은 범위의 데이터 정보를 알 수 없다.\n근처에 있는 점이 50개라면 색깔이 다른 점이 근처에 있다. 근처에 있는 점 개수가 너무 많아서 일부 범위의 데이터 구조를 파악할 수 없다."
  },
  {
    "objectID": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html#algorithm-1",
    "href": "posts/ml_basic/2023-05-20-Manifold learning Embedding.html#algorithm-1",
    "title": "Manifold learning Embedding",
    "section": "Algorithm",
    "text": "Algorithm\n\n모든 \\(i,j\\) 쌍에 관한 \\(x_i, x_j\\)의 유사도를 가우스 분포를 이용한 유사도로 나타낸다.\n\\(x_i\\)와 같은 개수의 점 \\(y_i\\)를 낮은 차원 공간에 무작위로 배치하고, 모든 \\(i,j\\)쌍에 관한 \\(y_i,y_j\\)의 유사도를 t분포를 이용해 나타낸다.\n1과 2에서 정의한 유사도 분포가 가능하면 같도록 데이터 포인트 \\(y_i\\)를 갱신한다.\n수렴 조건까지 과정 3을 반복한다.\n\n\nt분포는 헤비테일분포1이므로 높은 차원 공간에서는 중심에서 먼 부분의 비중이 높다.\n따라서 일부분의 정보를 유지하기 어려워 4차원 이상의 공간은 차원 축소가 제대로 되지 않을 수 있다.\n\n\nimport sklearn.datasets\nfrom sklearn.manifold import TSNE\n\n\n# 필기체 숫자 이미지 데이터세트 불러오기\ndata = sklearn.datasets.load_digits()\n\n\ndata.keys()\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n\n\n\ndata['data']\n\narray([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n       ...,\n       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n       [ 0.,  0., 10., ..., 12.,  1.,  0.]])\n\n\n\nnp.array(data['data']).shape\n\n(1797, 64)\n\n\n\n# 2차원으로 차원 축소\nn_components = 2\n\n\n# t-분포 확률적 임베딩 모델 생성\nmodel = TSNE(n_components = n_components)\n\n\n# 특징으로 학습한 2차원 공간 값 출력\nprint(model.fit_transform(data.data))\n\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n[[ -2.8487089  53.309692 ]\n [ 10.247521  -12.026601 ]\n [-14.381865  -19.028639 ]\n ...\n [ -6.171889  -11.055643 ]\n [-21.39586    15.081427 ]\n [-14.349473   -7.253504 ]]\n\n\n\ntsne = model.fit_transform(data.data)\n\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n\nfig, ax = plt.subplots(figsize=(15, 12), nrows=1)\nax.scatter(tsne[:, 0], tsne[:, 1],c=data.target,label = 'n_components=2')\nax.legend(fontsize=15)\nax.set_title(\"t-SNE Embedding of the UCI ML hand-written digits\")\n\nText(0.5, 1.0, 't-SNE Embedding of the UCI ML hand-written digits')"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html",
    "href": "posts/ml_basic/2023-04-09-Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering\nReference: Pattern\n경성 군집화 hard clustering = 한 셈플이 하나의 군집에 속하도록 강제하는 방식\n연성 군집화 soft clustering = 샘플마다 군집에 속하는 정도를 다르게 할 수 있음"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#최적화-문제로-해석",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#최적화-문제로-해석",
    "title": "Clustering",
    "section": "최적화 문제로 해석",
    "text": "최적화 문제로 해석\nk평균은 직관에 기초한 휴리스틱한 알고리즘으로 보이는데, 이면에는 이론적인 토대를 갖추고 있다.\nk평균의 목적함수\n\\(J(Z,A) = \\sum^n_{i=1} \\sum^k_{j=1} a_{ji} dist(x_i, z_j)\\)\nZ는 군집 중심으로 A는 샘플의 배정 정보를 나타내는 k*n 행렬이다. i번째 샘플이 j번째 군집에 배정되었다면 \\(a_{ji}\\)는 1이고, 그렇지 않으면 0이다.\nk-평균은 최적화 문제를 푸는 알고리즘으로 볼 수 있다.\n\n루프를 반복하면서 목적함수의 값이 작아지는 방향으로 해를 갱신한다.\n어떤 초기 군집 중심을 가지고 출발하더라도 수렴한다는 것은 증명되었으나\n초기 군집 중심이 달라지면 최종 결과가 달라지는 문제가 있다."
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#em-기초",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#em-기초",
    "title": "Clustering",
    "section": "EM 기초",
    "text": "EM 기초\nZ는 입출력단계에서 보이지 않는 은닉변수 latent variable\nEM 알고리즘 Expectation Maximazation algorithm\nE단계\n\n은닉변수를 추정하는 단계\n\nM단계\n\n매개변수를 추정하는 단계"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#친밀도-전파-알고리즘",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#친밀도-전파-알고리즘",
    "title": "Clustering",
    "section": "친밀도 전파 알고리즘",
    "text": "친밀도 전파 알고리즘\n소셜네트워크 자료에서 사용?"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#커널-클러스터링",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#커널-클러스터링",
    "title": "Clustering",
    "section": "커널 클러스터링",
    "text": "커널 클러스터링\n유클리드 거리로 표현되는 거리를 내적으로 표현해서 커널로 보내어 비선형으로 분리한 클러스터링 결과 얻기\n초기값의 영향을 많이 받아 결과가 바뀌기도"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#스펙트럴-클러스터링",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#스펙트럴-클러스터링",
    "title": "Clustering",
    "section": "스펙트럴 클러스터링",
    "text": "스펙트럴 클러스터링\n차원축소를 통래 초기값에 대한 의존성을 줄이려는 시도"
  },
  {
    "objectID": "posts/ml_basic/2023-04-09-Clustering.html#파라미터의-자동-결정",
    "href": "posts/ml_basic/2023-04-09-Clustering.html#파라미터의-자동-결정",
    "title": "Clustering",
    "section": "파라미터의 자동 결정",
    "text": "파라미터의 자동 결정\n직접 초기값 입력하면 거기에 의존하여 결과가 바뀌기도 하니 객관적으로 결정되게끔 파라미터 선택되게 하는 기법\n제곱 손실 상호정보량mutual information 사용 상호정보량보다 이상값에 민감하게 반응하지 않는디."
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Linear Regression, Logistic Regression.html",
    "href": "posts/ml_basic/2023-03-28-Linear Regression, Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Logistic Regression\n\n로지스틱 함수는 샘플이 어느 카테고리에 속할 확률을 추정하여 분류하는 회귀 알고리즘이다.\nRefernece: 핸즈 온 머신러닝\n\n로지스틱 회귀"
  },
  {
    "objectID": "posts/ml_basic/2023-05-09-EM_algorithm.html",
    "href": "posts/ml_basic/2023-05-09-EM_algorithm.html",
    "title": "Expectation Maximization(EM algorithm)",
    "section": "",
    "text": "Expectation Maximization"
  },
  {
    "objectID": "posts/ml_basic/2023-05-09-EM_algorithm.html#어떤-동전인지-알-때-expactation",
    "href": "posts/ml_basic/2023-05-09-EM_algorithm.html#어떤-동전인지-알-때-expactation",
    "title": "Expectation Maximization(EM algorithm)",
    "section": "어떤 동전인지 알 때 Expactation",
    "text": "어떤 동전인지 알 때 Expactation\n어떤 동전에서 어떤 면이 나올 기댓값을 직접 구할 수 있다.\n\n1: 앞 면\n2: 뒷 면\n\nA 동전\n\nN = 80\noutcomes = []\nfor i in range(N):\n    outcome = random.choice([1, 2])\n    outcomes.append(outcome)\nnum_of_ones = outcomes.count(1)\nprobability_of_one = num_of_ones / N\nprint(\"Number of ones:\", num_of_ones)\nprint(\"Probability of one:\", probability_of_one)\n\nNumber of ones: 37\nProbability of one: 0.4625\n\n\nB 동전\n\nN = 80\noutcomes = []\nfor i in range(N):\n    outcome = random.choice([1, 2])\n    outcomes.append(outcome)\nnum_of_ones = outcomes.count(1)\nprobability_of_one = num_of_ones / N\nprint(\"Number of ones:\", num_of_ones)\nprint(\"Probability of one:\", probability_of_one)\n\nNumber of ones: 45\nProbability of one: 0.5625"
  },
  {
    "objectID": "posts/ml_basic/2023-05-09-EM_algorithm.html#어떤-동전인지-모를-때-expactation",
    "href": "posts/ml_basic/2023-05-09-EM_algorithm.html#어떤-동전인지-모를-때-expactation",
    "title": "Expectation Maximization(EM algorithm)",
    "section": "어떤 동전인지 모를 때 Expactation",
    "text": "어떤 동전인지 모를 때 Expactation\n값이 어떤 동전에서 나오는지 모르는 경우\n\n10번 시도한 1 sequence: 앞/뒤/앞/뒤/뒤/앞/뒤/앞/앞/뒤\n사용되는 동전의 수 = hidden variable/latent variable\n\n임의의 초기값(랜덤 부여 가능)\n\n\\(\\theta_A = 0.4\\)\n\\(\\theta_B = 0.3\\)\n\n\nE step\n1 sequence에서 Hidden variable의 responsibility를 구한다.\nA 동전\n\n(0.4)**5*(0.6)**5\n\n0.0007962624\n\n\nB 동전\n\n(0.3)**5*(0.7)**5\n\n0.00040841009999999976\n\n\n1 sequence에서 A 동전을 사용했을 비율(responsibility)\n\nround(((0.4)**5*(0.6)**5)/((0.4)**5*(0.6)**5 + (0.3)**5*(0.7)**5),2)\n\n0.66\n\n\n1 sequence에서 B 동전을 사용했을 비율(responsibility)\n\nround(((0.3)**5*(0.7)**5)/((0.4)**5*(0.6)**5 + (0.3)**5*(0.7)**5),2)\n\n0.34\n\n\n\n\nM step\n1 sequence에서 A 동전 앞면\n\nround((0.66)*5,2)\n\n3.3\n\n\n1 sequence에서 A 동전 뒷면\n\nround((0.44)*5,2)\n\n2.2\n\n\n1 sequence에서 B 동전 앞면\n\nround((0.34)*5,2)\n\n1.7\n\n\n1 sequence에서 B 동전 뒷면\n\nround((0.66)*5,2)\n\n3.3\n\n\n\\(\\dots\\)\n이런 식으로 모든 sequence의 동전의 앞 뒷명면의 reponsibility를 사용하여 probability를 계산한다.\n\n\nUpdate\n\\(\\hat{\\theta}^{(1)}_A \\sim \\frac{14}{14+16} \\sim 0.47\\)\n\\(\\hat{\\theta}^{(1)}_B \\sim \\frac{14}{14+16} \\sim 0.38\\)\n\n\n…E/M step and Update Repeat\nLocal maximum으로 답 찾을 수 있음."
  },
  {
    "objectID": "posts/ml_basic/2023-03-31-Ridge Regression_note3_0331.html",
    "href": "posts/ml_basic/2023-03-31-Ridge Regression_note3_0331.html",
    "title": "Ridge Regression",
    "section": "",
    "text": "Lasso and Ridge\n\n\n능형회귀\n\n다중공선성이 \\(\\hat{\\beta}\\)에 대한 분산과 공분산을 모두 크게한다.\nref: Montgomery, D. C., Peck, E. A., & Vining, G. G. (2021). Introduction to linear regression analysis. John Wiley & Sons.\n아래와 같이 2개의 regressor가 존재하는 모형을 생각하자.\n\\(y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2 x_{i2}+\\epsilon_i, \\quad \\epsilon_i ~ \\sim iid N(0,\\sigma^2)\\)\n편의상 아래를 가정하자.\n\\(s_y^2=\\sum_{i=1}^{n}(y_i-\\bar{y})^2=1\\)\n\\(s_1^2=\\sum_{i=1}^{n}(x_{i1}-\\bar{x}_1)^2=1\\)\n\\(s_2^2=\\sum_{i=1}^{n}(x_{i2}-\\bar{x}_2)^2=1\\)\n최소제곱추정법으로 \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\)를 구하면 아래와 같다.\n\\(\\hat{\\beta_1}=\\frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^2}\\).\n\\(\\hat{\\beta_2}=\\frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^2}\\).\n단,\n\\(r_{1y}=\\frac{s_{1y}}{\\sqrt{s_1^2s_y^2}}=s_{1y}\\), \\(\\quad s_{1y}=\\sum_{i=1}^{n}(y_i-\\bar{y})(x_{i1}-\\bar{x}_1)\\)\n\\(r_{2y}=\\frac{s_{2y}}{\\sqrt{s_2^2s_y^2}}=s_{2y}\\), \\(\\quad s_{2y}=\\sum_{i=1}^{n}(y_i-\\bar{y})(x_{i2}-\\bar{x}_2)\\)\n\\(r_{12}=\\frac{s_{12}}{\\sqrt{s_1^2s_2^2}}=s_{12}\\). \\(\\quad s_{12}=\\sum_{i=1}^{n}(x_{i1}-\\bar{x}_1)(x_{i2}-\\bar{x}_2)\\)\n(관찰1)\n만약에 \\(x_1\\)과 \\(x_2\\)사이에 강한 선형관계가 있다면 \\(r_{12}\\approx 1\\) or \\(r_{12}\\approx -1\\)\n\\(\\hat{\\beta}_1\\)의 분모 = \\(1-r_{12}^2 \\approx 0\\)\n\\(\\hat{\\beta}_1\\)의 분자 \\(\\approx 0\\)\n\\(\\to\\) \\(\\hat{\\beta}_1\\)와 \\(\\hat{\\beta}_2\\)의 값이 불안정할것 같다.\n(관찰2)\n\\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\)의 분산과 공분산을 구해보자.\n\\(\\mbox{V}(\\hat{\\beta_1})=\\frac{1}{1-r_{12}^2}\\sigma^2\\)\n\\(\\mbox{V}(\\hat{\\beta_2})=\\frac{1}{1-r_{12}^2}\\sigma^2\\)\n\\(\\mbox{cov}\\big(\\hat{\\beta}_1,\\hat{\\beta}_2\\big)=\\frac{-r_{12}}{1-r_{12}^2}\\sigma^2\\)\n(관찰3)\n\\(\\mbox{cor}\\big(\\hat{\\beta}_1,\\hat{\\beta}_2\\big)=-r_{12}\\)\n\nimport rpy2 \n%load_ext rpy2.ipython\n\n\n%%R \nset.seed(999)\nn<-20000\ntoeic<-750+rnorm(n,sd=80)\ntoeic[toeic>990]<-990\ntoeic<-round(toeic)\nteps<-toeic + rnorm(n,sd=0.01)\ngpa<-3.5+rnorm(n,sd=0.3)\ngpa[gpa>4.5]<-4.5 \ngpa<-round(gpa,1)\nsal<-gpa*600+toeic*5+rnorm(n,sd=300)\nsal<-round(sal)\n\n\n%%R \ncor(toeic,teps)\n\n[1] 1\n\n\n(직관)\n토익이나 텝스점수나 그게 그거이다. (강한 상관관계)\n원래모형은 연봉=학점*600+토익*5+오차인데, 토익이나 텝스나 그게 그거이므로, 아래와 같은 모형들도 거의 참모형이라고 생각할 수 있다.\n\n연봉=학점*600+토익*2+텝스*3+오차\n연봉=학점*600+토익*1+텝스*4+오차\n연봉=학점*600+토익*(-5)+텝스*10+오차\n연봉=학점*600+토익*(-1000)+텝스*(1005)+오차\n연봉=학점*600+토익*(-10000)+텝스*(10005)+오차\n\n…\n결국에는 토익의 계수와 텝스의 계수를 더해서 5만되면 참모형\n\n%%R \nlm1<-lm(sal~toeic+teps+gpa)\nlm1\n\n\nCall:\nlm(formula = sal ~ toeic + teps + gpa)\n\nCoefficients:\n(Intercept)        toeic         teps          gpa  \n      50.38       194.70      -189.72       590.53  \n\n\n\n토익의 계수는 194.70, 텝스의 계수는 -189.72 로 추정되었다. 두개더하면 대략 5.\n몇번 더 시도를 해보자.\n(시도2)\n\n%%R \nset.seed(1)\nn<-20000\ntoeic<-750+rnorm(n,sd=80)\ntoeic[toeic>990]<-990\ntoeic<-round(toeic)\nteps<-toeic + rnorm(n,sd=0.01)\ngpa<-3.5+rnorm(n,sd=0.3)\ngpa[gpa>4.5]<-4.5 \ngpa<-round(gpa,1)\nsal<-gpa*600+toeic*5+rnorm(n,sd=300)\nsal<-round(sal)\n\n\n%%R \nlm1<-lm(sal~toeic+teps+gpa)\nlm1\n\n\nCall:\nlm(formula = sal ~ toeic + teps + gpa)\n\nCoefficients:\n(Intercept)        toeic         teps          gpa  \n     -45.82       -64.47        69.50       606.31  \n\n\n\n이번에는 토익의 계수는 -64.49, 텝스의 계수는 69.50 두개 더하면 대충 5\n(시도3)\n\n%%R \nset.seed(2)\nn<-20000\ntoeic<-750+rnorm(n,sd=80)\ntoeic[toeic>990]<-990\ntoeic<-round(toeic)\nteps<-toeic + rnorm(n,sd=0.01)\ngpa<-3.5+rnorm(n,sd=0.3)\ngpa[gpa>4.5]<-4.5 \ngpa<-round(gpa,1)\nsal<-gpa*600+toeic*5+rnorm(n,sd=300)\nsal<-round(sal)\n\n\n%%R \nlm1<-lm(sal~toeic+teps+gpa)\nlm1\n\n\nCall:\nlm(formula = sal ~ toeic + teps + gpa)\n\nCoefficients:\n(Intercept)        toeic         teps          gpa  \n      19.54       217.27      -212.31       603.10  \n\n\n\n토익의 계수는 217.27, 텝스의 계수는 -212.31 두개더하면 대충 5\n(시도4)\n\n%%R \nset.seed(3)\nn<-20000\ntoeic<-750+rnorm(n,sd=80)\ntoeic[toeic>990]<-990\ntoeic<-round(toeic)\nteps<-toeic + rnorm(n,sd=0.01)\ngpa<-3.5+rnorm(n,sd=0.3)\ngpa[gpa>4.5]<-4.5 \ngpa<-round(gpa,1)\nsal<-gpa*600+toeic*5+rnorm(n,sd=300)\nsal<-round(sal)\n\n\n%%R \nlm1<-lm(sal~toeic+teps+gpa)\nlm1\n\n\nCall:\nlm(formula = sal ~ toeic + teps + gpa)\n\nCoefficients:\n(Intercept)        toeic         teps          gpa  \n     -32.02      -152.82       157.85       600.85  \n\n\n\n토익의 계수는 -152.82, 텝스의 계수는 157.85 두개더하면 대충 5.\n[다중공선성의 특징]\n\n추정하는 \\(\\hat{\\beta_1}\\), \\(\\hat{\\beta_2}\\)가 어떤값일지 거의 예측안된다.\n\n\n5근처의 값이 나올때도 있고, 30근처의 값이 나오기도 하고, 100근처의 값이 나오기도 한다.\n\\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\)의 분산이 크다.\n\n\n그래도 \\(\\hat{\\beta}_1+\\hat{\\beta}_2\\approx 5\\)라는 공통점은 있음.\n\n\n\n수식적으로 그럴듯해 보여도, 모두 바람직한 모형은 아니다.\n아래는 모두 참모형이라고 생각되어지는 상황이다.\n\n\\(\\hat{\\beta}_1=2\\), \\(\\hat{\\beta}_2=3\\)\n\\(\\hat{\\beta}_1=5\\), \\(\\hat{\\beta}_2=0\\)\n\\(\\hat{\\beta}_1=10\\), \\(\\hat{\\beta}_2=-5\\)\n\n모두 참모형에 가깝지만 상식적으로 (3)은 용납할 수 잆음.\n이유1: (3)번과 같은 형태를 허용하면 \\(\\hat{\\beta}_1=10000, ~ \\hat{\\beta}_2=-9995\\) 와 같은식으로도 만들수 있음.\n이유2: (해석불가능한 모델)\n\\(\\hat{\\beta}_2\\)가 의미하는 것은 텝스점수가 얼마나 연봉에 영향을 주는지이다.\n즉 텝스점수 1점을 올리면 연봉이 5만원 깍임.\n상식적으로 말이 안된다.\n\n\n해결책\n논의를 간단하게 하기 위해서 모형을 좀 더 단순화 하자.\ngpa에 대응하는 계수 600을 우리가 이미 알고있다고 가정하자. (혹은 적절하게 추정했다고 가정하자.)\n그리고 오로지 연봉을 토익과 텝스만으로 추정한다고 하자.\n단순화된 모형은\nsal-600*gpa = toeic * β1 + teps * β2\n이제 아래와 같이 계수를 추정하였을 경우\n\n\\(\\hat{\\beta}_1=2\\), \\(\\hat{\\beta}_2=3\\)\n\\(\\hat{\\beta}_1=5\\), \\(\\hat{\\beta}_2=0\\)\n\\(\\hat{\\beta}_1=10\\), \\(\\hat{\\beta}_2=-5\\)\n\n(3)과 같은 상황이 발생하지 않도록 하여보자.\n\n%%R \nset.seed(2)\nn<-20000\ntoeic<-750+rnorm(n,sd=80)\ntoeic[toeic>990]<-990\ntoeic<-round(toeic)\nteps<-toeic + rnorm(n,sd=0.01)\ngpa<-3.5+rnorm(n,sd=0.3)\ngpa[gpa>4.5]<-4.5 \ngpa<-round(gpa,1)\nsal<-gpa*600+toeic*5+rnorm(n,sd=300)\nsal<-round(sal)\n\n\n%%R\ny<-sal-600*gpa\nlm(y~toeic+teps-1)\n\n\nCall:\nlm(formula = y ~ toeic + teps - 1)\n\nCoefficients:\n toeic    teps  \n 216.9  -211.9  \n\n\n\n\\(\\hat{\\beta}_1=216.9\\)\n\\(\\hat{\\beta}_2=-211.9\\)\n위의 결과는 \\(L(\\beta)\\)를 최소화하는 \\(\\beta\\)를 구한 결과임\n\n%%R \nX<-cbind(toeic,teps)\nL<-function(beta){\n    t(y-X%*%beta)%*%(y-X%*%beta)\n}\n\n\n%%R\nbeta<-c(216.9,-211.9)\nc<-L(beta)\n\n즉 \\(c=1798374986\\)이 \\(L\\)이 가질수 있는 최소값.\n\n%%R\nbeta1<-c(2,3)\nL1<-L(beta1)/c\nbeta2<-c(5,0)\nL2<-L(beta2)/c\nbeta3<-c(10,-5)\nL3<-L(beta3)/c\nbeta4<-c(104,-100)\nL4<-L(beta4)/c\nc(L1,L2,L3,L4)\n\n[1] 1.000052 1.000050 1.000048 7.321030\n\n\n\n\\(\\hat{\\beta}_1=2\\), \\(\\hat{\\beta}_2=3\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.000052\\)\n\\(\\hat{\\beta}_1=5\\), \\(\\hat{\\beta}_2=0\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.000050\\)\n\\(\\hat{\\beta}_1=10\\), \\(\\hat{\\beta}_2=-5\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.000048\\)\n\\(\\hat{\\beta}_1=104\\), \\(\\hat{\\beta}_2=-100\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=7.321030\\)\n\n이대로라면 위의 (1)-(4) 중에서 가장 적절한 해는 (3)이다.\n아이디어: \\(L(\\beta)\\)을 조금 바꾸자. 위의 손실함수에서 각각 \\(\\frac{1}{100000}\\big(\\beta_1^2+\\beta_2^2\\big)\\)를 더한다면?\n\n%%R\nbeta1<-c(2,3)\nbeta2<-c(5,0)\nbeta3<-c(10,-5)\nbeta4<-c(104,-100)\np1<-(beta1[1]^2+beta1[2]^2)/100000\np2<-(beta2[1]^2+beta2[2]^2)/100000\np3<-(beta3[1]^2+beta3[2]^2)/100000\np4<-(beta4[1]^2+beta4[2]^2)/100000\n\nL1<-L(beta1)/c+p1\nL2<-L(beta2)/c+p2\nL3<-L(beta3)/c+p3\nL4<-L(beta4)/c+p4\nc(L1,L2,L3,L4)\n\n[1] 1.000182 1.000300 1.001298 7.529190\n\n\n\n\\(\\hat{\\beta}_1=2\\), \\(\\hat{\\beta}_2=3\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.000182\\)\n\\(\\hat{\\beta}_1=5\\), \\(\\hat{\\beta}_2=0\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.000300\\)\n\\(\\hat{\\beta}_1=10\\), \\(\\hat{\\beta}_2=-5\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=1.001298\\)\n\\(\\hat{\\beta}_1=104\\), \\(\\hat{\\beta}_2=-100\\) \\(\\longrightarrow\\) \\(L(\\boldsymbol{\\beta})=7.529190\\)\n\n이렇게하면 이제 (수정된) \\(L\\)을 최소화하는 해는 (1)이다.\n\n\n해결책의 시각적 이해\n\n%%R\nβ1 = seq(-10,15,length=50)\nβ2 = seq(-10,15,length=50)\nL_ <- function(β1,β2){\n    rtn<-c()\n    for(k in 1:length(β2)){\n        rtn[k]<-sum((y-β1[k]*toeic-β2[k]*teps)^2)/c\n    }\n    rtn\n}\nz_=outer(β1,β2,L_)\n#install.packages(\"plot3D\")\nlibrary(plot3D) \nribbon3D(z=z_,contour=TRUE,xlab=\"β1\",ylab=\"β2\",zlab=\"L(β1,β2)\",main=\"original loss function\")\n\n\n\n\n\n%%R\nβ1 = seq(-10,15,length=50)\nβ2 = seq(-10,15,length=50)\npanelty_<-function(β1,β2){\n    λ<-1.5 # 1/100000\n    rtn<-c()\n    for(k in 1:length(β2)){\n        rtn[k]<-λ*sum(β1[k]^2+β2[k]^2)\n    }\n    rtn    \n}\n\nL_ <- function(β1,β2){\n    rtn<-c()\n    for(k in 1:length(β2)){\n        rtn[k]<-sum((y-β1[k]*toeic-β2[k]*teps)^2)/c\n    }\n    rtn\n}\nz_=outer(β1,β2,L_)\nz__=outer(β1,β2,panelty_)\n#install.packages(\"plot3D\")\nlibrary(plot3D) \nribbon3D(z=z_,contour=TRUE,xlab=\"β1\",ylab=\"β2\",main=\"그림1: L(β1,β2)\")\nribbon3D(z=z__,contour=TRUE,xlab=\"β1\",ylab=\"β2\",main=\"그림2: λ(β1^2+β2^2)\")\nribbon3D(z=z_+z__,contour=TRUE,xlab=\"β1\",ylab=\"β2\",main=\"그림3: L(β1,β2)+λ(β1^2+β2^2)\")\n\n\n\n\\(\\lambda\\)의 역할 \\((\\lambda>0)\\)\n[그림1]이랑 [그림2]을 섞어서 [그림3]을 만드는데, 각각을 얼만큼의 비율로 섞을지 결정한다.\n람다가 크면 그림2를 많이 반영한다. \\(\\to\\) 람다가 너무 크면 결국 \\(\\hat{\\beta}_1\\)와 \\(\\hat{\\beta}_2\\)의 추정값이 거의 0에 가깝게 된다.\n람다가 작으면 그림1을 많이 반영한다. \\(\\to\\) 람다가 너무 작게되면, 우리는 손실함수 \\(L(\\beta_1,\\beta_2)\\)에 아무런 수정도 하지않은 셈이된다.\n\n\n\\(\\lambda(\\beta_1^2+\\beta_2^2)\\)의 역할\n원래 \\(\\beta_1+\\beta_2=5\\)를 만족하는 어떠한 해도 솔루션이 되었음. 즉 [그림1]과 같은 상황\n[그림1]과 같은 상황에서 약간의 경사를 주어서 [그림3]과 같은 상황을 만듬\n\\(\\hat{\\beta}_1\\) 와 \\(\\hat{\\beta}_2\\) 의 값을 안정적으로 만들어 준다.\n그러면서도 딱히 원래 \\(L(\\beta_1,\\beta_2)\\)의 모양을 크게 해치지 않는다.\n(!) 그럴듯함. 우리가 원하는것 같다.\n이렇게 하면 다중공선성이 발생하는 상황에서 적절한 해를 구할 수 있을 것 같다.\n(관찰)\n\\(\\lambda\\big(\\beta_1^2+\\beta_2^2\\big)\\)은 아래와 같이 표현가능함.\n\\(\\lambda [\\beta_1,\\beta_2]\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix}=\\lambda\\beta^\\top \\beta\\)\n단, \\(\\beta=\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\).\n\n\n릿지에 대한 책의 설명들..\nStep 1. \\(\\hat{\\beta}^R=\\bf\\big(X^\\top X+\\lambda I \\big)^{-1}{\\bf X}^\\top y\\).\n일반적인 회귀분석과 다르게 아래와 같은 손실함수를 생각하자.\n\\(L={\\bf (y-X\\beta)^\\top}{\\bf (y-X\\beta)}+\\lambda \\beta^\\top \\beta=\\mbox{goodness of fit} + \\mbox{panalty}\\)\n\\(L\\)를 최소화하는 \\(\\hat{\\beta}\\)을 구하면 아래와 같다. 구분을 위해서 \\(\\hat{\\beta}^R\\)이라는 기호를쓰자.\n\\(\\hat{\\beta}^R=\\bf\\big(X^\\top X+\\lambda I \\big)^{-1}{\\bf X}^\\top y\\)\nStep 2 : \\(\\hat{\\beta}^R\\)이 \\(\\hat{\\beta}^{OLS}\\)보다 왜 좋은가?\n\\(\\hat{\\beta}^{OLS}={\\bf (X^\\top X)^{-1}X^\\top y}\\)\n\\(\\hat{\\beta}^{R}={\\bf (X^\\top X+\\lambda I)^{-1}X^\\top y}\\)\n\n\\(\\hat{\\beta}^{OLS}\\)는 \\(\\beta\\)에 대한 불편추정량 이지만, \\(\\hat{\\beta}^{R}\\)은 아니다.\n하지만 분산은 \\(\\hat{\\beta}^{OLS}\\)보다 \\(\\hat{\\beta}^R\\)이 더 작다.\n좋은 추정량은 bias와 분산이 모두 작아야하는데, \\(\\hat{\\beta}^{R}\\)은 바이어스는 \\(\\hat{\\beta}^{OLS}\\)보다 큰 상황이고, 분산은 \\(\\hat{\\beta}^{OLS}\\)보다 작은 상황이다.\n따라서 두 추정치중에서 뭐가 더 좋다고 말하기 애매한데, 이를 명확하게 말하기 위해서는 MSE(\\(=\\mbox{bias}^2 + \\mbox{variance}\\))를 비교해봐야한다.\nHoerl 과 Kennard는 \\(\\hat{\\beta}^{R}\\)이 \\(\\hat{\\beta}^{OLS}\\)보다 MSE가 작게되는 \\(\\lambda\\)값이 항상 존재한다고 한다고 밝혔다. (1970년)\n따라서 이러한 \\(\\lambda\\)를 잘 찾으면 항상 Ridge Regression이 Linear Regression 보다 좋다고 주장할 수 있다.\n\n(참고)\n\n일반적인 손실함수 \\(L={\\bf (y-X\\beta)^\\top (y-X\\beta)}\\)와 수정된 손실함수를 구분하기 위해서 아래와 같이 \\(\\tilde{L}\\)와 같은 기호를 쓰기도 한다.\n\n\\(\\tilde{L}={\\bf(y-X\\beta)^\\top(y-X\\beta)+\\lambda \\beta^\\top \\beta }\\)\n\n일반적인 손실함수 \\(L\\)을 최소화하는 \\(\\beta\\)와 수정된 손실함수 \\(\\tilde{L}\\)을 최소화하는 \\(\\beta\\)를 구분하기 위해서 아래와 같은 기호를 쓰기도 한다.\n\n\\(\\hat{\\beta}^{OLS}={\\bf (X^\\top X)^{-1}X^\\top y}\\)\n\\(\\hat{\\beta}^{R}={\\bf (X^\\top X+\\lambda I)^{-1}X^\\top y}\\)\n\n\\(\\lambda \\beta^\\top \\beta\\)를 패널티항(=벌점항), \\(L_2\\)-패널티, regularization term (정규화항) 이라고 부른다.\n\\(\\lambda \\beta^\\top \\beta\\)를 \\(\\lambda \\|\\beta\\|_2^2\\)로 표현하기도 한다. \\(\\| \\cdot \\|_2\\)는 벡터의 \\(L_2\\)-노름 이라고 한다.\n\n\n\\({\\bf x}=(x_1,x_2)\\) \\(\\to\\) \\(\\|{\\bf x}\\|_2^2:=x_1^2+x_2^2\\), \\(\\|{\\bf x}\\|_2:=\\sqrt{x_1^2+x_2^2}\\)\n\\(\\lambda \\beta^\\top \\beta = \\lambda(\\beta_1^2+\\beta_2^2)\\)\n\n\n(4)에서 유리하여 \\(\\lambda \\beta^\\top \\beta\\)를 \\(L_2\\)-패널티항이라고 부르기도 한다.\n참고로 \\(L_1\\)-패널티항도 있다. 그리고 \\(L_1\\)과 \\(L_2\\)패널티를 섞어서 쓰는 방법도 있다.\n\n\n\\(Loss=SSE+L_2\\mbox{-}panalty\\): Ridge\n\\(Loss=SSE+L_1\\mbox{-}panalty\\): Lasso\n\\(Loss=SSE+L_1\\mbox{-}panalty+L_2\\mbox{-}panalty\\): Elastic-net\n\n\n\\(L_2\\)-패널티항은 너무 큰 \\(\\beta_1,\\beta_2\\)를 구할때 패널티를 부여하여 되도록이면 작은 \\(\\beta_1,\\beta_2\\)을 선택하게끔 제약을 건다.\n일반적인 손실함수 \\(L\\)이지만 우리는 \\(\\tilde{L}\\)을 최소화해야 하므로 (i) 되도록이면 \\(\\beta_1,\\beta_2\\)의 값이 작을수록 좋다는 제약하에서 (ii) \\(L\\)를 최소화하는 2가지 역할을 수행해야 한다.\n\\(\\tilde{L}\\)를 최소화하는 문제를 제약된 조건하에서의 최소화문제라고 표현한다.\n(8)에서 (i)-(ii)의 역할중에서 어떤것을 더 중시할지 조율하는 역할을 \\(\\lambda\\)가 하는데, 이와 같은 이유로 \\(\\lambda\\)를 조율모수 (tuning parameter)라고 부르기도 한다.\n\\(\\lambda\\)를 하이퍼파라메터라고 부르기도 한다.\n\\(\\hat{\\beta}^{R}\\)의 값은 대체적으로 \\(\\hat{\\beta}^{OLS}\\)보다 작은값을 가진다. 이러한 이유로 \\(\\beta^{R}\\) shrinkage esitmator라고 부른다.\nbiased regression, shrinkage method, regularization method, panalty method.. 등 ridge를 표현하는 방법들은 다양하다.\n능형회귀를 수행함에 있어서 (이론적인 부분에서는 언급되지 않았으나) 변수의 표준화를 먼저 시행해야 한다."
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Attention is all you need.html",
    "href": "posts/ml_basic/2023-05-28-Attention is all you need.html",
    "title": "Transformers",
    "section": "",
    "text": "Attention is all you need\n\nref: 딥 러닝을 이용한 자연어 처리 입문, Attention is all you need\n\\(\\star\\) seq2seq 구조인 인코더-디코더를 따르면서 어텐션만으로 구현한 모델, RNN을 사용하지 않고 인코더-디코더 구조로 설계하였지만 RNN보다 우수한 성능을 보임\n\nhttps://github.com/huggingface/transformers\n\n\nhttps://github.com/huggingface/transformers/blob/main/README_ko.md\n\n\nseq2seq 의 한계\n\n인코더, 디코더로 구성되어 있는 seq2seq\n인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축\n\n\\(\\to\\) 입력 시퀀스의 정보가 일부 손실된다는 단점 존재\n\\(\\to\\) 이를 위해 어텐션 메카니즘 등장\n\n디코더는 이 벡터 표현을 통해 출력 시퀀스를 만듦\n\\(\\star\\) 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만든다면??\n\n\n\n트랜스포머의 주요 하이퍼파라미터"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis\nRefernece: 핸즈 온 머신러닝, 최규빈교수님 통계전산강의노트 사이킷런 홈페이지"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#분산-보존",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#분산-보존",
    "title": "Principal Component Analysis",
    "section": "1. 분산 보존",
    "text": "1. 분산 보존\n분산이 최대로 보존되는 차원 축소가 정보를 가장 적게 손실되어 합리적으로 보임"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#주성분",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#주성분",
    "title": "Principal Component Analysis",
    "section": "2. 주성분",
    "text": "2. 주성분\n분산이 큰 순서대로 차원의 수만큼 찾음\ni번째 축 = 주성분 PC principal component\n특이값 분해 SVD singular value decomposition= training set에서 주성분 찾는 방법\n교수님 lecture note with julia\n\n이론 : \\(X_{n \\times m} = U_{n \\times n} D_{n \\times m} (V_{m \\times m})^\\top\\)\n\nver 1 = \\(U, V\\)가 모두 직교 행렬\nver 2 = \\(U\\) 또는 \\(V\\)가 직교 행렬\n\n왜?\n\n데이터 매트릭스 \\(X\\)가 존재할때 정보는 유지하면서 비용을 줄이는 \\(Z\\)4를 찾고 싶다.\n\\(Z\\) 구하는 법\n\n\\(Z = \\tilde{U} \\tilde{D}\\) \\(\\to\\) \\(\\tilde{U} \\tilde{D} = X\\tilde{V}\\) \\(\\to\\) \\(Z = X\\tilde{V}\\)\n\\(X^\\top X = \\psi \\lambda \\psi^\\top\\)을 구해서 \\(Z = X\\tilde{\\psi}\\) \\(\\to\\) \\(\\hat{X} = Z\\tilde{\\psi}^\\top\\)\n\n\n\n\\(\\star\\) PCA는 데이터셋의 평균이 0이라고 가정"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#d차원으로-투영하기",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#d차원으로-투영하기",
    "title": "Principal Component Analysis",
    "section": "3. d차원으로 투영하기",
    "text": "3. d차원으로 투영하기\n\\(X_{d-proj,n \\times d} = X_{n \\times M} W_{d,n \\times d}\\)"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#설명된-분산의-비율",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#설명된-분산의-비율",
    "title": "Principal Component Analysis",
    "section": "4. 설명된 분산의 비율",
    "text": "4. 설명된 분산의 비율\n설명된 분산의 비율 explained variance ratio\n\n공분산 행렬의 고유값\n투영한 분산의 비율\n주성분 선택된 순으로 작아짐"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#적절한-차원-수-선택하기",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#적절한-차원-수-선택하기",
    "title": "Principal Component Analysis",
    "section": "5. 적절한 차원 수 선택하기",
    "text": "5. 적절한 차원 수 선택하기\n차원 수를 임의로 정하는 것보다는 충분한 분산5이 될 떄까지 선택"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#압축을-위한-pca",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#압축을-위한-pca",
    "title": "Principal Component Analysis",
    "section": "6. 압축을 위한 PCA",
    "text": "6. 압축을 위한 PCA\n참고\n중요한 특징만을 살리기 위해 PCA를 시도하여 차원 축소하였다.\n이후 원본 데이터로 돌아가려 할 때 특징은 살아있지만 완벽히 데이터셋이 일치하지 않는데,\n여기서 이 오류를 재구성 오차 = 재건 오류 reconstruction error 라고 한다.\n\\(X_{n \\times M} = X_{d-proj,n \\times d} (W_{d,n \\times d})^\\top\\)"
  },
  {
    "objectID": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#커널-pca",
    "href": "posts/ml_basic/2023-03-28-Principal Component Analysis.html#커널-pca",
    "title": "Principal Component Analysis",
    "section": "7. 커널 PCA",
    "text": "7. 커널 PCA\n차원 축소를 통해 비선형 투영 수행\n\\(\\zeta = \\Psi \\alpha\\)\n\n이 때, \\(||\\alpha_j|| = 1\\)로 정규화한다.\n그러기 위해 \\(\\alpha_j\\)를 \\(||\\zeta_j||\\)로 나누어 정규화\n\n\\(||\\zeta_j|| = \\sqrt{\\lambda}_j\\)\n\\(\\alpha_j \\to \\frac{1}{\\sqrt{\\lambda}_j} \\alpha_j, j=1, \\dots, m\\)\n\n\n특징 벡터로 내적하여 나오는 커널\\(K\\) 행렬로 중심화\n\n\\(K = HKH\\)\n\\(H = I_n - 1_{n \\times 1} /n\\)\n\n\\(\\alpha\\) 정규화 한 후 중심화하면\n\\((z_1, \\dots, z_n) = (\\frac{1}{\\sqrt{\\lambda_1}} \\alpha_1,\\dots , \\frac{1}{\\sqrt{\\lambda_m}}\\alpha_m)^\\top HKH\\)\n\n여기서 m개가 주성분!\n\n\n\n\nimage\n\n\n\\(\\star\\) 고유벡터\n\n선형 \\(C = \\psi \\psi^\\top\\)\n비선형 \\(K = \\psi^\\top \\psi\\)\n\n\\(\\psi\\)의 길이에 따라 고유값 문제의 표현을 다르게 하여 계산 시간 줄이기\n차원 수가 표본 수보다 큰 경우에는 커널 행렬을 사용하는 것이 효율적"
  },
  {
    "objectID": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html",
    "href": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html",
    "title": "Ensemble and Random Forest",
    "section": "",
    "text": "Ensemble and Random Forest\n직접 투표 hard voting\n간접 투표 soft voting\n약한학습기1 weak learner일지라도 큰 수의 법칙 law of large numbers에 의해 앙상블은 강한 학습기 strong learner가 될 수 있다.\n결정주분류 Decision-Making Classifier"
  },
  {
    "objectID": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html#에이다부스트",
    "href": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html#에이다부스트",
    "title": "Ensemble and Random Forest",
    "section": "에이다부스트",
    "text": "에이다부스트\n에이다부스트AdaBoost5\n\n과소적합했던 훈련 샘플의 가중치를 더 높여 새로운 예측기 만들기\n가중치 부여: 이전 모델에서 잘못 분류된 샘플에 더 높은 가중치를 부여하여 다음 모델에서 이 샘플이 더 중요하게 처리되도록 합니다.\n재샘플링: 이전 모델에서 잘못 분류된 샘플을 더 많이 선택하여 다음 모델에서 이 샘플이 더 많이 등장하도록 합니다.\n특성 선택: 이전 모델에서 잘못 분류된 샘플과 관련이 높은 특성을 더 많이 사용하여 다음 모델을 학습시킵니다.\n학습률을 줄여 해당 분류기가 전체 모델에 기여하는 확률을 줄임\n\n에이다부스트를 통해 학습을 수행하는 알고리즘6\n\n각 훈련 표본 \\(\\{x_i, y_i)\\}^n_{i=1}\\)에 대응하는 가중치 \\(\\{ w_i\\}^n_{i=1}\\)을 모두 같은 값으로, 강분류기 \\(f\\)는 0으로 초기화한다.\n\n\n\\(w_1,\\dots, w_n \\leftarrow 1/n, f \\leftarrow 0\\)\n\n\n\\(j=1,\\dots,b\\)에 대하여 아래의 과정을 반복한다.\n\n\n표본의 현재 가중치 \\(\\{ w_i\\}^n_{i=1}\\)에 대하여 가중 오분류율(0/1손실7에 가중치를 곱한 합)\\(R(\\psi)\\)가 최소가 되도록 약 분류기\\(\\psi_j\\)를 학습시킨다.\n\n\n\\(\\psi_j = argmin_{\\psi} R(\\psi), R(\\psi) = \\sum^n_{i=1} \\frac{w_i}{2}(1-\\psi(x_i)y_i)\\)\n\n\n약 분류기 \\(\\psi_j\\)의 가중치 \\(\\theta_j\\)를 다음 식과 같이 결정한다.\n\n\n\\(\\theta_j = \\frac{1}{2} log\\frac{1-R(\\psi_j)}{R(\\psi_j)}\\)\n\n\n강 분류기 \\(f\\)를 다음 식과 같이 업데이트 한다.\n\n\n\\(f \\leftarrow f+\\theta_j\\psi_j\\)\n\n\n표본의 가중치 \\(\\{ w_i\\}^n_{i=1}\\)을 다음 식과 같이 업데이트 한다.\n\n\n\\(w_i \\leftarrow \\frac{exp(-f(x_i)y_i)}{\\sum^n_{i'=1}exp(-f(x_{i'})y_{i'})}, \\forall i = 1,\\dots, n\\)"
  },
  {
    "objectID": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html#그레이디언트-부스팅",
    "href": "posts/ml_basic/2023-04-02-Ensemble and Random Forest.html#그레이디언트-부스팅",
    "title": "Ensemble and Random Forest",
    "section": "그레이디언트 부스팅",
    "text": "그레이디언트 부스팅\n\n그레이디언트 부스팅 gradient boosting\n\n이전 예측기가 만든 잔여 오차에 새로운 예측기 학습\n그레디언트 트리 부스팅 gradient tree boosting = 그레디언트 부스티드 회귀 트리 gradient boosted regression tree(GBRT)\n\n결정 트리를 기반 예측기로 사용\n\n확률적 그레디언트 부스팅 stochastic gradient boosting\n\n트리가 훈련할 때 사용할 훈련 샘플의 비율 지정 -> 분산 낮아질 것\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n에이다부스트(AdaBoost)는 처음에는 지수 손실 최소화를 통해 분류기를 학습합니다. 그러나 이후에는 분류 오차를 최소화하기 위해 다른 손실 함수나 분류기 학습 알고리즘을 사용하기도 합니다.\n예를 들어, 로지스틱 회귀(Logistic Regression)를 사용하여 분류기를 학습하면 로지스틱 손실 함수를 최소화하는 방향으로 학습됩니다. 또한, 그레디언트 부스팅(Gradient Boosting)을 사용하여 분류기를 학습할 수도 있습니다. 이 경우, 분류 오차를 최소화하는 방향으로 학습됩니다.\n따라서, 에이다부스트는 지수 손실 최소화 학습 관점에서만 볼 수 있는 것은 아니며, 다른 손실 함수나 학습 알고리즘을 사용하여 분류기를 학습할 수 있습니다. 그러나 초기에는 지수 손실 함수를 사용하여 가중치를 부여하는 방식으로 분류기를 학습하는 것이 일반적입니다."
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html",
    "href": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html",
    "title": "Sequence-to-Sequence, seq2seq",
    "section": "",
    "text": "Sequence-to-Sequence, seq2seq\nRef: 딥러닝을 이용한 자연어 처리 입문, keras"
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#teacher-forcing-교사-강요",
    "href": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#teacher-forcing-교사-강요",
    "title": "Sequence-to-Sequence, seq2seq",
    "section": "Teacher Forcing 교사 강요",
    "text": "Teacher Forcing 교사 강요\n\ntrain에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않음.\n이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법 사용\n\n이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측에도 잘못될 가능성이 높음\n이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 함\n이런 상황이 반복되면 훈련 시간이 느려질 수 있음\n\n\n교사 강요\n\nRNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법"
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#seq2seq-기계-번역기-훈련",
    "href": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#seq2seq-기계-번역기-훈련",
    "title": "Sequence-to-Sequence, seq2seq",
    "section": "seq2seq 기계 번역기 훈련",
    "text": "seq2seq 기계 번역기 훈련\n\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense\nfrom tensorflow.keras.models import Model\nimport numpy as np\n\nencoder_states 가 바로 컨텍스트 벡터\n\nencoder_inputs = Input(shape=(None, src_vocab_size))\nencoder_lstm = LSTM(units=256, return_state=True)\n\n# encoder_outputs은 여기서는 불필요\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n\n# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\nencoder_states = [state_h, state_c]\n\nreturn_state=True\n인코더의 내부 상태를 디코더로 넘겨주기 위해, False면 리턴하지 않음\n\ndecoder_inputs = Input(shape=(None, tar_vocab_size))\ndecoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n\n# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\ndecoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n\ndecoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\ndecoder_outputs = decoder_softmax_layer(decoder_outputs)\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n\n디코더는 은닉상태, 셀 상태를 사용하지 않기 때문에 _로 받아줌\n\nmodel.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)\n\nEpoch 1/40\n750/750 [==============================] - 100s 131ms/step - loss: 0.8574 - val_loss: 0.7819\nEpoch 2/40\n750/750 [==============================] - 98s 131ms/step - loss: 0.5758 - val_loss: 0.6667\nEpoch 3/40\n750/750 [==============================] - 144s 191ms/step - loss: 0.5051 - val_loss: 0.6056\nEpoch 4/40\n750/750 [==============================] - 171s 229ms/step - loss: 0.4589 - val_loss: 0.5569\nEpoch 5/40\n750/750 [==============================] - 159s 213ms/step - loss: 0.4232 - val_loss: 0.5200\nEpoch 6/40\n750/750 [==============================] - 154s 205ms/step - loss: 0.3958 - val_loss: 0.4920\nEpoch 7/40\n750/750 [==============================] - 109s 145ms/step - loss: 0.3745 - val_loss: 0.4701\nEpoch 8/40\n750/750 [==============================] - 105s 140ms/step - loss: 0.3572 - val_loss: 0.4552\nEpoch 9/40\n750/750 [==============================] - 119s 159ms/step - loss: 0.3429 - val_loss: 0.4391\nEpoch 10/40\n750/750 [==============================] - 163s 218ms/step - loss: 0.3306 - val_loss: 0.4275\nEpoch 11/40\n750/750 [==============================] - 164s 219ms/step - loss: 0.3201 - val_loss: 0.4184\nEpoch 12/40\n750/750 [==============================] - 160s 213ms/step - loss: 0.3111 - val_loss: 0.4119\nEpoch 13/40\n750/750 [==============================] - 133s 177ms/step - loss: 0.3031 - val_loss: 0.4011\nEpoch 14/40\n750/750 [==============================] - 101s 135ms/step - loss: 0.2959 - val_loss: 0.3956\nEpoch 15/40\n750/750 [==============================] - 101s 135ms/step - loss: 0.2893 - val_loss: 0.3928\nEpoch 16/40\n750/750 [==============================] - 141s 188ms/step - loss: 0.2832 - val_loss: 0.3868\nEpoch 17/40\n750/750 [==============================] - 173s 230ms/step - loss: 0.2778 - val_loss: 0.3798\nEpoch 18/40\n750/750 [==============================] - 161s 215ms/step - loss: 0.2727 - val_loss: 0.3781\nEpoch 19/40\n750/750 [==============================] - 152s 202ms/step - loss: 0.2679 - val_loss: 0.3755\nEpoch 20/40\n750/750 [==============================] - 117s 156ms/step - loss: 0.2635 - val_loss: 0.3704\nEpoch 21/40\n750/750 [==============================] - 105s 140ms/step - loss: 0.2593 - val_loss: 0.3687\nEpoch 22/40\n750/750 [==============================] - 123s 164ms/step - loss: 0.2553 - val_loss: 0.3656\nEpoch 23/40\n750/750 [==============================] - 162s 216ms/step - loss: 0.2517 - val_loss: 0.3626\nEpoch 24/40\n750/750 [==============================] - 162s 217ms/step - loss: 0.2482 - val_loss: 0.3618\nEpoch 25/40\n750/750 [==============================] - 157s 209ms/step - loss: 0.2448 - val_loss: 0.3602\nEpoch 26/40\n750/750 [==============================] - 135s 180ms/step - loss: 0.2414 - val_loss: 0.3577\nEpoch 27/40\n750/750 [==============================] - 98s 131ms/step - loss: 0.2385 - val_loss: 0.3569\nEpoch 28/40\n750/750 [==============================] - 98s 131ms/step - loss: 0.2355 - val_loss: 0.3562\nEpoch 29/40\n750/750 [==============================] - 150s 200ms/step - loss: 0.2325 - val_loss: 0.3536\nEpoch 30/40\n750/750 [==============================] - 172s 229ms/step - loss: 0.2299 - val_loss: 0.3517\nEpoch 31/40\n750/750 [==============================] - 161s 214ms/step - loss: 0.2271 - val_loss: 0.3520\nEpoch 32/40\n750/750 [==============================] - 156s 208ms/step - loss: 0.2246 - val_loss: 0.3518\nEpoch 33/40\n750/750 [==============================] - 110s 147ms/step - loss: 0.2222 - val_loss: 0.3504\nEpoch 34/40\n750/750 [==============================] - 107s 143ms/step - loss: 0.2199 - val_loss: 0.3494\nEpoch 35/40\n750/750 [==============================] - 128s 171ms/step - loss: 0.2177 - val_loss: 0.3502\nEpoch 36/40\n750/750 [==============================] - 168s 224ms/step - loss: 0.2152 - val_loss: 0.3493\nEpoch 37/40\n750/750 [==============================] - 166s 221ms/step - loss: 0.2131 - val_loss: 0.3479\nEpoch 38/40\n750/750 [==============================] - 154s 205ms/step - loss: 0.2111 - val_loss: 0.3492\nEpoch 39/40\n750/750 [==============================] - 122s 162ms/step - loss: 0.2091 - val_loss: 0.3499\nEpoch 40/40\n750/750 [==============================] - 98s 131ms/step - loss: 0.2070 - val_loss: 0.3497\n\n\n<keras.callbacks.History at 0x7fe721864a30>"
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#seq2seq-기계-번역기-동작",
    "href": "posts/ml_basic/2023-05-28-Sequence-to-Sequence, seq2seq.html#seq2seq-기계-번역기-동작",
    "title": "Sequence-to-Sequence, seq2seq",
    "section": "seq2seq 기계 번역기 동작",
    "text": "seq2seq 기계 번역기 동작\nSTEP 1. 번역하고자 하는 입력 문장이 인코더에 들어가 은닉 상태와 셀 상태를 얻는다.\nSTEP 2. 상태와 <sos>에 해당하는 \\t 를 디코더로 보낸다.\nSTEP 3. elzhejrk <eos>에 해당하는 \\n이 나올 때까지 다음 문자를 예측하는 행동을 반복한다.\n\nencoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n\n인코더 정의\n\n# 이전 시점의 상태들을 저장하는 텐서\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n# 뒤의 함수 decode_sequence()에 동작을 구현 예정\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n\n# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_softmax_layer(decoder_outputs)\ndecoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n\n\ndecoder_inputs\n\n<KerasTensor: shape=(None, None, 103) dtype=float32 (created by layer 'input_2')>\n\n\n\ndecoder_states_inputs\n\n[<KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_5')>,\n <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'input_6')>]\n\n\n디코더 설계\n\nindex_to_src = dict((i, char) for char, i in src_to_index.items())\nindex_to_tar = dict((i, char) for char, i in tar_to_index.items())\n\n인덱스에서 단어를 얻을 수 있도록 index_to_~설계\n\ndef decode_sequence(input_seq):\n    # 입력으로부터 인코더의 상태를 얻음\n    states_value = encoder_model.predict(input_seq)\n\n    # <SOS>에 해당하는 원-핫 벡터 생성\n    target_seq = np.zeros((1, 1, tar_vocab_size))\n    target_seq[0, 0, tar_to_index['\\t']] = 1.\n\n    stop_condition = False\n    decoded_sentence = \"\"\n\n    # stop_condition이 True가 될 때까지 루프 반복\n    while not stop_condition:\n        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n    # 예측 결과를 문자로 변환\n    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n    sampled_char = index_to_tar[sampled_token_index]\n\n    # 현재 시점의 예측 문자를 예측 문장에 추가\n    decoded_sentence += sampled_char\n\n    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n    if (sampled_char == '\\n' or\n        len(decoded_sentence) > max_tar_len):\n        stop_condition = True\n\n    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n    target_seq = np.zeros((1, 1, tar_vocab_size))\n    target_seq[0, 0, sampled_token_index] = 1.\n\n    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n    states_value = [h, c]\n\n    return decoded_sentence\n\n\nstep by step\n\ninput_seq = encoder_input[5:10]\n\n\nstates_value = encoder_model.predict(input_seq)\n\n1/1 [==============================] - 0s 31ms/step\n\n\n\n# <SOS>에 해당하는 원-핫 벡터 생성\ntarget_seq = np.zeros((1, 1, tar_vocab_size))\n\n\ntarget_seq\n\narray([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.]]])\n\n\n\ntarget_seq[0, 0, tar_to_index['\\t']] = 1.\n\n\ntarget_seq\n\narray([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0.]]])\n\n\n\nstop_condition = False\n\n\ndecoded_sentence = \"\"\n\n\n# Check dimensions of input arrays\nprint(target_seq.shape)            # Shape of target_seq\nprint(states_value[0].shape)       # Shape of states_value[0]\nprint(states_value[1].shape)       # Shape of states_value[1]\nprint(states_value[0][1,:].reshape(1,-1).shape)\nprint(states_value[1][1,:].reshape(1,-1).shape)\n\n(1, 1, 103)\n(5, 256)\n(5, 256)\n(1, 256)\n(1, 256)\n\n\n\nstates_value_tmp = [states_value[0][1,:].reshape(1,-1), states_value[1][1,:].reshape(1,-1)]\n\n\n# stop_condition이 True가 될 때까지 루프 반복\nwhile not stop_condition:\n    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n    output_tokens, h, c = decoder_model.predict([target_seq] + states_value_tmp)\n\n\noutput_tokens, h, c = decoder_model.predict([target_seq] + states_value_tmp)\n\n\n# 예측 결과를 문자로 변환\nsampled_token_index = np.argmax(output_tokens[0, -1, :])\nsampled_token_index\n\n\nsampled_char = index_to_tar[sampled_token_index]\nsampled_char\n\n\n# 현재 시점의 예측 문자를 예측 문장에 추가\ndecoded_sentence += sampled_char\n\n\n# <eos>에 도달하거나 최대 길이를 넘으면 중단.\nif (sampled_char == '\\n' or\n    len(decoded_sentence) > max_tar_len):\n    stop_condition = True\n\n\n# 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\ntarget_seq = np.zeros((1, 1, tar_vocab_size))\ntarget_seq\n\n\ntarget_seq[0, 0, sampled_token_index] = 1.\n\n\n# 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\nstates_value = [h, c]\n\n\nprint('입력 문장:', lines.src[5])\nprint('정답 문장:', lines.tar[5][2:len(lines.tar[5])-1]) # '\\t'와 '\\n'을 빼고 출력\nprint('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n\n\nfor seq_index in [3,4]: # 입력 문장의 인덱스\n    input_seq = encoder_input[seq_index:seq_index+1]\n    decoded_sentence = decode_sequence(input_seq)\n    # print(35 * \"-\")\n    print('입력 문장:', lines.src[seq_index])\n    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
  },
  {
    "objectID": "posts/ml_basic/2023-03-29-Lasso and Ridge.html",
    "href": "posts/ml_basic/2023-03-29-Lasso and Ridge.html",
    "title": "Lasso and Ridge",
    "section": "",
    "text": "Lasso and Ridge\n릿지ridge 회귀 = 능형회귀 = 티호노프 tikhonov 규제 = L2 노름\n\\(\\star\\) 숙지!\n릿지 회귀는 L2 규제를 사용하여 모델의 가중치가 커지지 않도록 제한합니다. 이 규제는 일반적으로 극단적인 가중치 값을 갖는 것을 방지하고, 모델의 일반화 성능을 향상시킵니다.\n하지만 릿지 회귀는 가중치를 완전히 0으로 만들지는 않습니다. 대신, 가중치를 작게 만들어 모델이 더 일반적인 패턴을 학습하도록 유도합니다. 이는 희소성을 가진 데이터에서 모델의 성능을 향상시키는 데 도움이 됩니다.\n반면, 라쏘 회귀는 L1 규제를 사용하여 가중치를 0으로 만들 수 있습니다. 이는 희귀 학습에서 유용합니다. 희소성을 가진 데이터에서는 많은 특성 중 일부만 중요하다는 것을 알 수 있습니다. 따라서, 라쏘 회귀는 이러한 특성을 선택하고 다른 특성을 제거하여 모델을 희소하게 만들 수 있습니다.\nRefernece: 책_The Elements of Statistical Learning, 그림과 수식으로 배우는 통통 머신러닝, 2021데이터과학 최규빈교수님 lecture 노트"
  },
  {
    "objectID": "posts/ml_basic/2023-03-29-Lasso and Ridge.html#예제",
    "href": "posts/ml_basic/2023-03-29-Lasso and Ridge.html#예제",
    "title": "Lasso and Ridge",
    "section": "예제)",
    "text": "예제)\n\\(y = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon, \\epsilon \\sim N(0,\\sigma^2)_{idd}\\)임 모형이 있다고 할 때,\n원래 모형은 \\(y = 5 x_{i1} + 600 x_{i2}\\)이다.\n이 때, \\(x_{i3}\\)이 \\(x_{i1}\\)과 거의 \\(1\\)의 상관관계를 가지고 있다면? \\(\\to\\) 다중공선성을 가지고 있다면?\n\\(\\beta_1 + \\beta_3 = 5\\)만 된다면 원래 모형에 근사하다고 나올 터, 심지어 음수가 나올 때 조차도.\n\n다중공선성의 특징\n\n추정하는 \\(\\beta\\)가 어떤 값인지 거의 예측이 안 된다.\n\n\\(\\beta\\)들의 분산이 크다. (다양한 값 나오고 그러니..)\n\n\\(\\beta\\)는 그래도 더하면 원래 값에 근사함.\n\n\n모두 참이라고 생각되는 모형 \\(\\to\\) 합은 일단 원히던 5임\n\n조건) 다중공선성 없는 \\(x_{i2}\\)는 \\(y\\)쪽으로 옮긴 상황. 즉 신경 안 써도 됌\n\n\n\\(\\beta_1=2,\\beta_2=3\\)\n\\(\\beta_1=5,\\beta_2=0\\)\n\\(\\beta_1=10,\\beta_2=-5\\)\n\\(\\beta_1=10000,\\beta_2=-9995\\)\n\n\\(\\dots\\)\n-> 음수 있는 해석 불가한 이상한 모형들 다 가능하겠다. 해석 불가능."
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html",
    "title": "Support Vector Machine",
    "section": "",
    "text": "Support Vector Machine\nReference : 핸즈 온 머신러닝, 머신러닝 도감, 사이킷런 홈페이지\n마진이 아래 식으로 구해지는 이유\n어딘가의 SVM lecture 노트임!\n힌지 손실 Hinge Loss"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#하드-마진-분류",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#하드-마진-분류",
    "title": "Support Vector Machine",
    "section": "1) 하드 마진 분류",
    "text": "1) 하드 마진 분류\n\n클래스 분류의 경계에 샘플이 없을 때 = 마진 오류가 하나도 없을때, 하드 마진 분류\n\n선형적으로 구분되어야 함\n이상치에 민감함\n\n\n이 때 생기는 이상치를 마진 오류로 본다.\n\n이상치 \\(\\to\\) 클래스 구분했을때 그어지는 선에 걸쳐지는 값들 \\(\\to\\) 마진 오류 margin violation\n\n적절한 균형 잡는 것이 필요\n\\(minimize_{w,b} ||w||^2_2\\)\nsubject to \\(y_i(w^\\top x_i - b) \\ge 1 \\forall_i \\in \\{1, \\dots , n \\}\\)\n\n거의 불가능..한 자료 구조.."
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#소프트-마진-분류",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#소프트-마진-분류",
    "title": "Support Vector Machine",
    "section": "2) 소프트 마진 분류",
    "text": "2) 소프트 마진 분류\n소프트 마진 분류 필요한 이유.\n\nhyper parameter C가 너무 크면 과대적합 가능성 존재\n\n손실함수는 힌지 손실 Hinge Loss\n\n주로 SVM과 함께 쓰임. \\(max(0,1-y), max(0,1+y)\\) 로 \\(y\\)의 범위가 [-1,1]에 오도록 함\n\n\\(minimize_{w,b,\\zeta} ||w||^2_2 + C\\sum^n_{i=1} \\zeta_i\\)\nsubject to \\(y_i(w^\\top x_i - b) \\ge 1-\\zeta_i, \\zeta_i \\ge 0 \\forall_i \\in \\{ 1 ,\\dots, n\\}\\)\n\n\\(1-\\zeta_i\\)로 잡음으로써 더 유연하게 이상치 정의 및 경계선 정의\n위처럼 선에 걸친게 마진 오류 margin violation"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#다항식-커널",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#다항식-커널",
    "title": "Support Vector Machine",
    "section": "1) 다항식 커널",
    "text": "1) 다항식 커널\n\n다항식은 잘 작동하기는 한데 설명변수가 너무 많아지면 오히려 모델을 느리게 만들어서 안 좋아.."
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#유사도-특성",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#유사도-특성",
    "title": "Support Vector Machine",
    "section": "2) 유사도 특성",
    "text": "2) 유사도 특성\n\n특정 랜드마크와 얼마나 닮았는지 유사도 함수 similarity function = similarity measure = similarity metric 로 계산1\n\n특정 랜드마크 -> 데이터 분포가 있을떄 뽑아진 임의의 점?\n\n\n유사도 함수의 종류\n\n유클리디안 거리 Euclidean Distance ; 모든 속성 고려\n코사인 유사도 Cosine Similarity ; 각으로 고려; -1~1사이\n마할라라비스 거리 ; 값들 사이의 공분산 이용\n민코스키 거리 Minkowski Distance ; 가장 큰 값만 고려"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#가우시안-rbf-커널",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#가우시안-rbf-커널",
    "title": "Support Vector Machine",
    "section": "3) 가우시안 RBF 커널2",
    "text": "3) 가우시안 RBF 커널2\n\nGaussian Radical Basis Function = Gaussian Kernel"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#계산-복잡도",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#계산-복잡도",
    "title": "Support Vector Machine",
    "section": "4) 계산 복잡도",
    "text": "4) 계산 복잡도\n\\(O(m\\times n)\\)\ncomputational complexity theory"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#결정-함수와-예측",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#결정-함수와-예측",
    "title": "Support Vector Machine",
    "section": "1) 결정 함수와 예측",
    "text": "1) 결정 함수와 예측\n선형 SVM을 훈련한다 = 마진 오류를 하나도 발생하지 않거나(하드 마진) 제한적인 마진 오류를 가지면서(소프트 마진) 가능한 한 마진을 크게 하는 \\(\\mathbb{w}\\)와 \\(b\\)를 찾는 것\n단순히 선형 회귀식 계산해서 0인지 1인지를 나눈다.\n\\(\\hat{y} = \\begin{cases} 0 & \\mathbb{w}^\\top \\mathbb{x} + b < 0 \\text{ 일 때} \\\\ 1 & \\mathbb{w}^\\top \\mathbb{x} + b \\ge 0 \\text{ 일 때} \\end{cases}\\)"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#목적-함수",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#목적-함수",
    "title": "Support Vector Machine",
    "section": "2) 목적 함수",
    "text": "2) 목적 함수\n결정 함수의 기울기는 가중치 벡터의 노름 \\(||w||\\)와 같다.\n\n마진 크게 = \\(||w||\\) 최소화\n\n하드 마진 선형 SVM 분류기의 목적 함수\n\\(minimize_{w,b} \\frac{1}{2} \\mathbb{w^\\top w}\\)\n\\(\\star\\) 왜 \\(\\frac{1}{2} \\mathbb{w^\\top w}\\)? \\(||w||\\)는 \\(w=0\\)에서 미분도 되지 않음\n소프트 마진 선형 SVM 분류기의 목적 함수\n\n슬랙 변수 slack variable \\(\\zeta^{(i)} \\ge 0\\) 도입\n\ni번째 샘플이 얼마나 마진을 위반할지 정함\n\\(x > b\\)\n\\(x = b + slack\\)\n\n\n-> 슬랙변수는 경계를 나눌때 생기는 최소한의 오차로 생각하자\n마진 오류 최소화 방법\n\n슬랙 변수 값을 작게 만들기\n마진 크게 하기 위해 \\(\\frac{1}{2} \\mathbb{w^\\top w}\\) 최소화\n\n\\(minimize_{w,b,\\zeta} \\frac{1}{2} \\mathbb{w^\\top w} + C \\sum^m_{i=1}\\zeta^{(i)}\\)\n\\(\\star\\) hyper parameter \\(C\\) \\(\\to\\) 두 목표object 사이의 트레이드 오프 정의"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#콰드라틱-프로그래밍",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#콰드라틱-프로그래밍",
    "title": "Support Vector Machine",
    "section": "3) 콰드라틱 프로그래밍",
    "text": "3) 콰드라틱 프로그래밍\n콰드라틱 프로그래밍 Quadratic Programming, QP = 하드 마진과 소프트 마진 문제는 모두 선형적인 제약 조건이 있는 볼록 함수의 이차 최적화 문제"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#쌍대-문제",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#쌍대-문제",
    "title": "Support Vector Machine",
    "section": "4) 쌍대 문제",
    "text": "4) 쌍대 문제"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#커널-svm",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#커널-svm",
    "title": "Support Vector Machine",
    "section": "5) 커널 SVM",
    "text": "5) 커널 SVM\n원래 선형 분리할 수 없는 비선형 데이터를 커널을 통해서 선형화 시킴!\n\\(\\star\\) 가우시안 커널이 복잡한 결정 경게를 학습한다고 데이터에 무조건 비선형 커널 기법을 적용하지 말고(분석에 의미가 없을 수 있음), 선형 커널을 이용한 분석으로 확린 후 커널 함수를 적용한 분석을 하는 것이 좋음\n\n\n\nimage\n\n\n커널의 종류\n\n선형 커널 linear kernel 예시, 선형, 비선형 포함\n\n\\(<x,x'>\\)\n\n시그모이드 커널 sigmoid kernel\n\n\\(\\tanh(\\gamma<x,x'>+r)\\)\nwhere \\(r\\) is specified by coef0.\n\n다항 커널 polynomial kernel\n\n\\((\\gamma<x,x'> + r)^d\\)\nwhere \\(d\\) is specified by parameter degree, \\(r\\) by coef0.\n\nRBF 커널 = 가우시안 커널 rbf kernel\n\n\\(exp(-\\gamma||x - x'||^2)\\)\nwhere \\(r\\) is specified by parameter gamma, must be grater than 0\n\n복잡한 결정 경계 다룰때 좋을 듯\n\n\n\n\n\n\nimage.png\n\n\n쓰는 법\nlinear_svc = svm.SVC(kernel='linear')\nlinear_svc.kernel\n선형 커널\nlinear_svc = svm.SVC(kernel='sigmoid')\nlinear_svc.kernel\n시그모이드 커널\nlinear_svc = svm.SVC(kernel='poly')\nlinear_svc.kernel\n다항 커널\nrbf_svc = svm.SVC(kernel='rbf')\nrbf_svc.kernel\nrbf 커널"
  },
  {
    "objectID": "posts/ml_basic/2023-03-23-Support Vector Machine.html#온라인-svm",
    "href": "posts/ml_basic/2023-03-23-Support Vector Machine.html#온라인-svm",
    "title": "Support Vector Machine",
    "section": "6) 온라인 SVM",
    "text": "6) 온라인 SVM"
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Attention Mechanism.html",
    "href": "posts/ml_basic/2023-05-28-Attention Mechanism.html",
    "title": "Attention Mechanism",
    "section": "",
    "text": "Attention Mechanism\nRef: 딥 러닝을 위한 자연어 처리 입문\nRNN에 기반한 seq2seq 모델의 문제점\n\\(\\to\\) 기계 번역 동안 입력 문장이 길면 품질이 떨어지는 현상 발생, 이의 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 들장한 어텐션."
  },
  {
    "objectID": "posts/ml_basic/2023-05-28-Attention Mechanism.html#dot-product-attention",
    "href": "posts/ml_basic/2023-05-28-Attention Mechanism.html#dot-product-attention",
    "title": "Attention Mechanism",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\n\n디코더의 마지막 LSTM 셀이 출력단어를 예측하기 위해 인코더의 모든 입력 단어들의 정보를 다시 참고하고자 함\n이 때, 소프트 맥스를 이용\n\n인코더에 입력된 값들이 출력 단어를 예측할 때 얼마나 도움이 되었는지 수치화한 결과\n다시 디코더로 전달\n디코더가 출력 단어를 더 정확하게 예측하게 도와줌\n\n\n\n1. 어텐션 스코어를 구한다.\n\n인코더의 은닉 시점 time stamp = \\(1,2, \\dots, N\\)\n인코더의 은닉 상태 hidden state = \\(s_t\\)\n\n단, 인코더와 디코더의 은닉 상태는 차원이 같음\n\n\n\n\n\n\n\n\nImportant\n\n\n\n디코더의 현재시점 t에서 필요한 입력값\nseq2seq\n\n이전 시점인 t-1의 은닉 상태\n이전 시점 t-1에서 나온 출력 단어\n\nattention\n\n이전 시점인 t-1의 은닉 상태\n이전 시점 t-1에서 나온 출력 단어\n어텐션 값 \\(a_t\\)\n\n\n\nattention score 어텐션 값\n\n현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 \\(s_t\\)와 얼마나 유사한지를 판단하는 스코어 값\n\n닷 프로덕트 어텐션은 은닉 상태 \\(s_t\\)를 전치(transpose)하고, 각 은닉 상태와 내적(dot product)를 수행하여 스칼라 결과 어텐션 스코어 score 추출\n\n닷 프러덕트 어텐션이라고 부르는 이유\n\n\\[score(s_t, h_i) = s_t^\\top h_i\\]\n\\[e^t = [s_t^\\top h_1, \\dots, s_t^\\top h_N]\\]\n\\(e^t\\)는 \\(s_t\\)와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값\n\n\n2. 소프트맥스 함수를 통해 어텐션 분포를 구한다.\n\n\\(e^t\\)에 소프트맥스 함수 적용해서 합이 1이 되는 확률 분포 도출\n\n각각 값은 어텐션 가중치 Attention weight\n분포는 어텐션 분포 Attention Distribution \\(\\alpha^t\\)\n\n\n\\[\\alpha^t = softmax(e^t)\\]\n\n\n3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구한다.\n\n어텐션의 최종 결과값을 얻기 위해 각 인코더의 은닉상태 \\(s_t\\)와 어텐션 가중치 값attention weight를 곱하고 가중합weighted sum인 어텐션 값attention value \\(a_t\\)을 구한다.\n\n\\[a_t = \\sum^N_{i=1} \\alpha_i^t h_i\\]\n\n\n\n\n\n\nTip\n\n\n\n어텐션 값 attention value \\(a_t\\)는 인코더의 문맥을 포함하고 있다고 하여 컨텐스트 벡터 context vector라고도 부른다.\n단, seq2seq에서는 인코더의 마지막 은닉 상태를 컨텍스트 벡터context vector라고 불렀음.\n\n\n\n\n4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.\n\n어텐션 값\\(a_t\\)와 디코더의 은닉 상태 \\(s_t\\)를 결합concatenate하여 하나의 벡터로 만드는 작업을 수행한다. \\(\\to\\) \\(v_t\\)\n아 \\(v_t\\)를 예측 연산의 입력으로 사용하무로서 인코더로부터 얻은 정보를 활용하여 \\(\\hat{y}\\)를 더 잘 예측할 수 있게 됨 \\(\\to\\) 어텐션 메커니즘의 핵심\n\n\n\n5. 출력층 연산의 입력이 되는 \\(\\tilde{s}_t\\)를 계산한다.\n\n어텐션 값과 디코더의 t시점의 은닉 상태를 연결하고\\(a_t s_t\\) tanh하이퍼볼릭탄젠트 함수를 통과하도록 해서 출력층 연산을 위한 새로운 벡터 \\(\\tilde{s}_t\\)를 얻는다.\n\n\n\n\n\n\n\nNote\n\n\n\nseq2seq\n\n출력층의 입력이 t시점의 은닉 상태인 \\(s_t\\)\n\nattention\n\n출력츨의 입력이 \\(\\tilde{s}_t\\)\n\n\n\n\\[\\tilde{s}_t = tanh(W_c[a_t;s_t] + b_c\\]\n\\(b_c\\)는 편향\n참고: 기계학습 특강 9주차 tanh\n\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ntanh = torch.nn.Tanh()\n\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\nplt.title(\"tanh(x)\", size=15)\n\nText(0.5, 1.0, 'tanh(x)')\n\n\n\n\n\n\n\n6. \\(\\tilde{s}_t\\)를 출력층의 입력으로 사용한다.\n\\[\\hat{y}_t = Softmax(W_y \\tilde{s}_t + b_y)\\]"
  },
  {
    "objectID": "posts/rl/index.html",
    "href": "posts/rl/index.html",
    "title": "Advanced Regression Analysis",
    "section": "",
    "text": "Those are posts of Advanced Regression Analysis."
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html",
    "href": "posts/rl/2023-03-02-graduation_test.html",
    "title": "Advanced Regression Analysis GT",
    "section": "",
    "text": "GT"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-1",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-1",
    "title": "Advanced Regression Analysis GT",
    "section": "(1)",
    "text": "(1)\n\\(\\beta\\)에 대한 최소제곱추정량(\\(LSE\\)) \\(\\hat{\\beta}\\)을 구하시오\nanswer\n\\(\\hat{\\beta} = argmin_{\\beta \\in R} S = \\sum^n_{i=1} (y_i - \\beta x_i)^2 \\to \\frac{\\partial S}{\\partial \\beta}|_{\\beta = \\hat{\\beta}} = 0\\)\n\\(\\frac{\\partial S}{\\partial \\beta} = \\sum^n_{i=1} (-2x_i)(y_i - \\beta x_i) \\to \\sum^n_{i=1} x_i(y_i - \\hat{\\beta}x_i) = 0 \\to \\hat{\\beta} = \\frac{\\sum^n_{i=1} x_iy_i}{\\sum^n_{i=1} x^2_i}\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-2",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-2",
    "title": "Advanced Regression Analysis GT",
    "section": "(2)",
    "text": "(2)\n\\(E(\\hat{\\beta})\\)을 구하시오\nanswer\n\\(\\hat{\\beta} = \\frac{\\sum^n_{i=1} x_i y_i}{\\sum^n_{i=1} x^2_i} = \\sum^n_{i=1} \\frac{x_i}{\\sum^n_{j=1} s^2_j} y_i\\)\n\\(\\to E(\\hat{\\beta})= \\sum^n_{i=1} \\frac{x_i}{\\sum^n_{i=1} x^2_j} E(y_i) = \\sum^n_{i=1}\\frac{x_i}{\\sum^n_{j=1} x^2_j} \\beta x_i = \\beta\\frac{\\sum^n_{i=1} x^2_{i=1} x^2_i}{\\sum^n_{i=1} x^2_i} = \\beta\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-3",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-3",
    "title": "Advanced Regression Analysis GT",
    "section": "(3)",
    "text": "(3)\n\\(Var(\\hat{\\beta})\\)을 구하시오\nanswer\n\\(Var(\\hat{\\beta}) = \\sum^n_{i=1} \\frac{x^2_i}{(\\sum^n_{j=1} x^2_j)^2} Var(y_i) = \\sum^n_{i=1} \\frac{x^2_i}{(\\sum^n_{j=1} x^2_j)^2} \\sigma^2 = \\sigma^2 \\frac{\\sum^n_{i=1} x^2_i}{(\\sum^n_{i=1} x^2_i)^2} = \\frac{\\sigma^2}{\\sum^n_{i=1} x^2_i}\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-5",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-5",
    "title": "Advanced Regression Analysis GT",
    "section": "(1)",
    "text": "(1)\n\\(\\sum^n_{j=1} Var(\\hat{y}_j) = (p+1) \\sigma^2\\)\nanswer\n\\(\\hat{\\bf{y}}= X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y = Hy\\)\n\\(Var(X\\hat{\\beta}) = XVar(\\hat{\\beta})X^\\top\\)\n\\(Var(\\hat{\\beta}) = Var((X^\\top X)^{-1} X^\\top y) = (X^\\top X)^{-1} X^\\top Var(y)X(X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top X(X^\\top X)^{-1} \\sigma^2 = (X^\\top X )^{-1} \\sigma^2\\)\n\\(Var(\\hat{y}) = Var(X\\hat{\\beta}) = X(X^\\top X)^{-1} X^\\top \\sigma^2\\)\n\\(\\to Var(\\hat{y}) = HVar(y) H^\\top = HH\\sigma^2 = H\\sigma^2\\)\n\\(\\star H^\\top = H, H^2 = H, Var(y) = I_n \\sigma^2\\)\n\\(\\sum^{n}_{j=1} Var(\\hat{y}_j) = tr(Var(\\hat{y})) = tr(H\\sigma^2) = \\sigma^2 tr(H) = tr((X^\\top X)^{-1} X^\\top X)\\sigma^2 = tr(I_{p+1})\\sigma^2 = (p+1)\\sigma^2\\)\n\\(\\star tr(X (X^\\top X)^{-1} X^\\top)\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-6",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-6",
    "title": "Advanced Regression Analysis GT",
    "section": "(2)",
    "text": "(2)\n\\(Cov(\\mathbf{e,y}) = \\sigma^2[I_n - X(X^\\top X)^{-1} X^\\top]\\)\nanswer\n\\(\\bf{e} = y - \\hat{y} = y - Hy = (I-H)y\\)\n\\(Cov(\\bf{e},y) = Cov((I-H)y,y) = (I-H)Var(y) = (I-H) \\sigma^2\\)\n\\(\\star Cov(Ax,y) = A Cov(X,Y)\\)\n\\(\\star Var(y) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-7",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-7",
    "title": "Advanced Regression Analysis GT",
    "section": "(3)",
    "text": "(3)\n\\(Cov(\\mathbf{e,\\hat{y}}) = O_n\\)\nanswer\n\\(Cov(e(I - H)\\bf{y},Hy) = (I-H) Cov(y,y)H^\\top = (I-H)H\\sigma^2 = \\mathbb{O}_n \\sigma^2\\)\n\\(\\star Cov(Ax,By) = ACov(X,Y)B^\\top\\)\n\\(\\star H^\\top= H\\)\n\\(\\star H^2 = H\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-8",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-8",
    "title": "Advanced Regression Analysis GT",
    "section": "(4)",
    "text": "(4)\n\\(Cov(\\mathbf{e,\\hat{\\beta}}) = O_{n \\times (p+1)}\\)\nanswer\n\\(Cov((I-H)\\bf{y},(X^\\top X)^{-1} y) = (I-H) Cov(y,y) X (X^\\top X)^{-1} = (I - X(X^\\top X)^{-1} X^\\top ) X (X^\\top X)^{-1} \\sigma^2 = \\{ X(X^\\top X)^{-1} - X(X^\\top X)^{-1} \\} \\sigma^2 = \\mathbb{O}_{n \\times ([+1)}\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-9",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-9",
    "title": "Advanced Regression Analysis GT",
    "section": "(5)",
    "text": "(5)\n\\(Cov(\\mathbf{\\epsilon, \\hat{\\beta}}) = \\sigma^2 X(X^\\top X)^{-1}\\)\nanswer\n\\(\\bf{\\hat{\\beta}} = (X^\\top X)^{-1} X^\\top y = (X^\\top X)^{-1} X^\\top (X\\beta + \\epsilon) = \\beta + (X^\\top X)^{-1} X^\\top \\epsilon\\)\n\\(Cov(\\bf{\\epsilon} , \\beta + (X^\\top X)^{-1} X^\\top \\epsilon ) = Cov(\\epsilon, \\beta) + Cov(\\epsilon, \\epsilon)X(X^\\top X)^{-1} = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\star Cov(\\epsilon, \\beta) = 0\\)\n\\(\\star Cov(\\epsilon, \\epsilon) = I_n \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-10",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-10",
    "title": "Advanced Regression Analysis GT",
    "section": "(6)",
    "text": "(6)\n\\(\\mathbf{e^\\top y} = SSE\\)\nanswer\n\\(\\sum^{n}_{j=1} e_j y_j = \\bf{e^\\top y} = \\{ (I - H)y\\}^\\top y = y^\\top (I-H)y\\)\n\\(\\star \\bf{e}^\\top (e_1, \\dots , e_n) = (y-\\hat{y})^\\top\\)\n\\(\\star \\bf{y}^\\top = (y_1 , \\dots , y_n)\\)\n\\(SSE = \\sum^n_{j=1} (y_i - \\hat{y}_j)^2 = (\\bf{y} - \\hat{y} ) ^\\top ( y - \\hat{y} ) = e^\\top e = \\{ (I - H)y \\}^\\top \\{ (I - H)y \\} = y^\\top (I - H) (I-H)y = y^\\top (I - H)y\\)\n\\(\\star I - H_H+H^2 = I-H, H^2 = H\\)\n\\(\\therefore \\sum^{n}_{j=1} e_j y_j = SSE\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-11",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-11",
    "title": "Advanced Regression Analysis GT",
    "section": "(7)",
    "text": "(7)\n\\(\\mathbf{e^\\top \\hat{y}}=0\\)\nanswer\n\\(\\sum^{n}_{j=1} \\bf{e_j \\hat{y}_j} = e^\\top \\hat{y} = y^\\top (I-H) Hy = y^\\top_{1\\times n} \\mathbb{O}_{n\\times n} y_{n \\times 1} = 0\\)\n\\(\\star H - H^2 = H - H = \\mathbb{O}_{n \\times n}\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-12",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-12",
    "title": "Advanced Regression Analysis GT",
    "section": "(8)",
    "text": "(8)\n\\(E(\\frac{SSE}{n-p-1}) = \\sigma^2\\)\nanswer\n정리 5.1\n\\(y \\sim N(\\mu,V)\\) 이면\n\\(E(y^\\top A y) = tr(AV) + \\mu^\\top A \\mu, cov(y,y^\\top A y) = 2VA\\mu\\)\n\\(\\frac{SSE}{\\sigma^2} = y^\\top \\frac{1}{\\sigma^2}(I-H)y\\)\n\\(B = \\frac{1}{\\sigma^2}(I-H)\\)\n\\(y \\sim N(X\\beta, I \\sigma^2)\\)\n\\(BV = \\frac{1}{\\sigma^2}(I-H)I\\sigma^2 = B, BV=B\\)\n\\(B, BV\\)는 멱등행렬\n\\(tr(BV) = tr(B) = tr(I-H) = tr(I) - tr(H) = n-p-1\\)\n\\(\\star\\) \\(X\\)가 \\(n \\times (p+1)\\) 행렬이고, \\(rank\\)가 \\(p+1\\), \\(\\therefore tr(H) = p+1\\)\n\\(\\mu = X B\\)\n\\(B = \\frac{1}{\\sigma^2}(I-H)\\)\n\\(B\\mu = \\frac{1}{\\sigma^2}(I-H) X \\beta = \\frac{1}{\\sigma^2}(X B - HXB) = 0\\)\n\\(\\star HX = X\\)\n\\(\\therefore \\mu^\\top B \\mu = 0\\)\n\\(E(\\frac{SSE}{\\sigma^2}) = n-p-1\\)\n\\(E(SSE) = (n-p-1)\\sigma^2\\)\n\\(\\therefore E(\\frac{SSE}{(n-p-1)}) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-14",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-14",
    "title": "Advanced Regression Analysis GT",
    "section": "(1)",
    "text": "(1)\n적합결여검정을 위한 가설을 제시하라."
  },
  {
    "objectID": "posts/rl/2023-03-02-graduation_test.html#section-15",
    "href": "posts/rl/2023-03-02-graduation_test.html#section-15",
    "title": "Advanced Regression Analysis GT",
    "section": "(2)",
    "text": "(2)\n적합결여검정 수행을 위한 과정과 결과 추론을 구체적으로 제시하라(계산 X)"
  },
  {
    "objectID": "posts/rl/2022-12-05-rl-CH11.html",
    "href": "posts/rl/2022-12-05-rl-CH11.html",
    "title": "고급회귀분석 실습 CH11",
    "section": "",
    "text": "chapter 11 변수선택"
  },
  {
    "objectID": "posts/rl/2022-12-05-rl-CH11.html#단계적선택법",
    "href": "posts/rl/2022-12-05-rl-CH11.html#단계적선택법",
    "title": "고급회귀분석 실습 CH11",
    "section": "단계적선택법",
    "text": "단계적선택법\n\nm0 = lm(y ~ 1, data = dt)\n\n\nadd1(m0, \n     scope = y ~  x1 + x2 + x3+ x4,\n     test = \"F\")  ## x4추가\n\n\n\nA anova: 5 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA2715.763171.44443       NA          NA\n    x1 11450.07631265.686763.5194712.6025180.0045520446\n    x2 11809.4267 906.336359.1779921.9606050.0006648249\n    x3 1 776.36261939.400569.06740 4.4034170.0597623242\n    x4 11831.8962 883.866958.8516422.7985200.0005762318\n\n\n\n\n\nm1 <- update(m0, ~ . +x4)\n\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.589  -8.228   1.495   4.726  17.524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 117.5679     5.2622  22.342 1.62e-10 ***\nx4           -0.7382     0.1546  -4.775 0.000576 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.964 on 11 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.645 \nF-statistic:  22.8 on 1 and 11 DF,  p-value: 0.0005762\n\n\n\nadd1(m1, \n     scope = y ~  x1 + x2 + x3+ x4,\n     test = \"F\")  ## x1추가\n\n\n\nA anova: 4 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA883.8669258.85164         NA          NA\n    x1 1809.10480 74.7621128.74170108.22390931.105281e-06\n    x2 1 14.98679868.8801360.62933  0.17248396.866842e-01\n    x3 1708.12891175.7380039.85258 40.29458028.375467e-05\n\n\n\n\n\nm2 <- update(m1, ~ . +x1)\n\n\nsummary(m2)  #제거 없음\n\n\nCall:\nlm(formula = y ~ x4 + x1, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0234 -1.4737  0.1371  1.7305  3.7701 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 103.09738    2.12398   48.54 3.32e-13 ***\nx4           -0.61395    0.04864  -12.62 1.81e-07 ***\nx1            1.43996    0.13842   10.40 1.11e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.734 on 10 degrees of freedom\nMultiple R-squared:  0.9725,    Adjusted R-squared:  0.967 \nF-statistic: 176.6 on 2 and 10 DF,  p-value: 1.581e-08\n\n\n\nadd1(m2, \n     scope = y ~  x1 + x2 + x3+ x4,\n     test = \"F\")  ## x2추가\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA      NA74.7621128.74170      NA        NA\n    x2 126.7893847.9727324.973885.0258650.05168735\n    x3 123.9259950.8361225.727554.2358460.06969226\n\n\n\n\n\nm3 <- update(m2, ~ . +x2)\n\n\nsummary(m3)  #x4 제거\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\nm4 <- update(m3, ~ . -x4)\n\n\nadd1(m4, \n     scope = y ~  x1 + x2 + x3+ x4,\n     test = \"F\") #stop\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA      NA57.9044825.41999      NA       NA\n    x3 19.79386948.1106125.011201.8321280.2088895\n    x4 19.93175447.9727324.973881.8632620.2053954\n\n\n\n\n\n#install.packages(\"leaps\")\n\n\nlibrary(leaps)\n\n\nfit<-regsubsets(y~., data=dt, nbest=1,nvmax=4,\n                # method=c(\"exhaustive\",\"backward\", \n                #          \"forward\", \"seqrep\")\n                method='forward',\n                )\n\nfull model, nbest = 1개만 추가 nvmax 최대 4개 포함 가능\n\na <- summary(fit)\n\n\nstr(a)\n\nList of 8\n $ which : logi [1:4, 1:5] TRUE TRUE TRUE TRUE FALSE TRUE ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:5] \"(Intercept)\" \"x1\" \"x2\" \"x3\" ...\n $ rsq   : num [1:4] 0.675 0.972 0.982 0.982\n $ rss   : num [1:4] 883.9 74.8 48 47.9\n $ adjr2 : num [1:4] 0.645 0.967 0.976 0.974\n $ cp    : num [1:4] 138.73 5.5 3.02 5\n $ bic   : num [1:4] -9.46 -39.01 -42.21 -39.68\n $ outmat: chr [1:4, 1:4] \" \" \"*\" \"*\" \"*\" ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1  ( 1 )\" \"2  ( 1 )\" \"3  ( 1 )\" \"4  ( 1 )\"\n  .. ..$ : chr [1:4] \"x1\" \"x2\" \"x3\" \"x4\"\n $ obj   :List of 28\n  ..$ np       : int 5\n  ..$ nrbar    : int 10\n  ..$ d        : num [1:5] 13 3362 390.2 154.7 10.5\n  ..$ rbar     : num [1:10] 30 7.4615 48.1538 11.7692 -0.0863 ...\n  ..$ thetab   : num [1:5] 95.423 -0.738 1.44 0.416 0.102\n  ..$ first    : int 2\n  ..$ last     : int 5\n  ..$ vorder   : int [1:5] 1 5 2 3 4\n  ..$ tol      : num [1:5] 1.80e-09 9.67e-08 2.36e-08 1.19e-07 3.72e-08\n  ..$ rss      : num [1:5] 2715.8 883.9 74.8 48 47.9\n  ..$ bound    : num [1:5] 2715.8 1265.7 57.9 48.1 47.9\n  ..$ nvmax    : int 5\n  ..$ ress     : num [1:5, 1] 2715.8 883.9 74.8 48 47.9\n  ..$ ir       : int 5\n  ..$ nbest    : num 1\n  ..$ lopt     : int [1:15, 1] 1 1 5 1 5 2 1 5 2 3 ...\n  ..$ il       : int 15\n  ..$ ier      : int 0\n  ..$ xnames   : chr [1:5] \"(Intercept)\" \"x1\" \"x2\" \"x3\" ...\n  ..$ method   : chr \"forward\"\n  ..$ force.in : Named logi [1:5] TRUE FALSE FALSE FALSE FALSE\n  .. ..- attr(*, \"names\")= chr [1:5] \"\" \"x1\" \"x2\" \"x3\" ...\n  ..$ force.out: Named logi [1:5] FALSE FALSE FALSE FALSE FALSE\n  .. ..- attr(*, \"names\")= chr [1:5] \"\" \"x1\" \"x2\" \"x3\" ...\n  ..$ sserr    : num 47.9\n  ..$ intercept: logi TRUE\n  ..$ lindep   : logi [1:5] FALSE FALSE FALSE FALSE FALSE\n  ..$ nullrss  : num 2716\n  ..$ nn       : int 13\n  ..$ call     : language regsubsets.formula(y ~ ., data = dt, nbest = 1, nvmax = 4, method = \"forward\",      )\n  ..- attr(*, \"class\")= chr \"regsubsets\"\n - attr(*, \"class\")= chr \"summary.regsubsets\"\n\n\n\nwith(summary(fit),\n     round(cbind(which,rss,rsq,adjr2, cp, bic),3))\n\n\n\nA matrix: 4 × 10 of type dbl\n\n    (Intercept)x1x2x3x4rssrsqadjr2cpbic\n\n\n    110001883.8670.6750.645138.731 -9.463\n    211001 74.7620.9720.967  5.496-39.008\n    311101 47.9730.9820.976  3.018-42.211\n    411111 47.8640.9820.974  5.000-39.675\n\n\n\n\n설명변수 하나 썼을때, 2개 썼을때,,, 선택되는 변수들\n설명변수2개.3개일때 Radj별로 차이 없어서 BIC 기준으로 2번째 꺼 선택\ncp <= p+1(각 줄의 cp가 2,3,4,5보다 커야 함) -> 2번째꺼 선택!\nstep- r 기본 함수\nm은 꼭 full model로 적어주기\n\n###Backward - AIC\nmodel_back = step(m, direction = \"backward\")\n\nStart:  AIC=26.94\ny ~ x1 + x2 + x3 + x4\n\n       Df Sum of Sq    RSS    AIC\n- x3    1    0.1091 47.973 24.974\n- x4    1    0.2470 48.111 25.011\n- x2    1    2.9725 50.836 25.728\n<none>              47.864 26.944\n- x1    1   25.9509 73.815 30.576\n\nStep:  AIC=24.97\ny ~ x1 + x2 + x4\n\n       Df Sum of Sq    RSS    AIC\n<none>               47.97 24.974\n- x4    1      9.93  57.90 25.420\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\nAIC 가장 작게하는 모형 기준으로 정렬되어 있음\n24.97 가장 작아 최종 모형 결정됌\n\nsummary(model_back)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n###Forward - AIC\nmodel_forward = step(\n  m0, \n  scope = y ~  x1 + x2 + x3+ x4, \n  direction = \"forward\")\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n<none>              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq    RSS    AIC\n+ x1    1    809.10  74.76 28.742\n+ x3    1    708.13 175.74 39.853\n<none>              883.87 58.852\n+ x2    1     14.99 868.88 60.629\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq    RSS    AIC\n+ x2    1    26.789 47.973 24.974\n+ x3    1    23.926 50.836 25.728\n<none>              74.762 28.742\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n<none>              47.973 24.974\n+ x3    1   0.10909 47.864 26.944\n\n\n정해진 식에서 설명변수만 추가하고\n\nsummary(model_forward)\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n###Step - AIC\nmodel_step = step(\n  m0, \n  scope =  y ~  x1 + x2 + x3+ x4, \n  direction = \"both\")\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n<none>              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq     RSS    AIC\n+ x1    1    809.10   74.76 28.742\n+ x3    1    708.13  175.74 39.853\n<none>               883.87 58.852\n+ x2    1     14.99  868.88 60.629\n- x4    1   1831.90 2715.76 71.444\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq     RSS    AIC\n+ x2    1     26.79   47.97 24.974\n+ x3    1     23.93   50.84 25.728\n<none>                74.76 28.742\n- x1    1    809.10  883.87 58.852\n- x4    1   1190.92 1265.69 63.519\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n<none>               47.97 24.974\n- x4    1      9.93  57.90 25.420\n+ x3    1      0.11  47.86 26.944\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\n\nsummary(model_step)\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\nround(cor(mtcars),2)\n\n\n\nA matrix: 11 × 11 of type dbl\n\n    mpgcyldisphpdratwtqsecvsamgearcarb\n\n\n    mpg 1.00-0.85-0.85-0.78 0.68-0.87 0.42 0.66 0.60 0.48-0.55\n    cyl-0.85 1.00 0.90 0.83-0.70 0.78-0.59-0.81-0.52-0.49 0.53\n    disp-0.85 0.90 1.00 0.79-0.71 0.89-0.43-0.71-0.59-0.56 0.39\n    hp-0.78 0.83 0.79 1.00-0.45 0.66-0.71-0.72-0.24-0.13 0.75\n    drat 0.68-0.70-0.71-0.45 1.00-0.71 0.09 0.44 0.71 0.70-0.09\n    wt-0.87 0.78 0.89 0.66-0.71 1.00-0.17-0.55-0.69-0.58 0.43\n    qsec 0.42-0.59-0.43-0.71 0.09-0.17 1.00 0.74-0.23-0.21-0.66\n    vs 0.66-0.81-0.71-0.72 0.44-0.55 0.74 1.00 0.17 0.21-0.57\n    am 0.60-0.52-0.59-0.24 0.71-0.69-0.23 0.17 1.00 0.79 0.06\n    gear 0.48-0.49-0.56-0.13 0.70-0.58-0.21 0.21 0.79 1.00 0.27\n    carb-0.55 0.53 0.39 0.75-0.09 0.43-0.66-0.57 0.06 0.27 1.00\n\n\n\n\n\nm_full <- lm(mpg~., mtcars)\n\n\nsummary(m_full)\n\n\nCall:\nlm(formula = mpg ~ ., data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 12.30337   18.71788   0.657   0.5181  \ncyl         -0.11144    1.04502  -0.107   0.9161  \ndisp         0.01334    0.01786   0.747   0.4635  \nhp          -0.02148    0.02177  -0.987   0.3350  \ndrat         0.78711    1.63537   0.481   0.6353  \nwt          -3.71530    1.89441  -1.961   0.0633 .\nqsec         0.82104    0.73084   1.123   0.2739  \nvs           0.31776    2.10451   0.151   0.8814  \nam           2.52023    2.05665   1.225   0.2340  \ngear         0.65541    1.49326   0.439   0.6652  \ncarb        -0.19942    0.82875  -0.241   0.8122  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.65 on 21 degrees of freedom\nMultiple R-squared:  0.869, Adjusted R-squared:  0.8066 \nF-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n\n\n모형은 유의한데 변수들은 유의하지 않아 다중공선성 의심\n\nfit<-regsubsets(mpg~., data=mtcars, nbest=1,nvmax=9,\n                # method=c(\"exhaustive\",\"backward\", \"forward\", \"seqrep\")\n                method='exhaustive',\n)\n\n변수 9개 이용 best 1개만 뽑기\n\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(mpg ~ ., data = mtcars, nbest = 1, nvmax = 9, \n    method = \"exhaustive\", )\n10 Variables  (and intercept)\n     Forced in Forced out\ncyl      FALSE      FALSE\ndisp     FALSE      FALSE\nhp       FALSE      FALSE\ndrat     FALSE      FALSE\nwt       FALSE      FALSE\nqsec     FALSE      FALSE\nvs       FALSE      FALSE\nam       FALSE      FALSE\ngear     FALSE      FALSE\ncarb     FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: exhaustive\n         cyl disp hp  drat wt  qsec vs  am  gear carb\n1  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n2  ( 1 ) \"*\" \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"  \" \" \n3  ( 1 ) \" \" \" \"  \" \" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n4  ( 1 ) \" \" \" \"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n5  ( 1 ) \" \" \"*\"  \"*\" \" \"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n6  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \" \"  \" \" \n7  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \" \" \n8  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \" \" \"*\" \"*\"  \"*\" \n9  ( 1 ) \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"  \"*\" \n\n\n\nwith(summary(fit),\n     round(cbind(which,rss,rsq,adjr2, cp, bic),3))\n\n\n\nA matrix: 9 × 16 of type dbl\n\n    (Intercept)cyldisphpdratwtqsecvsamgearcarbrssrsqadjr2cpbic\n\n\n    110000100000278.3220.7530.74511.627-37.795\n    211000100000191.1720.8300.819 1.219-46.348\n    310000110100169.2860.8500.834 0.103-46.773\n    410010110100160.0660.8580.837 0.790-45.099\n    510110110100153.4380.8640.838 1.846-42.987\n    610111110100150.0930.8670.835 3.370-40.227\n    710111110110148.5280.8680.830 5.147-37.096\n    810111110111147.8430.8690.823 7.050-33.779\n    910111111111147.5740.8690.815 9.011-30.371\n\n\n\n\n3번째 줄이 cp 가장 작음\nrss = sse\n3,4,5 들이 좋은 모형이라 할 수 있다.\nBIC -46.773(3번째)가 가장 큼 절댓값으로\n4번이 좋은 모형으로 본다면?\n\nfit_4 <- lm(mpg~hp+wt+qsec+am, mtcars)\n\n\nsummary(fit_4)\n\n\nCall:\nlm(formula = mpg ~ hp + wt + qsec + am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4975 -1.5902 -0.1122  1.1795  4.5404 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 17.44019    9.31887   1.871  0.07215 . \nhp          -0.01765    0.01415  -1.247  0.22309   \nwt          -3.23810    0.88990  -3.639  0.00114 **\nqsec         0.81060    0.43887   1.847  0.07573 . \nam           2.92550    1.39715   2.094  0.04579 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.435 on 27 degrees of freedom\nMultiple R-squared:  0.8579,    Adjusted R-squared:  0.8368 \nF-statistic: 40.74 on 4 and 27 DF,  p-value: 4.589e-11"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html",
    "href": "posts/rl/2022-11-21-rl-HW3.html",
    "title": "Regression HW 3",
    "section": "",
    "text": "고급회귀분석 과제, CH03,07\n고급회귀분석 세번째 과제입니다.\n제출 기한 : 11월 21일\n제출 방법\n(pdf 아닌 문서는 미제출로 간주)\n주의사항\n예) R에서 lm으로 beta의 추정량을 구하면 안 됨. 수업 시간에 배운 식으로 풀이를 적어야 함.\n************ R을 이용해서 푸는 문제는, R 코드도 같이 업로드."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-1",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-1",
    "title": "Regression HW 3",
    "section": "(1)",
    "text": "(1)\n\\(X^\\top X, X^\\top y, y^\\top y\\)와 \\((X^\\top X)^{-1}\\)을 구하시오.\n\nxx <- t(x) %*% x\n\n\nxx\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    14 62\n    62360\n\n\n\n\n\nxy <- t(x) %*% y\n\n\nxy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1253\n    6714\n\n\n\n\n\nyy <- t(y) %*% y\n\n\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    138197\n\n\n\n\n\nxx_i <- solve(xx)\n\n\nxx_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     0.30100334-0.05183946\n    -0.05183946 0.01170569"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-2",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-2",
    "title": "Regression HW 3",
    "section": "(2)",
    "text": "(2)\n\\(\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y\\)를 구하고, 적합된 회귀선형을 써 보아라.\n\nbetahat <- xx_i %*%  xy\n\n\nbetahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    29.10702\n    13.63712\n\n\n\n\n\\(y = 29.1 + 13.6x\\)\n\ncoef(lm(y~x, dt))\n\n(Intercept)29.1070234113712x13.6371237458194"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-3",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-3",
    "title": "Regression HW 3",
    "section": "(3)",
    "text": "(3)\n\\(\\sigma^2\\)을 MSE 로 추정할 경우, \\(\\hat{\\beta}\\)의 분산-공분산행렬의 추정은 \\(\\hat{Var}(\\hat{\\beta}) = (X^\\top X)^{-1}\\)(MSE)이다. 먼저 분산분석하여 MSE 를 구하고 \\(\\hat{Var}(\\hat{\\beta})\\)을 구하시오\n\nn <- 14\n\n\np <- 1\n\n\nIn = diag(rep(1,n))\n\n\nH = x %*% xx_i %*% t(x)\n\n\\(SSE = y^\\top(I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (In-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    10166.25\n\n\n\n\n\nsst = t(y) %*% y - n * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    26053.5\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15887.25\n\n\n\n\n\nmsr = ssr/p\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15887.25\n\n\n\n\n\nmse = sse/(n-p-1)\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    847.1876\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    18.75293\n\n\n\n\n\nqf(0.95,p,n-p-1)\n\n4.74722534672251\n\n\n분산분석표\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n15887.25\n1\n15887.25\n18.75293\n4.747225\n\n\n잔차\n10166.25\n12\n847.19\n\n\n\n\n계\n26053.5\n13\n\n\n\n\n\n\n\nhat_var_betahat <- mse[,] * xx_i\n\n\nvcov(lm(y~x, dt))\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    (Intercept)x\n\n\n    (Intercept)255.00629-43.917750\n    x-43.91775  9.916911"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-5",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-5",
    "title": "Regression HW 3",
    "section": "(1)",
    "text": "(1)\n선형회귀모형, \\(y_j = \\beta_0 + \\beta_1 x_{1j} + \\beta_2 x_{2j} + \\epsilon_j\\) 가 성립된다고 가정하고 데이터로부터 회귀모형을 추정하시오.\n\nx = matrix(c(rep(1,8),dt$x1, dt$x2), ncol=3)\n\n\ny = dt$y\n\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 3 × 3 of type dbl\n\n       8  1587  474\n    158731574594108\n     474 9410828136\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    82.8749894-0.134598917-0.945973490\n    -0.1345989 0.001242266-0.001887521\n    -0.9459735-0.001887521 0.022285408\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 3 × 1 of type dbl\n\n       901.9\n    179676.4\n     54034.3\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    110959\n\n\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    -554.3112232\n      -0.1797396\n      11.8599927\n\n\n\n\n\\(y = -554.31 - 0.1797 x_1 + 11.86 x_2\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-6",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-6",
    "title": "Regression HW 3",
    "section": "(2)",
    "text": "(2)\n오차분산이 \\(\\sigma^2 = 3\\)이라 하면, \\(Var(\\hat{\\beta}_0), Var(\\hat{\\beta}_1), Var(\\hat{\\beta}_2)\\)와 \\(Cov(\\hat{\\beta}_1, \\hat{\\beta}_2)\\)는 무엇인가?\n\\(var(\\hat{\\beta}) = (x^\\top x)^{-1} \\sigma^2\\)\n\nxx_i * 3\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    248.6249683-0.403796751-2.837920471\n     -0.4037968 0.003726798-0.005662562\n     -2.8379205-0.005662562 0.066856223\n\n\n\n\n\\(var(\\hat{\\beta}_0) = 248.6250\\)\n\\(var(\\hat{\\beta}_1) = 0.0037\\)\n\\(var(\\hat{\\beta}_2) = 0.0669\\)\n\\(cov(\\hat{\\beta}_1, \\hat{\\beta}_2) = -0.0057\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-7",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-7",
    "title": "Regression HW 3",
    "section": "(3)",
    "text": "(3)\n\\(x_1 = 200\\)°C이고 \\(x_2 = 59\\text{psi}\\)에서 평균 제품의 강도의 추정값 \\(\\hat{y}\\)는 얼마인가? 이의 분산을 \\(\\sigma^2 = 3\\)이라 가정하고 구하시오.\n\nnew_dt <- data.frame(x1=200, x2=59)\nnew_dt\n\n\n\nA data.frame: 1 × 2\n\n    x1x2\n    <dbl><dbl>\n\n\n    20059\n\n\n\n\n\npredict(m2, newdata = new_dt)\n\n1: 109.480424963516\n\n\n\nx0 <- c(1,200,59)\nx0\n\n\n120059\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    109.4804\n\n\n\n\n\\(\\hat{\\mu}_0 = 109.4804\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-8",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-8",
    "title": "Regression HW 3",
    "section": "(4)",
    "text": "(4)\n추정된 회귀계수 \\(\\hat{\\beta}_1, \\hat{\\beta}_2\\)의 의미는 무엇인가?\n\\(\\hat{\\beta}_1\\) : 공정압력이 일정하게 유지되었을 때, 공정온도가 1도씨 증가하면 강도는 0.1797만큼 감소한다.\n\\(\\hat{\\beta}_2\\) : 공정온도가 일정하게 유지되었을 때, 공정압력이 1psi 증가하면 강도는 11.8600만큼 증가한다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-9",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-9",
    "title": "Regression HW 3",
    "section": "(5)",
    "text": "(5)\n분산분석표를 작성하고 \\(\\alpha = 0.05\\)로 \\(F\\)-검정을 행하시오.\n\nanova(m2)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11 628.467 628.4670 1.3423710.29894191\n    x216311.7286311.727813.4815050.01441752\n    Residuals52340.884 468.1768       NA        NA\n\n\n\n\n\nI8 = diag(rep(1,8))\nI8\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    10000000\n    01000000\n    00100000\n    00010000\n    00001000\n    00000100\n    00000010\n    00000001\n\n\n\n\n\nH = x %*% xx_i %*% t(x)\nH\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    0.223303342 0.04734782240.0925307250 0.004932881 0.04854185 0.354021685 0.18034566 0.04897604\n    0.047347822 0.78758156140.0003377034 0.178850120 0.18539614 0.121730006-0.28766297-0.03358038\n    0.092530725 0.00033770340.1733021360 0.174906227 0.15025388 0.004944942 0.19895553 0.20476885\n    0.004932881 0.17885012000.1749062270 0.274444297 0.21838554-0.166837528 0.08255641 0.23276205\n    0.048541845 0.18539613810.1502538806 0.218385537 0.18446745-0.053127978 0.08195337 0.18412975\n    0.354021685 0.12173000620.0049449423-0.166837528-0.05312798 0.711046519 0.14493505-0.11671270\n    0.180345664-0.28766297200.1989555317 0.082556415 0.08195337 0.144935052 0.38255762 0.21635932\n    0.048976035-0.03358037940.2047688541 0.232762052 0.18412975-0.116712699 0.21635932 0.26329707\n\n\n\n\n\\(SSE = y^\\top (I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (I8-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2340.884\n\n\n\n\n\nsst = t(y) %*% y - 8 * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    9281.079\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    6940.195\n\n\n\n\n\nmsr = ssr/2\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3470.097\n\n\n\n\n\nmse = sse/5\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    468.1768\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.411938\n\n\n\n\n\nqf(0.95,2,5)\n\n5.78613504334997\n\n\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n6940.195\n2\n3470.097\n7.4119\n5.786135\n\n\n잔차\n2340.884\n5\n468.177\n\n\n\n\n계\n9281.079\n7\n\n\n\n\n\n\n결론 : 검정통계량의 관측값이 기각역에 속하므로 귀무가설 기각. 즉 모형이 유의수준 0.05하에서 유의하다고 할 수 있다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-10",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-10",
    "title": "Regression HW 3",
    "section": "(6)",
    "text": "(6)\n결정계수 \\(R^2\\)을 구하시오.\n\nsummary(m2)$r.squared\n\n0.747778895028607\n\n\n\\(R^2 = \\frac{SSR}{SST}\\)\n\nssr/sst\n# 0.7478\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.7477789"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-11",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-11",
    "title": "Regression HW 3",
    "section": "(7)",
    "text": "(7)\n수정된 결정계수 \\(R^2_{adj}\\) 을 구하시오.\n\nsummary(m2)$adj\n\n0.64689045304005\n\n\n\\(R^2_{adj} = 1-\\frac{SSE(n-p-1)}{SST/(n-1)}\\)\n\n1-(sse/5)/(sst/7)\n# 0.6469\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.6468905"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-12",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-12",
    "title": "Regression HW 3",
    "section": "(8)",
    "text": "(8)\n\\(\\sigma^2\\)의 추정값 MSE 를 구하시오.\n\\(\\sigma^2 = MSE\\)\n\nmse\n##468.17687\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    468.1768"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-13",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-13",
    "title": "Regression HW 3",
    "section": "(9)",
    "text": "(9)\nMSR의 기대값을 \\(\\sigma^2\\)과 \\(\\beta_0, \\beta_1, \\beta_2\\)의 함수로 표시하여라.\n\nI8 <- diag(rep(1,8))\nI8\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    10000000\n    01000000\n    00100000\n    00010000\n    00001000\n    00000100\n    00000010\n    00000001\n\n\n\n\n\none1 <- rep(1,8)\none1\n\n\n11111111\n\n\n\nJn <- one1 %*% t(one1)\nJn\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n\n\n\n\n\nt(x) %*% (I8 - Jn/8) %*% x\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    0  0.000 0.00\n    0923.87578.25\n    0 78.25051.50"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-15",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-15",
    "title": "Regression HW 3",
    "section": "(1)",
    "text": "(1)\n데이터로부터 회귀모형, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\\) 을 구하여라. 그리고 어째서 이 모형이 선택되었는가에 대하여 토의하시오.\n\nx = matrix(c(rep(1,10),dt$x1, dt$x2, dt$x3), ncol=4)\n\n\ny = dt$y\n\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 4 × 4 of type dbl\n\n     10  191  265  638\n    191 3971 504712620\n    265 5047 704917038\n    638126201703843324\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    42.8460778-0.274517258-1.666784957 0.1044984818\n    -0.2745173 0.005399764 0.009802163-0.0013851965\n    -1.6667850 0.009802163 0.067921641-0.0050213139\n     0.1044985-0.001385196-0.005021314 0.0008624387\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 4 × 1 of type dbl\n\n      34.6\n     686.3\n     916.0\n    2249.9\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    121.94\n\n\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 4 × 1 of type dbl\n\n     2.409213010\n     0.069788005\n    -0.024766597\n     0.005864434\n\n\n\n\n\\(\\hat{y} = 2.409 + 0.0698x_1 - 0.0248x_2 + 0.0059x_3\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-16",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-16",
    "title": "Regression HW 3",
    "section": "(2)",
    "text": "(2)\n\\(\\hat{\\beta}_1, \\hat{\\beta}_2\\)와 \\(\\hat{\\beta}_3\\)의 의미는 무엇인가?\n\\(\\hat{\\beta}_1\\) : 작업일수, 작업량이 변하지 않을 때, 평균온도가 1도씨 증가하면 물소비량은 69.8톤 증가한다.\n\\(\\hat{\\beta}_2\\) : 평균온도, 작업량이 변하지 않을 때, 작업일수가 1일 증가하면 물소비량은 24.8톤 감소한다.\n\\(\\hat{\\beta}_3\\) : 평균온도, 작업일수이 변하지 않을 때, 작업량이 1000톤 증가하면 물소비량은 5.9톤 증가한다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-17",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-17",
    "title": "Regression HW 3",
    "section": "(3)",
    "text": "(3)\n\\(Var(\\hat{\\beta}_3)\\)을 구하시오. \\(\\sigma^2\\)을 MSE 로 추정하면 \\(\\hat{Var}(\\hat{\\beta}_3)\\)은 무엇인가?\n\nsummary(m3)$coefficients\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept) 2.4092130101.125953765 2.13970870.076181015\n    x1 0.0697880050.012640155 5.52113530.001485354\n    x2-0.0247665970.044830038-0.55245540.600595338\n    x3 0.0058644340.005051602 1.16090580.289775583\n\n\n\n\n\\(var(\\beta_3) = (x^\\top x)^{-1}_{(4,4)} \\sigma^2\\)\n\nxx_i[4,4]\n## 0.0009*sigma^2\n\n0.000862438702127396\n\n\n\nhat_var_b3 <- xx_i[4,4] * mse\nhat_var_b3\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.4037738\n\n\n\n\n\\(var(\\hat{\\beta}_3) = 0.0000255\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-18",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-18",
    "title": "Regression HW 3",
    "section": "(4)",
    "text": "(4)\n분산분석표를 작성하고, 결정계수 \\(R^2\\)을 구하시오.\n\nanova(m3)\n\n\n\nA anova: 4 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x112.0043158872.00431588767.738585990.0001737553\n    x210.0022730710.002273071 0.076821530.7909535228\n    x310.0398771420.039877142 1.347702340.2897755827\n    Residuals60.1775339000.029588983         NA          NA\n\n\n\n\n\nI10 = diag(rep(1,10))\nI10\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1000000000\n    0100000000\n    0010000000\n    0001000000\n    0000100000\n    0000010000\n    0000001000\n    0000000100\n    0000000010\n    0000000001\n\n\n\n\n\nH = x %*% xx_i %*% t(x)\nH\n\n\n\nA matrix: 10 × 10 of type dbl\n\n     0.479007503-0.005805719-0.08066882-0.01958705 0.26225012 0.09655116-0.23996631 0.10866366 0.28730870 0.11224675\n    -0.005805719 0.186448902 0.16479959 0.27196857 0.01139329 0.10399383 0.06627062 0.18477003-0.01335934 0.02952023\n    -0.080668821 0.164799589 0.33179614 0.23355629 0.24558622-0.08987596 0.23998739 0.04338569-0.01509426-0.07347228\n    -0.019587049 0.271968566 0.23355629 0.48736361 0.01178437 0.05219047-0.10292798 0.28777554-0.11085118-0.11127263\n     0.262250120 0.011393292 0.24558622 0.01178437 0.58743384-0.22034983 0.06947229-0.08527926 0.20954991-0.09184095\n     0.096551163 0.103993827-0.08987596 0.05219047-0.22034983 0.36048096 0.08217297 0.20539705 0.10911272 0.30032662\n    -0.239966310 0.066270622 0.23998739-0.10292798 0.06947229 0.08217297 0.69646084-0.11029224 0.06925883 0.22956358\n     0.108663660 0.184770026 0.04338569 0.28777554-0.08527926 0.20539705-0.11029224 0.27283220 0.01617393 0.07657339\n     0.287308704-0.013359337-0.01509426-0.11085118 0.20954991 0.10911272 0.06925883 0.01617393 0.25886069 0.18903998\n     0.112246749 0.029520231-0.07347228-0.11127263-0.09184095 0.30032662 0.22956358 0.07657339 0.18903998 0.33931531\n\n\n\n\n\\(SSE = y^\\top (I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (I10-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1775339\n\n\n\n\n\nsst = t(y) %*% y - 10 * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.224\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.046466\n\n\n\n\n\nmsr = ssr/3\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.6821554\n\n\n\n\n\nmse = sse/6\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.02958898\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    23.05437\n\n\n\n\n\nqf(0.95,3,6)\n\n4.75706266308941\n\n\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n2.0465\n3\n0.6822\n23.0544\n4.7571\n\n\n잔차\n0.1775\n6\n0.0296\n\n\n\n\n계\n2.2240\n9\n\n\n\n\n\n\n\\(R^2\\)\n\nssr/sst\n#0.9202\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.9201736"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-19",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-19",
    "title": "Regression HW 3",
    "section": "(5)",
    "text": "(5)\n\\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 평균 물소비량을 추정하시오. 이 추정된 소비량의 표준편차의 추정값을 구하시오.\n\nnew_dt <- data.frame(x1=20, x2=27, x3 = 60)\nnew_dt\n\n\n\nA data.frame: 1 × 3\n\n    x1x2x3\n    <dbl><dbl><dbl>\n\n\n    202760\n\n\n\n\n\npredict(m3, newdata = new_dt)\n\n1: 3.48814105556645\n\n\n\nx0 <- c(1,20,27,60)\nx0\n\n\n1202760\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.488141\n\n\n\n\n\\(\\hat{\\mu}_0 = 3.4881\\)\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.005065205\n\n\n\n\n\nsqrt(var_mu0)\n# se(hat mu0) = 0.0712\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.07117026"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-20",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-20",
    "title": "Regression HW 3",
    "section": "(6)",
    "text": "(6)\n\\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 어느 한 달의 물소비량 \\(y_s\\) 를 예측하여라. 이 예측된 물소비량의 표준편차의 추정값을 구하시오.\n\nx0 <- c(1,20,27,60)\nx0\n\n\n1202760\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.488141\n\n\n\n\n\\(\\hat{y}_s = 3.4881\\)\n\nvar_ys <- (1+t(x0) %*% xx_i %*% x0) * mse\nvar_ys\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.03465419\n\n\n\n\n\nsqrt(var_ys)\n# se(hat ys) = 0.1862\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1861564"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-21",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-21",
    "title": "Regression HW 3",
    "section": "(7)",
    "text": "(7)\n\\(y_j = \\beta_0 + \\beta_1 x_{1j} + \\beta_2 x_{2j} + \\epsilon_j\\) 라 가정하고 회귀제곱합 SSR의 기대값을 \\(\\sigma^2\\)과 \\(\\beta_i, i = 0, 1, 2, 3\\)의 함수로 표시하여라.\n\nI10 <- diag(rep(1,10))\nI10\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1000000000\n    0100000000\n    0010000000\n    0001000000\n    0000100000\n    0000010000\n    0000001000\n    0000000100\n    0000000010\n    0000000001\n\n\n\n\n\none1 <- rep(1,10)\none1\n\n\n1111111111\n\n\n\nJn <- one1 %*% t(one1)\nJn\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n\n\n\n\n\nt(x) %*% (I10 - Jn/10) %*% x\n\n\n\nA matrix: 4 × 4 of type dbl\n\n     6.661338e-16 1.160183e-14 1.751377e-144.096723e-14\n    -1.065814e-14 3.229000e+02-1.450000e+014.342000e+02\n    -1.332268e-14-1.450000e+01 2.650000e+011.310000e+02\n    -1.421085e-14 4.342000e+02 1.310000e+022.619600e+03"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-22",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-22",
    "title": "Regression HW 3",
    "section": "(8)",
    "text": "(8)\n\\(\\beta_3\\)의 95% 신뢰구간을 구하시오. 이 신뢰구간의 의미를 해석하시오.\n\nconfint(m3)\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-0.3458966015.16432262\n    x1 0.0388586610.10071735\n    x2-0.1344617480.08492855\n    x3-0.0064963910.01822526\n\n\n\n\n\\(\\beta_3 \\pm t_{0.025}(6) se(\\hat{\\beta}_3)\\)\n\nb[4] - qt(0.975,6) * sqrt(hat_var_b3)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -1.548982\n\n\n\n\n\nb[4] + qt(0.975,6) * sqrt(hat_var_b3)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.56071\n\n\n\n\n\n#(-0.0065, 0.0182)\n\n이 신뢰구간이 \\(\\beta_3\\)을 포함하고 있을 것이라고 95% 확신한다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-23",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-23",
    "title": "Regression HW 3",
    "section": "(9)",
    "text": "(9)\n\\(\\beta_1\\)의 99% 신뢰구간을 구하시오. 이 신뢰구간의 의미를 해석하시오.\n\nconfint(m3, level=0.99)\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    0.5 %99.5 %\n\n\n    (Intercept)-1.765179536.58360555\n    x1 0.022925540.11665047\n    x2-0.190970740.14143754\n    x3-0.012864020.02459289\n\n\n\n\n\\(\\beta_1 \\pm t_{0.005}(6) se(\\hat{\\beta}_1)\\)\n\nhat_var_b1 = xx_i[2,2] * mse\nhat_var_b1\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.0001597735\n\n\n\n\n\nb[2] - qt(0.995,6) * sqrt(hat_var_b1)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.02292554\n\n\n\n\n\nb[2] + qt(0.995,6) * sqrt(hat_var_b1)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1166505\n\n\n\n\n\n#(0.0229, 0.1167)\n\n이 신뢰구간이 \\(\\beta_1\\)을 포함하고 있을 것이라고 99% 확신한다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-24",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-24",
    "title": "Regression HW 3",
    "section": "(10)",
    "text": "(10)\n\\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 평균 물소비량, \\(E(y)\\)의 95% 신뢰구간을 구하시오.\n\npredict(m3, newdata = new_dt, interval = c(\"confidence\"))\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    13.4881413.3139943.662288\n\n\n\n\n\nmu0 - qt(0.975,6) * sqrt(var_mu0)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.313994\n\n\n\n\n\nmu0 + qt(0.975,6) * sqrt(var_mu0)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.662288\n\n\n\n\n\n#(3.3140, 3.6623)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-25",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-25",
    "title": "Regression HW 3",
    "section": "(11)",
    "text": "(11)\n가설 \\(H_0 : \\beta_1 = 0, H_1 : \\beta_1 > 0\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23490 -0.07744 -0.02166  0.08840  0.23442 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  2.409213   1.125954   2.140  0.07618 . \nx1           0.069788   0.012640   5.521  0.00149 **\nx2          -0.024767   0.044830  -0.552  0.60060   \nx3           0.005864   0.005052   1.161  0.28978   \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.172 on 6 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.8803 \nF-statistic: 23.05 on 3 and 6 DF,  p-value: 0.001079\n\n\n검정통계량 : \\(t_0 = \\frac{\\hat{\\beta}_1}{se(\\hat{\\beta}_1)}\\)\n\nt0 <- b[2]/sqrt(hat_var_b1)\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    5.521135\n\n\n\n\n\\(t_0 = 5.5211\\)\n기각역 : \\(t_{0.05}(6) = 1.9432\\)\n\nqt(0.95,6)\n\n1.9431802805153\n\n\n결론 : \\(H_0\\) 기각 가능. 즉 유의수준 5%에서 \\(\\beta_2\\)는 0보다 크다고 할 수 있다.\n\\(T = \\frac{\\hat{\\beta}_i - \\beta^0_i}{\\hat{\\sigma} \\sqrt{c_{ii}}}\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-26",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-26",
    "title": "Regression HW 3",
    "section": "(12)",
    "text": "(12)\n가설 \\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\nlibrary(car) \n\nLoading required package: carData\n\n\n\n\nlinearHypothesis(m3, matrix(c(0,1,-1,0,0,0,1,-1), byrow = T, nrow=2),c(0,0))\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    181.1224764NA       NA      NA         NA\n    260.1775339 20.944942515.967810.003956509\n\n\n\n\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)\n\\(T= (0,1,-1,0) (0,0,1,-1)\\)\n\\(y= \\beta_0 + \\beta_1 x_1 + \\beta_1 x_2 + \\beta_1 x_3 + \\epsilon = \\beta_0 + \\beta_1(x_1+x_2+x_3)\\)\n\nz <- matrix(c(rep(1,10), dt$x1+dt$x2+dt$x3), ncol=2)\nz\n\n\n\nA matrix: 10 × 2 of type dbl\n\n    1101\n    1122\n    1133\n    1142\n    1126\n    1 87\n    1 95\n    1116\n    1 93\n    1 79\n\n\n\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n      10  1094\n    1094123754\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     3.04034002-0.0268769654\n    -0.02687697 0.0002456761\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n      34.6\n    3852.2\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    121.94\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    0.117334910.073997642 0.051297170 0.03272406 0.065742925 0.146226415 0.129716980.08637972 0.133844340 0.162735849\n    0.073997640.139003538 0.173054245 0.20091392 0.151385613 0.030660377 0.055424530.12043042 0.049233491 0.005896226\n    0.051297170.173054245 0.236831761 0.28901336 0.196246069-0.029874214 0.016509430.13826651 0.004913522-0.076257862\n    0.032724060.200913915 0.289013365 0.36109473 0.232950079-0.079402516-0.015330190.15285967-0.031348270-0.143474843\n    0.065742920.151385613 0.196246069 0.23295008 0.167698506 0.008647799 0.041273580.12691627 0.033117138-0.023977987\n    0.146226420.030660377-0.029874214-0.07940252 0.008647799 0.223270440 0.179245280.06367925 0.190251572 0.267295597\n    0.129716980.055424528 0.016509434-0.01533019 0.041273585 0.179245283 0.150943400.07665094 0.158018868 0.207547170\n    0.086379720.120430425 0.138266509 0.15285967 0.126916274 0.063679245 0.076650940.11070165 0.073408019 0.050707547\n    0.133844340.049233491 0.004913522-0.03134827 0.033117138 0.190251572 0.158018870.07340802 0.166077044 0.222484277\n    0.162735850.005896226-0.076257862-0.14347484-0.023977987 0.267295597 0.207547170.05070755 0.222484277 0.327044025\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1.66031840\n    0.01645047\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 10 × 1 of type dbl\n\n    -0.521816038\n     0.232724057\n     0.051768868\n     0.403714623\n    -0.633077830\n     0.008490566\n     0.276886792\n     0.031426887\n    -0.190212264\n     0.340094340\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.122476\n\n\n\n\n\nf0 = ((sse_rm - sse)/(2))/(sse/6)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15.96781\n\n\n\n\n\nqf(0.95,2,6)\n\n5.14325284978472\n\n\n검정통계량 : \\(\\frac{(SSE_{RM} - SSE_{FM})/r}{SSE_{FM}/(n-p-1)}\\)\n관측값 \\(f_0 = 15.968\\)\n기각역 : \\(f_0 > 5.1433\\)\n결론 : 기각할 수 있다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-27",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-27",
    "title": "Regression HW 3",
    "section": "(13)",
    "text": "(13)\n가설 \\(H_0 : \\beta_1 = \\beta_2 + 3\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)\n\\(T= (0,1,-1,0)\\)\n\\(y = \\beta_0 + (\\beta_2+3)x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\) \\(y - 3x_1 = \\beta_0 + \\beta_2(x_1 + x_2) + \\beta_3 x_3 + \\epsilon\\)\n\nyy <- y-3*dt$x1\nyy\n\n\n-27.2-68.1-71.1-79.6-41.9-50.9-62.5-62.4-33-41.7\n\n\n\nz <- matrix(c(rep(1,10), dt$x1+dt$x2,dt$x3), ncol=3)\nz\n\n\n\nA matrix: 10 × 3 of type dbl\n\n    13764\n    15072\n    15380\n    15488\n    14581\n    14245\n    14946\n    14769\n    13954\n    14039\n\n\n\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     10  456  638\n    4562111429658\n    6382965843324\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     6.76054651-0.160413550 0.0102556645\n    -0.16041355 0.005038964-0.0010871974\n     0.01025566-0.001087197 0.0006163093\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% yy\ngammahat\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    77.7140851\n    -3.1683286\n     0.2025345\n\n\n\n\n\ne <- yy - z %*% gammahat\ne\n\n\n\nA matrix: 10 × 1 of type dbl\n\n    -0.6481331\n    -1.9801368\n     2.9045732\n    -4.0473741\n     6.5554097\n    -4.6583347\n     5.7174312\n    -5.1775193\n     1.9138690\n    -0.5797851\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    157.327\n\n\n\n\n\nf0 = ((sse_rm - sse)/(1))/(sse/6)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    5311.082\n\n\n\n\n\nqf(0.95,1,6)\n\n5.9873776072737\n\n\n검정통계량 : \\(\\frac{(SSE_RM - SSE_FM)/r)}{(SSE_FM/(n-p-1))}\\)\n관측값 \\(f_0 = 5311.082\\)\n기각역 : \\(f_0 > 5.9874\\)\n결론 : 기각할 수 있다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-28",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-28",
    "title": "Regression HW 3",
    "section": "(14)",
    "text": "(14)\n\\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 \\(E(y)\\)에 관한 가설 \\(H_0 : E(y) = 3.5, H_1 : E(y) ̸\\neq 3.5\\)를 \\(\\alpha = 0.05\\)로 검정하시오.\nsy 풀이\n\\(T_0 = \\frac{\\hat{y} - E(y)}{\\sqrt{Var{\\hat{y}}}} = \\frac{\\hat{y} - E(Y)}{\\sqrt{x^\\top (X^\\top X)^{-1} x MSE}} \\sim t(n-1)\\)\n\nx_indi <- matrix(c(1,20,27,60),nrow=1,ncol=4)\n\n\nyhat <- x_indi %*% b\nround(yhat,5)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.48814\n\n\n\n\n\\(Var{y} = x_i (X^\\top X)^{-1} x_i^\\top \\sigma^2\\)\n\nvar_y <- ((x_indi %*%solve(t(x) %*% x) %*% t(x_indi) ) * mse)[,]\nround(var_y,5)\n\n0.00507\n\n\n\nt_0 <- (yhat - 3.5)/sqrt(var_y)\nround(t_0[,],5)\n\n-0.16663\n\n\n\nround(-pt(0.025,9),5)\n\n-0.5097\n\n\n\\(F_0=-0.16663\\) > \\(t_{0.025}(9)=-0.5097\\) 로 귀무가설을 기각하지 못하여 \\(E(y) = 3.5\\)라는 결론이 나온다."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-31",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-31",
    "title": "Regression HW 3",
    "section": "(1)",
    "text": "(1)\n\\(Cov(e, y) = \\sigma^2[I_n − X(X^\\top X)^{-1}X^\\top]\\)\n\\(\\bf{e} = y - \\hat{y} = y - Hy = (I-H)y\\)\n\\(Cov(\\bf{e},y) = Cov((I-H)y,y) = (I-H)Var(y) = (I-H) \\sigma^2\\)\n\\(\\star Cov(Ax,y) = A Cov(X,Y)\\)\n\\(\\star Var(y) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-32",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-32",
    "title": "Regression HW 3",
    "section": "(2)",
    "text": "(2)\n\\(Cov(e, \\hat{y}) = \\mathbb{O}_n\\)\n\\(Cov(e(I - H)\\bf{y},Hy) = (I-H) Cov(y,y)H^\\top = (I-H)H\\sigma^2 = \\mathbb{O}_n \\sigma^2\\)\n\\(\\star Cov(Ax,By) = ACov(X,Y)B^\\top\\)\n\\(\\star H^\\top= H\\)\n\\(\\star H^2 = H\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-33",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-33",
    "title": "Regression HW 3",
    "section": "(3)",
    "text": "(3)\n\\(Cov(e, \\hat{\\beta}) = \\mathbb{O}_{n\\times(k+1)}\\)\n\\(Cov((I-H)\\bf{y},(X^\\top X)^{-1} y) = (I-H) Cov(y,y) X (X^\\top X)^{-1} = (I - X(X^\\top X)^{-1} X^\\top ) X (X^\\top X)^{-1} \\sigma^2 = \\{ X(X^\\top X)^{-1} - X(X^\\top X)^{-1} \\} \\sigma^2 = \\mathbb{O}_{n \\times ([+1)}\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-34",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-34",
    "title": "Regression HW 3",
    "section": "(4)",
    "text": "(4)\n\\(Cov(\\epsilon, \\hat{\\beta}) = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\bf{\\hat{\\beta}} = (X^\\top X)^{-1} X^\\top y = (X^\\top X)^{-1} X^\\top (X\\beta + \\epsilon) = \\beta + (X^\\top X)^{-1} X^\\top \\epsilon\\)\n\\(Cov(\\bf{\\epsilon} , \\beta + (X^\\top X)^{-1} X^\\top \\epsilon ) = Cov(\\epsilon, \\beta) + Cov(\\epsilon, \\epsilon)X(X^\\top X)^{-1} = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\star Cov(\\epsilon, \\beta) = 0\\)\n\\(\\star Cov(\\epsilon, \\epsilon) = I_n \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-35",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-35",
    "title": "Regression HW 3",
    "section": "(5)",
    "text": "(5)\n\\(\\sum^{n}_{j=1} e_j y_j = SSE\\)\n\\(\\sum^{n}_{j=1} e_j y_j = \\bf{e^\\top y} = \\{ (I - H)y\\}^\\top y = y^\\top (I-H)y\\)\n\\(\\star \\bf{e}^\\top (e_1, \\dots , e_n) = (y-\\hat{y})^\\top\\)\n\\(\\star \\bf{y}^\\top = (y_1 , \\dots , y_n)\\)\n\\(SSE = \\sum^n_{j=1} (y_i - \\hat{y}_j)^2 = (\\bf{y} - \\hat{y} ) ^\\top ( y - \\hat{y} ) = e^\\top e = \\{ (I - H)y \\}^\\top \\{ (I - H)y \\} = y^\\top (I - H) (I-H)y = y^\\top (I - H)y\\)\n\\(\\star I - H_H+H^2 = I-H, H^2 = H\\)\n\\(\\therefore \\sum^{n}_{j=1} e_j y_j = SSE\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-36",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-36",
    "title": "Regression HW 3",
    "section": "(6)",
    "text": "(6)\n\\(\\sum^{n}_{j=1} e_j \\hat{y}_j = 0\\)\n\\(\\sum^{n}_{j=1} \\bf{e_j \\hat{y}_j} = e^\\top \\hat{y} = y^\\top (I-H) Hy = y^\\top_{1\\times n} \\mathbb{O}_{n\\times n} y_{n \\times 1} = 0\\)\n\\(\\star H - H^2 = H - H = \\mathbb{O}_{n \\times n}\\)"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-37",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-37",
    "title": "Regression HW 3",
    "section": "(1)",
    "text": "(1)\n\\(X^\\top X, X^\\top y y^\\top y\\)와 \\((X^\\top X)^{-1}\\) 을 구하시오.\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     61310\n    133323\n    102320\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     1.5232558-0.34883721-0.36046512\n    -0.3488372 0.23255814-0.09302326\n    -0.3604651-0.09302326 0.33720930\n\n\n\n\n\nxx_i2 <- round(xx_i,2)\nxx_i2\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     1.52-0.35-0.36\n    -0.35 0.23-0.09\n    -0.36-0.09 0.34\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    13\n    32\n    15\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-38",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-38",
    "title": "Regression HW 3",
    "section": "(2)",
    "text": "(2)\n\\(\\beta_0, \\beta_1, \\beta_2, \\sigma^2\\)를 추정하시오.\n\ncoef(m)\n\n(Intercept)3.23255813953489x11.51162790697674x2-2.6046511627907\n\n\n\nanova(m)$Mean[3]\n\n2.55813953488372\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 3 × 1 of type dbl\n\n     3.232558\n     1.511628\n    -2.604651\n\n\n\n\n\nb1 <- round(round(xx_i,2)%*% xy,2)\nb1\n\n\n\nA matrix: 3 × 1 of type dbl\n\n     3.16\n     1.46\n    -2.46\n\n\n\n\n\nhaty <- x %*% b\nhaty\n\n\n\nA matrix: 6 × 1 of type dbl\n\n     2.13953488\n     3.65116279\n    -0.46511628\n     5.16279070\n     2.55813953\n    -0.04651163\n\n\n\n\n\nhaty_2 <- round(x %*% b1,2)\nhaty_2\n\n\n\nA matrix: 6 × 1 of type dbl\n\n     2.16\n     3.62\n    -0.30\n     5.08\n     2.62\n     0.16\n\n\n\n\n\ne <- y - haty\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.1395349\n     1.3488372\n     0.4651163\n    -1.1627907\n     1.4418605\n    -0.9534884\n\n\n\n\n\ne_2 <- y - haty_2\ne_2\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.16\n     1.38\n     0.30\n    -1.08\n     1.38\n    -1.16\n\n\n\n\n\nsse <- t(e) %*% e\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.674419\n\n\n\n\n\nsse_2 <- t(e_2) %*% e_2\nsse_2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.7564\n\n\n\n\n\nmse <- sse / 3\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.55814\n\n\n\n\n\nmse_2 <- sse_2 / 3\nmse_2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.585467"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-39",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-39",
    "title": "Regression HW 3",
    "section": "(3)",
    "text": "(3)\n\\(x_1 = x_2 = 2\\)에서 \\(E(y)\\)의 95% 신뢰구간을 구하시오.\n\nnew_dt <- data.frame(x1=2, x2=2)\nnew_dt\n\n\n\nA data.frame: 1 × 2\n\n    x1x2\n    <dbl><dbl>\n\n\n    22\n\n\n\n\n\npredict(m, newdata = new_dt,interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    11.046512-1.3459823.439005\n\n\n\n\n\nx0 <- c(1,2,2)\n\n\nmu0 <- x0 %*% b\n\n\nt(x0) %*% xx_i %*% x0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.2209302\n\n\n\n\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.5651704\n\n\n\n\n\ntse <- qt(0.975,3) * sqrt(var_mu0)\ntse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.392494\n\n\n\n\n\nmu0 - (tse)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -1.345982\n\n\n\n\n\nmu0 + (tse)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.439005"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-40",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-40",
    "title": "Regression HW 3",
    "section": "(4)",
    "text": "(4)\n\\(\\beta_1\\)의 90% 신뢰구간을 구하시오.\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n      1       2       3       4       5       6 \n-1.1395  1.3488  0.4651 -1.1628  1.4419 -0.9535 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   3.2326     1.9740   1.638   0.2000  \nx1            1.5116     0.7713   1.960   0.1449  \nx2           -2.6047     0.9288  -2.804   0.0676 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.599 on 3 degrees of freedom\nMultiple R-squared:  0.7511,    Adjusted R-squared:  0.5852 \nF-statistic: 4.527 on 2 and 3 DF,  p-value: 0.1242\n\n\n\nconfint(m, level = 0.9)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)-1.4129961 7.8781124\n    x1-0.3035404 3.3267962\n    x2-4.7904032-0.4188991\n\n\n\n\n\nse_b1 <- sqrt(xx_i[2,2] * mse)\nse_b1\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.7713081\n\n\n\n\n\ntse <- qt(0.95,3) *se_b1\ntse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.815168\n\n\n\n\n\nb[2] - tse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -0.3035404\n\n\n\n\n\nb[2] + tse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.326796"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-41",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-41",
    "title": "Regression HW 3",
    "section": "(5)",
    "text": "(5)\n가설 \\(H_0: \\beta_1 = \\beta_2, H_1: \\beta_1 \\neq \\beta_2\\)을 유의수준 \\(\\alpha = 0.05\\)\n\nlibrary(car)\n\n\nlinearHypothesis(m, c(0,1,-1), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1430.092308NA      NA      NA        NA\n    23 7.674419 122.417898.7633570.05952979\n\n\n\n\n\nz <- matrix(c(1,1,1,1,1,1,x[,2]+x[,3]), ncol=2)\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     623\n    2399\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     1.5230769-0.35384615\n    -0.3538462 0.09230769\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    13\n    47\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 6 × 6 of type dbl\n\n     0.476923083.076923e-013.076923e-010.1384615-0.03076923-2.000000e-01\n     0.307692312.307692e-012.307692e-010.1538462 0.07692308 1.110223e-16\n     0.307692312.307692e-012.307692e-010.1538462 0.07692308 1.110223e-16\n     0.138461541.538462e-011.538462e-010.1692308 0.18461538 2.000000e-01\n    -0.030769237.692308e-027.692308e-020.1846154 0.29230769 4.000000e-01\n    -0.200000001.110223e-161.110223e-160.2000000 0.40000000 6.000000e-01\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n     3.1692308\n    -0.2615385\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.646154\n     2.615385\n    -2.384615\n     1.876923\n     2.138462\n    -2.600000\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    30.09231\n\n\n\n\n\nf0 = (sse_rm - sse)/(sse/3)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    8.763357\n\n\n\n\n\nqf(0.95,1,3)\n\n10.1279644860139\n\n\nT검정 방법\n\nq = c(0,1,-1)\n\n\nvar_qb = t(q) %*% xx_i %*% q*  mse\nvar_qb\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.933478\n\n\n\n\n\nse_qb = sqrt(var_qb)\nse_qb\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.390495\n\n\n\n\n\nt0 = (t(q) %*% b )/se_qb\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.960297\n\n\n\n\n\nt0^2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    8.763357"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-42",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-42",
    "title": "Regression HW 3",
    "section": "(6)",
    "text": "(6)\n가설 \\(H_0 : \\beta_1 = 0, H_1 : \\beta_1 \\neq 0\\)을 유의수준 \\(\\alpha = 0.1\\)로 검정하시오. 위 (3)의 질문의 결과와 어떠한 연관을 지을 수 있는가?\n\nt0 <- b[2] / se_b1\n\n\nqt(0.95,3)\n\n2.35336343480182"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-43",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-43",
    "title": "Regression HW 3",
    "section": "(7)",
    "text": "(7)\n\\(x_1 = 2, x_2 = 3\\)에서 가설 \\(H_0 : E(y) = 4,H_1 : E(y) \\neq 4\\)를 \\(\\alpha = 0.05\\)로 검정하시오\n\nnew_dt <- data.frame(x1=2, x2=2)\n\n\npredict(m, newdata = new_dt,interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    11.046512-1.3459823.439005\n\n\n\n\n\nx0 <- c(1,2,3)\n\n\nmu0 <- x0 %*% b\n\n\nt(x0) %*% xx_i %*% x0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.8139535\n\n\n\n\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.082207\n\n\n\n\n\nt0 <- (mu0-4)/ sqrt(var_mu0)\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -3.851834\n\n\n\n\n\nqt(0.975,3)\n\n3.18244630528371"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-44",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-44",
    "title": "Regression HW 3",
    "section": "(8)",
    "text": "(8)\n가설 \\(H_0 : \\beta_2 = 0, H_1: \\beta_2 <0\\)을 \\(\\alpha = 0.05\\) 로 검정하시오.\n\nse_b2 <- sqrt(xx_i[3,3] * mse)\nse_b2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.9287779\n\n\n\n\n\nt0<- b[3] / se_b2\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -2.804385"
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html#section-45",
    "href": "posts/rl/2022-11-21-rl-HW3.html#section-45",
    "title": "Regression HW 3",
    "section": "(9)",
    "text": "(9)\n가설 $H_0: _1 = 2_2, H_1 : _1 _2 $를 \\(\\alpha=0..05\\)로 검정하시오.\n\nlinearHypothesis(m, c(0,1,-2), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1430.797619NA     NA      NA        NA\n    23 7.674419 123.12329.0390690.05737101\n\n\n\n\n\nz <- matrix(c(1,1,1,1,1,1,2*x[,2]+x[,3]), ncol=2)\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     6 36\n    36244\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     1.4523810-0.21428571\n    -0.2142857 0.03571429\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    13\n    79\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 6 × 6 of type dbl\n\n     0.488095240.27380952 0.380952380.05952381-0.04761905-0.15476190\n     0.273809520.20238095 0.238095240.13095238 0.09523810 0.05952381\n     0.380952380.23809524 0.309523810.09523810 0.02380952-0.04761905\n     0.059523810.13095238 0.095238100.20238095 0.23809524 0.27380952\n    -0.047619050.09523810 0.023809520.23809524 0.30952381 0.38095238\n    -0.154761900.05952381-0.047619050.27380952 0.38095238 0.48809524\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1.95238095\n    0.03571429\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.059524\n     2.869048\n    -2.095238\n     1.797619\n     1.761905\n    -3.273810\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    30.79762\n\n\n\n\n\nf0 = (sse_rm - sse)/(sse/3)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    9.039069\n\n\n\n\n\nqf(0.95,1,3)\n\n10.1279644860139"
  },
  {
    "objectID": "posts/rl/2022-11-23-rl-CH10.html",
    "href": "posts/rl/2022-11-23-rl-CH10.html",
    "title": "고급회귀분석 실습 CH10",
    "section": "",
    "text": "CH10"
  },
  {
    "objectID": "posts/rl/2022-11-23-rl-CH10.html#데이터-입력",
    "href": "posts/rl/2022-11-23-rl-CH10.html#데이터-입력",
    "title": "고급회귀분석 실습 CH10",
    "section": "데이터 입력",
    "text": "데이터 입력\n\ndt <- data.frame(x = c(15,26,10,9,15,20,18,11,\n                       8,20,7,9,10,11,11,10,12,42,17,11,10),\n                 y = c(95,71,83,91,102,87,93,100,\n                       104,94,113,96,83,84,102,100,\n                       105,57,121,86,100))\n\n\n산점도\n\nplot(y~x, dt,pch  = 20,cex  = 2,col  = \"darkorange\")\n\n\n\n\n\n\n회귀적합\n\nmodel_reg <- lm(y~x, dt)\n\n\nsummary(model_reg)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.604  -8.731   1.396   4.523  30.285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 109.8738     5.0678  21.681 7.31e-15 ***\nx            -1.1270     0.3102  -3.633  0.00177 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.02 on 19 degrees of freedom\nMultiple R-squared:   0.41, Adjusted R-squared:  0.3789 \nF-statistic:  13.2 on 1 and 19 DF,  p-value: 0.001769\n\n\n\nplot(y~x, dt,pch  = 20,cex  = 2,col  = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\n\n잔차\n\nresidual <- model_reg$residuals  ## e_i = y_i - hat(y_i)\n## resid(model_reg)\n\n\n\n내적으로 표준화된 잔차\n\ns_residual<- rstandard(model_reg)\n\n\n# # 또는\n# s_xx <- sum((dt$x-mean(dt$x))^2)  #S_xx\n# h_ii <- 1/21 + (dt$x- mean(dt$x))^2/s_xx\n# ## h_ii <- influence(model_reg)$hat\n# hat_sigma <-  summary(model_reg)$sigma   #hat sigma\n# s_residual <- residual/(hat_sigma*sqrt(1-h_ii)) ## 내적\n\n\n\n외적으로 스튜던트화된 잔차\n\ns_residual_i <- rstudent(model_reg) ## 외적으로 스튜던트화 잔차\n\n\n# # 또는\n# hat_sigma_i <- sqrt(((21-1-1)*hat_sigma^2 - residual^2/(1-h_ii) )/(21-1-2))\n# ## hat_sigma_i <- influence(model_reg)$sigma\n# s_residual_i <-  residual/(hat_sigma_i*sqrt(1-h_ii)) ## 외적\n\n\n\n잔차그림\n\npar(mfrow = c(2, 2))\n\nplot(fitted(model_reg), residual, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"Residuals\", \n     main = \"residual plot\")\nabline(h=0, lty=2)\n\nplot(fitted(model_reg), s_residual, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals\", \n     ylim=c(min(-3, min(s_residual)), \n            max(3,max(s_residual))),\n     main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\n\nplot(fitted(model_reg), s_residual_i, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(s_residual_i)), \n            max(3,max(s_residual_i))),\n     main = \"studentized residual plot\")\nabline(h=c(-3,-2,0,2,3), lty=2)\n\nplot(fitted(model_reg), s_residual_i, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(s_residual_i)), \n            max(3,max(s_residual_i))),\n     main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,21-2),0,qt(0.975,21-2)), lty=2)\ntext (fitted(model_reg)[which(abs(s_residual_i)>qt(0.975,21-2))],\n      s_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))], \n      which(abs(s_residual_i)>qt(0.975,21-2)),adj = c(0,1))\n\n\n\n\n\n## 이상치 검정\nqt(0.975,21-2) #기각역 \n\n2.09302405440831\n\n\n\ns_residual_i\n\n10.1839684933793942-0.9415833513782013-1.510811922917994-0.81426336315943850.8328629175207956-0.03063182753708870.31124676473215880.22971574964993190.289910136925676100.617660260595883111.0508471635886512-0.34283148352928113-1.5108119229179914-1.27977575448039150.413153195694502160.127393415386012170.79828114441511618-0.845110861537551193.6069797213043920-1.07648107628971210.127393415386012\n\n\n\ns_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))]\n\n19: 3.60697972130439\n\n\n\n## 정규성 검정\npar(mfrow=c(1,2))\nhist(resid(model_reg),\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals\",\n     col    = \"darkorange\",\n     border = \"dodgerblue\",\n     breaks = 20)\n\n\n\n\n\nqqnorm(resid(model_reg), \n       main = \"Normal Q-Q Plot\", \n       col = \"darkgrey\",\n       pch=16)\nqqline(resid(model_reg), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\ngraphics.off()\n\n\n\n독립성 검정\n\nlmtest::dwtest(model_reg)\n\n\n    Durbin-Watson test\n\ndata:  model_reg\nDW = 2.0844, p-value = 0.5716\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n영향점\n\npar(mfrow=c(1,1))\nplot(y~x, dt,pch  = 20,cex  = 2,col  = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\n# influence(model_reg)\ninfluence.measures(model_reg)\n\nInfluence measures of\n     lm(formula = y ~ x, data = dt) :\n\n     dfb.1_    dfb.x    dffit cov.r   cook.d    hat inf\n1   0.01664  0.00328  0.04127 1.166 8.97e-04 0.0479    \n2   0.18862 -0.33480 -0.40252 1.197 8.15e-02 0.1545    \n3  -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n4  -0.20004  0.12788 -0.22433 1.115 2.56e-02 0.0705    \n5   0.07532  0.01487  0.18686 1.085 1.77e-02 0.0479    \n6   0.00113 -0.00503 -0.00857 1.201 3.88e-05 0.0726    \n7   0.00447  0.03266  0.07722 1.170 3.13e-03 0.0580    \n8   0.04430 -0.02250  0.05630 1.174 1.67e-03 0.0567    \n9   0.07907 -0.05427  0.08541 1.200 3.83e-03 0.0799    \n10 -0.02283  0.10141  0.17284 1.152 1.54e-02 0.0726    \n11  0.31560 -0.22889  0.33200 1.088 5.48e-02 0.0908    \n12 -0.08422  0.05384 -0.09445 1.183 4.68e-03 0.0705    \n13 -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n14 -0.24681  0.12536 -0.31367 0.992 4.76e-02 0.0567    \n15  0.07968 -0.04047  0.10126 1.159 5.36e-03 0.0567    \n16  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n17  0.13328 -0.05493  0.18717 1.096 1.79e-02 0.0521    \n18  0.83112 -1.11275 -1.15578 2.959 6.78e-01 0.6516   *\n19  0.14348  0.27317  0.85374 0.396 2.23e-01 0.0531   *\n20 -0.20761  0.10544 -0.26385 1.043 3.45e-02 0.0567    \n21  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n\n\n\nhatvalues(model_reg)\n\n10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n\n\ndffits(model_reg)  \n\n10.04127403575140562-0.4025206873025253-0.3911400454742154-0.22432853366080450.1868559838824216-0.0085717364067812270.077223952838937980.056303486522047690.085407472693718100.172840518129759110.33199685399425312-0.094449643042361813-0.39114004547421514-0.313673908094842150.101264129345836160.0329813827461469170.18716612805440518-1.15577873097521190.85373710713076620-0.263846244162542210.0329813827461469\n\n\n\ncooks.distance(model_reg)\n\n10.00089740639287069120.081497955150763530.071658144221383340.025615958245264150.017743662633501363.87762740910137e-0570.003130574802994980.0016682085781346990.00383194880672965100.0154395158127621110.0548101351203612120.00467762256482442130.0716581442213833140.0475978118328145150.00536121617564154160.000573584529113046170.017856495213809180.678112028575845190.223288273631179200.0345188940892692210.000573584529113046\n\n\n\ncovratio(model_reg)\n\n11.1658918168321921.1969989767629630.93634739734183941.1151026899392951.0850410825772861.2013199827549771.1701575789867381.1742372676080391.19966823450598101.15209128858604111.08783960928084121.18326164825873130.936347397341839140.992331347870996151.15904532932769161.18673688685713171.09643883044992182.95868271380702190.396431612340971201.04257281407241211.18673688685713\n\n\n\nsummary(influence.measures(model_reg))\n\nPotentially influential observations of\n     lm(formula = y ~ x, data = dt) :\n\n   dfb.1_ dfb.x   dffit   cov.r   cook.d hat    \n18  0.83  -1.11_* -1.16_*  2.96_*  0.68   0.65_*\n19  0.14   0.27    0.85    0.40_*  0.22   0.05  \n\n\n\n\n기각역\n\np <- 1\n\n\nn <- 21\n\n\n2*(p+1)/n #hat (h_ii)\n\n0.19047619047619\n\n\n\n2*sqrt((p+1)/(n-p-1)) #Dffits\n\n0.64888568452305\n\n\n\nqf(0.5, p+1, n-p-1) #Cook d\n\n0.719060569091733\n\n\n\npar(mfrow=c(2,2))\nplot(y~x, dt,pch  = 20,\n     cex  = 2,col  = \"darkorange\",\n     main = \"전체 데이터\")\nabline(model_reg, col='steelblue', lwd=2)\ntext (dt[18:19,],c('18', '19'),adj = c(0,0))\n\n## 18제거 전후 \nplot(y~x, dt,pch  = 20,\n     cex  = 2,col  = \"darkorange\",\n     main = \"18번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-18,]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(18)\"),\n       col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n## 19제거 전후 \nplot(y~x, dt,pch  = 20,\n     cex  = 2,col  = \"darkorange\",\n     main = \"19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-19,]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(19)\"),\n       col=c('steelblue', 'red'), lty=1, lwd=2)\n# not leverage and high influence, outlier\n\n\n## 18, 19제거 전후 \nplot(y~x, dt,pch  = 20,\n     cex  = 2,col  = \"darkorange\",\n     main = \"18,19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-c(18,19),]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(18,19)\"),\n       col=c('steelblue', 'red'), lty=1, lwd=2)\n\n\n\n\n\n\n회귀진단 그림\n\npar(mfrow = c(2, 2))\nplot(model_reg, pch=16)\n\n\n\n\n\n\nHitters\n\nlibrary(ISLR)\n\n\nhitters <- na.omit(Hitters)\n\n\ndim(hitters)\n\n\n26320\n\n\n\nhead(hitters)\n\n\n\nA data.frame: 6 × 20\n\n    AtBatHitsHmRunRunsRBIWalksYearsCAtBatCHitsCHmRunCRunsCRBICWalksLeagueDivisionPutOutsAssistsErrorsSalaryNewLeague\n    <int><int><int><int><int><int><int><int><int><int><int><int><int><fct><fct><int><int><int><dbl><fct>\n\n\n    -Alan Ashby315 81 7243839143449 835 69321414375NW632 4310475.0N\n    -Alvin Davis47913018667276 31624 457 63224266263AW880 8214480.0A\n    -Andre Dawson496141206578371156281575225828838354NE200 11 3500.0N\n    -Andres Galarraga321 8710394230 2 396 101 12 48 46 33NE805 40 4 91.5N\n    -Alfredo Griffin594169 47451351144081133 19501336194AW28242125750.0A\n    -Al Newman185 37 123 821 2 214  42  1 30  9 24NE 76127 7 70.0A\n\n\n\n\n\nreg_model <- lm(Salary ~ AtBat + Hits + HmRun, hitters)\n\n\nsummary(reg_model)\n\n\nCall:\nlm(formula = Salary ~ AtBat + Hits + HmRun, data = hitters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-884.75 -214.97  -58.05  175.88 1991.53 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  158.754     75.815   2.094  0.03724 *  \nAtBat         -1.564      0.641  -2.440  0.01536 *  \nHits           8.329      2.053   4.056 6.61e-05 ***\nHmRun          9.502      3.384   2.808  0.00536 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 398.9 on 259 degrees of freedom\nMultiple R-squared:  0.2271,    Adjusted R-squared:  0.2182 \nF-statistic: 25.37 on 3 and 259 DF,  p-value: 2.013e-14\n\n\n\nvcov(reg_model)\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    (Intercept)AtBatHitsHmRun\n\n\n    (Intercept)5747.87524-26.212753548.15869121.0663196\n    AtBat -26.21275  0.4108477-1.249666-0.4193544\n    Hits  48.15869 -1.2496656 4.216189 0.1404930\n    HmRun  21.06632 -0.4193544 0.14049311.4506363\n\n\n\n\n\npairs(hitters[,c(1,2,3,19)])\n\n\n\n\n\n## 잔차그림 \nresidual <- resid(reg_model)\nstad.res <- rstandard(reg_model)\nstu.res <- rstudent(reg_model)\n\n\npar(mfrow = c(2, 2))\nplot(fitted(reg_model), residual, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"Residuals\", \n     main = \"residual plot\")\nabline(h=0, lty=2)\n\nplot(fitted(reg_model), stad.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals\", \n     ylim=c(min(-3, min(stad.res)), \n            max(3,max(stad.res))),\n     main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\n\nplot(fitted(reg_model), stu.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(stu.res)), \n            max(3,max(stu.res))),\n     main = \"studentized residual plot\")\nabline(h=c(-2,0,2), lty=2)\n\nplot(fitted(reg_model), stu.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(stu.res)), \n            max(3,max(stu.res))),\n     main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,nrow(hitters)-4),0,qt(0.975,nrow(hitters)-4)), lty=2)\ntext (fitted(reg_model)[which(abs(stu.res)>qt(0.975,nrow(hitters)-4))],\n      stu.res[which(abs(stu.res)>qt(0.975,nrow(hitters)-4))], \n      which(abs(stu.res)>qt(0.975,nrow(hitters)-4)),adj = c(0,1))\n\n\n\n\n\nstu.res[abs(stu.res)>qt(0.975,nrow(hitters)-4)]\n\n-Dale Murphy2.74033829619024-Dave Winfield2.88089264730221-Eddie Murray4.31895378037471-George Brett2.05782174934042-Gary Carter3.24866258649007-Joe Carter-2.08071298999844-Jack Clark2.44291700843437-Jim Rice3.5583979165244-Keith Hernandez2.44056603081697-Kirby Puckett-2.29372779175543-Mike Schmidt5.33366401188885-Ozzie Smith3.5948319898282-Rickey Henderson2.20244386003162-Steve Garvey1.97931624811918-Steve Sax-2.30201279255003\n\n\n\nwhich(abs(stu.res)>qt(0.975,nrow(hitters)-4))\n\n-Dale Murphy63-Dave Winfield74-Eddie Murray77-George Brett82-Gary Carter84-Joe Carter110-Jack Clark111-Jim Rice127-Keith Hernandez140-Kirby Puckett148-Mike Schmidt173-Ozzie Smith183-Rickey Henderson200-Steve Garvey226-Steve Sax230\n\n\n\ninfluence(reg_model)\n\n\n\n    $hat\n        -Alan Ashby0.00540428715355245-Alvin Davis0.00589170670784522-Andre Dawson0.00832644882765465-Andres Galarraga0.00543756565246947-Alfredo Griffin0.0246137118709988-Al Newman0.0141225554276349-Argenis Salazar0.010928273983481-Andres Thomas0.00583280157863637-Andre Thornton0.0118104032344835-Alan Trammell0.00994920971431027-Alex Trevino0.0116023974454147-Andy VanSlyke0.00392214855226506-Alan Wiggins0.01133266253963-Bill Almon0.0120577739078933-Buddy Bell0.00926235909199221-Buddy Biancalana0.012342136255686-Bruce Bochy0.0204159574317937-Barry Bonds0.0140367315219278-Bobby Bonilla0.0110632944090634-Bob Brenly0.00872907614618226-Bill Buckner0.0139048245540849-Brett Butler0.023168413560765-Bob Dernier0.0102008268559032-Bo Diaz0.00579243488480162-Bill Doran0.015434760936016-Brian Downing0.00777622895232283-Billy Hatcher0.00709689247443314-Brook Jacoby0.0108511422285614-Bob Kearney0.0108810262666556-Bill Madlock0.00473465384983353-Bob Melvin0.00900012174203842-BillyJo Robidoux0.0133294228418443-Bill Schroeder0.0112763637277587-Chris Bando0.00978612142405515-Chris Brown0.0166260339272529-Carmen Castillo0.0130280396536268-Chili Davis0.00704454319748951-Carlton Fisk0.017822764593868-Curt Ford0.0109708972674672-Carney Lansford0.0107121199405846-Chet Lemon0.004977264802092-Candy Maldonado0.00764992031384351-Carmelo Martinez0.0089685788087143-Craig Reynolds0.00600742640963073-Cal Ripken0.0154842953466146-Cory Snyder0.0142384557435194-Chris Speier0.0176408073638382-Curt Wilkerson0.011580828583756-Dave Anderson0.0114107120327874-Don Baylor0.0352479469918076-Daryl Boston0.0120025169206652-Darnell Coles0.00768936788078936-Dave Concepcion0.00754097071158992-Doug DeCinces0.016164784133267-Darrell Evans0.0264058825441862-Dwight Evans0.0156073759122261-Damaso Garcia0.00737202574736494-Dan Gladden0.00732057564815775-Dave Henderson0.00493766296992215-Donnie Hill0.00802999130926279-Davey Lopes0.00867219904074439-Don Mattingly0.0903700111079589-Dale Murphy0.0205781074488089-Dwayne Murphy0.00499065937964354-Dave Parker0.0233401084346804-Dan Pasqua0.0148260514260301-Darrell Porter0.0212335628824219-Dick Schofield0.00692844758375807-Don Slaught0.00665433551016605-Darryl Strawberry0.017694944503085-Dale Sveum0.00583616542870044-Danny Tartabull0.0128297967927647-Denny Walling0.0124382760330085-Dave Winfield0.0133528039520951-Eric Davis0.0205166608140052-Eddie Milner0.00477555388471096-Eddie Murray0.0126894264569264-Ed Romero0.0120154735656705-Frank White0.0103217756795036-George Bell0.0344024597514498-Glenn Braggs0.0102378206814018-George Brett0.0070249800741032-Greg Brock0.0102483444612376-Gary Carter0.0133493295035423-Glenn Davis0.0233784484234977-Gary Gaetti0.0300641988907722-Greg Gagne0.00756816838324455-George Hendrick0.00993821578501312-Glenn Hubbard0.0144050547662455-Garth Iorg0.00750828700013946-Gary Matthews0.0119812939218614-Graig Nettles0.0135988964718854-Gary Pettis0.0180946787493926-Gary Redus0.00533116613569048-Garry Templeton0.0231472795912073-Greg Walker0.00954059659043919-Gary Ward0.0158776584493562-Glenn Wilson0.0102664940810771-Harold Baines0.0136874329182192-Hubie Brooks0.0245534617574867-Howard Johnson0.0111605903106341-Hal McRae0.00660970775119084-Harold Reynolds0.0266174645810819-Harry Spilman0.0178175584405383-Herm Winningham0.0125072976893885-Jesse Barfield0.0472745129906935-Juan Beniquez0.0098405887950288-John Cangelosi0.0185585370145323-Jose Canseco0.0385966269231462-Joe Carter0.0279123839207128-Jack Clark0.00976153800810466-Jose Cruz0.00621927886532388-Jody Davis0.0125836354713245-Jim Dwyer0.0159318273881519-Julio Franco0.0230480594602062-Jim Gantner0.00960427626759017-Johnny Grubb0.0249337849541658-Jack Howell0.0166783571884679-John Kruk0.0135348618245948-Jeffrey Leonard0.00621692194497029-Jim Morrison0.0104347491092473-John Moses0.00955816927339891-Jerry Mumphrey0.0114527911454454-Jim Presley0.0181732014300399-Johnny Ray0.0222264061248388-Jeff Reed0.0141048942273143-Jim Rice0.0341630071931888-Jerry Royster0.00786584848798418-John Russell0.00732221183848203-Juan Samuel0.011233958379014-John Shelby0.0105769969697364-Joel Skinner0.00820725725265926-Jim Sundberg0.0196190015932163-Jose Uribe0.0243170465990865-Joel Youngblood0.0127513647950321-Kevin Bass0.0219191326187113-Kal Daniels0.0194979541215839-Kirk Gibson0.020556919885839-Ken Griffey0.0157391819211111-Keith Hernandez0.0186125240001884-Kent Hrbek0.0192503945544296-Ken Landreaux0.00738914265279174-Kevin McReynolds0.0156330543753078-Kevin Mitchell0.00623609661997075-Keith Moreland0.0121555120488774-Ken Oberkfell0.0126467584348519-Ken Phelps0.0198333298123843-Kirby Puckett0.0552718094796791-Kurt Stillwell0.011990041252144-Leon Durham0.00780417454171992-Len Dykstra0.00857784296541703-Larry Herndon0.0064990040160584-Lee Lacy0.00742301328937459-Len Matuszek0.0129926810188161-Lloyd Moseby0.0154094589566827-Lance Parrish0.0168397241242445-Larry Parrish0.0195480106203573-Larry Sheets0.0102053644915363-Lou Whitaker0.0103736298972029-Mike Aldrete0.0108976777772544-Marty Barrett0.0291908907582424-Mike Davis0.00667586668426171-Mike Diaz0.0147105547379761-Mariano Duncan0.0115422666932657-Mike Easler0.010820500442813-Mel Hall0.00976796679777078-Mike Heath0.0081507738805316-Mike Kingery0.0111519990806471-Mike LaValliere0.00901026773489625-Mike Marshall0.013711182342755-Mike Pagliarulo0.0255437587312095-Mark Salas0.00824038954487028-Mike Schmidt0.0308861245462707-Mike Scioscia0.00698057071979755-Mickey Tettleton0.0135500510047471-Milt Thompson0.00617205972690294-Mitch Webster0.0169403513647917-Mookie Wilson0.00619444773498657-Marvell Wynne0.0063403620095783-Mike Young0.00474275439485779-Ozzie Guillen0.0271620505926552-Oddibe McDowell0.00964328420757499-Ozzie Smith0.0229502793106078-Ozzie Virgil0.0114067190696189-Phil Bradley0.0168810816259417-Phil Garner0.00535812295602376-Pete Incaviglia0.025161431654974-Paul Molitor0.00570944294085908-Pete Rose0.0126156272171943-Pat Sheridan0.00888970164559414-Pat Tabler0.026279450164693-Rafael Belliard0.0123374671998298-Rick Burleson0.00906906701865493-Randy Bush0.00495753180334833-Rick Cerone0.0104730023394748-Ron Cey0.0113956391454122-Rob Deer0.0395521316143585-Rick Dempsey0.0136529420759308-Ron Hassey0.0158151139459369-Rickey Henderson0.0194628445140604-Reggie Jackson0.00963058080211747-Ron Kittle0.0195094238607815-Ray Knight0.0100975869646124-Rick Leach0.0140249162441515-Rick Manning0.0115898080723435-Rance Mulliniks0.00450212481049989-Ron Oester0.0122552708544235-Rey Quinones0.0128781945206417-Rafael Ramirez0.0155619012974591-Ronn Reynolds0.0174189559060563-Ron Roenicke0.007216482671799-Ryne Sandberg0.0148031952946029-Rafael Santana0.0213487186421553-Rick Schu0.0124262441988659-Ruben Sierra0.00575359430765965-Roy Smalley0.0106124476383773-Robby Thompson0.0140106441432452-Rob Wilfong0.0109282567228258-Robin Yount0.0199323190913311-Steve Balboni0.032832608857617-Scott Bradley0.013819143124654-Sid Bream0.0064848822413407-Steve Buechele0.010047792882046-Shawon Dunston0.0154878353515676-Scott Fletcher0.0231584911833167-Steve Garvey0.0124707214778381-Steve Jeltz0.0294334036298904-Steve Lombardozzi0.0163703558064671-Spike Owen0.0361863260829997-Steve Sax0.056205798190448-Tony Bernazard0.0137201779926164-Tom Brookens0.00842727602684437-Tom Brunansky0.0155535640161956-Tony Fernandez0.0384149427739519-Tim Flannery0.00879815995334561-Tom Foley0.010178072996595-Tony Gwynn0.0441255560927934-Terry Harper0.00728359774271928-Tommy Herr0.0283062395596944-Tim Hulett0.0193407789424842-Terry Kennedy0.0326924785968483-Tito Landrum0.0124610368212244-Tim Laudner0.0137152102414128-Tom Paciorek0.0126750030530696-Tony Pena0.00921261317536243-Terry Pendleton0.0411002808615697-Tony Phillips0.00947681648702826-Terry Puhl0.013480153488892-Ted Simmons0.0183540764261599-Tim Teufel0.00754075413540749-Tim Wallach0.014724129247963-Vince Coleman0.0540676127419561-Von Hayes0.0192039404330803-Vance Law0.0113459171671123-Wally Backman0.022556121420949-Wade Boggs0.0736687853777228-Will Clark0.00548420107775056-Wally Joyner0.01320331340394-Willie McGee0.0115854961579362-Willie Randolph0.011798792067581-Wayne Tolleson0.0136448610296419-Willie Upshaw0.0189725147824934-Willie Wilson0.0210111323582833\n\n    $coefficients\n        \nA matrix: 263 × 4 of type dbl\n\n    (Intercept)AtBatHitsHmRun\n\n\n    -Alan Ashby 0.65924938-4.086660e-04-1.281158e-03-0.0083503671\n    -Alvin Davis 0.19498183-6.704304e-04-2.345890e-04-0.0516648778\n    -Andre Dawson 0.06576750 1.100954e-02-4.016280e-02-0.0970298280\n    -Andres Galarraga-4.21682830 1.761848e-02-3.703466e-02-0.0320576617\n    -Alfredo Griffin-0.77018454 2.388378e-03 9.127765e-03-0.0763167161\n    -Al Newman-2.01620004-2.341329e-03 2.006901e-02 0.0297530788\n    -Argenis Salazar-1.85154500-6.368273e-03 2.098820e-02 0.1194087080\n    -Andres Thomas-2.56590064-5.396923e-03 2.574708e-02 0.0672680286\n    -Andre Thornton 0.10303912 6.698207e-02-2.553939e-01 0.2464785215\n    -Alan Trammell 2.03660813-3.586078e-03-7.094059e-03-0.0733319660\n    -Alex Trevino 3.73771207-1.346635e-02 2.388866e-02-0.0125752009\n    -Andy VanSlyke-0.06284050 1.416493e-04-5.027719e-04-0.0013033606\n    -Alan Wiggins 6.26418239-7.921075e-03 6.495036e-03-0.1867461933\n    -Bill Almon-0.66310602 5.527425e-04 3.398486e-03-0.0058770693\n    -Buddy Bell 0.00952803-1.127743e-05-6.277069e-05-0.0002924174\n    -Buddy Biancalana-1.70759169 3.655244e-03-2.793180e-03 0.0165104010\n    -Bruce Bochy-4.44090060 1.869447e-02-2.742246e-02-0.0687457299\n    -Barry Bonds 0.65494620-4.598393e-02 1.642284e-01-0.0928787697\n    -Bobby Bonilla 0.21192170-2.260935e-02 4.828845e-02 0.2152155106\n    -Bob Brenly-0.27252701 6.233813e-03-1.957537e-02 0.0088001622\n    -Bill Buckner-0.45485652 2.949080e-03-5.416430e-03-0.0026034641\n    -Brett Butler-1.41235128 7.937832e-03 1.993044e-03-0.1295576882\n    -Bob Dernier 2.22713238 3.647360e-02-1.260762e-01-0.1530605674\n    -Bo Diaz-0.26184766 3.227393e-03 1.145628e-03-0.0465005599\n    -Bill Doran-0.02728711 1.626614e-04 5.683137e-05-0.0026675099\n    -Brian Downing-0.91568606 6.678274e-03-1.681666e-02 0.0729610721\n    -Billy Hatcher-0.20437836-1.872251e-02 4.266133e-02 0.1567504397\n    -Brook Jacoby 1.34598144 4.664552e-03-3.759005e-02 0.0063911208\n    -Bob Kearney-0.08820219 1.879015e-04-2.026282e-05-0.0003393462\n    -Bill Madlock 2.18222420-1.384198e-02 4.418954e-02-0.0163762368\n    ⋮⋮⋮⋮⋮\n    -Tony Fernandez 7.06537284 0.0565156366-0.351299510 0.483422041\n    -Tim Flannery-1.01933468 0.0045248195-0.020810894 0.076494461\n    -Tom Foley-1.28244788 0.0034576426-0.008433418 0.038790823\n    -Tony Gwynn 1.25204500 0.0641744467-0.275670947 0.116733745\n    -Terry Harper 0.54502374-0.0013766830 0.001170777 0.002694023\n    -Tommy Herr-5.99483011 0.0763894855-0.160940404-0.493581322\n    -Tim Hulett 3.59601814-0.0625005547 0.192278681-0.029916225\n    -Terry Kennedy26.55838372-0.1158123257 0.201345158 0.122066179\n    -Tito Landrum 1.13024912 0.0015617120-0.012017503-0.016340543\n    -Tim Laudner-1.98257829 0.0061722430-0.004098792-0.038386487\n    -Tom Paciorek-2.77226979 0.0144500766-0.034564760 0.012078742\n    -Tony Pena-0.98487182-0.0136270227 0.095360137-0.171708913\n    -Terry Pendleton 4.54802865-0.0638023359 0.153211680 0.316621222\n    -Tony Phillips 0.05485979-0.0024171333 0.005349894 0.018807172\n    -Terry Puhl13.38730788-0.0374062086 0.043150123-0.043522054\n    -Ted Simmons 6.09400873-0.0236603094 0.037567620 0.027284006\n    -Tim Teufel-0.67151496-0.0001857344 0.003288409 0.014743759\n    -Tim Wallach-1.61211169 0.0356716192-0.116871942 0.063226245\n    -Vince Coleman 4.75964847-0.0674930368 0.167442603 0.305677592\n    -Von Hayes-2.07154896-0.0374915002 0.170440619 0.020979381\n    -Vance Law 0.41809764 0.0241963127-0.078369026-0.080688279\n    -Wally Backman-0.46933529 0.0066426159-0.025722440 0.033033473\n    -Wade Boggs 2.53680391-0.1858604097 0.733772104-0.377519798\n    -Will Clark-2.78654493 0.0285231362-0.100429173 0.023143027\n    -Wally Joyner 4.73172625 0.0304243580-0.159518442-0.218513250\n    -Willie McGee-1.25566058 0.0201839892-0.045081166-0.110406929\n    -Willie Randolph-0.95975323 0.0075263046 0.014433543-0.211613919\n    -Wayne Tolleson 0.39638021-0.0070850671 0.009543190 0.087366853\n    -Willie Upshaw-5.67666776 0.0673426811-0.157285698-0.253588560\n    -Willie Wilson-5.06778235 0.0352735234-0.046913737-0.244613940\n\n\n\n    $sigma\n        -Alan Ashby399.634203569099-Alvin Davis399.492597339658-Andre Dawson399.357259506549-Andres Galarraga398.933958984569-Alfredo Griffin399.628837026824-Al Newman399.589183690645-Argenis Salazar399.45913958176-Andres Thomas399.186969057154-Andre Thornton397.63748684862-Alan Trammell399.305567442193-Alex Trevino399.47898208154-Andy VanSlyke399.654722352462-Alan Wiggins398.809647826131-Bill Almon399.649936999163-Buddy Bell399.656593829134-Buddy Biancalana399.617963490199-Bruce Bochy399.517436655539-Barry Bonds399.117135210789-Bobby Bonilla399.173453123254-Bob Brenly399.638214399165-Bill Buckner399.651742803744-Brett Butler399.574300125236-Bob Dernier398.830912043822-Bo Diaz399.52670812524-Bill Doran399.656542576923-Brian Downing399.435802080694-Billy Hatcher399.058122239888-Brook Jacoby399.469820860983-Bob Kearney399.656489418298-Bill Madlock399.199661955533-Bob Melvin399.466996751965-BillyJo Robidoux399.532114622873-Bill Schroeder399.617758642113-Chris Bando399.648029050811-Chris Brown398.616310438533-Carmen Castillo399.558389784502-Chili Davis399.561670241124-Carlton Fisk398.625143843004-Curt Ford399.431008718328-Carney Lansford398.926230716375-Chet Lemon399.478161122053-Candy Maldonado399.572887009363-Carmelo Martinez399.656443141091-Craig Reynolds399.648479206124-Cal Ripken398.612250898437-Cory Snyder397.956233002957-Chris Speier399.635873837542-Curt Wilkerson399.653274124473-Dave Anderson399.645838956195-Don Baylor399.332313144429-Daryl Boston399.320992119561-Darnell Coles397.824487497975-Dave Concepcion399.641570126188-Doug DeCinces399.539814233217-Darrell Evans399.581917564588-Dwight Evans399.431220069416-Damaso Garcia399.198095338432-Dan Gladden399.361682500835-Dave Henderson399.404765367991-Donnie Hill399.478033157219-Davey Lopes399.648557868608-Don Mattingly397.743646040557-Dale Murphy393.964283305825-Dwayne Murphy399.500396526051-Dave Parker399.565499922729-Dan Pasqua398.677165239333-Darrell Porter399.59464075442-Dick Schofield399.648624041364-Don Slaught399.643928622877-Darryl Strawberry398.303115961143-Dale Sveum399.190292541126-Danny Tartabull397.875762023231-Denny Walling399.624442702131-Dave Winfield393.379419635538-Eric Davis398.765458160105-Eddie Milner399.636464677271-Eddie Murray385.948066373798-Ed Romero399.540856668197-Frank White399.655475127278-George Bell399.628236270768-Glenn Braggs399.429525938891-George Brett396.416592064954-Greg Brock399.644129572991-Gary Carter391.724908196771-Glenn Davis397.825873937425-Gary Gaetti399.629327956353-Greg Gagne399.014594348095-George Hendrick399.441514335659-Glenn Hubbard399.472407530578-Garth Iorg399.654394940912-Gary Matthews399.539923897956-Graig Nettles399.462993329827-Gary Pettis399.584369549925-Gary Redus399.651883210097-Garry Templeton399.185460011545-Greg Walker399.656193673305-Gary Ward399.655966022193-Glenn Wilson399.648238491901-Harold Baines399.628458110819-Hubie Brooks399.631800047401-Howard Johnson399.637790944298-Hal McRae399.645132646166-Harold Reynolds399.438307895792-Harry Spilman399.5700147204-Herm Winningham399.545267959526-Jesse Barfield399.444717016645-Juan Beniquez399.600368849692-John Cangelosi399.346282156929-Jose Canseco398.02468475877-Joe Carter396.345005739808-Jack Clark395.113012819977-Jose Cruz399.530072034124-Jody Davis398.960009857206-Jim Dwyer399.650790727091-Julio Franco399.634941644984-Jim Gantner399.30120909998-Johnny Grubb399.509727897045-Jack Howell399.4451037316-John Kruk398.989538398825-Jeffrey Leonard398.974767418778-Jim Morrison398.505945585891-John Moses399.114195166899-Jerry Mumphrey399.613137347821-Jim Presley397.818000372507-Johnny Ray399.594524245333-Jeff Reed399.515249933965-Jim Rice390.196208465802-Jerry Royster399.603735704486-John Russell399.30666953672-Juan Samuel399.642278659429-John Shelby399.609869907446-Joel Skinner399.437654115013-Jim Sundberg398.584394963321-Jose Uribe399.579000294072-Joel Youngblood399.560223620739-Kevin Bass399.126373002493-Kal Daniels399.12015978781-Kirk Gibson397.975247493125-Ken Griffey399.53237331735-Keith Hernandez395.12160557951-Kent Hrbek398.360454780228-Ken Landreaux398.997937387274-Kevin McReynolds399.358715010895-Kevin Mitchell398.903463941795-Keith Moreland399.00989140517-Ken Oberkfell399.510017390875-Ken Phelps399.330435852697-Kirby Puckett395.642946606445-Kurt Stillwell399.496801962796-Leon Durham398.261636678748-Len Dykstra398.809639978878-Larry Herndon399.54653974752-Lee Lacy399.554332681094-Len Matuszek399.606362918436-Lloyd Moseby399.597604730663-Lance Parrish399.36277824941-Larry Parrish399.500451278616-Larry Sheets398.781597712039-Lou Whitaker399.145067821065-Mike Aldrete399.430603303995-Marty Barrett399.565465636137-Mike Davis399.592637413193-Mike Diaz399.145038102774-Mariano Duncan399.413058360128-Mike Easler399.640085387524-Mel Hall399.498717574299-Mike Heath399.142076130073-Mike Kingery399.369015887524-Mike LaValliere399.451525535306-Mike Marshall399.448864415595-Mike Pagliarulo398.597729793777-Mark Salas399.472597477717-Mike Schmidt379.292250579782-Mike Scioscia398.572806263645-Mickey Tettleton399.527735602914-Milt Thompson399.392147441609-Mitch Webster398.347346283402-Mookie Wilson399.385964043788-Marvell Wynne399.519163457209-Mike Young399.615621049389-Ozzie Guillen399.242206269624-Oddibe McDowell398.425124104216-Ozzie Smith390.008483574748-Ozzie Virgil399.232670321634-Phil Bradley399.640200646453-Phil Garner399.656525133287-Pete Incaviglia398.140184239167-Paul Molitor397.429741229718-Pete Rose398.280795633753-Pat Sheridan399.582502621366-Pat Tabler399.497740921433-Rafael Belliard399.553174081354-Rick Burleson399.65322242696-Randy Bush399.521546684067-Rick Cerone399.62878918092-Ron Cey397.973834039668-Rob Deer398.730746953908-Rick Dempsey399.637209844612-Ron Hassey399.634427225238-Rickey Henderson395.951714392753-Reggie Jackson399.652720587124-Ron Kittle399.652676908848-Ray Knight399.438845758554-Rick Leach399.450896732263-Rick Manning399.642943281909-Rance Mulliniks399.654922439105-Ron Oester399.44241283954-Rey Quinones399.486378350133-Rafael Ramirez398.766407058046-Ronn Reynolds399.653501886057-Ron Roenicke399.544472584493-Ryne Sandberg399.642443719266-Rafael Santana399.65494440862-Rick Schu399.363722079039-Ruben Sierra398.636609618147-Roy Smalley399.518322924908-Robby Thompson398.579898644176-Rob Wilfong399.625113730433-Robin Yount399.428710993818-Steve Balboni398.360706238665-Scott Bradley399.146876060407-Sid Bream398.620806071406-Steve Buechele398.846032824028-Shawon Dunston398.593654297952-Scott Fletcher399.442652563701-Steve Garvey396.65636455604-Steve Jeltz399.58260056726-Steve Lombardozzi399.272349470646-Spike Owen399.656235335859-Steve Sax395.614338602903-Tony Bernazard399.156464351707-Tom Brookens399.649134852872-Tom Brunansky399.408904165723-Tony Fernandez397.817017292335-Tim Flannery399.556718137648-Tom Foley399.616988419874-Tony Gwynn399.184364023896-Terry Harper399.64941906511-Tommy Herr398.657363966685-Tim Hulett399.145382261934-Terry Kennedy396.840786363831-Tito Landrum399.631568221619-Tim Laudner399.609029909848-Tom Paciorek399.564853400074-Tony Pena398.575984199143-Terry Pendleton399.33123547191-Tony Phillips399.651379214157-Terry Puhl397.688749711276-Ted Simmons399.38288948683-Tim Teufel399.640405908538-Tim Wallach399.377418080339-Vince Coleman399.412859480921-Von Hayes398.995072376167-Vance Law399.446049912437-Wally Backman399.646230007602-Wade Boggs398.079495263796-Will Clark398.533481524676-Wally Joyner397.186759775293-Willie McGee399.471318558753-Willie Randolph399.198237604745-Wayne Tolleson399.598450704678-Willie Upshaw398.81428332845-Willie Wilson399.12738883149\n\n    $wt.res\n        -Alan Ashby67.7800562087087-Alvin Davis-183.345293019541-Andre Dawson-247.375182642532-Andres Galarraga-384.813327418291-Alfredo Griffin74.7315286332977-Al Newman-117.07197395248-Argenis Salazar-200.665020800462-Andres Thomas-310.205917546153-Andre Thornton640.648615530166-Alan Trammell-267.656083228693-Alex Trevino190.25352752117-Andy VanSlyke-19.6536365997382-Alan Wiggins415.330079297421-Bill Almon-36.8517287339844-Buddy Bell-1.35241541753995-Buddy Biancalana-88.7106348656366-Bruce Bochy-167.655635510768-Barry Bonds-331.081398558356-Bobby Bonilla-313.806646549598-Bob Brenly61.3099969638083-Bill Buckner31.4371793372293-Brett Butler128.754601606279-Bob Dernier410.323289380803-Bo Diaz163.180113095525-Bill Doran3.49626524769039-Brian Downing212.526647532097-Billy Hatcher-349.932396304933-Brook Jacoby-195.171249913134-Bob Kearney-4.80870487790564-Bill Madlock306.156580178178-Bob Melvin-196.82474768404-BillyJo Robidoux-159.141935211524-Bill Schroeder-88.993441680264-Chris Bando-41.8422889642001-Chris Brown-459.010482523421-Carmen Castillo-141.376931971695-Chili Davis139.416761251418-Carlton Fisk456.781868012672-Curt Ford-214.474379654876-Carney Lansford385.836455594581-Chet Lemon191.330626342117-Candy Maldonado-130.882535697752-Carmelo Martinez-5.71193253559322-Craig Reynolds40.8066844825446-Cal Ripken460.170947533131-Cory Snyder-587.304792470144-Chris Speier-64.8017087996083-Curt Wilkerson-26.0478888596196-Dave Anderson-46.8442929092329-Don Baylor253.953744692514-Daryl Boston-261.440549141057-Darnell Coles-611.603935126123-Dave Concepcion-55.4674758077263-Doug DeCinces153.921018529722-Darrell Evans-122.448748207027-Dwight Evans213.870844722575-Damaso Garcia306.274048898031-Dan Gladden-245.665992427704-Dave Henderson-227.29254394042-Donnie Hill-191.105371028528-Davey Lopes40.554165593647-Don Mattingly598.319277598617-Dale Murphy1068.42965654213-Dwayne Murphy179.014700419564-Dave Parker135.45070641864-Dan Pasqua-445.807053482183-Darrell Porter-111.828849079933-Dick Schofield-40.4225403345766-Don Slaught-50.9535759790935-Darryl Strawberry523.179287889077-Dale Sveum-309.106462763598-Danny Tartabull-601.440367261628-Denny Walling-80.9282759858087-Dave Winfield1125.69219116458-Eric Davis-424.032103920511-Eddie Milner-64.2884097306572-Eddie Murray1656.28214419654-Ed Romero153.55543035292-Frank White-15.1706074829492-George Bell75.1556565287132-Glenn Braggs-215.25760276689-George Brett812.88430477159-Greg Brock-50.4565048468612-Gary Carter1264.0594565789-Glenn Davis-606.520649566213-Gary Gaetti-73.8606997041783-Greg Gagne-362.338576182642-George Hendrick209.531632083408-Glenn Hubbard193.466934012087-Garth Iorg-21.2576387351008-Gary Matthews154.175498873463-Graig Nettles-198.429297664118-Gary Pettis-120.935011745943-Gary Redus-31.1141247211481-Garry Templeton307.986106586873-Greg Walker9.14124868112292-Gary Ward-11.3679009173431-Glenn Wilson-41.3180421898229-Harold Baines75.6599531231978-Hubie Brooks70.6338894140339-Howard Johnson-61.9357958533628-Hal McRae-48.474066324933-Harold Reynolds-209.301620327783-Harry Spilman-132.424978895246-Herm Winningham-150.563795884441-Jesse Barfield204.007229803057-Juan Beniquez-107.15312797439-John Cangelosi-250.565658249054-Jose Canseco-568.233122374574-Joe Carter-813.089352277853-Jack Clark960.505689659938-Jose Cruz161.019061085732-Jody Davis376.459891359261-Jim Dwyer-34.3434710507558-Julio Franco-66.0601688331282-Jim Gantner269.358699127131-Johnny Grubb-171.838022127467-Jack Howell-207.067953446457-John Kruk-368.223751958345-Jeffrey Leonard-373.653104622136-Jim Morrison-484.228919189103-John Moses-332.734871510109-Jerry Mumphrey94.1298253578462-Jim Presley-609.438149799754-Johnny Ray-111.877142843771-Jeff Reed-169.510879992981-Jim Rice1364.55006368818-Jerry Royster-103.999639155523-John Russell-267.590083275267-Juan Samuel-54.0437192880373-John Shelby-97.6467732452258-Joel Skinner-211.587726780924-Jim Sundberg465.279368155062-Jose Uribe-124.950915616423-Joel Youngblood140.070604499256-Kevin Bass-326.921657688736-Kal Daniels-329.236976542924-Kirk Gibson582.14402580365-Ken Griffey158.782267361893-Keith Hernandez955.303999211567-Kent Hrbek511.590308251624-Ken Landreaux367.038202402703-Kevin McReynolds-245.862477419598-Kevin Mitchell-392.683613419869-Keith Moreland362.820757982595-Ken Oberkfell172.746852451584-Ken Phelps-256.714055315183-Kirby Puckett-882.06124667897-Kurt Stillwell-180.42432563195-Leon Durham533.788943932952-Len Dykstra-415.910238696257-Larry Herndon-150.156223654492-Lee Lacy-144.676170013163-Len Matuszek-101.120476392784-Lloyd Moseby109.445769501835-Lance Parrish244.030830326028-Larry Parrish-177.669210096705-Larry Sheets-422.384856127069-Lou Whitaker-323.000076354027-Mike Aldrete-214.674900219887-Marty Barrett-135.06978793439-Mike Davis114.463998835635-Mike Diaz-322.300899557089-Mariano Duncan-222.776983156224-Mike Easler-58.0459834544182-Mel Hall-179.541420331312-Mike Heath324.306227919313-Mike Kingery-242.124936548447-Mike LaValliere-204.694391158619-Mike Marshall205.52862360045-Mike Pagliarulo-460.981610247946-Mark Salas-193.970967587303-Mike Schmidt1991.53081105811-Mike Scioscia470.789169935772-Mickey Tettleton-161.898200722661-Milt Thompson-232.770673155091-Mitch Webster-514.771716718755-Mookie Wilson235.472665331111-Marvell Wynne-167.805169548164-Mike Young-91.7107068988726-Ozzie Guillen-288.259770879492-Oddibe McDowell-501.121239537188-Ozzie Smith1385.83327291096-Ozzie Virgil293.907257636743-Phil Bradley-57.6656386051236-Phil Garner3.99086175422808-Pete Incaviglia-551.609693048556-Paul Molitor674.785595483477-Pete Rose528.830102649674-Pat Sheridan-123.060530334477-Pat Tabler-178.587959125593-Rafael Belliard-145.132631319866-Rick Burleson26.2827152966923-Randy Bush-166.459873267454-Rick Cerone-75.3361148411943-Ron Cey585.105514035373-Rob Deer-427.981756862369-Rick Dempsey62.805798909155-Ron Hassey-67.0869313322284-Rickey Henderson863.533325149634-Reggie Jackson-28.1581764674054-Ron Kittle-28.1745560525503-Ray Knight-210.810121188791-Rick Leach-204.488532219717-Rick Manning52.7655700191927-Rance Mulliniks-18.5732077775455-Ron Oester208.848830444002-Rey Quinones-186.130873802358-Rafael Ramirez424.877109067089-Ronn Reynolds-25.0665888740822-Ron Roenicke-151.504821453376-Ryne Sandberg-53.6343768278081-Rafael Santana-18.2946008972326-Rick Schu-244.184962221945-Ruben Sierra-457.021589556042-Roy Smalley167.955201592851-Robby Thompson-467.584330386837-Rob Wilfong80.1407632780496-Robin Yount214.584722465971-Steve Balboni-507.986296633222-Scott Bradley-321.867266069112-Sid Bream-460.374516763279-Steve Buechele-406.584104350852-Shawon Dunston-464.243720588218-Scott Fletcher-207.576715133793-Steve Garvey780.197594958407-Steve Jeltz-121.697951720275-Steve Lombardozzi-279.11845097516-Spike Owen-8.54596640943839-Steve Sax-884.745594770179-Tony Bernazard-318.843538559565-Tom Brookens-39.0776713143429-Tom Brunansky224.212014148684-Tony Fernandez-603.284068119108-Tim Flannery-142.880059322374-Tom Foley-89.9212735988276-Tony Gwynn-305.015041218915-Terry Harper38.3489586433606-Tommy Herr447.1941065268-Tim Hulett-321.434595545371-Terry Kennedy748.146213034522-Tito Landrum71.4017150776481-Tim Laudner-98.3640208138739-Tom Paciorek-136.670586581832-Tony Pena469.570742925334-Terry Pendleton-253.602476951105-Tony Phillips-32.6650132185081-Terry Puhl631.949441316095-Ted Simmons235.352792139137-Tim Teufel-57.5752552727432-Tim Wallach238.131704064234-Vince Coleman-218.02109227197-Von Hayes365.639256842471-Vance Law207.163816279912-Wally Backman-45.7255454440639-Wade Boggs548.344227020477-Will Clark-479.603323533331-Wally Joyner-707.855886902391-Willie McGee194.315202710728-Willie Randolph305.542963369166-Wayne Tolleson-108.755647406025-Willie Upshaw412.589715777102-Willie Wilson326.760120586328\n\n\n\n\n\ninfluence.measures(reg_model)\n\nInfluence measures of\n     lm(formula = Salary ~ AtBat + Hits + HmRun, data = hitters) :\n\n                      dfb.1_  dfb.AtBt  dfb.Hits  dfb.HmRn     dffit cov.r\n-Alan Ashby         8.68e-03 -6.36e-04 -6.23e-04 -2.46e-03  0.012536 1.021\n-Alvin Davis        2.57e-03 -1.04e-03 -1.14e-04 -1.52e-02 -0.035436 1.018\n-Andre Dawson       8.66e-04  1.72e-02 -1.95e-02 -2.86e-02 -0.056997 1.018\n-Andres Galarraga  -5.56e-02  2.75e-02 -1.80e-02 -9.47e-03 -0.071519 1.006\n-Alfredo Griffin   -1.01e-02  3.72e-03  4.44e-03 -2.25e-02  0.030079 1.041\n-Al Newman         -2.65e-02 -3.65e-03  9.76e-03  8.78e-03 -0.035316 1.029\n-Argenis Salazar   -2.44e-02 -9.92e-03  1.02e-02  3.52e-02 -0.053094 1.023\n-Andres Thomas     -3.38e-02 -8.41e-03  1.25e-02  1.99e-02 -0.059697 1.012\n-Andre Thornton     1.36e-03  1.05e-01 -1.25e-01  7.31e-02  0.177184 0.987\n-Alan Trammell      2.68e-02 -5.59e-03 -3.45e-03 -2.16e-02 -0.067532 1.019\n-Alex Trevino       4.92e-02 -2.10e-02  1.16e-02 -3.71e-03  0.051902 1.024\n-Andy VanSlyke     -8.27e-04  2.21e-04 -2.44e-04 -3.84e-04 -0.003092 1.020\n-Alan Wiggins       8.26e-02 -1.24e-02  3.16e-03 -5.52e-02  0.112136 1.010\n-Bill Almon        -8.73e-03  8.61e-04  1.65e-03 -1.73e-03 -0.010249 1.028\n-Buddy Bell         1.25e-04 -1.76e-05 -3.05e-05 -8.62e-05 -0.000329 1.025\n-Buddy Biancalana  -2.25e-02  5.69e-03 -1.36e-03  4.87e-03 -0.024970 1.027\n-Bruce Bochy       -5.85e-02  2.91e-02 -1.33e-02 -2.03e-02 -0.061210 1.034\n-Barry Bonds        8.63e-03 -7.17e-02  7.99e-02 -2.74e-02 -0.099680 1.019\n-Bobby Bonilla      2.79e-03 -3.52e-02  2.35e-02  6.36e-02 -0.083613 1.017\n-Bob Brenly        -3.59e-03  9.71e-03 -9.52e-03  2.60e-03  0.014460 1.024\n-Bill Buckner      -5.99e-03  4.59e-03 -2.63e-03 -7.68e-04  0.009406 1.030\n-Brett Butler      -1.86e-02  1.24e-02  9.69e-04 -3.82e-02  0.050210 1.038\n-Bob Dernier        2.94e-02  5.69e-02 -6.14e-02 -4.52e-02  0.104980 1.009\n-Bo Diaz           -3.45e-03  5.03e-03  5.57e-04 -1.37e-02  0.031266 1.019\n-Bill Doran        -3.59e-04  2.53e-04  2.76e-05 -7.87e-04  0.001104 1.032\n-Brian Downing     -1.21e-02  1.04e-02 -8.18e-03  2.15e-02  0.047287 1.019\n-Billy Hatcher     -2.69e-03 -2.92e-02  2.08e-02  4.63e-02 -0.074400 1.011\n-Brook Jacoby       1.77e-02  7.27e-03 -1.83e-02  1.89e-03 -0.051453 1.023\n-Bob Kearney       -1.16e-03  2.93e-04 -9.85e-06 -1.00e-04 -0.001269 1.027\n-Bill Madlock       2.88e-02 -2.16e-02  2.15e-02 -4.84e-03  0.053022 1.011\n-Bob Melvin        -2.82e-02 -1.33e-02  2.01e-02  9.44e-03 -0.047168 1.021\n-BillyJo Robidoux  -4.00e-02  5.53e-03  2.48e-03  1.13e-02 -0.046609 1.027\n-Bill Schroeder    -1.80e-02 -2.22e-03  7.72e-03 -2.78e-03 -0.023918 1.026\n-Chris Bando       -8.41e-03  3.17e-03 -2.29e-03  4.15e-03 -0.010460 1.025\n-Chris Brown       -5.22e-02  1.06e-01 -1.23e-01  4.74e-02 -0.150988 1.011\n-Carmen Castillo   -3.90e-02  2.32e-02 -1.46e-02 -9.22e-03 -0.040920 1.027\n-Chili Davis       -8.54e-03  2.74e-03  3.53e-03 -7.85e-03  0.029494 1.021\n-Carlton Fisk      -4.14e-02  1.34e-01 -1.36e-01  3.39e-03  0.155755 1.013\n-Curt Ford         -4.91e-02  1.14e-02 -2.96e-03  1.54e-02 -0.056865 1.022\n-Carney Lansford   -4.05e-02 -5.02e-03  2.46e-02  1.04e-02  0.101187 1.012\n-Chet Lemon         4.79e-03  1.53e-02 -1.64e-02  1.31e-03  0.033959 1.017\n-Candy Maldonado   -4.62e-03 -6.67e-03  1.02e-02 -1.75e-02 -0.028870 1.022\n-Carmelo Martinez  -1.11e-03  1.25e-04  1.93e-04 -3.20e-04 -0.001366 1.025\n-Craig Reynolds     4.84e-03  9.61e-04 -1.67e-03 -2.32e-03  0.007962 1.022\n-Cal Ripken        -6.35e-02 -1.31e-03  2.06e-02  6.02e-02  0.145912 1.010\n-Cory Snyder       -3.92e-02  3.85e-02 -1.48e-02 -1.52e-01 -0.178644 0.996\n-Chris Speier      -2.15e-02  1.29e-02 -8.23e-03 -3.79e-03 -0.021924 1.033\n-Curt Wilkerson    -4.92e-03 -1.98e-04  8.58e-04  3.42e-03 -0.007096 1.027\n-Dave Anderson     -1.04e-02  1.91e-03 -2.65e-04  4.45e-03 -0.012665 1.027\n-Don Baylor        -4.88e-02  7.07e-02 -7.44e-02  7.34e-02  0.123757 1.046\n-Daryl Boston      -6.99e-02  3.31e-02 -1.91e-02 -1.14e-03 -0.072599 1.021\n-Darnell Coles      3.33e-02 -1.21e-02  2.66e-03 -5.92e-02 -0.135855 0.987\n-Dave Concepcion   -7.21e-03  5.03e-04 -2.47e-04  6.59e-03 -0.012144 1.023\n-Doug DeCinces     -1.06e-02  1.46e-02 -1.71e-02  3.55e-02  0.049785 1.030\n-Darrell Evans      1.10e-02 -2.15e-02  2.57e-02 -3.68e-02 -0.051147 1.042\n-Dwight Evans      -1.75e-02  1.92e-02 -2.10e-02  4.68e-02  0.067953 1.027\n-Damaso Garcia      1.25e-02 -9.79e-03  1.97e-02 -4.08e-02  0.066363 1.014\n-Dan Gladden       -2.67e-02  1.19e-02 -1.44e-02  3.06e-02 -0.053020 1.017\n-Dave Henderson    -1.61e-02  4.05e-03  2.35e-04 -1.89e-02 -0.040187 1.016\n-Donnie Hill       -2.50e-02  1.62e-02 -1.76e-02  2.21e-02 -0.043215 1.020\n-Davey Lopes        8.74e-03 -4.75e-03  3.20e-03  3.12e-04  0.009532 1.024\n-Don Mattingly      6.38e-03 -3.78e-01  4.24e-01  1.54e-01  0.497138 1.074\n-Dale Murphy       -1.73e-01  1.10e-01 -9.41e-02  2.43e-01  0.397212 0.925\n-Dwayne Murphy      1.95e-02  2.57e-03 -6.47e-03 -7.29e-04  0.031814 1.018\n-Dave Parker       -2.20e-02  6.86e-03 -4.20e-03  3.43e-02  0.053028 1.038\n-Dan Pasqua        -1.01e-01  9.04e-02 -6.41e-02 -8.27e-02 -0.138206 1.011\n-Darrell Porter    -3.70e-02  2.10e-02 -1.01e-02 -2.10e-02 -0.041665 1.036\n-Dick Schofield     1.66e-03 -5.59e-03  5.19e-03  4.62e-04 -0.008478 1.023\n-Don Slaught       -7.59e-03  3.20e-03 -1.18e-03 -4.75e-03 -0.010470 1.022\n-Darryl Strawberry -9.32e-03  2.04e-02 -3.76e-02  1.47e-01  0.177875 1.006\n-Dale Sveum        -3.46e-02 -1.10e-02  1.72e-02  1.20e-02 -0.059502 1.012\n-Danny Tartabull    2.40e-02 -4.75e-03  9.49e-03 -1.28e-01 -0.173445 0.993\n-Denny Walling     -1.18e-02  1.88e-02 -1.86e-02 -4.02e-03 -0.022870 1.028\n-Dave Winfield     -1.37e-01  1.24e-01 -1.08e-01  1.71e-01  0.335145 0.907\n-Eric Davis        -3.51e-02  4.56e-02 -2.41e-02 -1.39e-01 -0.155503 1.019\n-Eddie Milner      -1.32e-03 -2.68e-03  3.12e-03 -3.65e-03 -0.011170 1.020\n-Eddie Murray       5.79e-02 -3.17e-01  3.64e-01  9.46e-02  0.489635 0.778\n-Ed Romero          2.63e-02  1.13e-02 -1.72e-02 -1.35e-02  0.042641 1.026\n-Frank White        1.53e-03 -6.98e-04  3.17e-04 -1.63e-03 -0.003897 1.026\n-George Bell       -5.58e-03 -1.77e-02  2.05e-02  1.95e-02  0.036125 1.051\n-Glenn Braggs      -4.78e-02  7.01e-03  3.28e-03  6.51e-03 -0.055092 1.021\n-George Brett       4.71e-02 -9.45e-02  9.83e-02  5.71e-02  0.173086 0.958\n-Greg Brock        -5.17e-03 -2.81e-03  5.50e-03 -7.43e-03 -0.012914 1.026\n-Gary Carter       -6.17e-02  1.13e-01 -1.35e-01  2.62e-01  0.377879 0.877\n-Glenn Davis        6.84e-02 -4.07e-02  4.56e-02 -1.81e-01 -0.238690 1.002\n-Gary Gaetti        5.82e-03  6.93e-03 -6.77e-03 -2.60e-02 -0.033040 1.047\n-Greg Gagne         2.08e-02 -5.44e-02  4.75e-02  1.55e-02 -0.079601 1.010\n-George Hendrick    4.09e-02 -2.49e-02  1.37e-02  2.89e-02  0.052819 1.021\n-Glenn Hubbard     -5.46e-03  4.28e-02 -3.89e-02 -3.31e-02  0.058976 1.027\n-Garth Iorg        -2.36e-03 -1.37e-04  1.10e-04  2.77e-03 -0.004644 1.023\n-Gary Matthews      1.44e-02 -4.54e-03 -3.11e-03  3.47e-02  0.042751 1.026\n-Graig Nettles     -8.56e-03 -3.21e-02  4.14e-02 -2.49e-02 -0.058726 1.026\n-Gary Pettis        1.61e-02 -2.34e-02  1.45e-02  2.93e-02 -0.041462 1.033\n-Gary Redus        -2.88e-03 -1.19e-03  2.01e-03 -9.42e-04 -0.005715 1.021\n-Garry Templeton   -3.95e-02  7.73e-02 -5.56e-02 -8.91e-02  0.120165 1.030\n-Greg Walker        1.82e-03 -1.20e-03  7.46e-04  1.07e-03  0.002256 1.025\n-Gary Ward         -1.60e-03  2.50e-03 -2.82e-03  1.37e-03 -0.003642 1.032\n-Glenn Wilson       5.65e-03 -4.08e-03  1.71e-03  2.31e-03 -0.010584 1.026\n-Harold Baines     -3.89e-03 -9.03e-03  1.20e-02  6.58e-03  0.022457 1.029\n-Hubie Brooks       1.82e-02 -2.57e-02  2.35e-02  9.08e-03  0.028393 1.041\n-Howard Johnson    -1.45e-02  4.60e-03 -4.42e-04 -5.91e-03 -0.016557 1.027\n-Hal McRae         -7.99e-03  1.15e-03  4.67e-04  5.00e-04 -0.009927 1.022\n-Harold Reynolds    2.01e-02 -6.84e-02  5.94e-02  5.59e-02 -0.087826 1.039\n-Harry Spilman     -4.44e-02  2.41e-02 -1.43e-02 -5.96e-03 -0.045041 1.032\n-Herm Winningham   -3.67e-02  2.79e-03  6.44e-03  1.44e-03 -0.042678 1.026\n-Jesse Barfield    -1.17e-02 -2.82e-02  2.26e-02  1.02e-01  0.116556 1.061\n-Juan Beniquez     -1.65e-02  1.75e-02 -1.81e-02  7.94e-03 -0.026865 1.025\n-John Cangelosi     1.54e-02 -6.05e-02  5.05e-02  5.93e-02 -0.087092 1.028\n-Jose Canseco       1.15e-01 -1.54e-01  1.63e-01 -1.84e-01 -0.291733 1.022\n-Joe Carter         9.69e-02  1.31e-01 -1.72e-01 -1.61e-01 -0.352580 0.977\n-Jack Clark         2.03e-01 -3.27e-02 -2.58e-02  6.29e-02  0.242548 0.936\n-Jose Cruz         -2.58e-03  2.92e-04  5.81e-03 -1.40e-02  0.031982 1.019\n-Jody Davis        -4.09e-02  6.34e-02 -6.09e-02  4.15e-02  0.107199 1.014\n-Jim Dwyer         -1.04e-02  4.29e-03 -1.47e-03 -3.30e-03 -0.011022 1.032\n-Julio Franco       5.20e-03  8.60e-03 -1.50e-02  1.13e-02 -0.025687 1.039\n-Jim Gantner       -1.31e-02  1.36e-02  1.21e-03 -4.45e-02  0.066750 1.018\n-Johnny Grubb      -5.67e-02  5.67e-02 -4.45e-02 -3.00e-02 -0.069655 1.039\n-Jack Howell       -6.71e-02  3.52e-02 -2.12e-02 -3.63e-03 -0.068083 1.028\n-John Kruk         -8.31e-02  7.74e-02 -7.30e-02  2.61e-02 -0.108842 1.016\n-Jeffrey Leonard   -4.71e-02  2.71e-02 -2.69e-02  2.78e-02 -0.074305 1.008\n-Jim Morrison       3.10e-02 -5.12e-03 -1.49e-04 -7.27e-02 -0.125433 1.003\n-John Moses        -7.36e-03 -2.96e-02  2.05e-02  6.08e-02 -0.082292 1.014\n-Jerry Mumphrey     1.80e-02 -1.75e-02  1.72e-02 -6.72e-03  0.025500 1.026\n-Jim Presley        1.02e-01 -7.23e-02  5.83e-02 -1.10e-01 -0.210342 0.997\n-Johnny Ray         8.54e-03  9.75e-03 -2.07e-02  2.48e-02 -0.042689 1.037\n-Jeff Reed         -4.76e-02  1.35e-02 -3.51e-03  6.30e-03 -0.051111 1.027\n-Jim Rice          -4.77e-02 -4.34e-01  5.39e-01  5.10e-02  0.669238 0.868\n-Jerry Royster     -1.99e-02  5.33e-03 -1.91e-03  3.92e-03 -0.023265 1.023\n-John Russell      -3.18e-02 -7.34e-03  1.91e-02 -2.43e-02 -0.057767 1.016\n-Juan Samuel        8.35e-03 -7.32e-03  4.38e-03  2.18e-03 -0.014496 1.027\n-John Shelby        1.47e-03 -1.95e-02  2.03e-02  1.73e-03 -0.025399 1.025\n-Joel Skinner      -1.96e-02 -2.06e-02  2.41e-02  1.72e-02 -0.048386 1.020\n-Jim Sundberg      -3.39e-02  1.46e-01 -1.49e-01 -7.63e-03  0.166777 1.014\n-Jose Uribe         1.27e-02 -4.06e-02  3.59e-02  2.86e-02 -0.049979 1.039\n-Joel Youngblood    3.87e-02 -1.59e-02  7.41e-03  2.04e-03  0.040097 1.027\n-Kevin Bass         1.28e-02  7.12e-02 -9.02e-02 -1.74e-02 -0.123985 1.027\n-Kal Daniels       -1.09e-01  8.83e-02 -6.92e-02 -1.37e-02 -0.117476 1.025\n-Kirk Gibson        2.27e-02 -2.47e-02 -2.73e-03  1.92e-01  0.214128 1.003\n-Ken Griffey        7.61e-03 -3.34e-02  3.51e-02  2.31e-02  0.050656 1.029\n-Keith Hernandez   -1.52e-02 -1.83e-01  2.48e-01 -6.77e-02  0.336103 0.945\n-Kent Hrbek        -4.24e-02  1.99e-02 -2.45e-02  1.39e-01  0.181680 1.009\n-Ken Landreaux      6.06e-02 -1.48e-02  7.99e-03 -2.84e-02  0.079663 1.010\n-Kevin McReynolds   1.30e-02  2.02e-02 -2.35e-02 -4.94e-02 -0.078198 1.026\n-Kevin Mitchell    -5.85e-02  3.95e-02 -2.86e-02 -2.47e-02 -0.078225 1.007\n-Keith Moreland    -5.18e-02  3.89e-02 -1.36e-02 -4.43e-02  0.101486 1.015\n-Ken Oberkfell     -1.13e-02  1.44e-02 -3.38e-03 -3.68e-02  0.049249 1.026\n-Ken Phelps        -2.88e-02  2.99e-03  1.60e-02 -7.95e-02 -0.092367 1.029\n-Kirby Puckett      5.34e-02  3.55e-01 -4.12e-01 -2.14e-01 -0.554805 0.992\n-Kurt Stillwell    -2.29e-02 -1.51e-02  1.71e-02  2.94e-02 -0.050053 1.025\n-Leon Durham       -1.79e-02  2.93e-02 -3.04e-02  6.50e-02  0.119335 0.995\n-Len Dykstra       -2.56e-02  4.61e-02 -5.92e-02  3.91e-02 -0.097424 1.007\n-Larry Herndon     -2.34e-02  1.15e-03  4.27e-03 -1.07e-03 -0.030495 1.020\n-Lee Lacy           1.21e-03  7.57e-03 -1.36e-02  1.06e-02 -0.031430 1.021\n-Len Matuszek      -2.73e-02  1.32e-02 -6.25e-03 -9.11e-03 -0.029224 1.028\n-Lloyd Moseby      -1.91e-02  2.26e-02 -1.93e-02  7.31e-03  0.034531 1.030\n-Lance Parrish      3.46e-02 -1.47e-02 -2.45e-03  6.80e-02  0.080653 1.027\n-Larry Parrish     -5.09e-03  1.29e-02 -6.60e-03 -5.55e-02 -0.063419 1.033\n-Larry Sheets      -6.01e-02  4.13e-02 -2.12e-02 -7.80e-02 -0.108104 1.008\n-Lou Whitaker       4.27e-02 -2.92e-02  1.69e-02 -1.65e-02 -0.083285 1.016\n-Mike Aldrete      -4.90e-02  1.23e-02 -4.10e-03  1.57e-02 -0.056724 1.022\n-Marty Barrett      2.22e-02 -7.08e-03 -9.51e-03  4.39e-02 -0.059492 1.044\n-Mike Davis        -3.44e-03  3.40e-03 -2.79e-03  1.11e-02  0.023562 1.021\n-Mike Diaz         -8.72e-02  4.92e-02 -2.48e-02 -4.78e-02 -0.099399 1.020\n-Mariano Duncan     5.10e-03 -4.73e-02  4.63e-02  1.88e-02 -0.060623 1.022\n-Mike Easler       -1.58e-03  9.03e-03 -1.11e-02  3.61e-04 -0.015274 1.026\n-Mel Hall          -1.28e-02  2.82e-02 -2.81e-02 -2.03e-02 -0.044855 1.022\n-Mike Heath         4.06e-02  2.41e-02 -3.66e-02  6.25e-04  0.073957 1.013\n-Mike Kingery      -5.95e-02  2.18e-02 -1.13e-02  1.12e-02 -0.064746 1.021\n-Mike LaValliere   -2.20e-02 -1.68e-02  1.94e-02  2.36e-02 -0.049084 1.021\n-Mike Marshall      2.08e-02  1.06e-02 -2.35e-02  4.29e-02  0.061086 1.025\n-Mike Pagliarulo    4.30e-02 -8.89e-02  1.04e-01 -1.29e-01 -0.189683 1.020\n-Mark Salas        -3.31e-02 -2.78e-03  1.21e-02 -4.68e-03 -0.044444 1.020\n-Mike Schmidt       9.44e-01 -4.08e-01  1.81e-01  5.15e-02  0.952182 0.690\n-Mike Scioscia      2.27e-02  3.67e-02 -3.22e-02 -5.76e-02  0.099382 1.001\n-Mickey Tettleton  -3.39e-02 -4.09e-03  1.65e-02 -1.62e-02 -0.047818 1.027\n-Milt Thompson     -3.20e-02 -8.94e-04  6.04e-03  1.09e-02 -0.046072 1.017\n-Mitch Webster      4.95e-02  8.30e-03 -5.39e-02  1.03e-01 -0.171094 1.006\n-Mookie Wilson      2.45e-02 -2.53e-02  2.69e-02 -8.88e-03  0.046693 1.016\n-Marvell Wynne     -2.80e-02  9.59e-03 -4.99e-03  2.78e-03 -0.033658 1.019\n-Mike Young        -5.67e-03 -4.87e-03  5.52e-03  3.00e-03 -0.015880 1.020\n-Ozzie Guillen      4.79e-02 -7.69e-02  5.21e-02  9.14e-02 -0.122317 1.035\n-Oddibe McDowell    6.49e-02 -5.61e-02  3.61e-02 -8.15e-03 -0.124714 1.000\n-Ozzie Smith       -9.51e-02  7.13e-02  5.76e-02 -4.69e-01  0.550953 0.855\n-Ozzie Virgil       1.25e-02  4.37e-02 -5.55e-02  2.95e-02  0.079533 1.019\n-Phil Bradley      -3.04e-04  1.08e-02 -1.43e-02  4.26e-03 -0.019070 1.033\n-Phil Garner        5.74e-04 -2.01e-04  1.01e-04  3.37e-05  0.000735 1.021\n-Pete Incaviglia    5.99e-02 -7.98e-02  9.24e-02 -1.65e-01 -0.225440 1.011\n-Paul Molitor       2.29e-02 -2.78e-02  4.57e-02 -5.30e-02  0.129029 0.977\n-Pete Rose          9.07e-02  3.25e-02 -4.72e-02 -7.08e-02  0.151041 1.001\n-Pat Sheridan      -2.45e-02  2.13e-03  3.72e-03  3.06e-04 -0.029298 1.023\n-Pat Tabler        -1.35e-02  4.82e-02 -6.01e-02  2.96e-02 -0.074424 1.040\n-Rafael Belliard   -1.36e-02 -1.55e-02  1.55e-02  2.68e-02 -0.040850 1.026\n-Rick Burleson      5.39e-03 -3.42e-03  2.82e-03 -1.20e-03  0.006320 1.025\n-Randy Bush        -1.61e-02  4.00e-03 -3.97e-03  1.09e-02 -0.029482 1.018\n-Rick Cerone       -1.81e-02  6.80e-03 -3.47e-03  2.24e-03 -0.019496 1.026\n-Ron Cey            1.32e-01 -8.12e-02  4.58e-02  7.98e-02  0.158755 0.993\n-Rob Deer           2.11e-02 -7.19e-02  1.01e-01 -1.81e-01 -0.222258 1.038\n-Rick Dempsey       3.82e-03  1.10e-02 -1.40e-02  5.00e-03  0.018617 1.029\n-Ron Hassey        -1.30e-02  1.82e-02 -1.81e-02  4.33e-04 -0.021450 1.031\n-Rickey Henderson  -1.39e-01  1.03e-01 -8.99e-02  1.80e-01  0.310296 0.961\n-Reggie Jackson    -1.87e-05 -3.39e-03  4.09e-03 -3.46e-03 -0.006982 1.025\n-Ron Kittle        -6.74e-04 -4.78e-03  6.41e-03 -6.15e-03 -0.010043 1.036\n-Ray Knight        -3.94e-03  2.58e-02 -3.50e-02  1.44e-02 -0.053574 1.021\n-Rick Leach        -5.26e-02  4.51e-02 -3.92e-02  5.45e-03 -0.061488 1.026\n-Rick Manning       1.35e-02 -5.40e-03  2.01e-03  3.37e-03  0.014381 1.027\n-Rance Mulliniks   -1.84e-03  5.85e-05  3.26e-04 -4.75e-04 -0.003132 1.020\n-Ron Oester        -2.26e-02  3.44e-02 -2.27e-02 -3.44e-02  0.058600 1.024\n-Rey Quinones      -1.38e-02 -2.98e-02  3.16e-02  2.70e-02 -0.053564 1.025\n-Rafael Ramirez    -4.85e-02  1.08e-01 -9.09e-02 -6.30e-02  0.135017 1.013\n-Ronn Reynolds     -8.04e-03  2.36e-03 -3.87e-04 -3.01e-04 -0.008425 1.034\n-Ron Roenicke      -2.42e-02  4.54e-04  3.78e-03  7.77e-03 -0.032447 1.021\n-Ryne Sandberg      8.20e-03 -1.64e-03 -2.84e-03  5.71e-03 -0.016574 1.031\n-Rafael Santana     6.45e-04 -5.08e-03  4.65e-03  4.18e-03 -0.006834 1.038\n-Rick Schu         -6.57e-02  3.72e-02 -2.24e-02 -1.53e-02 -0.069016 1.022\n-Ruben Sierra      -3.57e-02  9.90e-03  1.59e-03 -4.98e-02 -0.087465 1.001\n-Roy Smalley       -5.91e-03  2.12e-02 -2.40e-02  2.31e-02  0.043772 1.024\n-Robby Thompson     5.19e-02 -4.75e-02  1.27e-02  9.53e-02 -0.140832 1.008\n-Rob Wilfong        8.69e-03  9.61e-03 -1.13e-02 -8.49e-03  0.021196 1.026\n-Robin Yount        1.89e-03 -4.12e-02  5.64e-02 -2.92e-02  0.077390 1.032\n-Steve Balboni      6.22e-02 -1.33e-01  1.52e-01 -1.51e-01 -0.238906 1.023\n-Scott Bradley     -8.83e-02  6.52e-02 -5.21e-02  2.83e-03 -0.096123 1.019\n-Sid Bream          3.29e-02 -2.99e-02  1.65e-02 -4.34e-03 -0.093611 1.001\n-Steve Buechele     1.92e-02 -6.31e-02  6.69e-02 -3.78e-02 -0.103221 1.009\n-Shawon Dunston     8.37e-02 -1.12e-01  9.27e-02  6.61e-03 -0.147228 1.010\n-Scott Fletcher     7.47e-03  1.75e-02 -3.68e-02  5.70e-02 -0.080957 1.035\n-Steve Garvey      -1.04e-01  1.28e-01 -1.12e-01  7.07e-02  0.222426 0.968\n-Steve Jeltz        1.18e-02 -4.20e-02  3.67e-02  3.48e-02 -0.053836 1.045\n-Steve Lombardozzi  2.34e-02 -7.74e-02  7.16e-02  3.31e-02 -0.090932 1.025\n-Spike Owen         1.61e-03 -3.28e-03  2.63e-03  2.82e-03 -0.004220 1.054\n-Steve Sax          4.74e-02  2.88e-01 -4.15e-01  2.57e-01 -0.561771 0.992\n-Tony Bernazard     1.30e-02  4.32e-02 -6.01e-02 -2.28e-03 -0.094867 1.019\n-Tom Brookens      -6.87e-03  2.70e-03 -2.15e-03  3.78e-03 -0.009053 1.024\n-Tom Brunansky     -3.76e-02  3.98e-02 -3.40e-02  2.41e-02  0.071115 1.027\n-Tony Fernandez     9.34e-02  8.84e-02 -1.72e-01  1.43e-01 -0.309101 1.018\n-Tim Flannery      -1.34e-02  7.05e-03 -1.01e-02  2.26e-02 -0.033840 1.023\n-Tom Foley         -1.69e-02  5.38e-03 -4.10e-03  1.14e-02 -0.022935 1.025\n-Tony Gwynn         1.65e-02  1.00e-01 -1.34e-01  3.45e-02 -0.167916 1.052\n-Terry Harper       7.18e-03 -2.14e-03  5.69e-04  7.95e-04  0.008249 1.023\n-Tommy Herr        -7.91e-02  1.19e-01 -7.84e-02 -1.46e-01  0.194226 1.024\n-Tim Hulett         4.74e-02 -9.74e-02  9.36e-02 -8.84e-03 -0.114204 1.025\n-Terry Kennedy      3.52e-01 -1.82e-01  9.86e-02  3.63e-02  0.352395 0.992\n-Tito Landrum       1.49e-02  2.43e-03 -5.84e-03 -4.82e-03  0.020196 1.028\n-Tim Laudner       -2.61e-02  9.61e-03 -1.99e-03 -1.13e-02 -0.029228 1.029\n-Tom Paciorek      -3.65e-02  2.25e-02 -1.68e-02  3.56e-03 -0.039003 1.027\n-Tony Pena         -1.30e-02 -2.13e-02  4.65e-02 -5.08e-02  0.114130 1.003\n-Terry Pendleton    5.99e-02 -9.94e-02  7.45e-02  9.35e-02 -0.134267 1.052\n-Tony Phillips      7.22e-04 -3.76e-03  2.60e-03  5.55e-03 -0.008033 1.025\n-Terry Puhl         1.77e-01 -5.85e-02  2.11e-02 -1.29e-02  0.187017 0.990\n-Ted Simmons        8.03e-02 -3.69e-02  1.83e-02  8.05e-03  0.081328 1.029\n-Tim Teufel        -8.84e-03 -2.89e-04  1.60e-03  4.35e-03 -0.012606 1.023\n-Tim Wallach       -2.12e-02  5.56e-02 -5.68e-02  1.87e-02  0.073433 1.025\n-Vince Coleman      6.27e-02 -1.05e-01  8.14e-02  9.02e-02 -0.134179 1.068\n-Von Hayes         -2.73e-02 -5.85e-02  8.30e-02  6.20e-03  0.129480 1.022\n-Vance Law          5.51e-03  3.77e-02 -3.81e-02 -2.38e-02  0.055877 1.023\n-Wally Backman     -6.18e-03  1.03e-02 -1.25e-02  9.74e-03 -0.017580 1.039\n-Wade Boggs         3.35e-02 -2.91e-01  3.58e-01 -1.12e-01  0.403607 1.062\n-Will Clark        -3.68e-02  4.45e-02 -4.90e-02  6.85e-03 -0.089611 0.998\n-Wally Joyner       6.27e-02  4.77e-02 -7.80e-02 -6.49e-02 -0.207522 0.979\n-Willie McGee      -1.65e-02  3.14e-02 -2.19e-02 -3.26e-02  0.052971 1.024\n-Willie Randolph   -1.26e-02  1.17e-02  7.02e-03 -6.25e-02  0.084131 1.018\n-Wayne Tolleson     5.22e-03 -1.10e-02  4.64e-03  2.58e-02 -0.032231 1.028\n-Willie Upshaw     -7.49e-02  1.05e-01 -7.66e-02 -7.50e-02  0.145254 1.018\n-Willie Wilson     -6.68e-02  5.50e-02 -2.28e-02 -7.22e-02  0.121217 1.026\n                     cook.d     hat inf\n-Alan Ashby        3.94e-05 0.00540    \n-Alvin Davis       3.15e-04 0.00589    \n-Andre Dawson      8.14e-04 0.00833    \n-Andres Galarraga  1.28e-03 0.00544    \n-Alfredo Griffin   2.27e-04 0.02461    \n-Al Newman         3.13e-04 0.01412    \n-Argenis Salazar   7.07e-04 0.01093    \n-Andres Thomas     8.92e-04 0.00583    \n-Andre Thornton    7.80e-03 0.01181    \n-Alan Trammell     1.14e-03 0.00995    \n-Alex Trevino      6.75e-04 0.01160    \n-Andy VanSlyke     2.40e-06 0.00392    \n-Alan Wiggins      3.14e-03 0.01133    \n-Bill Almon        2.64e-05 0.01206    \n-Buddy Bell        2.71e-08 0.00926    \n-Buddy Biancalana  1.56e-04 0.01234    \n-Bruce Bochy       9.40e-04 0.02042    \n-Barry Bonds       2.49e-03 0.01404    \n-Bobby Bonilla     1.75e-03 0.01106    \n-Bob Brenly        5.25e-05 0.00873    \n-Bill Buckner      2.22e-05 0.01390    \n-Brett Butler      6.32e-04 0.02317    \n-Bob Dernier       2.75e-03 0.01020    \n-Bo Diaz           2.45e-04 0.00579    \n-Bill Doran        3.06e-07 0.01543    \n-Brian Downing     5.61e-04 0.00778    \n-Billy Hatcher     1.39e-03 0.00710    \n-Brook Jacoby      6.64e-04 0.01085    \n-Bob Kearney       4.04e-07 0.01088    \n-Bill Madlock      7.04e-04 0.00473    \n-Bob Melvin        5.58e-04 0.00900    \n-BillyJo Robidoux  5.45e-04 0.01333    \n-Bill Schroeder    1.44e-04 0.01128    \n-Chris Bando       2.75e-05 0.00979    \n-Chris Brown       5.69e-03 0.01663    \n-Carmen Castillo   4.20e-04 0.01303    \n-Chili Davis       2.18e-04 0.00704    \n-Carlton Fisk      6.06e-03 0.01782    \n-Curt Ford         8.11e-04 0.01097    \n-Carney Lansford   2.56e-03 0.01071    \n-Chet Lemon        2.89e-04 0.00498    \n-Candy Maldonado   2.09e-04 0.00765    \n-Carmelo Martinez  4.68e-07 0.00897    \n-Craig Reynolds    1.59e-05 0.00601    \n-Cal Ripken        5.32e-03 0.01548    \n-Cory Snyder       7.94e-03 0.01424    \n-Chris Speier      1.21e-04 0.01764    \n-Curt Wilkerson    1.26e-05 0.01158    \n-Dave Anderson     4.03e-05 0.01141    \n-Don Baylor        3.84e-03 0.03525    \n-Daryl Boston      1.32e-03 0.01200    \n-Darnell Coles     4.59e-03 0.00769    \n-Dave Concepcion   3.70e-05 0.00754    \n-Doug DeCinces     6.22e-04 0.01616    \n-Darrell Evans     6.56e-04 0.02641    \n-Dwight Evans      1.16e-03 0.01561    \n-Damaso Garcia     1.10e-03 0.00737    \n-Dan Gladden       7.04e-04 0.00732    \n-Dave Henderson    4.05e-04 0.00494    \n-Donnie Hill       4.68e-04 0.00803    \n-Davey Lopes       2.28e-05 0.00867    \n-Don Mattingly     6.14e-02 0.09037   *\n-Dale Murphy       3.85e-02 0.02058   *\n-Dwayne Murphy     2.54e-04 0.00499    \n-Dave Parker       7.05e-04 0.02334    \n-Dan Pasqua        4.77e-03 0.01483    \n-Darrell Porter    4.36e-04 0.02123    \n-Dick Schofield    1.80e-05 0.00693    \n-Don Slaught       2.75e-05 0.00665    \n-Darryl Strawberry 7.89e-03 0.01769    \n-Dale Sveum        8.86e-04 0.00584    \n-Danny Tartabull   7.48e-03 0.01283    \n-Denny Walling     1.31e-04 0.01244    \n-Dave Winfield     2.73e-02 0.01335   *\n-Eric Davis        6.04e-03 0.02052    \n-Eddie Milner      3.13e-05 0.00478    \n-Eddie Murray      5.61e-02 0.01269   *\n-Ed Romero         4.56e-04 0.01202    \n-Frank White       3.81e-06 0.01032    \n-George Bell       3.27e-04 0.03440   *\n-Glenn Braggs      7.61e-04 0.01024    \n-George Brett      7.40e-03 0.00702    \n-Greg Brock        4.18e-05 0.01025    \n-Gary Carter       3.44e-02 0.01335   *\n-Glenn Davis       1.42e-02 0.02338    \n-Gary Gaetti       2.74e-04 0.03006   *\n-Greg Gagne        1.59e-03 0.00757    \n-George Hendrick   6.99e-04 0.00994    \n-Glenn Hubbard     8.72e-04 0.01441    \n-Garth Iorg        5.41e-06 0.00751    \n-Gary Matthews     4.58e-04 0.01198    \n-Graig Nettles     8.65e-04 0.01360    \n-Gary Pettis       4.31e-04 0.01809    \n-Gary Redus        8.20e-06 0.00533    \n-Garry Templeton   3.62e-03 0.02315    \n-Greg Walker       1.28e-06 0.00954    \n-Gary Ward         3.33e-06 0.01588    \n-Glenn Wilson      2.81e-05 0.01027    \n-Harold Baines     1.27e-04 0.01369    \n-Hubie Brooks      2.02e-04 0.02455    \n-Howard Johnson    6.88e-05 0.01116    \n-Hal McRae         2.47e-05 0.00661    \n-Harold Reynolds   1.93e-03 0.02662    \n-Harry Spilman     5.09e-04 0.01782    \n-Herm Winningham   4.57e-04 0.01251    \n-Jesse Barfield    3.41e-03 0.04727   *\n-Juan Beniquez     1.81e-04 0.00984    \n-John Cangelosi    1.90e-03 0.01856    \n-Jose Canseco      2.12e-02 0.03860    \n-Joe Carter        3.07e-02 0.02791    \n-Jack Clark        1.44e-02 0.00976   *\n-Jose Cruz         2.57e-04 0.00622    \n-Jody Davis        2.87e-03 0.01258    \n-Jim Dwyer         3.05e-05 0.01593    \n-Julio Franco      1.66e-04 0.02305    \n-Jim Gantner       1.12e-03 0.00960    \n-Johnny Grubb      1.22e-03 0.02493    \n-Jack Howell       1.16e-03 0.01668    \n-John Kruk         2.96e-03 0.01353    \n-Jeffrey Leonard   1.38e-03 0.00622    \n-Jim Morrison      3.93e-03 0.01043    \n-John Moses        1.69e-03 0.00956    \n-Jerry Mumphrey    1.63e-04 0.01145    \n-Jim Presley       1.10e-02 0.01817    \n-Johnny Ray        4.57e-04 0.02223    \n-Jeff Reed         6.55e-04 0.01410    \n-Jim Rice          1.07e-01 0.03416   *\n-Jerry Royster     1.36e-04 0.00787    \n-John Russell      8.36e-04 0.00732    \n-Juan Samuel       5.27e-05 0.01123    \n-John Shelby       1.62e-04 0.01058    \n-Joel Skinner      5.87e-04 0.00821    \n-Jim Sundberg      6.94e-03 0.01962    \n-Jose Uribe        6.27e-04 0.02432    \n-Joel Youngblood   4.03e-04 0.01275    \n-Kevin Bass        3.85e-03 0.02192    \n-Kal Daniels       3.45e-03 0.01950    \n-Kirk Gibson       1.14e-02 0.02056    \n-Ken Griffey       6.44e-04 0.01574    \n-Keith Hernandez   2.77e-02 0.01861   *\n-Kent Hrbek        8.23e-03 0.01925    \n-Ken Landreaux     1.59e-03 0.00739    \n-Kevin McReynolds  1.53e-03 0.01563    \n-Kevin Mitchell    1.53e-03 0.00624    \n-Keith Moreland    2.58e-03 0.01216    \n-Ken Oberkfell     6.08e-04 0.01265    \n-Ken Phelps        2.14e-03 0.01983    \n-Kirby Puckett     7.57e-02 0.05527   *\n-Kurt Stillwell    6.28e-04 0.01199    \n-Leon Durham       3.55e-03 0.00780    \n-Len Dykstra       2.37e-03 0.00858    \n-Larry Herndon     2.33e-04 0.00650    \n-Lee Lacy          2.48e-04 0.00742    \n-Len Matuszek      2.14e-04 0.01299    \n-Lloyd Moseby      2.99e-04 0.01541    \n-Lance Parrish     1.63e-03 0.01684    \n-Larry Parrish     1.01e-03 0.01955    \n-Larry Sheets      2.92e-03 0.01021    \n-Lou Whitaker      1.74e-03 0.01037    \n-Mike Aldrete      8.07e-04 0.01090    \n-Marty Barrett     8.88e-04 0.02919    \n-Mike Davis        1.39e-04 0.00668    \n-Mike Diaz         2.47e-03 0.01471    \n-Mariano Duncan    9.21e-04 0.01154    \n-Mike Easler       5.85e-05 0.01082    \n-Mel Hall          5.05e-04 0.00977    \n-Mike Heath        1.37e-03 0.00815    \n-Mike Kingery      1.05e-03 0.01115    \n-Mike LaValliere   6.04e-04 0.00901    \n-Mike Marshall     9.36e-04 0.01371    \n-Mike Pagliarulo   8.98e-03 0.02554    \n-Mark Salas        4.95e-04 0.00824    \n-Mike Schmidt      2.05e-01 0.03089   *\n-Mike Scioscia     2.47e-03 0.00698    \n-Mickey Tettleton  5.73e-04 0.01355    \n-Milt Thompson     5.32e-04 0.00617    \n-Mitch Webster     7.30e-03 0.01694    \n-Mookie Wilson     5.46e-04 0.00619    \n-Marvell Wynne     2.84e-04 0.00634    \n-Mike Young        6.33e-05 0.00474    \n-Ozzie Guillen     3.75e-03 0.02716    \n-Oddibe McDowell   3.88e-03 0.00964    \n-Ozzie Smith       7.25e-02 0.02295   *\n-Ozzie Virgil      1.58e-03 0.01141    \n-Phil Bradley      9.13e-05 0.01688    \n-Phil Garner       1.36e-07 0.00536    \n-Pete Incaviglia   1.27e-02 0.02516    \n-Paul Molitor      4.13e-03 0.00571    \n-Pete Rose         5.69e-03 0.01262    \n-Pat Sheridan      2.15e-04 0.00889    \n-Pat Tabler        1.39e-03 0.02628    \n-Rafael Belliard   4.19e-04 0.01234    \n-Rick Burleson     1.00e-05 0.00907    \n-Randy Bush        2.18e-04 0.00496    \n-Rick Cerone       9.54e-05 0.01047    \n-Ron Cey           6.27e-03 0.01140    \n-Rob Deer          1.23e-02 0.03955    \n-Rick Dempsey      8.70e-05 0.01365    \n-Ron Hassey        1.15e-04 0.01582    \n-Rickey Henderson  2.37e-02 0.01946    \n-Reggie Jackson    1.22e-05 0.00963    \n-Ron Kittle        2.53e-05 0.01951    \n-Ray Knight        7.20e-04 0.01010    \n-Rick Leach        9.48e-04 0.01402    \n-Rick Manning      5.19e-05 0.01159    \n-Rance Mulliniks   2.46e-06 0.00450    \n-Ron Oester        8.61e-04 0.01226    \n-Rey Quinones      7.19e-04 0.01288    \n-Rafael Ramirez    4.55e-03 0.01556    \n-Ronn Reynolds     1.78e-05 0.01742    \n-Ron Roenicke      2.64e-04 0.00722    \n-Ryne Sandberg     6.89e-05 0.01480    \n-Rafael Santana    1.17e-05 0.02135    \n-Rick Schu         1.19e-03 0.01243    \n-Ruben Sierra      1.91e-03 0.00575    \n-Roy Smalley       4.81e-04 0.01061    \n-Robby Thompson    4.95e-03 0.01401    \n-Rob Wilfong       1.13e-04 0.01093    \n-Robin Yount       1.50e-03 0.01993    \n-Steve Balboni     1.42e-02 0.03283    \n-Scott Bradley     2.31e-03 0.01382    \n-Sid Bream         2.19e-03 0.00648    \n-Steve Buechele    2.66e-03 0.01005    \n-Shawon Dunston    5.41e-03 0.01549    \n-Scott Fletcher    1.64e-03 0.02316    \n-Steve Garvey      1.22e-02 0.01247    \n-Steve Jeltz       7.27e-04 0.02943    \n-Steve Lombardozzi 2.07e-03 0.01637    \n-Spike Owen        4.47e-06 0.03619   *\n-Steve Sax         7.76e-02 0.05621   *\n-Tony Bernazard    2.25e-03 0.01372    \n-Tom Brookens      2.06e-05 0.00843    \n-Tom Brunansky     1.27e-03 0.01555    \n-Tony Fernandez    2.38e-02 0.03841    \n-Tim Flannery      2.87e-04 0.00880    \n-Tom Foley         1.32e-04 0.01018    \n-Tony Gwynn        7.06e-03 0.04413   *\n-Terry Harper      1.71e-05 0.00728    \n-Tommy Herr        9.42e-03 0.02831    \n-Tim Hulett        3.26e-03 0.01934    \n-Terry Kennedy     3.07e-02 0.03269    \n-Tito Landrum      1.02e-04 0.01246    \n-Tim Laudner       2.14e-04 0.01372    \n-Tom Paciorek      3.82e-04 0.01268    \n-Tony Pena         3.25e-03 0.00921    \n-Terry Pendleton   4.52e-03 0.04110   *\n-Tony Phillips     1.62e-05 0.00948    \n-Terry Puhl        8.69e-03 0.01348    \n-Ted Simmons       1.66e-03 0.01835    \n-Tim Teufel        3.99e-05 0.00754    \n-Tim Wallach       1.35e-03 0.01472    \n-Vince Coleman     4.51e-03 0.05407   *\n-Von Hayes         4.19e-03 0.01920    \n-Vance Law         7.83e-04 0.01135    \n-Wally Backman     7.76e-05 0.02256    \n-Wade Boggs        4.06e-02 0.07367   *\n-Will Clark        2.00e-03 0.00548    \n-Wally Joyner      1.07e-02 0.01320    \n-Willie McGee      7.04e-04 0.01159    \n-Willie Randolph   1.77e-03 0.01180    \n-Wayne Tolleson    2.61e-04 0.01364    \n-Willie Upshaw     5.27e-03 0.01897    \n-Willie Wilson     3.68e-03 0.02101    \n\n\n\nsummary(influence.measures(reg_model))\n\nPotentially influential observations of\n     lm(formula = Salary ~ AtBat + Hits + HmRun, data = hitters) :\n\n                 dfb.1_ dfb.AtBt dfb.Hits dfb.HmRn dffit   cov.r   cook.d\n-Don Mattingly    0.01  -0.38     0.42     0.15     0.50_*  1.07_*  0.06 \n-Dale Murphy     -0.17   0.11    -0.09     0.24     0.40_*  0.92_*  0.04 \n-Dave Winfield   -0.14   0.12    -0.11     0.17     0.34    0.91_*  0.03 \n-Eddie Murray     0.06  -0.32     0.36     0.09     0.49_*  0.78_*  0.06 \n-George Bell     -0.01  -0.02     0.02     0.02     0.04    1.05_*  0.00 \n-Gary Carter     -0.06   0.11    -0.13     0.26     0.38_*  0.88_*  0.03 \n-Gary Gaetti      0.01   0.01    -0.01    -0.03    -0.03    1.05_*  0.00 \n-Jesse Barfield  -0.01  -0.03     0.02     0.10     0.12    1.06_*  0.00 \n-Jack Clark       0.20  -0.03    -0.03     0.06     0.24    0.94_*  0.01 \n-Jim Rice        -0.05  -0.43     0.54     0.05     0.67_*  0.87_*  0.11 \n-Keith Hernandez -0.02  -0.18     0.25    -0.07     0.34    0.94_*  0.03 \n-Kirby Puckett    0.05   0.36    -0.41    -0.21    -0.55_*  0.99    0.08 \n-Mike Schmidt     0.94  -0.41     0.18     0.05     0.95_*  0.69_*  0.20 \n-Ozzie Smith     -0.10   0.07     0.06    -0.47     0.55_*  0.85_*  0.07 \n-Spike Owen       0.00   0.00     0.00     0.00     0.00    1.05_*  0.00 \n-Steve Sax        0.05   0.29    -0.41     0.26    -0.56_*  0.99    0.08 \n-Tony Gwynn       0.02   0.10    -0.13     0.03    -0.17    1.05_*  0.01 \n-Terry Pendleton  0.06  -0.10     0.07     0.09    -0.13    1.05_*  0.00 \n-Vince Coleman    0.06  -0.11     0.08     0.09    -0.13    1.07_*  0.00 \n-Wade Boggs       0.03  -0.29     0.36    -0.11     0.40_*  1.06_*  0.04 \n                 hat    \n-Don Mattingly    0.09_*\n-Dale Murphy      0.02  \n-Dave Winfield    0.01  \n-Eddie Murray     0.01  \n-George Bell      0.03  \n-Gary Carter      0.01  \n-Gary Gaetti      0.03  \n-Jesse Barfield   0.05_*\n-Jack Clark       0.01  \n-Jim Rice         0.03  \n-Keith Hernandez  0.02  \n-Kirby Puckett    0.06_*\n-Mike Schmidt     0.03  \n-Ozzie Smith      0.02  \n-Spike Owen       0.04  \n-Steve Sax        0.06_*\n-Tony Gwynn       0.04  \n-Terry Pendleton  0.04  \n-Vince Coleman    0.05_*\n-Wade Boggs       0.07_*\n\n\n\n\n기각역\n\np <- 3\n\n\nn <- nrow(hitters)\n\n\n2*(p+1)/n #hat (h_ii)\n\n0.0304182509505703\n\n\n\n3*sqrt((p+1)/(n-p-1)) #Dffits\n\n0.37282185960072\n\n\n\n2*sqrt((p+1)/(n)) #Dffits\n\n0.246650566391283\n\n\n\nqf(0.5, p+1, n-p-1) #Cook d\n\n0.841375151461702\n\n\n\n\n변수 변환\n\nhitters$log_Salary <- log(hitters$Salary)\n\n\nreg_model_2 <- lm(log_Salary ~ AtBat + Hits + HmRun, hitters)\n\n\nsummary(reg_model_2)\n\n\nCall:\nlm(formula = log_Salary ~ AtBat + Hits + HmRun, data = hitters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2664 -0.6572  0.1122  0.5639  2.5886 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.103575   0.149615  34.111  < 2e-16 ***\nAtBat       -0.002179   0.001265  -1.722 0.086196 .  \nHits         0.014012   0.004052   3.458 0.000636 ***\nHmRun        0.016540   0.006678   2.477 0.013895 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.7872 on 259 degrees of freedom\nMultiple R-squared:  0.2253,    Adjusted R-squared:  0.2163 \nF-statistic:  25.1 on 3 and 259 DF,  p-value: 2.729e-14\n\n\n\n\n잔차그림\n\nresidual <- resid(reg_model_2)\nstad.res <- rstandard(reg_model_2)\nstu.res <- rstudent(reg_model_2)\n\n\npar(mfrow = c(2, 2))\nplot(fitted(reg_model_2), residual, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"Residuals\", \n     main = \"residual plot\")\nabline(h=0, lty=2)\n\nplot(fitted(reg_model_2), stad.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals\", \n     ylim=c(min(-3, min(stad.res)), \n            max(3,max(stad.res))),\n     main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\n\nplot(fitted(reg_model_2), stu.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(stu.res)), \n            max(3,max(stu.res))),\n     main = \"studentized residual plot\")\nabline(h=c(-2,0,2), lty=2)\n\nplot(fitted(reg_model_2), stu.res, \n     pch=20,cex  = 2,col  = \"darkorange\",\n     xlab = \"Fitted\", ylab = \"S_Residuals_(i)\", \n     ylim=c(min(-3, min(stu.res)), \n            max(3,max(stu.res))),\n     main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,nrow(hitters)-4),0,qt(0.975,nrow(hitters)-4)), lty=2)\ntext (fitted(reg_model_2)[which(abs(stu.res)>qt(0.975,nrow(hitters)-4))],\n      stu.res[which(abs(stu.res)>qt(0.975,nrow(hitters)-4))], \n      which(abs(stu.res)>qt(0.975,nrow(hitters)-4)),adj = c(0,1))\n\n\n\n\n\nstu.res[abs(stu.res)>qt(0.975,nrow(hitters)-4)]\n\n-Cory Snyder-2.16182826573383-Darnell Coles-2.09872610566858-Jack Clark2.12385831075298-Mike Schmidt3.40826964934749-Ozzie Smith2.0286158569726-Steve Sax-3.00934037017267-Terry Kennedy2.19849337509094\n\n\n\nwhich(abs(stu.res)>qt(0.975,nrow(hitters)-4))\n\n-Cory Snyder46-Darnell Coles52-Jack Clark111-Mike Schmidt173-Ozzie Smith183-Steve Sax230-Terry Kennedy241\n\n\n\ninfluence(reg_model_2)\n\n\n\n    $hat\n        -Alan Ashby0.00540428715355245-Alvin Davis0.00589170670784522-Andre Dawson0.00832644882765465-Andres Galarraga0.00543756565246947-Alfredo Griffin0.0246137118709988-Al Newman0.0141225554276349-Argenis Salazar0.010928273983481-Andres Thomas0.00583280157863637-Andre Thornton0.0118104032344835-Alan Trammell0.00994920971431027-Alex Trevino0.0116023974454147-Andy VanSlyke0.00392214855226506-Alan Wiggins0.01133266253963-Bill Almon0.0120577739078933-Buddy Bell0.00926235909199221-Buddy Biancalana0.012342136255686-Bruce Bochy0.0204159574317937-Barry Bonds0.0140367315219278-Bobby Bonilla0.0110632944090634-Bob Brenly0.00872907614618226-Bill Buckner0.0139048245540849-Brett Butler0.023168413560765-Bob Dernier0.0102008268559032-Bo Diaz0.00579243488480162-Bill Doran0.015434760936016-Brian Downing0.00777622895232283-Billy Hatcher0.00709689247443314-Brook Jacoby0.0108511422285614-Bob Kearney0.0108810262666556-Bill Madlock0.00473465384983353-Bob Melvin0.00900012174203842-BillyJo Robidoux0.0133294228418443-Bill Schroeder0.0112763637277587-Chris Bando0.00978612142405515-Chris Brown0.0166260339272529-Carmen Castillo0.0130280396536268-Chili Davis0.00704454319748951-Carlton Fisk0.017822764593868-Curt Ford0.0109708972674672-Carney Lansford0.0107121199405846-Chet Lemon0.004977264802092-Candy Maldonado0.00764992031384351-Carmelo Martinez0.0089685788087143-Craig Reynolds0.00600742640963073-Cal Ripken0.0154842953466146-Cory Snyder0.0142384557435194-Chris Speier0.0176408073638382-Curt Wilkerson0.011580828583756-Dave Anderson0.0114107120327874-Don Baylor0.0352479469918076-Daryl Boston0.0120025169206652-Darnell Coles0.00768936788078936-Dave Concepcion0.00754097071158992-Doug DeCinces0.016164784133267-Darrell Evans0.0264058825441862-Dwight Evans0.0156073759122261-Damaso Garcia0.00737202574736494-Dan Gladden0.00732057564815775-Dave Henderson0.00493766296992215-Donnie Hill0.00802999130926279-Davey Lopes0.00867219904074439-Don Mattingly0.0903700111079589-Dale Murphy0.0205781074488089-Dwayne Murphy0.00499065937964354-Dave Parker0.0233401084346804-Dan Pasqua0.0148260514260301-Darrell Porter0.0212335628824219-Dick Schofield0.00692844758375807-Don Slaught0.00665433551016605-Darryl Strawberry0.017694944503085-Dale Sveum0.00583616542870044-Danny Tartabull0.0128297967927647-Denny Walling0.0124382760330085-Dave Winfield0.0133528039520951-Eric Davis0.0205166608140052-Eddie Milner0.00477555388471096-Eddie Murray0.0126894264569264-Ed Romero0.0120154735656705-Frank White0.0103217756795036-George Bell0.0344024597514498-Glenn Braggs0.0102378206814018-George Brett0.0070249800741032-Greg Brock0.0102483444612376-Gary Carter0.0133493295035423-Glenn Davis0.0233784484234977-Gary Gaetti0.0300641988907722-Greg Gagne0.00756816838324455-George Hendrick0.00993821578501312-Glenn Hubbard0.0144050547662455-Garth Iorg0.00750828700013946-Gary Matthews0.0119812939218614-Graig Nettles0.0135988964718854-Gary Pettis0.0180946787493926-Gary Redus0.00533116613569048-Garry Templeton0.0231472795912073-Greg Walker0.00954059659043919-Gary Ward0.0158776584493562-Glenn Wilson0.0102664940810771-Harold Baines0.0136874329182192-Hubie Brooks0.0245534617574867-Howard Johnson0.0111605903106341-Hal McRae0.00660970775119084-Harold Reynolds0.0266174645810819-Harry Spilman0.0178175584405383-Herm Winningham0.0125072976893885-Jesse Barfield0.0472745129906935-Juan Beniquez0.0098405887950288-John Cangelosi0.0185585370145323-Jose Canseco0.0385966269231462-Joe Carter0.0279123839207128-Jack Clark0.00976153800810466-Jose Cruz0.00621927886532388-Jody Davis0.0125836354713245-Jim Dwyer0.0159318273881519-Julio Franco0.0230480594602062-Jim Gantner0.00960427626759017-Johnny Grubb0.0249337849541658-Jack Howell0.0166783571884679-John Kruk0.0135348618245948-Jeffrey Leonard0.00621692194497029-Jim Morrison0.0104347491092473-John Moses0.00955816927339891-Jerry Mumphrey0.0114527911454454-Jim Presley0.0181732014300399-Johnny Ray0.0222264061248388-Jeff Reed0.0141048942273143-Jim Rice0.0341630071931888-Jerry Royster0.00786584848798418-John Russell0.00732221183848203-Juan Samuel0.011233958379014-John Shelby0.0105769969697364-Joel Skinner0.00820725725265926-Jim Sundberg0.0196190015932163-Jose Uribe0.0243170465990865-Joel Youngblood0.0127513647950321-Kevin Bass0.0219191326187113-Kal Daniels0.0194979541215839-Kirk Gibson0.020556919885839-Ken Griffey0.0157391819211111-Keith Hernandez0.0186125240001884-Kent Hrbek0.0192503945544296-Ken Landreaux0.00738914265279174-Kevin McReynolds0.0156330543753078-Kevin Mitchell0.00623609661997075-Keith Moreland0.0121555120488774-Ken Oberkfell0.0126467584348519-Ken Phelps0.0198333298123843-Kirby Puckett0.0552718094796791-Kurt Stillwell0.011990041252144-Leon Durham0.00780417454171992-Len Dykstra0.00857784296541703-Larry Herndon0.0064990040160584-Lee Lacy0.00742301328937459-Len Matuszek0.0129926810188161-Lloyd Moseby0.0154094589566827-Lance Parrish0.0168397241242445-Larry Parrish0.0195480106203573-Larry Sheets0.0102053644915363-Lou Whitaker0.0103736298972029-Mike Aldrete0.0108976777772544-Marty Barrett0.0291908907582424-Mike Davis0.00667586668426171-Mike Diaz0.0147105547379761-Mariano Duncan0.0115422666932657-Mike Easler0.010820500442813-Mel Hall0.00976796679777078-Mike Heath0.0081507738805316-Mike Kingery0.0111519990806471-Mike LaValliere0.00901026773489625-Mike Marshall0.013711182342755-Mike Pagliarulo0.0255437587312095-Mark Salas0.00824038954487028-Mike Schmidt0.0308861245462707-Mike Scioscia0.00698057071979755-Mickey Tettleton0.0135500510047471-Milt Thompson0.00617205972690294-Mitch Webster0.0169403513647917-Mookie Wilson0.00619444773498657-Marvell Wynne0.0063403620095783-Mike Young0.00474275439485779-Ozzie Guillen0.0271620505926552-Oddibe McDowell0.00964328420757499-Ozzie Smith0.0229502793106078-Ozzie Virgil0.0114067190696189-Phil Bradley0.0168810816259417-Phil Garner0.00535812295602376-Pete Incaviglia0.025161431654974-Paul Molitor0.00570944294085908-Pete Rose0.0126156272171943-Pat Sheridan0.00888970164559414-Pat Tabler0.026279450164693-Rafael Belliard0.0123374671998298-Rick Burleson0.00906906701865493-Randy Bush0.00495753180334833-Rick Cerone0.0104730023394748-Ron Cey0.0113956391454122-Rob Deer0.0395521316143585-Rick Dempsey0.0136529420759308-Ron Hassey0.0158151139459369-Rickey Henderson0.0194628445140604-Reggie Jackson0.00963058080211747-Ron Kittle0.0195094238607815-Ray Knight0.0100975869646124-Rick Leach0.0140249162441515-Rick Manning0.0115898080723435-Rance Mulliniks0.00450212481049989-Ron Oester0.0122552708544235-Rey Quinones0.0128781945206417-Rafael Ramirez0.0155619012974591-Ronn Reynolds0.0174189559060563-Ron Roenicke0.007216482671799-Ryne Sandberg0.0148031952946029-Rafael Santana0.0213487186421553-Rick Schu0.0124262441988659-Ruben Sierra0.00575359430765965-Roy Smalley0.0106124476383773-Robby Thompson0.0140106441432452-Rob Wilfong0.0109282567228258-Robin Yount0.0199323190913311-Steve Balboni0.032832608857617-Scott Bradley0.013819143124654-Sid Bream0.0064848822413407-Steve Buechele0.010047792882046-Shawon Dunston0.0154878353515676-Scott Fletcher0.0231584911833167-Steve Garvey0.0124707214778381-Steve Jeltz0.0294334036298904-Steve Lombardozzi0.0163703558064671-Spike Owen0.0361863260829997-Steve Sax0.056205798190448-Tony Bernazard0.0137201779926164-Tom Brookens0.00842727602684437-Tom Brunansky0.0155535640161956-Tony Fernandez0.0384149427739519-Tim Flannery0.00879815995334561-Tom Foley0.010178072996595-Tony Gwynn0.0441255560927934-Terry Harper0.00728359774271928-Tommy Herr0.0283062395596944-Tim Hulett0.0193407789424842-Terry Kennedy0.0326924785968483-Tito Landrum0.0124610368212244-Tim Laudner0.0137152102414128-Tom Paciorek0.0126750030530696-Tony Pena0.00921261317536243-Terry Pendleton0.0411002808615697-Tony Phillips0.00947681648702826-Terry Puhl0.013480153488892-Ted Simmons0.0183540764261599-Tim Teufel0.00754075413540749-Tim Wallach0.014724129247963-Vince Coleman0.0540676127419561-Von Hayes0.0192039404330803-Vance Law0.0113459171671123-Wally Backman0.022556121420949-Wade Boggs0.0736687853777228-Will Clark0.00548420107775056-Wally Joyner0.01320331340394-Willie McGee0.0115854961579362-Willie Randolph0.011798792067581-Wayne Tolleson0.0136448610296419-Willie Upshaw0.0189725147824934-Willie Wilson0.0210111323582833\n\n    $coefficients\n        \nA matrix: 263 × 4 of type dbl\n\n    (Intercept)AtBatHitsHmRun\n\n\n    -Alan Ashby 4.817418e-03-2.986297e-06-9.361969e-06-6.101971e-05\n    -Alvin Davis 5.783712e-06-1.988686e-08-6.958574e-09-1.532526e-06\n    -Andre Dawson 3.051739e-05 5.108640e-06-1.863631e-05-4.502371e-05\n    -Andres Galarraga-1.394196e-02 5.825139e-05-1.224464e-04-1.059912e-04\n    -Alfredo Griffin-3.880218e-03 1.203274e-05 4.598602e-05-3.844865e-04\n    -Al Newman-1.699795e-02-1.973901e-05 1.691955e-04 2.508389e-04\n    -Argenis Salazar-8.046090e-03-2.767402e-05 9.120649e-05 5.189035e-04\n    -Andres Thomas-1.089006e-02-2.290534e-05 1.092744e-04 2.854954e-04\n    -Andre Thornton 1.934670e-04 1.257660e-04-4.795294e-04 4.627899e-04\n    -Alan Trammell 1.368784e-03-2.410167e-06-4.767847e-06-4.928569e-05\n    -Alex Trevino 1.506924e-02-5.429191e-05 9.631131e-05-5.069911e-05\n    -Andy VanSlyke 1.018986e-03-2.296905e-06 8.152666e-06 2.113456e-05\n    -Alan Wiggins 1.700556e-02-2.150357e-05 1.763226e-05-5.069653e-04\n    -Bill Almon 1.543927e-03-1.286964e-06-7.912781e-06 1.368373e-05\n    -Buddy Bell-1.705882e-03 2.019092e-06 1.123836e-05 5.235392e-05\n    -Buddy Biancalana-3.897104e-03 8.342080e-06-6.374658e-06 3.768041e-05\n    -Bruce Bochy-1.330506e-02 5.600915e-05-8.215845e-05-2.059641e-04\n    -Barry Bonds 2.279533e-03-1.600465e-04 5.715951e-04-3.232635e-04\n    -Bobby Bonilla 6.803360e-04-7.258320e-05 1.550213e-04 6.909102e-04\n    -Bob Brenly-1.918921e-03 4.389361e-05-1.378344e-04 6.196381e-05\n    -Bill Buckner-3.908655e-03 2.534192e-05-4.654425e-05-2.237198e-05\n    -Brett Butler-5.102074e-03 2.867516e-05 7.199807e-06-4.680230e-04\n    -Bob Dernier 5.841442e-03 9.566493e-05-3.306795e-04-4.014555e-04\n    -Bo Diaz-9.247399e-04 1.139785e-05 4.045893e-06-1.642212e-04\n    -Bill Doran-2.368204e-03 1.411712e-05 4.932303e-06-2.315089e-04\n    -Brian Downing-2.439010e-03 1.778817e-05-4.479265e-05 1.943382e-04\n    -Billy Hatcher-6.440503e-04-5.899958e-05 1.344372e-04 4.939621e-04\n    -Brook Jacoby 3.516924e-04 1.218804e-06-9.821930e-06 1.669940e-06\n    -Bob Kearney 4.747801e-03-1.011448e-05 1.090719e-06 1.826653e-05\n    -Bill Madlock 5.821589e-03-3.692670e-05 1.178859e-04-4.368741e-05\n    ⋮⋮⋮⋮⋮\n    -Tony Fernandez 1.052602e-02 8.419720e-05-5.233673e-04 7.202039e-04\n    -Tim Flannery-4.068231e-05 1.805885e-07-8.305763e-07 3.052943e-06\n    -Tom Foley-9.240389e-05 2.491327e-07-6.076509e-07 2.794985e-06\n    -Tony Gwynn 1.174950e-03 6.022291e-05-2.586965e-04 1.095459e-04\n    -Terry Harper 6.264040e-03-1.582243e-05 1.345592e-05 3.096281e-05\n    -Tommy Herr-1.253907e-02 1.597799e-04-3.366306e-04-1.032398e-03\n    -Tim Hulett 7.975363e-03-1.386157e-04 4.264418e-04-6.634915e-05\n    -Terry Kennedy 5.997923e-02-2.615496e-04 4.547162e-04 2.756732e-04\n    -Tito Landrum 5.790292e-03 8.000686e-06-6.156594e-05-8.371297e-05\n    -Tim Laudner-1.164993e-04 3.626904e-07-2.408513e-07-2.255649e-06\n    -Tom Paciorek-2.044713e-03 1.065779e-05-2.549355e-05 8.908786e-06\n    -Tony Pena-1.740733e-03-2.408537e-05 1.685463e-04-3.034906e-04\n    -Terry Pendleton 1.289931e-02-1.809589e-04 4.345455e-04 8.980146e-04\n    -Tony Phillips-4.086001e-04 1.800300e-05-3.984644e-05-1.400773e-04\n    -Terry Puhl 3.040861e-02-8.496636e-05 9.801338e-05-9.885820e-05\n    -Ted Simmons 2.260963e-02-8.778308e-05 1.393812e-04 1.012275e-04\n    -Tim Teufel 1.132868e-03 3.133402e-07-5.547655e-06-2.487321e-05\n    -Tim Wallach-4.706632e-03 1.041449e-04-3.412129e-04 1.845919e-04\n    -Vince Coleman 1.460109e-02-2.070472e-04 5.136607e-04 9.377216e-04\n    -Von Hayes-2.691757e-03-4.871620e-05 2.214694e-04 2.726047e-05\n    -Vance Law 1.466223e-03 8.485382e-05-2.748316e-04-2.829650e-04\n    -Wally Backman 3.033118e-03-4.292845e-05 1.662334e-04-2.134815e-04\n    -Wade Boggs 2.336669e-03-1.711974e-04 6.758830e-04-3.477363e-04\n    -Will Clark-7.253841e-03 7.425048e-05-2.614339e-04 6.024516e-05\n    -Wally Joyner 9.890247e-03 6.359295e-05-3.334252e-04-4.567361e-04\n    -Willie McGee-4.103649e-03 6.596369e-05-1.473306e-04-3.608230e-04\n    -Willie Randolph-2.369271e-03 1.857962e-05 3.563100e-05-5.223953e-04\n    -Wayne Tolleson-2.531200e-04 4.524374e-06-6.094079e-06-5.579062e-05\n    -Willie Upshaw-1.162878e-02 1.379530e-04-3.222032e-04-5.194818e-04\n    -Willie Wilson-1.005121e-02 6.995994e-05-9.304662e-05-4.851564e-04\n\n\n\n    $sigma\n        -Alan Ashby0.788087989207047-Alvin Davis0.788694226481284-Andre Dawson0.788661650873323-Andres Galarraga0.784684794952982-Alfredo Griffin0.788337114130146-Al Newman0.786262541299155-Argenis Salazar0.786802916611473-Andres Thomas0.784398464793053-Andre Thornton0.7850881428335-Alan Trammell0.788613981103893-Alex Trevino0.787230271479112-Andy VanSlyke0.788443711432741-Alan Wiggins0.785528352743374-Bill Almon0.788675988334844-Buddy Bell0.788548875937367-Buddy Biancalana0.788592315971829-Bruce Bochy0.788061155890424-Barry Bonds0.785378068169425-Bobby Bonilla0.786168555191178-Bob Brenly0.788232201709835-Bill Buckner0.788512427646438-Brett Butler0.788149915578498-Bob Dernier0.785813660772349-Bo Diaz0.787873065428321-Bill Doran0.788464477186018-Brian Downing0.787900316001232-Billy Hatcher0.7856791859616-Brook Jacoby0.788687839162256-Bob Kearney0.788527835166416-Bill Madlock0.787045649653056-Bob Melvin0.786484980327064-BillyJo Robidoux0.785740087144776-Bill Schroeder0.788596702820322-Chris Bando0.788610055889589-Chris Brown0.787125654887832-Carmen Castillo0.788679721825813-Chili Davis0.78811228983753-Carlton Fisk0.786089349695099-Curt Ford0.785317749724202-Carney Lansford0.78778198480267-Chet Lemon0.787566707678007-Candy Maldonado0.788678389348269-Carmelo Martinez0.788477865126042-Craig Reynolds0.788261244175587-Cal Ripken0.787863635256176-Cory Snyder0.781646564196697-Chris Speier0.788648618053139-Curt Wilkerson0.788684111170213-Dave Anderson0.788692872574602-Don Baylor0.787874897329411-Daryl Boston0.785236358622793-Darnell Coles0.782046884888717-Dave Concepcion0.78863268553584-Doug DeCinces0.788090534667608-Darrell Evans0.788671904266285-Dwight Evans0.787971280700316-Damaso Garcia0.787113204199-Dan Gladden0.788263659758164-Dave Henderson0.78862647579947-Donnie Hill0.788631269315016-Davey Lopes0.788158992681009-Don Mattingly0.788660291719989-Dale Murphy0.786078844438097-Dwayne Murphy0.787489141251169-Dave Parker0.788494124485019-Dan Pasqua0.78505429388851-Darrell Porter0.788693101647752-Dick Schofield0.788545491516604-Don Slaught0.788514203312617-Darryl Strawberry0.786807858453041-Dale Sveum0.784020227306241-Danny Tartabull0.784072466595428-Denny Walling0.788557073242349-Dave Winfield0.785184552147012-Eric Davis0.787925190716413-Eddie Milner0.78856906723733-Eddie Murray0.783899833933777-Ed Romero0.787764264758121-Frank White0.788565273918371-George Bell0.788680010666088-Glenn Braggs0.785303522751869-George Brett0.785626689374899-Greg Brock0.788564990901053-Gary Carter0.783947247342071-Glenn Davis0.785504133898955-Gary Gaetti0.788690454472167-Greg Gagne0.78675852038659-George Hendrick0.787283500279756-Glenn Hubbard0.787525873996843-Garth Iorg0.788525285594942-Gary Matthews0.787775386753804-Graig Nettles0.788339141078181-Gary Pettis0.788691757229353-Gary Redus0.788514562427462-Garry Templeton0.787032933366841-Greg Walker0.788261767785901-Gary Ward0.788375741945489-Glenn Wilson0.788592135020008-Harold Baines0.788499761629795-Hubie Brooks0.788078317104342-Howard Johnson0.78863904171628-Hal McRae0.78860563198926-Harold Reynolds0.78581978830625-Harry Spilman0.788529822154808-Herm Winningham0.786989228651752-Jesse Barfield0.788524019432133-Juan Beniquez0.788626689107663-John Cangelosi0.786082973427543-Jose Canseco0.78466538475743-Joe Carter0.783583328956248-Jack Clark0.781888793397349-Jose Cruz0.787913602863617-Jody Davis0.787233018430974-Jim Dwyer0.788610664564018-Julio Franco0.788655136326588-Jim Gantner0.787466856890579-Johnny Grubb0.788685819316832-Jack Howell0.786838108703902-John Kruk0.785844834141431-Jeffrey Leonard0.785209714654203-Jim Morrison0.78730398923047-John Moses0.784286439448128-Jerry Mumphrey0.78789542704568-Jim Presley0.78512023158689-Johnny Ray0.788673142619015-Jeff Reed0.786167187870066-Jim Rice0.786639414899379-Jerry Royster0.788692120188429-John Russell0.787635584676733-Juan Samuel0.788612862554493-John Shelby0.788694081135428-Joel Skinner0.787016999672412-Jim Sundberg0.785803777422643-Jose Uribe0.788454677863981-Joel Youngblood0.78759185955881-Kevin Bass0.788498434532896-Kal Daniels0.785309017100759-Kirk Gibson0.786610160831-Ken Griffey0.788248228927591-Keith Hernandez0.786279047403214-Kent Hrbek0.787346614303741-Ken Landreaux0.786148743042568-Kevin McReynolds0.788651014227156-Kevin Mitchell0.786045224892181-Keith Moreland0.787485053806796-Ken Oberkfell0.787826973031995-Ken Phelps0.788551952316283-Kirby Puckett0.783872240140299-Kurt Stillwell0.7858150616285-Leon Durham0.786610355579879-Len Dykstra0.787240122046691-Larry Herndon0.788610518123156-Lee Lacy0.788681477057886-Len Matuszek0.788691711033513-Lloyd Moseby0.788267571453955-Lance Parrish0.78727738986167-Larry Parrish0.788692526849978-Larry Sheets0.786319900331611-Lou Whitaker0.788437446953203-Mike Aldrete0.785653958052293-Marty Barrett0.78869060236359-Mike Davis0.788144626007235-Mike Diaz0.785494591022498-Mariano Duncan0.787670235764994-Mike Easler0.78858498201182-Mel Hall0.788691070813871-Mike Heath0.78641840420225-Mike Kingery0.784895250874634-Mike LaValliere0.786759899681058-Mike Marshall0.787367322394461-Mike Pagliarulo0.786242684868245-Mark Salas0.787818100074675-Mike Schmidt0.771516966354341-Mike Scioscia0.785772232406313-Mickey Tettleton0.787722898084898-Milt Thompson0.787614518459424-Mitch Webster0.786320351821897-Mookie Wilson0.787408039816236-Marvell Wynne0.788617570645859-Mike Young0.788666377633358-Ozzie Guillen0.787456625441716-Oddibe McDowell0.786275765146475-Ozzie Smith0.782478446295041-Ozzie Virgil0.786851189258332-Phil Bradley0.788613223218722-Phil Garner0.788345387520181-Pete Incaviglia0.785251799394364-Paul Molitor0.785615452318523-Pete Rose0.784449682022727-Pat Sheridan0.788567349137213-Pat Tabler0.788691562821052-Rafael Belliard0.787880816142943-Rick Burleson0.788226027049855-Randy Bush0.788677326352094-Rick Cerone0.788690776433792-Ron Cey0.785017275015262-Rob Deer0.787149192514477-Rick Dempsey0.788228157249776-Ron Hassey0.788502502037285-Rickey Henderson0.786492679287534-Reggie Jackson0.788491779433869-Ron Kittle0.788509683612867-Ray Knight0.788689546024379-Rick Leach0.788600745065715-Rick Manning0.788136514799188-Rance Mulliniks0.788440138683998-Ron Oester0.787699869276881-Rey Quinones0.785330598643259-Rafael Ramirez0.786430947960566-Ronn Reynolds0.788694050873503-Ron Roenicke0.788489466758294-Ryne Sandberg0.788642960122429-Rafael Santana0.788686776152599-Rick Schu0.78767529465952-Ruben Sierra0.784033196925155-Roy Smalley0.78783231649691-Robby Thompson0.785279183689162-Rob Wilfong0.78824466462335-Robin Yount0.788045263286404-Steve Balboni0.782942077266366-Scott Bradley0.785495385383538-Sid Bream0.786713951592677-Steve Buechele0.785895356612354-Shawon Dunston0.785628626425154-Scott Fletcher0.788684304146463-Steve Garvey0.785934312873395-Steve Jeltz0.788106821061602-Steve Lombardozzi0.785997102203074-Spike Owen0.78861285067638-Steve Sax0.775206275387361-Tony Bernazard0.788531731187791-Tom Brookens0.78856550988956-Tom Brunansky0.788008475661489-Tony Fernandez0.786627372789857-Tim Flannery0.788694218978449-Tom Foley0.788694195380288-Tony Gwynn0.788483659759081-Terry Harper0.78821331159661-Tommy Herr0.786478697056646-Tim Hulett0.787419866834543-Terry Kennedy0.781408804911603-Tito Landrum0.788361296031385-Tim Laudner0.788694216357093-Tom Paciorek0.788669010558951-Tony Pena0.786984130261975-Terry Pendleton0.787367431923881-Tony Phillips0.788547450382495-Terry Puhl0.783545249747788-Ted Simmons0.786783423090058-Tim Teufel0.788670940584408-Tim Wallach0.787487929036171-Vince Coleman0.787531460932664-Von Hayes0.788128574373877-Vance Law0.787381401320859-Wally Backman0.788474746198881-Wade Boggs0.788017299170636-Will Clark0.784833631339206-Wally Joyner0.783224300001469-Willie McGee0.787691098008247-Willie Randolph0.787278372318702-Wayne Tolleson0.788682284024342-Willie Upshaw0.786902984445929-Willie Wilson0.787639393843468\n\n    $wt.res\n        -Alan Ashby0.495297931132064-Alvin Davis-0.00543853956028187-Andre Dawson-0.114786848849309-Andres Galarraga-1.27229531063985-Alfredo Griffin0.376500219971959-Al Newman-0.986997038544126-Argenis Salazar-0.872011685580675-Andres Thomas-1.31655958876089-Andre Thornton1.20288647089233-Alan Trammell-0.179889013669906-Alex Trevino0.767040183099494-Andy VanSlyke0.318692333260666-Alan Wiggins1.12750852603613-Bill Almon0.0858028115614627-Buddy Bell0.24213414596455-Buddy Biancalana-0.202457390121689-Bruce Bochy-0.502300973698605-Barry Bonds-1.15232499810785-Bobby Bonilla-1.00741907658686-Bob Brenly0.431696722119547-Bill Buckner0.270144689736068-Brett Butler0.465121886329366-Bob Dernier1.07621794940745-Bo Diaz0.576286113193403-Bill Doran0.303435223205402-Brian Downing0.566083438554661-Billy Hatcher-1.10272955204631-Brook Jacoby-0.0509964324186427-Bob Kearney0.258845886332418-Bill Madlock0.816743698554709-Bob Melvin-0.943280569128019-BillyJo Robidoux-1.08812574734023-Bill Schroeder-0.198162260202405-Chris Bando0.184247170768769-Chris Brown-0.791926357419985-Carmen Castillo-0.076520019321227-Chili Davis0.484874205163271-Carlton Fisk1.01956500509913-Curt Ford-1.16454157784575-Carney Lansford0.605880918884638-Chet Lemon0.675486069950702-Candy Maldonado0.080158182728543-Carmelo Martinez0.295431177832217-Craig Reynolds0.418487879923019-Cal Ripken0.576751757234697-Cory Snyder-1.67771253764946-Chris Speier0.135138292021446-Curt Wilkerson0.0640179564537409-Dave Anderson0.0239608031254833-Don Baylor0.567051895161756-Daryl Boston-1.17784829349113-Darnell Coles-1.63497974751705-Dave Concepcion0.157749106577464-Doug DeCinces0.491576588246525-Darrell Evans0.094198452230805-Dwight Evans0.538069833035215-Damaso Garcia0.798792036697478-Dan Gladden-0.417043656520249-Dave Henderson-0.165724241440638-Donnie Hill-0.159512381291457-Davey Lopes0.464638898408859-Don Mattingly0.11220097338442-Dale Murphy1.02018138621454-Dwayne Murphy0.69831097433263-Dave Parker0.282051460542273-Dan Pasqua-1.20666029338326-Darrell Porter0.0218442236260971-Dick Schofield0.245223583138745-Don Slaught0.269809338287861-Darryl Strawberry0.867888978101088-Dale Sveum-1.3731294906825-Danny Tartabull-1.36064602790244-Denny Walling0.234834114695889-Dave Winfield1.1858080084532-Eric Davis-0.553561319997254-Eddie Milner0.225206411062808-Eddie Murray1.38584665743496-Ed Romero0.611330145173554-Frank White0.22795362953804-George Bell0.0749333382164492-Glenn Braggs-1.16741951227088-George Brett1.11230975245218-Greg Brock0.228211945679811-Gary Carter1.3785370379044-Glenn Davis-1.12488820474076-Gary Gaetti0.0389587510529879-Greg Gagne-0.883671415014483-George Hendrick0.753613498492403-Glenn Hubbard0.684333906230093-Garth Iorg0.261264686417221-Gary Matthews0.607676452682857-Graig Nettles-0.3775445392702-Gary Pettis0.0318736856719171-Gary Redus0.269719691207034-Garry Templeton0.812264750944759-Greg Walker0.417490899934653-Gary Ward0.35715267367006-Glenn Wilson0.202849728341657-Harold Baines0.279422827143254-Hubie Brooks0.494402098669122-Howard Johnson0.149118513263513-Hal McRae0.189325638832865-Harold Reynolds-1.06612201690011-Harry Spilman-0.256392787789728-Herm Winningham-0.82733714649685-Jesse Barfield0.256934056623595-Juan Beniquez0.1650552994761-John Cangelosi-1.02042756801519-Jose Canseco-1.2539225887893-Joe Carter-1.4196420952158-Jack Clark1.65249602766754-Jose Cruz0.5617695116793-Jody Davis0.765940435992398-Jim Dwyer0.183009814363443-Julio Franco0.124781398953848-Jim Gantner0.703096854658612-Johnny Grubb0.058009620489294-Jack Howell-0.861355915223531-John Kruk-1.06858519077484-Jeffrey Leonard-1.18582417695497-Jim Morrison-0.747938383666698-John Moses-1.33106708932092-Jerry Mumphrey0.566769738808514-Jim Presley-1.19367328735601-Johnny Ray0.0917534055033484-Jeff Reed-1.00614045295719-Jim Rice0.89813533469396-Jerry Royster-0.0296642100248916-John Russell-0.65377314201143-Juan Samuel0.181019658560744-John Shelby0.00937888697530022-Joel Skinner-0.822363835805777-Jim Sundberg1.07291795974715-Jose Uribe-0.308435009470071-Joel Youngblood0.665300948412535-Kevin Bass-0.279201727455344-Kal Daniels-1.16100578596754-Kirk Gibson0.910846136755323-Ken Griffey0.422644006978702-Keith Hernandez0.981404296809048-Kent Hrbek0.733106231900006-Ken Landreaux1.013233117435-Kevin McReynolds-0.13168074893095-Kevin Mitchell-1.03419617740358-Keith Moreland0.696970267256109-Ken Oberkfell0.590183918257675-Ken Phelps-0.238278110767064-Kirby Puckett-1.35951525760211-Kurt Stillwell-1.07498379497533-Leon Durham0.91671397527474-Len Dykstra-0.76562648945727-Larry Herndon-0.184045754021215-Lee Lacy0.0719691618070511-Len Matuszek0.0322454099378014-Lloyd Moseby0.413450878076408-Lance Parrish0.752605367472884-Larry Parrish0.0265959016015816-Larry Sheets-0.97724062248002-Lou Whitaker-0.321604031482053-Mike Aldrete-1.1052029976205-Marty Barrett0.0382193829021389-Mike Davis0.471304347125559-Mike Diaz-1.13155430682788-Mariano Duncan-0.641624213888835-Mike Easler0.209771608294576-Mel Hall0.0360716698727379-Mike Heath0.957777537445586-Mike Kingery-1.23497391828789-Mike LaValliere-0.882714891032766-Mike Marshall0.729508294750628-Mike Pagliarulo-0.985255175700308-Mark Salas-0.594515609271486-Mike Schmidt2.58861124879423-Mike Scioscia1.08567675933308-Mickey Tettleton-0.624284207184772-Milt Thompson-0.660623431696097-Mitch Webster-0.973817706371425-Mookie Wilson0.720969866122022-Marvell Wynne-0.176143684321503-Mike Young0.106344726946941-Ozzie Guillen-0.699732692751414-Oddibe McDowell-0.986547428059776-Ozzie Smith1.56902741497459-Ozzie Virgil0.860616801361628-Phil Bradley0.18010186908473-Phil Garner0.375770369109625-Pete Incaviglia-1.16736890038625-Paul Molitor1.11507910094352-Pete Rose1.30423694379153-Pat Sheridan-0.226276705627592-Pat Tabler0.032931772331738-Rafael Belliard-0.571670571974861-Rick Burleson0.434495984612881-Randy Bush-0.0829048675935128-Rick Cerone0.037666782549679-Ron Cey1.21487599177564-Rob Deer-0.776752363629396-Rick Dempsey0.432503050620572-Ron Hassey0.277148590255229-Rickey Henderson0.936653904475687-Reggie Jackson0.285682776968674-Ron Kittle0.27140018527723-Ray Knight-0.0437607252858133-Rick Leach-0.19374552988617-Rick Manning0.473591985068541-Rance Mulliniks0.320862346100393-Ron Oester0.63205053109618-Rey Quinones-1.1612071824936-Rafael Ramirez0.951563162601697-Ronn Reynolds-0.00997277788536639-Ron Roenicke-0.287659066189093-Ryne Sandberg0.143469410559807-Rafael Santana0.054739436545673-Rick Schu-0.639752203826063-Ruben Sierra-1.37128568405934-Roy Smalley0.588969901920232-Robby Thompson-1.16935774286535-Rob Wilfong0.425364489884647-Robin Yount0.508689021003016-Steve Balboni-1.50194677789557-Scott Bradley-1.13192581459033-Sid Bream-0.894261240233036-Steve Buechele-1.0609567729455-Shawon Dunston-1.10721059377824-Scott Fletcher-0.0630363308078193-Steve Garvey1.05227053225554-Steve Jeltz-0.481622738162541-Steve Lombardozzi-1.03819695469136-Spike Owen0.178734002234065-Steve Sax-2.26635137322017-Tony Bernazard-0.255431800411695-Tom Brookens0.227962979641856-Tom Brunansky0.524067383819616-Tony Fernandez-0.898774781417493-Tim Flannery-0.00570243569300807-Tom Foley-0.00647907478902504-Tony Gwynn-0.286233774284674-Terry Harper0.440750379047368-Tommy Herr0.93537253898349-Tim Hulett-0.712887845356798-Terry Kennedy1.68960717126566-Tito Landrum0.365792597371603-Tim Laudner-0.00578002022214194-Tom Paciorek-0.100802635353543-Tony Pena0.829952885309686-Terry Pendleton-0.719278147835191-Tony Phillips0.243291598575366-Terry Puhl1.43544207712413-Ted Simmons0.873191963164239-Tim Teufel0.0971313685052364-Tim Wallach0.695236162812252-Vince Coleman-0.668819368913094-Von Hayes0.475109192778092-Vance Law0.726500821547648-Wally Backman0.295505090214498-Wade Boggs0.505083990007538-Will Clark-1.2484874081587-Wally Joyner-1.47955927048621-Willie McGee0.63504531094061-Willie Randolph0.754270908480397-Wayne Tolleson0.0694490523617452-Willie Upshaw0.845199148995055-Willie Wilson0.648081520500237\n\n\n\n\n\ninfluence.measures(reg_model_2)\n\nInfluence measures of\n     lm(formula = log_Salary ~ AtBat + Hits + HmRun, data = hitters) :\n\n                      dfb.1_  dfb.AtBt  dfb.Hits  dfb.HmRn     dffit cov.r\n-Alan Ashby         3.22e-02 -2.36e-03 -2.31e-03 -9.13e-03  0.046453 1.015\n-Alvin Davis        3.86e-05 -1.57e-05 -1.71e-06 -2.29e-04 -0.000532 1.022\n-Andre Dawson       2.04e-04  4.03e-03 -4.59e-03 -6.73e-03 -0.013393 1.024\n-Andres Galarraga  -9.35e-02  4.62e-02 -3.03e-02 -1.59e-02 -0.120216 0.980\n-Alfredo Griffin   -2.59e-02  9.50e-03  1.13e-02 -5.75e-02  0.076818 1.037\n-Al Newman         -1.14e-01 -1.56e-02  4.18e-02  3.76e-02 -0.151315 1.005\n-Argenis Salazar   -5.38e-02 -2.19e-02  2.25e-02  7.77e-02 -0.117140 1.007\n-Andres Thomas     -7.30e-02 -1.82e-02  2.71e-02  4.29e-02 -0.128939 0.978\n-Andre Thornton     1.30e-03  9.97e-02 -1.19e-01  6.95e-02  0.168499 0.991\n-Alan Trammell      9.13e-03 -1.90e-03 -1.17e-03 -7.37e-03 -0.022981 1.025\n-Alex Trevino       1.01e-01 -4.29e-02  2.38e-02 -7.59e-03  0.106184 1.012\n-Andy VanSlyke      6.80e-03 -1.81e-03  2.01e-03  3.16e-03  0.025414 1.017\n-Alan Wiggins       1.14e-01 -1.70e-02  4.36e-03 -7.61e-02  0.154552 0.995\n-Bill Almon         1.03e-02 -1.02e-03 -1.95e-03  2.05e-03  0.012092 1.028\n-Buddy Bell        -1.14e-02  1.59e-03  2.77e-03  7.83e-03  0.029828 1.024\n-Buddy Biancalana  -2.60e-02  6.58e-03 -1.57e-03  5.63e-03 -0.028878 1.027\n-Bruce Bochy       -8.88e-02  4.42e-02 -2.03e-02 -3.08e-02 -0.092971 1.030\n-Barry Bonds        1.53e-02 -1.27e-01  1.41e-01 -4.85e-02 -0.176307 0.996\n-Bobby Bonilla      4.55e-03 -5.75e-02  3.83e-02  1.04e-01 -0.136291 1.001\n-Bob Brenly        -1.28e-02  3.47e-02 -3.40e-02  9.27e-03  0.051620 1.020\n-Bill Buckner      -2.61e-02  2.00e-02 -1.15e-02 -3.34e-03  0.040969 1.028\n-Brett Butler      -3.41e-02  2.26e-02  1.77e-03 -7.00e-02  0.091957 1.034\n-Bob Dernier        3.91e-02  7.58e-02 -8.17e-02 -6.02e-02  0.139750 0.996\n-Bo Diaz           -6.18e-03  9.00e-03  9.98e-04 -2.46e-02  0.055993 1.013\n-Bill Doran        -1.58e-02  1.11e-02  1.22e-03 -3.46e-02  0.048561 1.029\n-Brian Downing     -1.63e-02  1.40e-02 -1.10e-02  2.91e-02  0.063853 1.015\n-Billy Hatcher     -4.31e-03 -4.67e-02  3.32e-02  7.41e-02 -0.119083 0.992\n-Brook Jacoby       2.35e-03  9.62e-04 -2.42e-03  2.50e-04 -0.006809 1.027\n-Bob Kearney        3.17e-02 -7.98e-03  2.69e-04  2.73e-03  0.034619 1.025\n-Bill Madlock       3.89e-02 -2.92e-02  2.91e-02 -6.54e-03  0.071745 1.003\n-Bob Melvin        -6.87e-02 -3.23e-02  4.89e-02  2.30e-02 -0.114816 1.002\n-BillyJo Robidoux  -1.39e-01  1.92e-02  8.61e-03  3.91e-02 -0.162044 0.999\n-Bill Schroeder    -2.04e-02 -2.51e-03  8.72e-03 -3.14e-03 -0.026988 1.026\n-Chris Bando        1.88e-02 -7.08e-03  5.11e-03 -9.26e-03  0.023341 1.025\n-Chris Brown       -4.56e-02  9.28e-02 -1.07e-01  4.15e-02 -0.131922 1.016\n-Carmen Castillo   -1.07e-02  6.37e-03 -4.01e-03 -2.53e-03 -0.011220 1.029\n-Chili Davis       -1.51e-02  4.82e-03  6.22e-03 -1.38e-02  0.052004 1.017\n-Carlton Fisk      -4.68e-02  1.52e-01 -1.53e-01  3.84e-03  0.176295 1.007\n-Curt Ford         -1.35e-01  3.14e-02 -8.18e-03  4.26e-02 -0.157044 0.992\n-Carney Lansford   -3.22e-02 -3.99e-03  1.95e-02  8.29e-03  0.080463 1.017\n-Chet Lemon         8.58e-03  2.73e-02 -2.94e-02  2.34e-03  0.060812 1.009\n-Candy Maldonado    1.43e-03  2.07e-03 -3.17e-03  5.44e-03  0.008958 1.023\n-Carmelo Martinez   2.92e-02 -3.28e-03 -5.06e-03  8.39e-03  0.035805 1.023\n-Craig Reynolds     2.52e-02  5.00e-03 -8.66e-03 -1.21e-02  0.041397 1.017\n-Cal Ripken        -4.02e-02 -8.32e-04  1.31e-02  3.82e-02  0.092525 1.023\n-Cory Snyder       -5.69e-02  5.60e-02 -2.15e-02 -2.21e-01 -0.259816 0.959\n-Chris Speier       2.27e-02 -1.37e-02  8.69e-03  4.00e-03  0.023168 1.033\n-Curt Wilkerson     6.13e-03  2.46e-04 -1.07e-03 -4.26e-03  0.008837 1.027\n-Dave Anderson      2.70e-03 -4.95e-04  6.86e-05 -1.15e-03  0.003283 1.027\n-Don Baylor        -5.52e-02  8.00e-02 -8.42e-02  8.30e-02  0.140061 1.044\n-Daryl Boston      -1.60e-01  7.59e-02 -4.38e-02 -2.60e-03 -0.166329 0.992\n-Darnell Coles      4.52e-02 -1.65e-02  3.62e-03 -8.05e-02 -0.184747 0.956\n-Dave Concepcion    1.04e-02 -7.25e-04  3.56e-04 -9.50e-03  0.017502 1.023\n-Doug DeCinces     -1.71e-02  2.36e-02 -2.76e-02  5.75e-02  0.080608 1.026\n-Darrell Evans     -4.28e-03  8.38e-03 -1.00e-02  1.43e-02  0.019935 1.043\n-Dwight Evans      -2.23e-02  2.45e-02 -2.68e-02  5.97e-02  0.086661 1.024\n-Damaso Garcia      1.66e-02 -1.30e-02  2.61e-02 -5.40e-02  0.087782 1.007\n-Dan Gladden       -2.30e-02  1.02e-02 -1.24e-02  2.63e-02 -0.045601 1.019\n-Dave Henderson    -5.96e-03  1.49e-03  8.68e-05 -6.98e-03 -0.014840 1.020\n-Donnie Hill       -1.06e-02  6.84e-03 -7.43e-03  9.34e-03 -0.018272 1.023\n-Davey Lopes        5.08e-02 -2.76e-02  1.86e-02  1.82e-03  0.055380 1.019\n-Don Mattingly      6.03e-04 -3.58e-02  4.01e-02  1.46e-02  0.047017 1.116\n-Dale Murphy       -8.26e-02  5.28e-02 -4.50e-02  1.16e-01  0.190083 1.010\n-Dwayne Murphy      3.86e-02  5.09e-03 -1.28e-02 -1.44e-03  0.062959 1.008\n-Dave Parker       -2.32e-02  7.24e-03 -4.43e-03  3.62e-02  0.055955 1.038\n-Dan Pasqua        -1.39e-01  1.24e-01 -8.81e-02 -1.14e-01 -0.189970 0.993\n-Darrell Porter     3.66e-03 -2.08e-03  1.00e-03  2.08e-03  0.004123 1.038\n-Dick Schofield    -5.11e-03  1.72e-02 -1.59e-02 -1.42e-03  0.026066 1.021\n-Don Slaught        2.04e-02 -8.60e-03  3.17e-03  1.27e-02  0.028100 1.021\n-Darryl Strawberry -7.83e-03  1.72e-02 -3.16e-02  1.23e-01  0.149374 1.014\n-Dale Sveum        -7.82e-02 -2.49e-02  3.90e-02  2.71e-02 -0.134583 0.974\n-Danny Tartabull    2.76e-02 -5.45e-03  1.09e-02 -1.46e-01 -0.199116 0.982\n-Denny Walling      1.73e-02 -2.76e-02  2.74e-02  5.91e-03  0.033631 1.027\n-Dave Winfield     -7.23e-02  6.53e-02 -5.71e-02  9.04e-02  0.176875 0.993\n-Eric Davis        -2.32e-02  3.01e-02 -1.59e-02 -9.16e-02 -0.102739 1.029\n-Eddie Milner       2.34e-03  4.75e-03 -5.54e-03  6.48e-03  0.019830 1.019\n-Eddie Murray       2.38e-02 -1.31e-01  1.50e-01  3.90e-02  0.201707 0.980\n-Ed Romero          5.30e-02  2.29e-02 -3.47e-02 -2.73e-02  0.086099 1.018\n-Frank White       -1.17e-02  5.32e-03 -2.41e-03  1.24e-02  0.029675 1.025\n-George Bell       -2.82e-03 -8.96e-03  1.04e-02  9.84e-03  0.018250 1.052\n-Glenn Braggs      -1.32e-01  1.93e-02  9.04e-03  1.79e-02 -0.151972 0.991\n-George Brett       3.25e-02 -6.52e-02  6.79e-02  3.94e-02  0.119507 0.991\n-Greg Brock         1.19e-02  6.44e-03 -1.26e-02  1.70e-02  0.029601 1.025\n-Gary Carter       -3.36e-02  6.16e-02 -7.35e-02  1.43e-01  0.205920 0.981\n-Glenn Davis        6.42e-02 -3.82e-02  4.28e-02 -1.70e-01 -0.224204 1.007\n-Gary Gaetti       -1.56e-03 -1.85e-03  1.81e-03  6.96e-03  0.008830 1.047\n-Greg Gagne         2.58e-02 -6.73e-02  5.87e-02  1.92e-02 -0.098456 1.003\n-George Hendrick    7.46e-02 -4.55e-02  2.49e-02  5.27e-02  0.096385 1.011\n-Glenn Hubbard     -9.79e-03  7.68e-02 -6.98e-02 -5.95e-02  0.105819 1.018\n-Garth Iorg         1.47e-02  8.55e-04 -6.87e-04 -1.73e-02  0.028927 1.022\n-Gary Matthews      2.88e-02 -9.08e-03 -6.21e-03  6.93e-02  0.085459 1.018\n-Graig Nettles     -8.25e-03 -3.09e-02  3.99e-02 -2.40e-02 -0.056618 1.026\n-Gary Pettis       -2.15e-03  3.13e-03 -1.94e-03 -3.91e-03  0.005536 1.034\n-Gary Redus         1.27e-02  5.23e-03 -8.83e-03  4.14e-03  0.025109 1.019\n-Garry Templeton   -5.28e-02  1.03e-01 -7.44e-02 -1.19e-01  0.160741 1.022\n-Greg Walker        4.22e-02 -2.78e-02  1.73e-02  2.48e-02  0.052231 1.021\n-Gary Ward          2.54e-02 -3.99e-02  4.49e-02 -2.19e-02  0.058005 1.029\n-Glenn Wilson      -1.41e-02  1.01e-02 -4.25e-03 -5.75e-03  0.026334 1.025\n-Harold Baines     -7.27e-03 -1.69e-02  2.25e-02  1.23e-02  0.042035 1.028\n-Hubie Brooks       6.44e-02 -9.14e-02  8.34e-02  3.22e-02  0.100778 1.035\n-Howard Johnson     1.77e-02 -5.61e-03  5.40e-04  7.21e-03  0.020201 1.026\n-Hal McRae          1.58e-02 -2.28e-03 -9.24e-04 -9.89e-04  0.019648 1.021\n-Harold Reynolds    5.20e-02 -1.77e-01  1.54e-01  1.45e-01 -0.227396 1.013\n-Harry Spilman     -4.36e-02  2.37e-02 -1.40e-02 -5.84e-03 -0.044190 1.032\n-Herm Winningham   -1.02e-01  7.79e-03  1.80e-02  4.02e-03 -0.119059 1.011\n-Jesse Barfield    -7.48e-03 -1.80e-02  1.44e-02  6.49e-02  0.074362 1.064\n-Juan Beniquez      1.29e-02 -1.37e-02  1.41e-02 -6.20e-03  0.020968 1.025\n-John Cangelosi     3.20e-02 -1.25e-01  1.04e-01  1.23e-01 -0.180186 1.008\n-Jose Canseco       1.29e-01 -1.72e-01  1.82e-01 -2.06e-01 -0.326554 1.014\n-Joe Carter         8.56e-02  1.16e-01 -1.52e-01 -1.42e-01 -0.311377 0.992\n-Jack Clark         1.77e-01 -2.84e-02 -2.24e-02  5.46e-02  0.210870 0.957\n-Jose Cruz         -4.56e-03  5.17e-04  1.03e-02 -2.48e-02  0.056579 1.014\n-Jody Davis        -4.21e-02  6.53e-02 -6.28e-02  4.28e-02  0.110534 1.013\n-Jim Dwyer          2.81e-02 -1.16e-02  3.96e-03  8.92e-03  0.029766 1.031\n-Julio Franco      -4.98e-03 -8.23e-03  1.44e-02 -1.08e-02  0.024587 1.039\n-Jim Gantner       -1.74e-02  1.79e-02  1.61e-03 -5.88e-02  0.088350 1.013\n-Johnny Grubb       9.70e-03 -9.70e-03  7.62e-03  5.12e-03  0.011911 1.041\n-Jack Howell       -1.42e-01  7.44e-02 -4.47e-02 -7.68e-03 -0.143773 1.014\n-John Kruk         -1.22e-01  1.14e-01 -1.08e-01  3.84e-02 -0.160368 1.000\n-Jeffrey Leonard   -7.59e-02  4.37e-02 -4.34e-02  4.49e-02 -0.119820 0.986\n-Jim Morrison       2.43e-02 -4.00e-03 -1.17e-04 -5.68e-02 -0.098066 1.012\n-John Moses        -1.50e-02 -6.03e-02  4.18e-02  1.24e-01 -0.167527 0.980\n-Jerry Mumphrey     5.49e-02 -5.36e-02  5.25e-02 -2.05e-02  0.077875 1.019\n-Jim Presley        1.01e-01 -7.18e-02  5.79e-02 -1.09e-01 -0.208752 0.997\n-Johnny Ray        -3.55e-03 -4.05e-03  8.60e-03 -1.03e-02  0.017739 1.038\n-Jeff Reed         -1.44e-01  4.06e-02 -1.06e-02  1.90e-02 -0.154169 1.004\n-Jim Rice          -1.56e-02 -1.42e-01  1.76e-01  1.67e-02  0.218494 1.030\n-Jerry Royster     -2.88e-03  7.71e-04 -2.76e-04  5.67e-04 -0.003362 1.024\n-John Russell      -3.94e-02 -9.09e-03  2.37e-02 -3.00e-02 -0.071551 1.012\n-Juan Samuel       -1.42e-02  1.24e-02 -7.43e-03 -3.70e-03  0.024606 1.026\n-John Shelby       -7.14e-05  9.49e-04 -9.87e-04 -8.43e-05  0.001236 1.026\n-Joel Skinner      -3.87e-02 -4.07e-02  4.76e-02  3.40e-02 -0.095446 1.007\n-Jim Sundberg      -3.97e-02  1.70e-01 -1.74e-01 -8.92e-03  0.195072 1.006\n-Jose Uribe         1.59e-02 -5.09e-02  4.49e-02  3.58e-02 -0.062522 1.038\n-Joel Youngblood    9.32e-02 -3.83e-02  1.78e-02  4.92e-03  0.096620 1.017\n-Kevin Bass         5.52e-03  3.08e-02 -3.90e-02 -7.53e-03 -0.053599 1.036\n-Kal Daniels       -1.96e-01  1.58e-01 -1.24e-01 -2.46e-02 -0.210543 1.001\n-Kirk Gibson        1.80e-02 -1.96e-02 -2.16e-03  1.52e-01  0.169506 1.015\n-Ken Griffey        1.03e-02 -4.51e-02  4.74e-02  3.11e-02  0.068343 1.027\n-Keith Hernandez   -7.84e-03 -9.44e-02  1.28e-01 -3.50e-02  0.173514 1.010\n-Kent Hrbek        -3.07e-02  1.44e-02 -1.77e-02  1.01e-01  0.131723 1.021\n-Ken Landreaux      8.49e-02 -2.08e-02  1.12e-02 -3.98e-02  0.111615 0.997\n-Kevin McReynolds   3.53e-03  5.46e-03 -6.38e-03 -1.34e-02 -0.021208 1.031\n-Kevin Mitchell    -7.82e-02  5.29e-02 -3.82e-02 -3.31e-02 -0.104551 0.995\n-Keith Moreland    -5.04e-02  3.79e-02 -1.32e-02 -4.31e-02  0.098780 1.016\n-Ken Oberkfell     -1.96e-02  2.49e-02 -5.85e-03 -6.38e-02  0.085324 1.020\n-Ken Phelps        -1.36e-02  1.41e-03  7.52e-03 -3.74e-02 -0.043416 1.035\n-Kirby Puckett      4.15e-02  2.77e-01 -3.20e-01 -1.66e-01 -0.431602 1.024\n-Kurt Stillwell    -6.94e-02 -4.57e-02  5.18e-02  8.92e-02 -0.151611 0.998\n-Leon Durham       -1.56e-02  2.55e-02 -2.64e-02  5.65e-02  0.103762 1.002\n-Len Dykstra       -2.38e-02  4.30e-02 -5.53e-02  3.65e-02 -0.090853 1.009\n-Larry Herndon     -1.45e-02  7.11e-04  2.65e-03 -6.66e-04 -0.018937 1.021\n-Lee Lacy          -3.05e-04 -1.91e-03  3.43e-03 -2.66e-03  0.007921 1.023\n-Len Matuszek       4.42e-03 -2.14e-03  1.01e-03  1.47e-03  0.004722 1.029\n-Lloyd Moseby      -3.65e-02  4.33e-02 -3.69e-02  1.40e-02  0.066128 1.027\n-Lance Parrish      5.42e-02 -2.31e-02 -3.84e-03  1.06e-01  0.126178 1.018\n-Larry Parrish      3.86e-04 -9.79e-04  5.01e-04  4.21e-03  0.004809 1.036\n-Larry Sheets      -7.05e-02  4.84e-02 -2.48e-02 -9.15e-02 -0.126844 1.002\n-Lou Whitaker       2.15e-02 -1.47e-02  8.50e-03 -8.32e-03 -0.041981 1.024\n-Mike Aldrete      -1.28e-01  3.22e-02 -1.07e-02  4.11e-02 -0.148469 0.996\n-Marty Barrett     -3.19e-03  1.02e-03  1.36e-03 -6.29e-03  0.008528 1.046\n-Mike Davis        -7.19e-03  7.11e-03 -5.83e-03  2.32e-02  0.049188 1.017\n-Mike Diaz         -1.56e-01  8.78e-02 -4.42e-02 -8.52e-02 -0.177330 0.998\n-Mariano Duncan     7.45e-03 -6.91e-02  6.76e-02  2.74e-02 -0.088537 1.017\n-Mike Easler        2.90e-03 -1.65e-02  2.03e-02 -6.61e-04  0.027973 1.026\n-Mel Hall           1.30e-03 -2.87e-03  2.86e-03  2.06e-03  0.004565 1.026\n-Mike Heath         6.08e-02  3.62e-02 -5.48e-02  9.36e-04  0.110857 1.001\n-Mike Kingery      -1.54e-01  5.65e-02 -2.93e-02  2.90e-02 -0.168032 0.988\n-Mike LaValliere   -4.82e-02 -3.67e-02  4.25e-02  5.17e-02 -0.107468 1.005\n-Mike Marshall      3.75e-02  1.90e-02 -4.23e-02  7.73e-02  0.109998 1.016\n-Mike Pagliarulo    4.65e-02 -9.63e-02  1.13e-01 -1.40e-01 -0.205529 1.017\n-Mark Salas        -5.14e-02 -4.32e-03  1.87e-02 -7.28e-03 -0.069072 1.015\n-Mike Schmidt       6.04e-01 -2.61e-01  1.16e-01  3.29e-02  0.608455 0.879\n-Mike Scioscia      2.65e-02  4.29e-02 -3.77e-02 -6.73e-02  0.116250 0.993\n-Mickey Tettleton  -6.64e-02 -8.00e-03  3.22e-02 -3.16e-02 -0.093520 1.019\n-Milt Thompson     -4.60e-02 -1.29e-03  8.69e-03  1.57e-02 -0.066305 1.011\n-Mitch Webster      4.75e-02  7.95e-03 -5.16e-02  9.84e-02 -0.163968 1.008\n-Mookie Wilson      3.80e-02 -3.93e-02  4.17e-02 -1.38e-02  0.072513 1.009\n-Marvell Wynne     -1.49e-02  5.10e-03 -2.65e-03  1.48e-03 -0.017899 1.021\n-Mike Young         3.33e-03  2.86e-03 -3.24e-03 -1.76e-03  0.009330 1.020\n-Ozzie Guillen      5.89e-02 -9.46e-02  6.41e-02  1.12e-01 -0.150538 1.031\n-Oddibe McDowell    6.48e-02 -5.60e-02  3.60e-02 -8.13e-03 -0.124412 1.001\n-Ozzie Smith       -5.37e-02  4.02e-02  3.25e-02 -2.65e-01  0.310911 0.976\n-Ozzie Virgil       1.85e-02  6.50e-02 -8.25e-02  4.39e-02  0.118163 1.008\n-Phil Bradley       4.81e-04 -1.71e-02  2.26e-02 -6.75e-03  0.030182 1.032\n-Phil Garner        2.74e-02 -9.59e-03  4.81e-03  1.61e-03  0.035079 1.017\n-Pete Incaviglia    6.42e-02 -8.56e-02  9.92e-02 -1.77e-01 -0.241899 1.006\n-Paul Molitor       1.92e-02 -2.32e-02  3.82e-02 -4.43e-02  0.107865 0.990\n-Pete Rose          1.14e-01  4.07e-02 -5.91e-02 -8.86e-02  0.189130 0.985\n-Pat Sheridan      -2.28e-02  1.98e-03  3.47e-03  2.85e-04 -0.027297 1.023\n-Pat Tabler         1.26e-03 -4.50e-03  5.61e-03 -2.76e-03  0.006952 1.043\n-Rafael Belliard   -2.71e-02 -3.09e-02  3.09e-02  5.36e-02 -0.081600 1.020\n-Rick Burleson      4.52e-02 -2.87e-02  2.37e-02 -1.01e-02  0.052975 1.020\n-Randy Bush        -4.05e-03  1.01e-03 -1.00e-03  2.74e-03 -0.007438 1.020\n-Rick Cerone        4.58e-03 -1.72e-03  8.78e-04 -5.67e-04  0.004939 1.026\n-Ron Cey            1.39e-01 -8.55e-02  4.82e-02  8.40e-02  0.167109 0.990\n-Rob Deer           1.94e-02 -6.61e-02  9.31e-02 -1.66e-01 -0.204332 1.041\n-Rick Dempsey       1.33e-02  3.85e-02 -4.89e-02  1.75e-02  0.065001 1.025\n-Ron Hassey         2.71e-02 -3.82e-02  3.80e-02 -9.06e-04  0.044913 1.030\n-Rickey Henderson  -7.61e-02  5.65e-02 -4.91e-02  9.83e-02  0.169443 1.013\n-Reggie Jackson     9.62e-05  1.74e-02 -2.10e-02  1.78e-02  0.035902 1.023\n-Ron Kittle         3.29e-03  2.33e-02 -3.13e-02  3.00e-02  0.049032 1.034\n-Ray Knight        -4.14e-04  2.71e-03 -3.68e-03  1.51e-03 -0.005632 1.026\n-Rick Leach        -2.53e-02  2.17e-02 -1.88e-02  2.61e-03 -0.029509 1.029\n-Rick Manning       6.13e-02 -2.46e-02  9.16e-03  1.53e-02  0.065449 1.022\n-Rance Mulliniks    1.61e-02 -5.13e-04 -2.86e-03  4.16e-03  0.027430 1.018\n-Ron Oester        -3.47e-02  5.27e-02 -3.48e-02 -5.28e-02  0.089931 1.018\n-Rey Quinones      -4.39e-02 -9.47e-02  1.00e-01  8.58e-02 -0.169986 0.994\n-Rafael Ramirez    -5.50e-02  1.23e-01 -1.03e-01 -7.16e-02  0.153327 1.008\n-Ronn Reynolds     -1.62e-03  4.75e-04 -7.79e-05 -6.08e-05 -0.001698 1.034\n-Ron Roenicke      -2.33e-02  4.36e-04  3.63e-03  7.47e-03 -0.031217 1.021\n-Ryne Sandberg     -1.11e-02  2.22e-03  3.85e-03 -7.74e-03  0.022466 1.030\n-Rafael Santana    -9.78e-04  7.70e-03 -7.05e-03 -6.34e-03  0.010362 1.038\n-Rick Schu         -8.73e-02  4.94e-02 -2.98e-02 -2.04e-02 -0.091678 1.018\n-Ruben Sierra      -5.44e-02  1.51e-02  2.43e-03 -7.59e-02 -0.133435 0.974\n-Roy Smalley       -1.05e-02  3.77e-02 -4.26e-02  4.11e-02  0.077840 1.018\n-Robby Thompson     6.59e-02 -6.03e-02  1.61e-02  1.21e-01 -0.178764 0.995\n-Rob Wilfong        2.34e-02  2.59e-02 -3.04e-02 -2.28e-02  0.057036 1.022\n-Robin Yount        2.27e-03 -4.95e-02  6.78e-02 -3.51e-02  0.092987 1.029\n-Steve Balboni      9.36e-02 -2.00e-01  2.28e-01 -2.28e-01 -0.359398 0.990\n-Scott Bradley     -1.58e-01  1.17e-01 -9.31e-02  5.05e-03 -0.171774 0.997\n-Sid Bream          3.24e-02 -2.95e-02  1.63e-02 -4.27e-03 -0.092135 1.002\n-Steve Buechele     2.54e-02 -8.36e-02  8.86e-02 -5.01e-02 -0.136695 0.997\n-Shawon Dunston     1.01e-01 -1.35e-01  1.12e-01  8.00e-03 -0.178151 1.000\n-Scott Fletcher     1.15e-03  2.69e-03 -5.66e-03  8.76e-03 -0.012451 1.040\n-Steve Garvey      -7.05e-02  8.68e-02 -7.66e-02  4.81e-02  0.151404 1.000\n-Steve Jeltz        2.37e-02 -8.42e-02  7.36e-02  6.98e-02 -0.108023 1.040\n-Steve Lombardozzi  4.43e-02 -1.46e-01  1.35e-01  6.25e-02 -0.171813 1.005\n-Spike Owen        -1.71e-02  3.48e-02 -2.79e-02 -2.99e-02  0.044732 1.053\n-Steve Sax          6.20e-02  3.77e-01 -5.42e-01  3.35e-01 -0.734384 0.937\n-Tony Bernazard     5.29e-03  1.75e-02 -2.44e-02 -9.23e-04 -0.038471 1.028\n-Tom Brookens       2.03e-02 -7.98e-03  6.36e-03 -1.12e-02  0.026764 1.023\n-Tom Brunansky     -4.45e-02  4.71e-02 -4.03e-02  2.86e-02  0.084252 1.024\n-Tony Fernandez     7.04e-02  6.66e-02 -1.29e-01  1.08e-01 -0.232886 1.034\n-Tim Flannery      -2.71e-04  1.42e-04 -2.05e-04  4.56e-04 -0.000684 1.025\n-Tom Foley         -6.16e-04  1.97e-04 -1.50e-04  4.18e-04 -0.000837 1.026\n-Tony Gwynn         7.84e-03  4.75e-02 -6.37e-02  1.64e-02 -0.079776 1.060\n-Terry Harper       4.18e-02 -1.25e-02  3.32e-03  4.63e-03  0.048072 1.018\n-Tommy Herr        -8.39e-02  1.26e-01 -8.31e-02 -1.55e-01  0.205925 1.022\n-Tim Hulett         5.33e-02 -1.10e-01  1.05e-01 -9.93e-03 -0.128391 1.022\n-Terry Kennedy      4.04e-01 -2.08e-01  1.13e-01  4.16e-02  0.404173 0.975\n-Tito Landrum       3.86e-02  6.32e-03 -1.52e-02 -1.25e-02  0.052448 1.025\n-Tim Laudner       -7.77e-04  2.86e-04 -5.93e-05 -3.37e-04 -0.000870 1.030\n-Tom Paciorek      -1.36e-02  8.41e-03 -6.28e-03  1.33e-03 -0.014574 1.028\n-Tony Pena         -1.16e-02 -1.90e-02  4.16e-02 -4.55e-02  0.102164 1.007\n-Terry Pendleton    8.62e-02 -1.43e-01  1.07e-01  1.34e-01 -0.193138 1.045\n-Tony Phillips     -2.73e-03  1.42e-02 -9.82e-03 -2.09e-02  0.030323 1.024\n-Terry Puhl         2.04e-01 -6.75e-02  2.43e-02 -1.49e-02  0.215607 0.977\n-Ted Simmons        1.51e-01 -6.94e-02  3.44e-02  1.52e-02  0.153167 1.015\n-Tim Teufel         7.56e-03  2.47e-04 -1.37e-03 -3.72e-03  0.010776 1.023\n-Tim Wallach       -3.14e-02  8.23e-02 -8.42e-02  2.76e-02  0.108729 1.018\n-Vince Coleman      9.75e-02 -1.64e-01  1.27e-01  1.40e-01 -0.208761 1.061\n-Von Hayes         -1.80e-02 -3.85e-02  5.46e-02  4.08e-03  0.085175 1.030\n-Vance Law          9.80e-03  6.71e-02 -6.78e-02 -4.24e-02  0.099409 1.014\n-Wally Backman      2.02e-02 -3.39e-02  4.10e-02 -3.19e-02  0.057586 1.037\n-Wade Boggs         1.56e-02 -1.35e-01  1.67e-01 -5.20e-02  0.187803 1.089\n-Will Clark        -4.86e-02  5.89e-02 -6.47e-02  9.05e-03 -0.118454 0.982\n-Wally Joyner       6.64e-02  5.05e-02 -8.27e-02 -6.87e-02 -0.219968 0.973\n-Willie McGee      -2.74e-02  5.21e-02 -3.63e-02 -5.40e-02  0.087794 1.017\n-Willie Randolph   -1.58e-02  1.47e-02  8.79e-03 -7.82e-02  0.105311 1.013\n-Wayne Tolleson    -1.69e-03  3.57e-03 -1.50e-03 -8.34e-03  0.010428 1.030\n-Willie Upshaw     -7.78e-02  1.09e-01 -7.95e-02 -7.78e-02  0.150806 1.017\n-Willie Wilson     -6.71e-02  5.53e-02 -2.29e-02 -7.26e-02  0.121829 1.026\n                     cook.d     hat inf\n-Alan Ashby        5.41e-04 0.00540    \n-Alvin Davis       7.11e-08 0.00589    \n-Andre Dawson      4.50e-05 0.00833    \n-Andres Galarraga  3.59e-03 0.00544    \n-Alfredo Griffin   1.48e-03 0.02461    \n-Al Newman         5.71e-03 0.01412    \n-Argenis Salazar   3.43e-03 0.01093    \n-Andres Thomas     4.13e-03 0.00583    \n-Andre Thornton    7.06e-03 0.01181    \n-Alan Trammell     1.33e-04 0.00995    \n-Alex Trevino      2.82e-03 0.01160    \n-Andy VanSlyke     1.62e-04 0.00392    \n-Alan Wiggins      5.95e-03 0.01133    \n-Bill Almon        3.67e-05 0.01206    \n-Buddy Bell        2.23e-04 0.00926    \n-Buddy Biancalana  2.09e-04 0.01234    \n-Bruce Bochy       2.17e-03 0.02042    \n-Barry Bonds       7.74e-03 0.01404    \n-Bobby Bonilla     4.63e-03 0.01106    \n-Bob Brenly        6.68e-04 0.00873    \n-Bill Buckner      4.21e-04 0.01390    \n-Brett Butler      2.12e-03 0.02317    \n-Bob Dernier       4.87e-03 0.01020    \n-Bo Diaz           7.85e-04 0.00579    \n-Bill Doran        5.91e-04 0.01543    \n-Brian Downing     1.02e-03 0.00778    \n-Billy Hatcher     3.53e-03 0.00710    \n-Brook Jacoby      1.16e-05 0.01085    \n-Bob Kearney       3.01e-04 0.01088    \n-Bill Madlock      1.29e-03 0.00473    \n-Bob Melvin        3.29e-03 0.00900    \n-BillyJo Robidoux  6.54e-03 0.01333    \n-Bill Schroeder    1.83e-04 0.01128    \n-Chris Bando       1.37e-04 0.00979    \n-Chris Brown       4.35e-03 0.01663    \n-Carmen Castillo   3.16e-05 0.01303    \n-Chili Davis       6.78e-04 0.00704    \n-Carlton Fisk      7.75e-03 0.01782    \n-Curt Ford         6.14e-03 0.01097    \n-Carney Lansford   1.62e-03 0.01071    \n-Chet Lemon        9.25e-04 0.00498    \n-Candy Maldonado   2.01e-05 0.00765    \n-Carmelo Martinez  3.22e-04 0.00897    \n-Craig Reynolds    4.30e-04 0.00601    \n-Cal Ripken        2.14e-03 0.01548    \n-Cory Snyder       1.66e-02 0.01424    \n-Chris Speier      1.35e-04 0.01764    \n-Curt Wilkerson    1.96e-05 0.01158    \n-Dave Anderson     2.70e-06 0.01141    \n-Don Baylor        4.91e-03 0.03525    \n-Daryl Boston      6.88e-03 0.01200    \n-Darnell Coles     8.42e-03 0.00769    \n-Dave Concepcion   7.69e-05 0.00754    \n-Doug DeCinces     1.63e-03 0.01616    \n-Darrell Evans     9.97e-05 0.02641    \n-Dwight Evans      1.88e-03 0.01561    \n-Damaso Garcia     1.93e-03 0.00737    \n-Dan Gladden       5.21e-04 0.00732    \n-Dave Henderson    5.53e-05 0.00494    \n-Donnie Hill       8.38e-05 0.00803    \n-Davey Lopes       7.69e-04 0.00867    \n-Don Mattingly     5.55e-04 0.09037   *\n-Dale Murphy       9.01e-03 0.02058    \n-Dwayne Murphy     9.92e-04 0.00499    \n-Dave Parker       7.85e-04 0.02334    \n-Dan Pasqua        8.97e-03 0.01483    \n-Darrell Porter    4.27e-06 0.02123    \n-Dick Schofield    1.70e-04 0.00693    \n-Don Slaught       1.98e-04 0.00665    \n-Darryl Strawberry 5.57e-03 0.01769    \n-Dale Sveum        4.49e-03 0.00584    \n-Danny Tartabull   9.83e-03 0.01283    \n-Denny Walling     2.84e-04 0.01244    \n-Dave Winfield     7.78e-03 0.01335    \n-Eric Davis        2.64e-03 0.02052    \n-Eddie Milner      9.87e-05 0.00478    \n-Eddie Murray      1.01e-02 0.01269    \n-Ed Romero         1.86e-03 0.01202    \n-Frank White       2.21e-04 0.01032    \n-George Bell       8.36e-05 0.03440   *\n-Glenn Braggs      5.75e-03 0.01024    \n-George Brett      3.56e-03 0.00702    \n-Greg Brock        2.20e-04 0.01025    \n-Gary Carter       1.05e-02 0.01335    \n-Glenn Davis       1.25e-02 0.02338    \n-Gary Gaetti       1.96e-05 0.03006   *\n-Greg Gagne        2.42e-03 0.00757    \n-George Hendrick   2.32e-03 0.00994    \n-Glenn Hubbard     2.80e-03 0.01441    \n-Garth Iorg        2.10e-04 0.00751    \n-Gary Matthews     1.83e-03 0.01198    \n-Graig Nettles     8.04e-04 0.01360    \n-Gary Pettis       7.69e-06 0.01809    \n-Gary Redus        1.58e-04 0.00533    \n-Garry Templeton   6.46e-03 0.02315    \n-Greg Walker       6.84e-04 0.00954    \n-Gary Ward         8.44e-04 0.01588    \n-Glenn Wilson      1.74e-04 0.01027    \n-Harold Baines     4.43e-04 0.01369    \n-Hubie Brooks      2.54e-03 0.02455    \n-Howard Johnson    1.02e-04 0.01116    \n-Hal McRae         9.69e-05 0.00661    \n-Harold Reynolds   1.29e-02 0.02662    \n-Harry Spilman     4.90e-04 0.01782    \n-Herm Winningham   3.54e-03 0.01251    \n-Jesse Barfield    1.39e-03 0.04727   *\n-Juan Beniquez     1.10e-04 0.00984    \n-John Cangelosi    8.09e-03 0.01856    \n-Jose Canseco      2.65e-02 0.03860    \n-Joe Carter        2.40e-02 0.02791    \n-Jack Clark        1.10e-02 0.00976    \n-Jose Cruz         8.02e-04 0.00622    \n-Jody Davis        3.05e-03 0.01258    \n-Jim Dwyer         2.22e-04 0.01593    \n-Julio Franco      1.52e-04 0.02305    \n-Jim Gantner       1.95e-03 0.00960    \n-Johnny Grubb      3.56e-05 0.02493    \n-Jack Howell       5.16e-03 0.01668    \n-John Kruk         6.41e-03 0.01353    \n-Jeffrey Leonard   3.57e-03 0.00622    \n-Jim Morrison      2.41e-03 0.01043    \n-John Moses        6.96e-03 0.00956    \n-Jerry Mumphrey    1.52e-03 0.01145    \n-Jim Presley       1.08e-02 0.01817    \n-Johnny Ray        7.90e-05 0.02223    \n-Jeff Reed         5.93e-03 0.01410    \n-Jim Rice          1.19e-02 0.03416    \n-Jerry Royster     2.84e-06 0.00787    \n-John Russell      1.28e-03 0.00732    \n-Juan Samuel       1.52e-04 0.01123    \n-John Shelby       3.83e-07 0.01058    \n-Joel Skinner      2.28e-03 0.00821    \n-Jim Sundberg      9.48e-03 0.01962    \n-Jose Uribe        9.80e-04 0.02432    \n-Joel Youngblood   2.34e-03 0.01275    \n-Kevin Bass        7.21e-04 0.02192    \n-Kal Daniels       1.10e-02 0.01950    \n-Kirk Gibson       7.17e-03 0.02056    \n-Ken Griffey       1.17e-03 0.01574    \n-Keith Hernandez   7.51e-03 0.01861    \n-Kent Hrbek        4.34e-03 0.01925    \n-Ken Landreaux     3.11e-03 0.00739    \n-Kevin McReynolds  1.13e-04 0.01563    \n-Kevin Mitchell    2.72e-03 0.00624    \n-Keith Moreland    2.44e-03 0.01216    \n-Ken Oberkfell     1.82e-03 0.01265    \n-Ken Phelps        4.73e-04 0.01983    \n-Kirby Puckett     4.62e-02 0.05527   *\n-Kurt Stillwell    5.73e-03 0.01199    \n-Leon Durham       2.69e-03 0.00780    \n-Len Dykstra       2.06e-03 0.00858    \n-Larry Herndon     9.00e-05 0.00650    \n-Lee Lacy          1.57e-05 0.00742    \n-Len Matuszek      5.59e-06 0.01299    \n-Lloyd Moseby      1.10e-03 0.01541    \n-Lance Parrish     3.98e-03 0.01684    \n-Larry Parrish     5.80e-06 0.01955    \n-Larry Sheets      4.01e-03 0.01021    \n-Lou Whitaker      4.42e-04 0.01037    \n-Mike Aldrete      5.49e-03 0.01090    \n-Marty Barrett     1.83e-05 0.02919    \n-Mike Davis        6.06e-04 0.00668    \n-Mike Diaz         7.83e-03 0.01471    \n-Mariano Duncan    1.96e-03 0.01154    \n-Mike Easler       1.96e-04 0.01082    \n-Mel Hall          5.23e-06 0.00977    \n-Mike Heath        3.07e-03 0.00815    \n-Mike Kingery      7.02e-03 0.01115    \n-Mike LaValliere   2.88e-03 0.00901    \n-Mike Marshall     3.03e-03 0.01371    \n-Mike Pagliarulo   1.05e-02 0.02554    \n-Mark Salas        1.19e-03 0.00824    \n-Mike Schmidt      8.89e-02 0.03089   *\n-Mike Scioscia     3.37e-03 0.00698    \n-Mickey Tettleton  2.19e-03 0.01355    \n-Milt Thompson     1.10e-03 0.00617    \n-Mitch Webster     6.71e-03 0.01694    \n-Mookie Wilson     1.32e-03 0.00619    \n-Marvell Wynne     8.04e-05 0.00634    \n-Mike Young        2.18e-05 0.00474    \n-Ozzie Guillen     5.67e-03 0.02716    \n-Oddibe McDowell   3.86e-03 0.00964    \n-Ozzie Smith       2.39e-02 0.02295    \n-Ozzie Virgil      3.49e-03 0.01141    \n-Phil Bradley      2.29e-04 0.01688    \n-Phil Garner       3.09e-04 0.00536    \n-Pete Incaviglia   1.46e-02 0.02516    \n-Paul Molitor      2.90e-03 0.00571    \n-Pete Rose         8.88e-03 0.01262    \n-Pat Sheridan      1.87e-04 0.00889    \n-Pat Tabler        1.21e-05 0.02628    \n-Rafael Belliard   1.67e-03 0.01234    \n-Rick Burleson     7.03e-04 0.00907    \n-Randy Bush        1.39e-05 0.00496    \n-Rick Cerone       6.12e-06 0.01047    \n-Ron Cey           6.94e-03 0.01140    \n-Rob Deer          1.04e-02 0.03955    \n-Rick Dempsey      1.06e-03 0.01365    \n-Ron Hassey        5.06e-04 0.01582    \n-Rickey Henderson  7.17e-03 0.01946    \n-Reggie Jackson    3.23e-04 0.00963    \n-Ron Kittle        6.03e-04 0.01951    \n-Ray Knight        7.96e-06 0.01010    \n-Rick Leach        2.18e-04 0.01402    \n-Rick Manning      1.07e-03 0.01159    \n-Rance Mulliniks   1.89e-04 0.00450    \n-Ron Oester        2.02e-03 0.01226    \n-Rey Quinones      7.19e-03 0.01288    \n-Rafael Ramirez    5.87e-03 0.01556    \n-Ronn Reynolds     7.24e-07 0.01742    \n-Ron Roenicke      2.44e-04 0.00722    \n-Ryne Sandberg     1.27e-04 0.01480    \n-Rafael Santana    2.69e-05 0.02135    \n-Rick Schu         2.10e-03 0.01243    \n-Ruben Sierra      4.42e-03 0.00575    \n-Roy Smalley       1.52e-03 0.01061    \n-Robby Thompson    7.95e-03 0.01401    \n-Rob Wilfong       8.15e-04 0.01093    \n-Robin Yount       2.17e-03 0.01993    \n-Steve Balboni     3.19e-02 0.03283    \n-Scott Bradley     7.35e-03 0.01382    \n-Sid Bream         2.12e-03 0.00648    \n-Steve Buechele    4.66e-03 0.01005    \n-Shawon Dunston    7.90e-03 0.01549    \n-Scott Fletcher    3.89e-05 0.02316    \n-Steve Garvey      5.71e-03 0.01247    \n-Steve Jeltz       2.92e-03 0.02943    \n-Steve Lombardozzi 7.36e-03 0.01637    \n-Spike Owen        5.02e-04 0.03619   *\n-Steve Sax         1.31e-01 0.05621   *\n-Tony Bernazard    3.71e-04 0.01372    \n-Tom Brookens      1.80e-04 0.00843    \n-Tom Brunansky     1.78e-03 0.01555    \n-Tony Fernandez    1.35e-02 0.03841    \n-Tim Flannery      1.17e-07 0.00880    \n-Tom Foley         1.76e-07 0.01018    \n-Tony Gwynn        1.60e-03 0.04413   *\n-Terry Harper      5.79e-04 0.00728    \n-Tommy Herr        1.06e-02 0.02831    \n-Tim Hulett        4.12e-03 0.01934    \n-Terry Kennedy     4.02e-02 0.03269   *\n-Tito Landrum      6.90e-04 0.01246    \n-Tim Laudner       1.90e-07 0.01372    \n-Tom Paciorek      5.33e-05 0.01268    \n-Tony Pena         2.61e-03 0.00921    \n-Terry Pendleton   9.33e-03 0.04110    \n-Tony Phillips     2.31e-04 0.00948    \n-Terry Puhl        1.15e-02 0.01348    \n-Ted Simmons       5.86e-03 0.01835    \n-Tim Teufel        2.91e-05 0.00754    \n-Tim Wallach       2.96e-03 0.01472    \n-Vince Coleman     1.09e-02 0.05407   *\n-Von Hayes         1.82e-03 0.01920    \n-Vance Law         2.47e-03 0.01135    \n-Wally Backman     8.32e-04 0.02256    \n-Wade Boggs        8.84e-03 0.07367   *\n-Will Clark        3.49e-03 0.00548    \n-Wally Joyner      1.20e-02 0.01320    \n-Willie McGee      1.93e-03 0.01159    \n-Willie Randolph   2.77e-03 0.01180    \n-Wayne Tolleson    2.73e-05 0.01364    \n-Willie Upshaw     5.68e-03 0.01897    \n-Willie Wilson     3.71e-03 0.02101    \n\n\n\nsummary(influence.measures(reg_model_2))\n\nPotentially influential observations of\n     lm(formula = log_Salary ~ AtBat + Hits + HmRun, data = hitters) :\n\n                dfb.1_ dfb.AtBt dfb.Hits dfb.HmRn dffit   cov.r   cook.d\n-Don Mattingly   0.00  -0.04     0.04     0.01     0.05    1.12_*  0.00 \n-George Bell     0.00  -0.01     0.01     0.01     0.02    1.05_*  0.00 \n-Gary Gaetti     0.00   0.00     0.00     0.01     0.01    1.05_*  0.00 \n-Jesse Barfield -0.01  -0.02     0.01     0.06     0.07    1.06_*  0.00 \n-Kirby Puckett   0.04   0.28    -0.32    -0.17    -0.43_*  1.02    0.05 \n-Mike Schmidt    0.60  -0.26     0.12     0.03     0.61_*  0.88_*  0.09 \n-Spike Owen     -0.02   0.03    -0.03    -0.03     0.04    1.05_*  0.00 \n-Steve Sax       0.06   0.38    -0.54     0.34    -0.73_*  0.94_*  0.13 \n-Tony Gwynn      0.01   0.05    -0.06     0.02    -0.08    1.06_*  0.00 \n-Terry Kennedy   0.40  -0.21     0.11     0.04     0.40_*  0.97    0.04 \n-Vince Coleman   0.10  -0.16     0.13     0.14    -0.21    1.06_*  0.01 \n-Wade Boggs      0.02  -0.14     0.17    -0.05     0.19    1.09_*  0.01 \n                hat    \n-Don Mattingly   0.09_*\n-George Bell     0.03  \n-Gary Gaetti     0.03  \n-Jesse Barfield  0.05_*\n-Kirby Puckett   0.06_*\n-Mike Schmidt    0.03  \n-Spike Owen      0.04  \n-Steve Sax       0.06_*\n-Tony Gwynn      0.04  \n-Terry Kennedy   0.03  \n-Vince Coleman   0.05_*\n-Wade Boggs      0.07_*\n\n\n\nsummary(influence.measures(reg_model))\n\nPotentially influential observations of\n     lm(formula = Salary ~ AtBat + Hits + HmRun, data = hitters) :\n\n                 dfb.1_ dfb.AtBt dfb.Hits dfb.HmRn dffit   cov.r   cook.d\n-Don Mattingly    0.01  -0.38     0.42     0.15     0.50_*  1.07_*  0.06 \n-Dale Murphy     -0.17   0.11    -0.09     0.24     0.40_*  0.92_*  0.04 \n-Dave Winfield   -0.14   0.12    -0.11     0.17     0.34    0.91_*  0.03 \n-Eddie Murray     0.06  -0.32     0.36     0.09     0.49_*  0.78_*  0.06 \n-George Bell     -0.01  -0.02     0.02     0.02     0.04    1.05_*  0.00 \n-Gary Carter     -0.06   0.11    -0.13     0.26     0.38_*  0.88_*  0.03 \n-Gary Gaetti      0.01   0.01    -0.01    -0.03    -0.03    1.05_*  0.00 \n-Jesse Barfield  -0.01  -0.03     0.02     0.10     0.12    1.06_*  0.00 \n-Jack Clark       0.20  -0.03    -0.03     0.06     0.24    0.94_*  0.01 \n-Jim Rice        -0.05  -0.43     0.54     0.05     0.67_*  0.87_*  0.11 \n-Keith Hernandez -0.02  -0.18     0.25    -0.07     0.34    0.94_*  0.03 \n-Kirby Puckett    0.05   0.36    -0.41    -0.21    -0.55_*  0.99    0.08 \n-Mike Schmidt     0.94  -0.41     0.18     0.05     0.95_*  0.69_*  0.20 \n-Ozzie Smith     -0.10   0.07     0.06    -0.47     0.55_*  0.85_*  0.07 \n-Spike Owen       0.00   0.00     0.00     0.00     0.00    1.05_*  0.00 \n-Steve Sax        0.05   0.29    -0.41     0.26    -0.56_*  0.99    0.08 \n-Tony Gwynn       0.02   0.10    -0.13     0.03    -0.17    1.05_*  0.01 \n-Terry Pendleton  0.06  -0.10     0.07     0.09    -0.13    1.05_*  0.00 \n-Vince Coleman    0.06  -0.11     0.08     0.09    -0.13    1.07_*  0.00 \n-Wade Boggs       0.03  -0.29     0.36    -0.11     0.40_*  1.06_*  0.04 \n                 hat    \n-Don Mattingly    0.09_*\n-Dale Murphy      0.02  \n-Dave Winfield    0.01  \n-Eddie Murray     0.01  \n-George Bell      0.03  \n-Gary Carter      0.01  \n-Gary Gaetti      0.03  \n-Jesse Barfield   0.05_*\n-Jack Clark       0.01  \n-Jim Rice         0.03  \n-Keith Hernandez  0.02  \n-Kirby Puckett    0.06_*\n-Mike Schmidt     0.03  \n-Ozzie Smith      0.02  \n-Spike Owen       0.04  \n-Steve Sax        0.06_*\n-Tony Gwynn       0.04  \n-Terry Pendleton  0.04  \n-Vince Coleman    0.05_*\n-Wade Boggs       0.07_*\n\n\n\nplot(reg_model, pch=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(reg_model_2, pch=16)"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html",
    "href": "posts/rl/2022-10-23-rl-HW2.html",
    "title": "Regression HW 2",
    "section": "",
    "text": "고급회귀분석 과제, CH03,04,05\n고급회귀분석 두번째 과제입니다.\n제출 기한 : 10월 23일\n(마지막 문제는 R을 이용해서 풀이해도 됨)\n제출 방법\n(pdf 아닌 문서는 미제출로 간주)\n주의사항\n예) R에서 lm으로 beta의 추정량을 구하면 안 됨. 수업 시간에 배운 식으로 풀이를 적어야 함.\n************ R을 이용해서 푸는 문제는, R 코드도 같이 업로드."
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-4",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-4",
    "title": "Regression HW 2",
    "section": "(1)",
    "text": "(1)\n선형회귀모형 (\\(y = \\beta_0 + \\beta_1 x + \\epsilon\\))이 타당한가를 유의수준 \\(\\alpha = 0.05\\)를 사용하여 적합결여검정을 행하라.\nAnswer\n\\(H_0 : E(Y|X = x) = \\beta_0 + \\beta_1 x\\)\n\\(H_1 : E(Y|X=x) \\neq \\beta_0 + \\beta_1 x\\)\n\ndf_4 = data.frame(x = c(0,0,3,3,6,6,9,9,12,12),\n                  y = c(8.5,8.4,7.9,8.1,7.8,7.6,7.3,7.0,6.8,6.7))\ndf_4\n\n\n\nA data.frame: 10 × 2\n\n    xy\n    <dbl><dbl>\n\n\n     08.5\n     08.4\n     37.9\n     38.1\n     67.8\n     67.6\n     97.3\n     97.0\n    126.8\n    126.7\n\n\n\n\n산점도\n\nplot(df_4$x,df_4$y,xlab='x(경과시간)',ylab='y(신선도)')\n\n\n\n\n우하향하는 모습이다.\n\ndf_4$x_barx = df_4$x - mean(df_4$x)\ndf_4$y_bary = df_4$y - mean(df_4$y) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_4$x_barx2 <- df_4$x_barx^2\ndf_4$y_bary2 <- df_4$y_bary^2\ndf_4$xy <-df_4$x_barx * df_4$y_bary\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_4\n\n\n\nA data.frame: 10 × 7\n\n    xyx_barxy_baryx_barx2y_bary2xy\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     08.5-6 0.89360.7921-5.34\n     08.4-6 0.79360.6241-4.74\n     37.9-3 0.29 90.0841-0.87\n     38.1-3 0.49 90.2401-1.47\n     67.8 0 0.19 00.0361 0.00\n     67.6 0-0.01 00.0001 0.00\n     97.3 3-0.31 90.0961-0.93\n     97.0 3-0.61 90.3721-1.83\n    126.8 6-0.81360.6561-4.86\n    126.7 6-0.91360.8281-5.46\n\n\n\n\n\nround(colSums(df_4),3)\n\nx60y76.1x_barx0y_bary0x_barx2180y_bary23.729xy-25.5\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_4 <- as.numeric(colSums(df_4)[7]/colSums(df_4)[5])\nbeta0_4 <- mean(df_4$y) - beta1_4 *  mean(df_4$x)\n\n\ncat(\"hat beta0 = \", beta0_4)\ncat(\"\\nhat beta1 = \", beta1_4)\n\nhat beta0 =  8.46\nhat beta1 =  -0.1416667\n\n\n\\(\\hat{y} = 8.46 - 0.1417x\\)\n\nSST_4 = sum((df_4$y - mean(df_4$y))^2)\n\n\nSSR_4 = sum( ( ( beta0_4 + beta1_4 *df_4$x)-mean(df_4$y) )^2 )\nMSR_4 = SSR_4/1\n\n\nSSE_4 = sum( ( df_4$y-( beta0_4 + beta1_4 *df_4$x))^2 )\nMSE_4 = SSE_4/8\n\n\ncat(\"SST = \", SST_4,\", df = 9\")\ncat(\"\\nSSR = \", SSR_4,\", df = 1\")\ncat(\"\\nSSE = \", SSE_4, \", df = 8\")\n\nSST =  3.729 , df = 9\nSSR =  3.6125 , df = 1\nSSE =  0.1165 , df = 8\n\n\n\nF_4 = MSR_4 / MSE_4\nF_4\n\n248.068669527898\n\n\n\nqf(0.95,1,8)\n\n5.31765507157871\n\n\n\\(H_0 : \\beta_1 = 0\\)\n\\(H_1 : \\beta_1 \\neq 0\\)\nF값이 유의수준 0.05에서 기준 F보다 크기 때문에 \\(H_0\\) 기각하고, \\(\\beta_1\\) 은 유의미하다.\n\ndf_4_ex <- cbind(df_4[c(1,3,5,7,9),c(1,2)],df_4[c(2,4,6,8,10),c(2)])\n\n\ncolnames(df_4_ex) <- c('x','y1','y2')\n\n\ndf_4_ex$ymean <- (df_4_ex$y1+df_4_ex$y2)/2\ndf_4_ex$y1_ymean2 <- (df_4_ex$y1 - df_4_ex$ymean)^2\ndf_4_ex$y2_ymean2 <- (df_4_ex$y2 - df_4_ex$ymean)^2\ndf_4_ex$yhat <- 8.46 - 0.146667 * df_4_ex$x\ndf_4_ex\n\n\n\nA data.frame: 5 × 7\n\n    xy1y2ymeany1_ymean2y2_ymean2yhat\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1 08.58.48.450.00250.00258.460000\n    3 37.98.18.000.01000.01008.019999\n    5 67.87.67.700.01000.01007.579998\n    7 97.37.07.150.02250.02257.139997\n    9126.86.76.750.00250.00256.699996\n\n\n\n\n\\(\\hat{y} = 8.46 - 01417x\\)\n\nSSPE_4 = sum(df_4_ex$y1_ymean2) + sum(df_4_ex$y2_ymean2)\nSSPE_4\n\n0.0949999999999998\n\n\n\nSSLF_4 = SSE_4 - SSPE_4\nSSLF_4\n\n0.0214999999999996\n\n\n\nF_4_0 = SSLF_4/3 / (SSPE_4 / 5)\nF_4_0\n\n0.377192982456135\n\n\n\ncat(\"유의수준 5%에서 \",qf(0.95,3,5), \"보다 \",F_4_0,\"값이 작기 때문에 귀무가설을 기각하지 못한다. 따라서 선형회귀모형은 타당하다.\")\n\n유의수준 5%에서  5.409451 보다  0.377193 값이 작기 때문에 귀무가설을 기각하지 못한다. 따라서 선형회귀모형은 타당하다.\n\n\n\\(H_0 : E(Y|X = x) = \\beta_0 + \\beta_1 x\\) 기각못함"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-5",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-5",
    "title": "Regression HW 2",
    "section": "(2)",
    "text": "(2)\n선형회귀모형이 타당한 경우, 신선도의 점수가 시간당 얼마만큼이나 떨어지는가를 95% 신뢰계수를 가지고 구간추정하라(즉, \\(\\beta_1\\)의 구간추정).\nAnswer\n\\(\\hat{\\beta}_1\\)의 \\(100(1-\\alpha)\\)%의 신뢰구간\n\\(\\hat{\\beta}_1 \\pm t_{\\alpha/2}(n-2)\\frac{\\sqrt{MSE}}{\\sqrt{S_{xx}}}\\)\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\ncat(\"beta1 is \",beta1_4)\ncat(\"\\nMSE is \",MSE_4)\ncat(\"\\nSxx is \",sum(df_4$x_barx2))\n\nbeta1 is  -0.1416667\nMSE is  0.0145625\nSxx is  180\n\n\n\ncat(\"95% 신뢰계수는 (\",beta1_4-qt(0.975,8)*sqrt(MSE_4/sum(df_4$x_barx2)),\"-\",beta1_4+qt(0.975,8)*sqrt(MSE_4/sum(df_4$x_barx2)),\") 이다.\")\n\n95% 신뢰계수는 ( -0.1624082 - -0.1209251 ) 이다.\n\n\n신뢰계수가 0을 포함하지 않는다. 신뢰구간에서 \\(\\beta_1\\)이 유의미함을 알 수 있다.\n\n\n두 타이어회사 A, B에서 생산되는 타이어를 비교하기 위하여 고속도로에서 트럭이 달리는 상황을 모의실험(simulated experiment)하여 다음의 데이터를 얻었다. \\(x\\)는 트럭이 달리는 속도이고 \\(y\\)는 타이어가 마모되기까지의 총 주행거리이다.\n\n\n\n\n\\(x_{1j}\\)\n10\n20\n30\n40\n50\n60\n70\n\n\n\n\n\\(y_{1j}(A)\\)\n9.8\n12.5\n14.9\n16.5\n22.4\n24.1\n25.8\n\n\n\\(y_{2j}(B)\\)\n15.0\n14.5\n16.5\n19.1\n22.3\n20.8\n22.4\n\n\n\n\n산점도를 그리시오.\n\nAnswer\n\ndf_5 = data.frame(x = c(10,20,30,40,50,60,70),\n                  yA = c(9.8,12.5,14.9,16.5,22.4,24.1,25.8),\n                  yB = c(15.0,14.5,16.5,19.1,22.3,20.8,22.4))\ndf_5\n\n\n\nA data.frame: 7 × 3\n\n    xyAyB\n    <dbl><dbl><dbl>\n\n\n    10 9.815.0\n    2012.514.5\n    3014.916.5\n    4016.519.1\n    5022.422.3\n    6024.120.8\n    7025.822.4\n\n\n\n\n\nplot(df_5$yA~df_5$x,\n     xlab = \"x\",\n     ylab = \"yA(orange),yB(blue)\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\npar(new=TRUE)\nplot(df_5$yB~df_5$x,\n     xlab='',\n     ylab='',\n     pch  = 16,\n     cex  = 1,\n     col  = \"blue\")\n\n\n\n\n\n각 회사별로 속도와 총주행거리 간의 회귀모형을 구한다면, 두 개의 직선이 동일하다고 볼수 있는가? 유의수준 \\(\\alpha = 0.05\\)로 가설검정하시오.\n\nAnswer\n\\(H_0: \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1: \\beta_{01} \\ne \\beta_{02} \\text{ or } \\beta_{11} \\ne \\beta_{12}\\)\n\ndf_5_A <- df_5\n\n\ndf_5_A$x_barx = df_5_A$x - mean(df_5_A$x)\ndf_5_A$yA_baryA = df_5_A$yA - mean(df_5_A$yA) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_A$x_barx2 <- df_5_A$x_barx^2\ndf_5_A$yA_baryA2 <- df_5_A$yA_baryA^2\ndf_5_A$xyA <-df_5_A$x_barx * df_5_A$yA_baryA\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_A\n\n\n\nA data.frame: 7 × 8\n\n    xyAyBx_barxyA_baryAx_barx2yA_baryA2xyA\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.815.0-30-8.290067.24246\n    2012.514.5-20-5.540030.25110\n    3014.916.5-10-3.1100 9.61 31\n    4016.519.1  0-1.5  0 2.25  0\n    5022.422.3 10 4.410019.36 44\n    6024.120.8 20 6.140037.21122\n    7025.822.4 30 7.890060.84234\n\n\n\n\n\nround(colSums(df_5_A),3)\n\nx280yA126yB130.6x_barx0yA_baryA0x_barx22800yA_baryA2226.76xyA787\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xyA <- as.numeric(colSums(df_5_A)[8]/colSums(df_5_A)[6])\nbeta0_xyA <- mean(df_5_A$yA) - beta1_xyA *  mean(df_5_A$x)\n\n\ncat(\"xyA hat beta0 = \", beta0_xyA)\ncat(\"\\nxyA hat beta1 = \", beta1_xyA)\n\nxyA hat beta0 =  6.757143\nxyA hat beta1 =  0.2810714\n\n\n\\(yA = 6.757143 + 0.2810714x\\)\n\nSST_A = sum((df_5_A$yA - mean(df_5_A$yA))^2)\n\n\nSSR_A = sum( ( ( 6.757143 + 0.2810714 *df_5_A$x)-mean(df_5_A$yA) )^2 )\n\n\nSSE_A = sum( ( df_5_A$yA-( 6.757143 + 0.2810714 *df_5_A$x))^2 )\n\n\ncat(\"yA SST = \", SST_A,\", df = 6\")\ncat(\"\\nyA SSR = \", SSR_A,\", df = 1\")\ncat(\"\\nyA SSE = \", SSE_A, \", df = 5\")\n\nyA SST =  226.76 , df = 6\nyA SSR =  221.2032 , df = 1\nyA SSE =  5.556786 , df = 5\n\n\n\ndf_5_B <- df_5\n\n\ndf_5_B$x_barx = df_5_B$x - mean(df_5_B$x)\ndf_5_B$yB_baryB = df_5_B$yB - mean(df_5_B$yB) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_B$x_barx2 <- df_5_B$x_barx^2\ndf_5_B$yB_baryB2 <- df_5_B$yB_baryB^2\ndf_5_B$xyB <-df_5_B$x_barx * df_5_B$yB_baryB\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_B\n\n\n\nA data.frame: 7 × 8\n\n    xyAyBx_barxyB_baryBx_barx2yB_baryB2xyB\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.815.0-30-3.657142990013.3746939109.71429\n    2012.514.5-20-4.157142940017.2818367 83.14286\n    3014.916.5-10-2.1571429100 4.6532653 21.57143\n    4016.519.1  0 0.4428571  0 0.1961224  0.00000\n    5022.422.3 10 3.642857110013.2704082 36.42857\n    6024.120.8 20 2.1428571400 4.5918367 42.85714\n    7025.822.4 30 3.742857190014.0089796112.28571\n\n\n\n\n\nround(colSums(df_5_B),3)\n\nx280yA126yB130.6x_barx0yB_baryB0x_barx22800yB_baryB267.377xyB406\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xyB <- as.numeric(colSums(df_5_B)[8]/colSums(df_5_B)[6])\nbeta0_xyB <- mean(df_5_B$yB) - beta1_xyB *  mean(df_5_B$x)\n\n\ncat(\"xyB hat beta0 = \", beta0_xyB)\ncat(\"\\nxyB hat beta1 = \", beta1_xyB)\n\nxyB hat beta0 =  12.85714\nxyB hat beta1 =  0.145\n\n\n\\(yB = 12.85714 + 0.145x\\)\n\nSST_B = sum((df_5_B$yB - mean(df_5_B$yB))^2)\n\n\nSSR_B = sum( ( ( 12.85714 + 0.145*df_5_B$x)-mean(df_5_B$yB) )^2 )\n\n\nSSE_B = sum( ( df_5_B$yB-( 12.85712 + 0.145 *df_5_B$x))^2 )\n\n\ncat(\"yB SST = \", SST_B,\", df = 6\")\ncat(\"\\nyB SSR = \", SSR_B,\", df = 1\")\ncat(\"\\nyB SSE = \", SSE_B, \", df = 5\")\n\nyB SST =  67.37714 , df = 6\nyB SSR =  58.87 , df = 1\nyB SSE =  8.507143 , df = 5\n\n\n\na <- df_5[,c(1,2)]\n\n\ncolnames(a) <- c('x','y')\n\n\nb <- df_5[,c(1,3)]\n\n\ncolnames(b) <- c('x','y')\n\n\ndf_5_AB <- rbind(a,b)\ndf_5_AB\n\n\n\nA data.frame: 14 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    10 9.8\n    2012.5\n    3014.9\n    4016.5\n    5022.4\n    6024.1\n    7025.8\n    1015.0\n    2014.5\n    3016.5\n    4019.1\n    5022.3\n    6020.8\n    7022.4\n\n\n\n\n\ndf_5_AB$x_barx = df_5_AB$x - mean(df_5_AB$x)\ndf_5_AB$y_bary = df_5_AB$y - mean(df_5_AB$y) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_AB$x_barx2 <- df_5_AB$x_barx^2\ndf_5_AB$y_bary2 <- df_5_AB$y_bary^2\ndf_5_AB$xy <-df_5_AB$x_barx * df_5_AB$y_bary\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_AB\n\n\n\nA data.frame: 14 × 7\n\n    xyx_barxy_baryx_barx2y_bary2xy\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.8-30-8.528571490072.736531255.85714\n    2012.5-20-5.828571440033.972245116.57143\n    3014.9-10-3.428571410011.755102 34.28571\n    4016.5  0-1.8285714  0 3.343673  0.00000\n    5022.4 10 4.071428610016.576531 40.71429\n    6024.1 20 5.771428640033.309388115.42857\n    7025.8 30 7.471428690055.822245224.14286\n    1015.0-30-3.328571490011.079388 99.85714\n    2014.5-20-3.828571440014.657959 76.57143\n    3016.5-10-1.8285714100 3.343673 18.28571\n    4019.1  0 0.7714286  0 0.595102  0.00000\n    5022.3 10 3.971428610015.772245 39.71429\n    6020.8 20 2.4714286400 6.107959 49.42857\n    7022.4 30 4.071428690016.576531122.14286\n\n\n\n\n\nround(colSums(df_5_AB),3)\n\nx560y256.6x_barx0y_bary0x_barx25600y_bary2295.649xy1193\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xy <- as.numeric(colSums(df_5_AB)[7]/colSums(df_5_AB)[5])\nbeta0_xy <- mean(df_5_AB$y) - beta1_xy *  mean(df_5_AB$x)\n\n\ncat(\"xy hat beta0 = \", beta0_xy)\ncat(\"\\nxy hat beta1 = \", beta1_xy)\n\nxy hat beta0 =  9.807143\nxy hat beta1 =  0.2130357\n\n\n\\(\\text{y} = 9.807143 +0.2130357\\text{x}\\)\n\nSST_5 = sum((df_5_AB$y - mean(df_5_AB$y))^2)\n\n\nSSR_5 = sum( ( (9.807143 + 0.2130357*df_5_AB$x)-mean(df_5_AB$y) )^2 )\n\n\nSSE_5 = sum( ( df_5_AB$y-(9.807143  +0.2130357 *df_5_AB$x))^2 )\n\n\ncat(\"y = \",round(beta0_xy,4), \"+ \",round(beta1_xy,4) ,\"x\")\n\ny =  9.8071 +  0.213 x\n\n\n\ncat(\"SST = \", SST_5,\", df = 13\")\ncat(\"\\nSSR = \", SSR_5,\", df = 1\")\ncat(\"\\nSSE = \", SSE_5, \", df = 12\")\n\nSST =  295.6486 , df = 13\nSSR =  254.1516 , df = 1\nSSE =  41.49696 , df = 12\n\n\n\ncat(\"yA = \",round(beta0_xyA,4), \"+ \",round(beta1_xyA,4) ,\"x\")\n\nyA =  6.7571 +  0.2811 x\n\n\n\ncat(\"yA SST = \", SST_A,\", df = 6\")\ncat(\"\\nyA SSR = \", SSR_A,\", df = 1\")\ncat(\"\\nyA SSE = \", SSE_A, \", df = 5\")\n\nyA SST =  226.76 , df = 6\nyA SSR =  221.2032 , df = 1\nyA SSE =  5.556786 , df = 5\n\n\n\ncat(\"yB = \",round(beta0_xyB,4), \"+ \",round(beta1_xyB,4) ,\"x\")\n\nyB =  12.8571 +  0.145 x\n\n\n\ncat(\"yB SST = \", SST_B,\", df = 6\")\ncat(\"\\nyB SSR = \", SSR_B,\", df = 1\")\ncat(\"\\nyB SSE = \", SSE_B, \", df = 5\")\n\nyB SST =  67.37714 , df = 6\nyB SSR =  58.87 , df = 1\nyB SSE =  8.507143 , df = 5\n\n\n가설\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1 : \\beta_{01} \\neq \\beta_{02} \\text{ or } \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(F_0 = \\frac{SSE(R) - SSE(F)}{df_R-df_F} \\times \\frac{df_F}{SSE(F)}\\)\n\nSSE_5_F = SSE_A + SSE_B\ndf_5_F = 5 -2 + 5 -2\n\n\nSSE_5_R = SSE_5\ndf_5_R = 5 -1 + 5 -1\n\n\nF_5_0 = (SSE_5_R - SSE_5_F)/(df_5_R - df_5_F) / (SSE_5_F/df_5_F)\nF_5_0\n\n5.8517864828756\n\n\n\ndf_5_R - df_5_F\n\n2\n\n\n\ndf_5_F\n\n6\n\n\n\nF_5_stan = qf(0.95,2,10)\nF_5_stan\n\n4.1028210151304\n\n\n\ncat(F_5_0, \" 는 유의수준 0.05에서 F값 \", F_5_stan , \" 보다 크다.\")\n\ncat(\"\\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\")\n\ncat(\"\\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\")\n\n5.851786  는 유의수준 0.05에서 F값  4.102821  보다 크다.\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\n\n\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\) 기각"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-6",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-6",
    "title": "Regression HW 2",
    "section": "(3)",
    "text": "(3)\n관심의 대상이 \\(x\\)가 증가함에 따라 \\(y\\) 가 얼마나 증가하는가에 있다. 두 회사의 타이어에 대하여 각각 회귀모형을 적합했을 때, 기울기가 같은지 유의수준 5%로 검정하시오.\nAnswer\n기울기 비교에 대한 가설\n\\(H_0 : \\beta_{11} = \\beta_{12} \\text{ vs. } H_1 : \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(t_0 = \\frac{ \\hat{\\beta}_{11} - \\hat{\\beta}_{12} }{ \\sqrt{ \\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) } }\\)\n\\(\\text{Degree of Freedom} = t((n_1 - 1) + (n_2 - 1))\\)\n\\(\\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) = MSE(F) [\\frac{1}{\\sum(x_{1j} - \\bar{x}_1)^2} + \\frac{1}{\\sum(x_{2j} - \\bar{x}_2)^2}]\\)\n\nround(beta1_xyA,4)\n\n0.2811\n\n\n\nround(beta1_xyB,4)\n\n0.145\n\n\n\nSSE_5_F\n\n14.063928575095\n\n\n\nMSE_5_F = SSE_5_F / df_5_F\nMSE_5_F\n\n2.34398809584917\n\n\n\nsum(df_5_A$x_barx2)\n\n2800\n\n\n\nsum(df_5_B$x_barx2)\n\n2800\n\n\n\nvar_5_diff = MSE_5_F * (1/sum(df_5_A$x_barx2) + 1/sum(df_5_B$x_barx2))\nvar_5_diff\n\n0.00167427721132084\n\n\n\nt_5_0 = (beta1_xyA - beta1_xyB)/sqrt(var_5_diff)\nt_5_0\n\n3.32547173820247\n\n\n\nqt(0.95,df_5_F)\n\n1.9431802805153\n\n\n\ncat(t_5_0,\"는 \",qt(0.95,df_5_F),\"보다 크다. 따라서 유의수준 5%에서 귀무가설을 기각하여 두 회귀모형의 기울기가 다르다고 할 수 있다.\")\n\n3.325472 는  1.94318 보다 크다. 따라서 유의수준 5%에서 귀무가설을 기각하여 두 회귀모형의 기울기가 다르다고 할 수 있다.\n\n\n\\(H_0 : \\beta_{11} = \\beta_{12}\\) 기각"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-8",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-8",
    "title": "Regression HW 2",
    "section": "(1)",
    "text": "(1)\nHigh와 Year, Low와 Year, 그리고 High와 Low에 대해 산점도를 그리시오.\nAnswer\n\ndf_6 = data.frame(Year = c(1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978),\n           High = c(25.82, 25.35, 24.29, 24.05, 24.89, 25.35, 25.23, 25.06, 27.13, 27.36, 26.65, 27.13, 27.49, 27.08, 27.51, 27.54, 26.21),\n           Low = c(18.24, 16.50, 20.26, 20.97, 19.43, 19.31, 20.85, 19.54, 20.49, 21.91, 22.51, 18.81, 19.42, 19.10, 18.80, 18.80, 17.57))\ndf_6\n\n\n\nA data.frame: 17 × 3\n\n    YearHighLow\n    <dbl><dbl><dbl>\n\n\n    196225.8218.24\n    196325.3516.50\n    196424.2920.26\n    196524.0520.97\n    196624.8919.43\n    196725.3519.31\n    196825.2320.85\n    196925.0619.54\n    197027.1320.49\n    197127.3621.91\n    197226.6522.51\n    197327.1318.81\n    197427.4919.42\n    197527.0819.10\n    197627.5118.80\n    197727.5418.80\n    197826.2117.57\n\n\n\n\n\nplot(df_6$High,df_6$Year,\n     xlab = \"Year\",\n     ylab = \"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n양의 기울기로 선형 관계를 갖는 모습이다.\n\nplot(df_6$Low~df_6$Year,\n     xlab = \"Year\",\n     ylab = \"Low\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n이차함수 모양의 연관이 있는 모양새이다.\n\nplot(df_6$Low~df_6$High,\n    xlab = \"Low\",\n     ylab = \"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n관련이 있는지 모르겠다."
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-9",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-9",
    "title": "Regression HW 2",
    "section": "(2)",
    "text": "(2)\nYear에 대한 High, Year에 대한 Low, 그리고 Low에 대한 High의 회귀모형을 구하시오. 3개 회귀모형의 결과를 요약하고, 각 모형별로 회귀계수의 의미를 설명하시오.\nAnswer\nYear에 대한 High의 회귀모형\n\ndf_6_YearHigh <- df_6\n\n\ndf_6_YearHigh$Year_barYear = df_6_YearHigh$Year - mean(df_6_YearHigh$Year)\ndf_6_YearHigh$High_barHigh = df_6_YearHigh$High - mean(df_6_YearHigh$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_YearHigh$Year_barYear2 <- df_6_YearHigh$Year_barYear^2\ndf_6_YearHigh$High_barHigh2 <- df_6_YearHigh$High_barHigh^2\ndf_6_YearHigh$YearHigh <-df_6_YearHigh$Year_barYear * df_6_YearHigh$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_YearHigh\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowYear_barYearHigh_barHighYear_barYear2High_barHigh2YearHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-8-0.30588235640.093564014 2.4470588\n    196325.3516.50-7-0.77588235490.601993426 5.4311765\n    196424.2920.26-6-1.83588235363.37046401411.0152941\n    196524.0520.97-5-2.07588235254.30928754310.3794118\n    196624.8919.43-4-1.23588235161.527405190 4.9435294\n    196725.3519.31-3-0.77588235 90.601993426 2.3276471\n    196825.2320.85-2-0.89588235 40.802605190 1.7917647\n    196925.0619.54-1-1.06588235 11.136105190 1.0658824\n    197027.1320.49 0 1.00411765 01.008252249 0.0000000\n    197127.3621.91 1 1.23411765 11.523046367 1.2341176\n    197226.6522.51 2 0.52411765 40.274699308 1.0482353\n    197327.1318.81 3 1.00411765 91.008252249 3.0123529\n    197427.4919.42 4 1.36411765161.860816955 5.4564706\n    197527.0819.10 5 0.95411765250.910340484 4.7705882\n    197627.5118.80 6 1.38411765361.915781661 8.3047059\n    197727.5418.80 7 1.41411765491.999728720 9.8988235\n    197826.2117.57 8 0.08411765640.007075779 0.6729412\n\n\n\n\n\nround(colSums(df_6_YearHigh),3)\n\nYear33490High444.14Low332.51Year_barYear0High_barHigh0Year_barYear2408High_barHigh222.951YearHigh73.8\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_YearHigh <- as.numeric(colSums(df_6_YearHigh)[8]/colSums(df_6_YearHigh)[6])\nbeta0_YearHigh <- mean(df_6_YearHigh$High) - beta1_YearHigh *  mean(df_6_YearHigh$Year)\n\n\ncat(\"YearHigh hat beta0 = \", beta0_YearHigh)\ncat(\"\\nYearHigh hat beta1 = \", beta1_YearHigh)\n\nYearHigh hat beta0 =  -330.2124\nYearHigh hat beta1 =  0.1808824\n\n\n\\(\\text{High} = -330.21235 + 0.18088\\text{Year}\\)\nR결과와 비교\n\nsummary(lm(df_6$High~df_6$Year))\n\n\nCall:\nlm(formula = df_6$High ~ df_6$Year)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3629 -0.5341  0.1479  0.4903  1.1412 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -330.21235   78.03319  -4.232 0.000725 ***\ndf_6$Year      0.18088    0.03961   4.567 0.000371 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8001 on 15 degrees of freedom\nMultiple R-squared:  0.5816,    Adjusted R-squared:  0.5537 \nF-statistic: 20.85 on 1 and 15 DF,  p-value: 0.0003708\n\n\nR결과 해석 - beta0과 beta1이 5%보다 유의확률이 작아 유의미하다. - 모형의 설명력은 50%정도에 머문다. - 모형의 p값이 0.05보다 작아 유의미하다고 볼 수 있다.\nYear에 대한 Low의 회귀모형\n\ndf_6_YearLow <- df_6\n\n\ndf_6_YearLow$Year_barYear = df_6_YearLow$Year - mean(df_6_YearLow$Year)\ndf_6_YearLow$Low_barLow = df_6_YearLow$Low - mean(df_6_YearLow$Low) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_YearLow$Year_barYear2 <- df_6_YearLow$Year_barYear^2\ndf_6_YearLow$Low_barLow2 <- df_6_YearLow$Low_barLow^2\ndf_6_YearLow$YearLow <-df_6_YearLow$Year_barYear * df_6_YearLow$Low_barLow\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_YearLow\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowYear_barYearLow_barLowYear_barYear2Low_barLow2YearLow\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-8-1.31941176641.7408474048 10.55529412\n    196325.3516.50-7-3.05941176499.3600003460 21.41588235\n    196424.2920.26-6 0.70058824360.4908238754 -4.20352941\n    196524.0520.97-5 1.41058824251.9897591696 -7.05294118\n    196624.8919.43-4-0.12941176160.0167474048  0.51764706\n    196725.3519.31-3-0.24941176 90.0622062284  0.74823529\n    196825.2320.85-2 1.29058824 41.6656179931 -2.58117647\n    196925.0619.54-1-0.01941176 10.0003768166  0.01941176\n    197027.1320.49 0 0.93058824 00.8659944637  0.00000000\n    197127.3621.91 1 2.35058824 15.5252650519  2.35058824\n    197226.6522.51 2 2.95058824 48.7059709343  5.90117647\n    197327.1318.81 3-0.74941176 90.5616179931 -2.24823529\n    197427.4919.42 4-0.13941176160.0194356401 -0.55764706\n    197527.0819.10 5-0.45941176250.2110591696 -2.29705882\n    197627.5118.80 6-0.75941176360.5767062284 -4.55647059\n    197727.5418.80 7-0.75941176490.5767062284 -5.31588235\n    197826.2117.57 8-1.98941176643.9577591696-15.91529412\n\n\n\n\n\nround(colSums(df_6_YearLow),3)\n\nYear33490High444.14Low332.51Year_barYear0Low_barLow0Year_barYear2408Low_barLow236.327YearLow-3.22\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_YearLow <- as.numeric(colSums(df_6_YearLow)[8]/colSums(df_6_YearLow)[6])\nbeta0_YearLow <- mean(df_6_YearLow$Low) - beta1_YearLow *  mean(df_6_YearLow$Year)\n\n\ncat(\"YearLow hat beta0 = \", beta0_YearLow)\ncat(\"\\nYearLow hat beta1 = \", beta1_YearLow)\n\nYearLow hat beta0 =  35.10696\nYearLow hat beta1 =  -0.007892157\n\n\n\\(\\text{Low} = 35.106961 -0.007892\\text{Year}\\)\nR결과와 비교\n\nsummary(lm(df_6$Low~df_6$Year))\n\n\nCall:\nlm(formula = df_6$Low ~ df_6$Year)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1147 -0.7121 -0.1610  0.9306  2.9664 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  35.106961 151.723912   0.231     0.82\ndf_6$Year    -0.007892   0.077017  -0.102     0.92\n\nResidual standard error: 1.556 on 15 degrees of freedom\nMultiple R-squared:  0.0006996, Adjusted R-squared:  -0.06592 \nF-statistic: 0.0105 on 1 and 15 DF,  p-value: 0.9197\n\n\nR결과 해석 - beta0과 beta1이 5%보다 유의확률이 커 유의미하지 않다. - 모형의 설명력은 굉장히 낮았다. - 모형의 p값이 0.05보다 커 유의미하지 않다.\nLow에 대한 High의 회귀모형\n\ndf_6_LowHigh <- df_6\n\n\ndf_6_LowHigh$Low_barLow = df_6_LowHigh$Low - mean(df_6_LowHigh$Low)\ndf_6_LowHigh$High_barHigh = df_6_LowHigh$High - mean(df_6_LowHigh$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_LowHigh$Low_barLow2 <- df_6_LowHigh$Low_barLow^2\ndf_6_LowHigh$High_barHigh2 <- df_6_LowHigh$High_barHigh^2\ndf_6_LowHigh$LowHigh <-df_6_LowHigh$Low_barLow * df_6_LowHigh$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_LowHigh\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-1.31941176-0.305882351.74084740480.093564014 0.40358478\n    196325.3516.50-3.05941176-0.775882359.36000034600.601993426 2.37374360\n    196424.2920.26 0.70058824-1.835882350.49082387543.370464014-1.28619758\n    196524.0520.97 1.41058824-2.075882351.98975916964.309287543-2.92821522\n    196624.8919.43-0.12941176-1.235882350.01674740481.527405190 0.15993772\n    196725.3519.31-0.24941176-0.775882350.06220622840.601993426 0.19351419\n    196825.2320.85 1.29058824-0.895882351.66561799310.802605190-1.15621522\n    196925.0619.54-0.01941176-1.065882350.00037681661.136105190 0.02069066\n    197027.1320.49 0.93058824 1.004117650.86599446371.008252249 0.93442007\n    197127.3621.91 2.35058824 1.234117655.52526505191.523046367 2.90090242\n    197226.6522.51 2.95058824 0.524117658.70597093430.274699308 1.54645536\n    197327.1318.81-0.74941176 1.004117650.56161799311.008252249-0.75249758\n    197427.4919.42-0.13941176 1.364117650.01943564011.860816955-0.19017405\n    197527.0819.10-0.45941176 0.954117650.21105916960.910340484-0.43833287\n    197627.5118.80-0.75941176 1.384117650.57670622841.915781661-1.05111522\n    197727.5418.80-0.75941176 1.414117650.57670622841.999728720-1.07389758\n    197826.2117.57-1.98941176 0.084117653.95775916960.007075779-0.16734464\n\n\n\n\n\nround(colSums(df_6_LowHigh),3)\n\nYear33490High444.14Low332.51Low_barLow0High_barHigh0Low_barLow236.327High_barHigh222.951LowHigh-0.511\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_LowHigh <- as.numeric(colSums(df_6_LowHigh)[8]/colSums(df_6_LowHigh)[6])\nbeta0_LowHigh <- mean(df_6_LowHigh$High) - beta1_LowHigh *  mean(df_6_LowHigh$Low)\n\n\ncat(\"LowHigh hat beta0 = \", beta0_LowHigh)\ncat(\"\\nLowHigh hat beta1 = \", beta1_LowHigh)\n\nLowHigh hat beta0 =  26.40088\nLowHigh hat beta1 =  -0.01405959\n\n\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\)\nR결과와 비교\n\nsummary(lm(df_6$Low~df_6$High))\n\n\nCall:\nlm(formula = df_6$Low ~ df_6$High)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0767 -0.7279 -0.1569  0.9529  2.9623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 20.14079    8.49368   2.371   0.0315 *\ndf_6$High   -0.02225    0.32478  -0.069   0.9463  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.556 on 15 degrees of freedom\nMultiple R-squared:  0.0003129, Adjusted R-squared:  -0.06633 \nF-statistic: 0.004695 on 1 and 15 DF,  p-value: 0.9463\n\n\nR결과 해석 - beta0은 유의미 하지만 beta1이 5%보다 유의확률이 커 유의미하지 않다. - 모형의 설명력은 굉장히 낮았다. - 모형의 p값이 0.05보다 커 유의미하지 않다.\n모형 정리\n\\(\\text{Low} = 35.106961 -0.007892\\text{Year}\\) 모형의 의미 - \\(\\beta_0 = 35.106961\\): 해당 모형에서 시간에 영향을 받지 않았을때의 아마존 강의 최저 수위가 35.106961m이다. - \\(\\beta_1 = -0.007892\\): 아마존 강의 최저수위가 0.007892m만큼 감소한 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 감소하는 것을 의미하지만, 0에 가까운 값으로서 영향이 미세해 보인다.\n\\(\\text{High} = -330.21235 + 0.18088\\text{Year}\\) 모형의 의미 - \\(\\beta_0 = -330.21235\\): 해당 모형에서 시간에 영향을 받지 않았을때의 아마존 강의 최고 수위가 -330.21235m이다. - \\(\\beta_1 = 0.18088\\): 아마존 강의 최고수위가 0.18088m만큼 증가된 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 0.18088m 늘어난 것을 나타낼 수 있다. 이 모형에서도 영향이 크게 끼치지 않는 것 같다.\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\) 모형의 의미 - \\(\\beta_0 = -26.40088\\): 해당 모형에서 최저수위의 영향을 받지 않았을 때의 아마존 강의 최고수위가 026.40088m 라는 것을 의미한다. - \\(\\beta_1 = -0.014061\\): 아마존 강의 최고수위가 최저수위가 1m 증가함에 따라 -0.0014061m 만큼 감소함을 의미한다. 아마존 강의 최저수위는 최고수위에 미치는 영향이 미미해 보인다."
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-10",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-10",
    "title": "Regression HW 2",
    "section": "(3)",
    "text": "(3)\n이 자료를 근거로 우리는 삼림파괴가 아마존 강 수위의 변화를 일으킨다고 할 수 있는가?\nAnswer\n\n1970년대 이후 삼림파괴가 이루어졌다. 구간을 나누어 보지 않는 이상 삼림파괴가 아마존 강 수위의 변화를 일으켰다는 판단은 섣불러 보인다.\n모형만 봐도 시간과 아마존 강의 최고수위 및 최저수위의 영향과 아마존 강의 최저수위 및 최고수위 간의 영향이 거의 없어보인다. 모두 1도 넘지 않았기도 하다.\n따라서 삼림파괴가 아마존 강 수위의 변화를 일으켰다고 2번의 근거로는 할 수 없다."
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-11",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-11",
    "title": "Regression HW 2",
    "section": "(4)",
    "text": "(4)\n아마존강의 최저수위와 최고수위와의 산점도를 1960년대, 1970년대 자료별로 다르게 그리고, 각각의 회귀선을 적합하시오.\nAnswer\n1960년대 아마존강의 최저수위와 최고수위와의 산점도\n\nplot(df_6$High[df_6$Year<1970]~df_6$Low[df_6$Year<1970],\n     xlab = \"Low\",\n     ylab =\"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n1960년대 아마존강의 최저수위와 최고수위와의 회귀선\n\ndf_6_1960 <- df_6[df_6$Year<1970,]\n\n\ndf_6_1960$Low_barLow = df_6_1960$Low - mean(df_6_1960$Low)\ndf_6_1960$High_barHigh = df_6_1960$High - mean(df_6_1960$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_1960$Low_barLow2 <- df_6_1960$Low_barLow^2\ndf_6_1960$High_barHigh2 <- df_6_1960$High_barHigh^2\ndf_6_1960$LowHigh <-df_6_1960$Low_barLow * df_6_1960$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\nround(colSums(df_6_1960),3)\n\nYear15724High200.04Low155.1Low_barLow0High_barHigh0Low_barLow215.09High_barHigh22.392LowHigh-3.761\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\ndf_6_1960\n\n\n\nA data.frame: 8 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1196225.8218.24-1.1475 0.8151.316756250.664225-0.9352125\n    2196325.3516.50-2.8875 0.3458.337656250.119025-0.9961875\n    3196424.2920.26 0.8725-0.7150.761256250.511225-0.6238375\n    4196524.0520.97 1.5825-0.9552.504306250.912025-1.5112875\n    5196624.8919.43 0.0425-0.1150.001806250.013225-0.0048875\n    6196725.3519.31-0.0775 0.3450.006006250.119025-0.0267375\n    7196825.2320.85 1.4625 0.2252.138906250.050625 0.3290625\n    8196925.0619.54 0.1525 0.0550.023256250.003025 0.0083875\n\n\n\n\n\nbeta1_1960 <- as.numeric(colSums(df_6_1960)[8]/colSums(df_6_1960)[6])\nbeta0_1960 <- mean(df_6_1960$High) - beta1_1960 *  mean(df_6_1960$Low)\n\n\ncat(\"hat beta0 1960 = \", round(beta0_1960,4))\ncat(\"\\nhat beta1 1960 = \", round(beta1_1960,4))\n\nhat beta0 1960 =  29.8367\nhat beta1 1960 =  -0.2492\n\n\n\ncat(\"회귀선은 다음과 같았다. 1960 High = \",round(beta0_1960,4),\" + \", round(beta1_1960,4),\"Low\")\n\n회귀선은 다음과 같았다. 1960 High =  29.8367  +  -0.2492 Low\n\n\nR결과 비교\n\nsummary(lm(df_6_1960$High~df_6_1960$Low))\n\n\nCall:\nlm(formula = df_6_1960$High ~ df_6_1960$Low)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5606 -0.4053 -0.0057  0.3765  0.5895 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    29.8367     2.4640  12.109 1.93e-05 ***\ndf_6_1960$Low  -0.2492     0.1268  -1.966   0.0969 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4925 on 6 degrees of freedom\nMultiple R-squared:  0.3918,    Adjusted R-squared:  0.2904 \nF-statistic: 3.864 on 1 and 6 DF,  p-value: 0.09691\n\n\n\nSST_1960 = sum((df_6_1960$High - mean(df_6_1960$High))^2)\n\n\nSSR_1960 = sum( ( (29.8367  +  -0.2492*df_6_1960$Low)-mean(df_6_1960$High) )^2 )\n\n\nSSE_1960 = sum( ( df_6_1960$High-(29.8367  +  -0.2492*df_6_1960$Low))^2 )\n\n\ncat(\"1960 SST = \", SST_1960,\", df = 7\")\ncat(\"\\n1960 SSR = \", SSR_1960,\", df = 1\")\ncat(\"\\n1960 SSE = \", SSE_1960, \", df = 6\")\n\n1960 SST =  2.3924 , df = 7\n1960 SSR =  0.9370965 , df = 1\n1960 SSE =  1.455164 , df = 6\n\n\nR결과 비교\n\nanova(lm(df_6_1960$High~df_6_1960$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6_1960$Low10.93723730.93723733.8644640.09690958\n    Residuals61.45516270.2425271      NA        NA\n\n\n\n\n1970년대 아마존강의 최저수위와 최고수위와의 산점도\n\nplot(df_6$High[df_6$Year>=1970]~df_6$Low[df_6$Year>=1970],\n     xlab = \"Low\",\n     ylab =\"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n1970년대 아마존강의 최저수위와 최고수위와의 회귀선\n\ndf_6_1970 <- df_6[df_6$Year>=1970,]\n\n\ndf_6_1970$Low_barLow = df_6_1970$Low - mean(df_6_1970$Low)\ndf_6_1970$High_barHigh = df_6_1970$High - mean(df_6_1970$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_1970$Low_barLow2 <- df_6_1970$Low_barLow^2\ndf_6_1970$High_barHigh2 <- df_6_1970$High_barHigh^2\ndf_6_1970$LowHigh <-df_6_1970$Low_barLow * df_6_1970$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\nround(colSums(df_6_1970),3)\n\nYear17766High244.1Low177.41Low_barLow0High_barHigh0Low_barLow220.79High_barHigh21.574LowHigh0.338\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\ndf_6_1970\n\n\n\nA data.frame: 9 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    9197027.1320.49 0.7777778 0.0077777780.604938276.049383e-05 0.006049383\n    10197127.3621.91 2.1977778 0.2377777784.830227165.653827e-02 0.522582716\n    11197226.6522.51 2.7977778-0.4722222227.827560492.229938e-01-1.321172840\n    12197327.1318.81-0.9022222 0.0077777780.814004946.049383e-05-0.007017284\n    13197427.4919.42-0.2922222 0.3677777780.085393831.352605e-01-0.107472840\n    14197527.0819.10-0.6122222-0.0422222220.374816051.782716e-03 0.025849383\n    15197627.5118.80-0.9122222 0.3877777780.832149381.503716e-01-0.353739506\n    16197727.5418.80-0.9122222 0.4177777780.832149381.745383e-01-0.381106173\n    17197826.2117.57-2.1422222-0.9122222224.589116058.321494e-01 1.954182716\n\n\n\n\n\nbeta1_1970 <- as.numeric(colSums(df_6_1970)[8]/colSums(df_6_1970)[6])\nbeta0_1970 <- mean(df_6_1970$High) - beta1_1970 *  mean(df_6_1970$Low)\n\n\ncat(\"hat beta0 1970 = \", round(beta0_1970,4))\ncat(\"\\nhat beta1 1970 = \", round(beta1_1970,4))\n\nhat beta0 1970 =  26.8016\nhat beta1 1970 =  0.0163\n\n\nR결과 비교\n\nsummary(lm(df_6_1970$High~df_6_1970$Low))\n\n\nCall:\nlm(formula = df_6_1970$High ~ df_6_1970$Low)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.87738 -0.03226  0.02245  0.37253  0.43262 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   26.80160    2.05235  13.059  3.6e-06 ***\ndf_6_1970$Low  0.01627    0.10381   0.157     0.88    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4733 on 7 degrees of freedom\nMultiple R-squared:  0.003495,  Adjusted R-squared:  -0.1389 \nF-statistic: 0.02455 on 1 and 7 DF,  p-value: 0.8799\n\n\n\ncat(\"회귀선은 다음과 같았다. 1970 High = \",round(beta0_1970,4),\" + \", round(beta1_1970,4),\"Low\")\n\n회귀선은 다음과 같았다. 1970 High =  26.8016  +  0.0163 Low\n\n\n\nSST_1970 = sum((df_6_1970$High - mean(df_6_1970$High))^2)\n\n\nSSR_1970 = sum( ( (26.8016  +  0.0163 *df_6_1970$Low)-mean(df_6_1970$High) )^2 )\n\n\nSSE_1970 = sum( ( df_6_1970$High-(26.8016  +  0.0163 *df_6_1970$Low))^2 )\n\n\ncat(\"1970 SST = \", SST_1970,\", df = 8\")\ncat(\"\\n1970 SSR = \", SSR_1970,\", df = 1\")\ncat(\"\\n1970 SSE = \", SSE_1970, \", df = 7\")\n\n1970 SST =  1.573756 , df = 8\n1970 SSR =  0.005528037 , df = 1\n1970 SSE =  1.56826 , df = 7\n\n\nR결과 비교\n\nanova(lm(df_6_1970$High~df_6_1970$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6_1970$Low10.0055001070.0055001070.024550050.8799168\n    Residuals71.5682554490.224036493        NA       NA"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-12",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-12",
    "title": "Regression HW 2",
    "section": "(5)",
    "text": "(5)\n아마존강의 최저수위와 최고수위와의 관계가 1960년대와 1970년대에 따라 차이가 있는가? 두 회귀모형의 동일성 여부를 유의수준 \\(\\alpha = 0.01\\)에서 검정하시오.\nAnswer\n가설\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1 : \\beta_{01} \\neq \\beta_{02} \\text{ pr } \\beta_{11} \\neq \\beta_{12}\\)\n(2)에서 구했던 것\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\)\n\nSST = sum((df_6$High - mean(df_6$High))^2)\n\n\nSSR = sum( ( (26.40088 - 0.01406 *df_6$Low)-mean(df_6$High) )^2 )\n\n\nSSE = sum( ( df_6$High-(26.40088 - 0.01406 *df_6$Low))^2 )\n\n\ncat(\"SST = \", SST,\", df = 16\")\ncat(\"\\nSSR = \", SSR,\", df = 1\")\ncat(\"\\nSSE = \", SSE, \", df = 15\")\n\nSST =  22.95141 , df = 16\nSSR =  0.007181232 , df = 1\nSSE =  22.94423 , df = 15\n\n\nR결과와 비교\n\nanova(lm(df_6$High~df_6$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6$Low 1 0.0071808110.0071808110.004694520.9462794\n    Residuals1522.9442309541.529615397        NA       NA\n\n\n\n\n\\(\\text{1960 High} = 29.8367 + -0.2492\\text{Low}\\)\n\ncat(\"1960 SST = \", SST_1960,\", df = 7\")\ncat(\"\\n1960 SSR = \", SSR_1960,\", df = 1\")\ncat(\"\\n1960 SSE = \", SSE_1960, \", df = 6\")\n\n1960 SST =  2.3924 , df = 7\n1960 SSR =  0.9370965 , df = 1\n1960 SSE =  1.455164 , df = 6\n\n\n\\(\\text{1970 High} = 26.8016 + 0.0163\\text{Low}\\)\n\ncat(\"1970 SST = \", SST_1970,\", df = 8\")\ncat(\"\\n1970 SSR = \", SSR_1970,\", df = 1\")\ncat(\"\\n1970 SSE = \", SSE_1970, \", df = 7\")\n\n1970 SST =  1.573756 , df = 8\n1970 SSR =  0.005528037 , df = 1\n1970 SSE =  1.56826 , df = 7\n\n\n검정통계량\n\\(F_0 = \\frac{SSE(R) - SSE(F)}{df_R} - \\times \\frac{df_F}{SSE(F)}\\)\n\nSSE_F = SSE_1960 + SSE_1970\ndf_F = 6 + 7\n\n\nSSE_R = SSE\ndf_R = 15\n\n\nF_0 = (SSE_R - SSE_F)/(df_R - df_F) / (SSE_F/df_F)\nF_0\n\n42.8273639841797\n\n\n\ndf_R - df_F\n\n2\n\n\n\ndf_F\n\n13\n\n\n\nF_stan = qf(0.95,2,13)\n\n\ncat(F_0, \" 는 유의수준 0.05에서 F값 \", F_stan , \" 보다 크다.\")\n\ncat(\"\\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\")\n\ncat(\"\\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\")\n\n42.82736  는 유의수준 0.05에서 F값  3.805565  보다 크다.\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다."
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html#section-13",
    "href": "posts/rl/2022-10-23-rl-HW2.html#section-13",
    "title": "Regression HW 2",
    "section": "(6)",
    "text": "(6)\n(4)에서 구한 두 회귀모형의 기울기가 같은지 유의수준 \\(\\alpha = 0.01\\)에서 검정하시오.\nAnswer\n기울기 비교에 대한 가설\n\\(H_0 : \\beta_{11} = \\beta_{12} \\text{ vs. } H_1 : \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(t_0 = \\frac{ \\hat{\\beta}_{11} - \\hat{\\beta}_{12} }{ \\sqrt{ \\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) } }\\)\n\\(\\text{Degree of Freedom} = t((n_1 - 1) + (n_2 - 1))\\)\n\\(\\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) = MSE(F) [\\frac{1}{\\sum(x_{1j} - \\bar{x}_1)^2} + \\frac{1}{\\sum(x_{2j} - \\bar{x}_2)^2}]\\)\n\nround(beta1_1960,4)\n\n-0.2492\n\n\n\nround(beta1_1970,4)\n\n0.0163\n\n\n\nSSE_F\n\n3.02342329210101\n\n\n\nMSE_F = SSE_F / df_F\nMSE_F\n\n0.232571022469308\n\n\n\nsum(df_6_1960$Low_barLow2)\n\n15.08995\n\n\n\nsum(df_6_1970$Low_barLow2)\n\n20.7903555555556\n\n\n\nvar_diff = MSE_F * (1/sum(df_6_1960$Low_barLow2) + 1/sum(df_6_1970$Low_barLow2))\nvar_diff\n\n0.026598798385161\n\n\n\nt_0 = (beta1_1960 - beta1_1970)/sqrt(var_diff)\nt_0\n\n-1.62782282241728\n\n\n\nqt(0.995,df_F)\n\n3.01227583871658\n\n\n\ncat(t_0,\"는 \",qt(0.995,df_F),\"보다 작다.\")\n\ncat(\"\\n따라서 유의수준 1%에서 귀무가설을 기각하지 못하여 두 회귀모형의 기울기가 같다고 할 수 있다.\")\n\n-1.627823 는  3.012276 보다 작다.\n따라서 유의수준 1%에서 귀무가설을 기각하지 못하여 두 회귀모형의 기울기가 같다고 할 수 있다.\n\n\n\\(H_0 : \\beta_{11} = \\beta_{12}\\) 채택"
  },
  {
    "objectID": "posts/rl/2022-12-11-rl-Ch10.html",
    "href": "posts/rl/2022-12-11-rl-Ch10.html",
    "title": "고급회귀분석 CH10",
    "section": "",
    "text": "이상치\n\n\nlibrary(ggplot2)\n\n\n회귀진단\n\n오차항의 검토\n적절한 모형의 선택\n독립변수들 간의 상관관계 검토\n지렛대점(leverage point)의 검출\n이상치(outlier) 화인\n영향점(influential observation)의 검출\n\n추정된 회귀직선\n\\[\\hat{y} = X(X^\\top X)^{-1} X^\\top y = Hy\\]\nH: hat matrix, \\(n \\times n\\) matrix\n\\(Var(\\hat{y}) = \\sigma^2 H\\)\n\\(Var(e) = (I_N - H) \\sigma^2\\)\n\\(h_{i,j} = x_i^\\top (X^\\top X) ^{-1} x_j\\)\n\\(h_{ij}\\) : H의 대각원소\n\\(H = X(X^\\top X)^{-1} X^\\top\\)\n\\(HH = H\\)\n\\(rank(H) = p+1\\)\n\\(tr(H) = \\sum^n_{i=1}h_{ii} = p+1\\)\n\\(H\\) : 양반정치행렬 positive definite\n\\(0 \\le h_{ii} <1 , -\\frac{1}{2} \\le h_{ij} \\le -\\frac{1}{2}\\)\n\\(p=1, h_{ii} = \\frac{1}{n} + \\frac{(x_{u} - \\bar{x})^2}{S_{xx}}\\)\n\\(p>1, h_{ii} = \\frac{1}{n} + (x_i - \\bar{x})^\\top (X^\\top X)^{-1} (x_i - \\bar{x})\\)\n\n지렛대점(leverage point)\n\\(h_{ii} > s\\bar{h}\\) 이면, i번째 관측치가 leverage point로 고려 가능\n\\[\\bar{h} = \\frac{1}{n} \\sum^n_{i=1}h_{ii} = \\frac{p+1}{n}\\]\n\n\n잔차(Residual)\n\\(e = y − \\hat{y} = y − X \\hat{\\beta}\\)\n\\(= y − X(X^\\top X)^{-1} X^\\top y\\)\n\\(= (In − X(X^\\to X)^{-1}X^\\to )y = (I_n − H)y\\)\n\\(E(e) = 0_n, Var(e) = (I_n − H)σ^2\\)\n\\(E(e_i) = 0, Var(e_i) = (1 − h_{ii})σ^2\\)\n\\(Cov(e_i,e_j) = −h_{ij} σ^2\\)\n\\(ρ_{ij} = \\frac{−h_{ij} σ^2}{ \\sqrt{(1 − h_{ii})(1 − h_{jj})σ^4} }=\\frac{ −h_{ij}}{\\sqrt{(1 − h_{ii})(1 − h_{jj})}}\\)\n\n\n표준화잔차(standardized residual)\n\\(y \\sim N(Xβ,I_nσ^2)\\)\n\\(e \\sim N(0_n,(I_n−H)σ^2)\\)\n\\(e_i \\sim N(0,(1−h_{ii})σ^2)\\)\n\\(\\star \\frac{e_i}{\\sigma\\sqrt{1-h_{ii}}} \\sim N(0,1)\\)\n\n내적 스튜던트화 잔차(internally studentized residual)\n\\(r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1−h_{ii}}}\\)\n\\(\\hat{\\sigma}^2 = MSE\\)\n이러한 표준화 잔차에 대한 표본분포는 분자와 분모가 서로 독립이 아니기 때문에 \\(t\\) 분포로 근사할 수 없는데, \\(|e_i|\\)가 큰 경우, \\(s\\)도 역시 커지게 되기 때문이다.\n\n\n외적 스튜던트화 잔차(externally studentized residual)\n\\(r_i^{*} = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1 − h_{ii}}} \\sim t(n-p-1-1)\\)\n\\(\\hat{\\sigma}^2_{(i)} = [(n-p-1)\\hat{\\sigma} - \\frac{e^2_{(i)}}{1-h_{ii}}]/(n-p-2)\\)\n\\(|r_i^{*}| ≥ t_{α/2}(n − p − 2)\\) 이면 유의수준\\(\\alpha\\)에서 \\(y_i\\)를 이상점이라고 판정\n\\(\\star\\)\n\\(t_{i} = \\frac{y_i - \\tilde{y}_i}{\\hat{\\sigma}\\sqrt{1 + x_i^\\top [X{i}^\\top X(i)]^{-1} x_i}} \\sim t(n-p-2)\\)\n\\(|r_i^{*}| \\sim t_{i}\\)\n\n\n\n예제 10.1\n\ndf = data.frame('x' =  c(15, 26, 10, 9, 15, 20, 18, 11, 8, 20, 7, 9, 10, 11, 11, 10, 12, 42, 17, 11, 10),\n    'y' = c(95, 71, 83, 91, 102, 87, 93, 100, 104, 94, 113, 96, 83, 84, 102, 100, 105, 57, 121, 86, 100))\n\n\nmodel = lm(y~x,df)\n\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.604  -8.731   1.396   4.523  30.285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 109.8738     5.0678  21.681 7.31e-15 ***\nx            -1.1270     0.3102  -3.633  0.00177 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.02 on 19 degrees of freedom\nMultiple R-squared:   0.41, Adjusted R-squared:  0.3789 \nF-statistic:  13.2 on 1 and 19 DF,  p-value: 0.001769\n\n\n\\(y = 109.8738 -1.1270 x\\)\n\nplot(df$x,df$y)\nabline(model, col='steelblue', lwd=2)\n\n\n\n\n\nanova(model)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x 11604.0811604.080913.201820.001768622\n    Residuals192308.586 121.5045      NA         NA\n\n\n\n\n\nqf(0.95,1,19)\n\n4.3807496923318\n\n\n\\(F_0 = 13.20 > F_{0.05}(1,19) = 4.38\\), 회귀직선은 유의하다\n\\(\\sigma^2 \\sim MSE\\)\n\nMSE = 121.505\n\n\nsqrt(MSE)\n\n11.0229306447968\n\n\n손계산\n\nres = df$y - (109.874 - 1.127*df$x)\n\n\nh = 1/21 + (df$x - mean(df$x))^2/(1604.08)\n\n\nri = res/(sqrt(MSE)*sqrt(1-h))\n\n\nri2 = res/(((19*sqrt(MSE) - res^2/(1-h))/18)*sqrt(1-h))\n\n\nrstudent(model)\n\n10.1839684933793942-0.9415833513782013-1.510811922917994-0.81426336315943850.8328629175207956-0.03063182753708870.31124676473215880.22971574964993190.289910136925676100.617660260595883111.0508471635886512-0.34283148352928113-1.5108119229179914-1.27977575448039150.413153195694502160.127393415386012170.79828114441511618-0.845110861537551193.6069797213043920-1.07648107628971210.127393415386012\n\n\n\nrstudent(model)[abs(rstudent(model))>qt(0.975,258)]\n\n19: 3.60697972130439\n\n\n\nround(data.frame(rep(1:21),res,h,ri,ri2),4)\n\n\n\nA data.frame: 21 × 5\n\n    rep.1.21.reshriri2\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n     1  2.0310.0479 0.1888  0.1827\n     2 -9.5720.1318-0.9319 -1.7796\n     3-15.6040.0596-1.4598  5.8540\n     4 -8.7310.0657-0.8194 -1.2717\n     5  9.0310.0479 0.8396  1.3459\n     6 -0.3340.0673-0.0314 -0.0297\n     7  3.4120.0558 0.3185  0.3207\n     8  2.5230.0547 0.2354  0.2304\n     9  3.1420.0730 0.2961  0.2955\n    10  6.6660.0673 0.6262  0.7679\n    11 11.0150.0816 1.0427  2.6755\n    12 -3.7310.0657-0.3502 -0.3571\n    13-15.6040.0596-1.4598  5.8540\n    14-13.4770.0547-1.2575-14.4335\n    15  4.5230.0547 0.4220  0.4459\n    16  1.3960.0596 0.1306  0.1250\n    17  8.6500.0512 0.8056  1.2241\n    18 -5.5400.5232-0.7278 -0.9954\n    19 30.2850.0519 2.8216 -0.7386\n    20-11.4770.0547-1.0709 -3.0318\n    21  1.3960.0596 0.1306  0.1250\n\n\n\n\n\\(2\\bar{h}\\)\n\n2*(2/21)\n\n0.19047619047619\n\n\n18번째가 2hbar 보다 크니까 지렛대점\n\nqt(0.975,18)\n\n2.10092204024104\n\n\n19번째가 ri2 t보다 크니까 이상점\n\nsummary(influence.measures( model))\n\nPotentially influential observations of\n     lm(formula = y ~ x, data = df) :\n\n   dfb.1_ dfb.x   dffit   cov.r   cook.d hat    \n18  0.83  -1.11_* -1.16_*  2.96_*  0.68   0.65_*\n19  0.14   0.27    0.85    0.40_*  0.22   0.05  \n\n\n\n\n영향점(influential observation)\n\nDIFFITS\n\\(DIFFITS(i) = (\\frac{h_{ii}}{10h_{ii}})^{1/2} r^{*}_i\\)\n\\(DIFFITS(i) \\ge 2(\\frac{p+1}{n})^{1/2}\\) 이 되는 i번째 관측칠가 영향점이라고 말함\n\n\nCook의 통계량\n\\(D(i) = \\frac{r^2_i}{p+1} \\frac{h_{ii}}{1-h_{ii}}\\)\n대략적으로 \\(D(i) \\ge F_{0.5}(p+1,n-p-1)\\)이면 영향을 크게 주는 측정값으로 의심\nh\n\nhatvalues(model)\n\n10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n\ndiffits\n\ndffits(model)\n\n10.04127403575140562-0.4025206873025253-0.3911400454742154-0.22432853366080450.1868559838824216-0.0085717364067812270.077223952838937980.056303486522047690.085407472693718100.172840518129759110.33199685399425312-0.094449643042361813-0.39114004547421514-0.313673908094842150.101264129345836160.0329813827461469170.18716612805440518-1.15577873097521190.85373710713076620-0.263846244162542210.0329813827461469\n\n\ncooksdistance, D\n\ncooks.distance(model)\n\n10.00089740639287069120.081497955150763530.071658144221383340.025615958245264150.017743662633501363.87762740910137e-0570.003130574802994980.0016682085781346990.00383194880672965100.0154395158127621110.0548101351203612120.00467762256482442130.0716581442213833140.0475978118328145150.00536121617564154160.000573584529113046170.017856495213809180.678112028575845190.223288273631179200.0345188940892692210.000573584529113046\n\n\nCOVRATIO\n\ncovratio(model) \n\n11.1658918168321921.1969989767629630.93634739734183941.1151026899392951.0850410825772861.2013199827549771.1701575789867381.1742372676080391.19966823450598101.15209128858604111.08783960928084121.18326164825873130.936347397341839140.992331347870996151.15904532932769161.18673688685713171.09643883044992182.95868271380702190.396431612340971201.04257281407241211.18673688685713\n\n\n\ninfluence.measures( model)\n\nInfluence measures of\n     lm(formula = y ~ x, data = df) :\n\n     dfb.1_    dfb.x    dffit cov.r   cook.d    hat inf\n1   0.01664  0.00328  0.04127 1.166 8.97e-04 0.0479    \n2   0.18862 -0.33480 -0.40252 1.197 8.15e-02 0.1545    \n3  -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n4  -0.20004  0.12788 -0.22433 1.115 2.56e-02 0.0705    \n5   0.07532  0.01487  0.18686 1.085 1.77e-02 0.0479    \n6   0.00113 -0.00503 -0.00857 1.201 3.88e-05 0.0726    \n7   0.00447  0.03266  0.07722 1.170 3.13e-03 0.0580    \n8   0.04430 -0.02250  0.05630 1.174 1.67e-03 0.0567    \n9   0.07907 -0.05427  0.08541 1.200 3.83e-03 0.0799    \n10 -0.02283  0.10141  0.17284 1.152 1.54e-02 0.0726    \n11  0.31560 -0.22889  0.33200 1.088 5.48e-02 0.0908    \n12 -0.08422  0.05384 -0.09445 1.183 4.68e-03 0.0705    \n13 -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n14 -0.24681  0.12536 -0.31367 0.992 4.76e-02 0.0567    \n15  0.07968 -0.04047  0.10126 1.159 5.36e-03 0.0567    \n16  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n17  0.13328 -0.05493  0.18717 1.096 1.79e-02 0.0521    \n18  0.83112 -1.11275 -1.15578 2.959 6.78e-01 0.6516   *\n19  0.14348  0.27317  0.85374 0.396 2.23e-01 0.0531   *\n20 -0.20761  0.10544 -0.26385 1.043 3.45e-02 0.0567    \n21  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html",
    "href": "posts/rl/2023-02-22-rl-mid_term.html",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "",
    "text": "중간고사"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-1",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-1",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(1)",
    "text": "(1)\n\\(\\beta\\)에 대한 최소제곱추정량(\\(LSE\\)) \\(\\hat{\\beta}\\)을 구하시오\nanswer\n\\(\\hat{\\beta} = argmin_{\\beta \\in R} S = \\sum^n_{i=1} (y_i - \\beta x_i)^2 \\to \\frac{\\partial S}{\\partial \\beta}|_{\\beta = \\hat{\\beta}} = 0\\)\n\\(\\frac{\\partial S}{\\partial \\beta} = \\sum^n_{i=1} (-2x_i)(y_i - \\beta x_i) \\to \\sum^n_{i=1} x_i(y_i - \\hat{\\beta}x_i) = 0 \\to \\hat{\\beta} = \\frac{\\sum^n_{i=1} x_iy_i}{\\sum^n_{i=1} x^2_i}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-2",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-2",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(2)",
    "text": "(2)\n\\(E(\\hat{\\beta})\\)을 구하시오\nanswer\n\\(\\hat{\\beta} = \\frac{\\sum^n_{i=1} x_i y_i}{\\sum^n_{i=1} x^2_i} = \\sum^n_{i=1} \\frac{x_i}{\\sum^n_{j=1} s^2_j} y_i\\)\n\\(\\to E(\\hat{\\beta})= \\sum^n_{i=1} \\frac{x_i}{\\sum^n_{i=1} x^2_j} E(y_i) = \\sum^n_{i=1}\\frac{x_i}{\\sum^n_{j=1} x^2_j} \\beta x_i = \\beta\\frac{\\sum^n_{i=1} x^2_{i=1} x^2_i}{\\sum^n_{i=1} x^2_i} = \\beta\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-3",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-3",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(3)",
    "text": "(3)\n\\(Var(\\hat{\\beta})\\)을 구하시오\nanswer\n\\(Var(\\hat{\\beta}) = \\sum^n_{i=1} \\frac{x^2_i}{(\\sum^n_{j=1} x^2_j)^2} Var(y_i) = \\sum^n_{i=1} \\frac{x^2_i}{(\\sum^n_{j=1} x^2_j)^2} \\sigma^2 = \\sigma^2 \\frac{\\sum^n_{i=1} x^2_i}{(\\sum^n_{i=1} x^2_i)^2} = \\frac{\\sigma^2}{\\sum^n_{i=1} x^2_i}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-4",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-4",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(4)",
    "text": "(4)\n\\(\\hat{\\beta}\\)의 표준오차를 구하시오.\nanswer\n\\(s.e.(\\hat{\\beta}) = \\sqrt{Var(\\hat{\\beta})} = \\frac{\\sigma}{\\sqrt{\\sum^n_{i=1}x^2_i}}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-6",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-6",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(1)",
    "text": "(1)\n최소제곱법에 의한 회귀직선을 적합시키고, 결과를 해석하시오.\nanswer\n\\(\\hat{\\beta} = \\frac{\\sum^n_{i=1} x_i y_i}{\\sum^n_{i=1} x^2_i} = \\frac{18207}{9103} = 2\\)\n\\(y = 2x\\)\n사내평가점수가 1점 올라갈 때마다 영업실적이 2백만원 증가한다."
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-7",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-7",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(2)",
    "text": "(2)\n분산분석표를 작성하고, 유의수준 \\(\\alpha=0.05\\)하에서, 회귀직선에 대한 유의성 검정을 수행하시오.\nanswer\n\\(SST = \\sum^n_{i=1} y^2_i = 36961\\)\n\\(SSR = \\sum^n_{i=1} \\hat{y}^2_i = (\\beta x)^2= \\frac{(\\sum^n_{i=1} x_i y_i)^2}{\\sum^n_{i=1} x^2_{i}} = \\frac{(18207)^2}{9103} = 36416\\)\n분산분석표\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n\n\n\n\n회귀\n36416\n1\n36416\n669.01\n\n\n잔차\n545\n10\n54.5\n\n\n\n계\n36961\n11\n\n\n\n\n\n- 가설 : \\(H_0 : \\beta = 0 \\text{ vs. } H_1 : \\beta = 0\\)\n- 기각역 : \\(F_0 > F_{0.01}(1,10) = 4.96\\)\n- 결론: \\(F_0\\) 값이 기각역에 속하므로 귀무가설을 기각할 수 있다. 즉 유의수준 \\(5\\)%에서 회귀직선이 유의하다고 할 수 있다.\n\nround(qf(0.95,1,10),2)\n\n4.96"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-8",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-8",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(3)",
    "text": "(3)\n결정계수를 구하시오\nanswer\n\\(R^2 = \\frac{SSR}{SST} = \\frac{36416}{36961} = 0.9865\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-9",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-9",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(4)",
    "text": "(4)\n\\(\\beta\\)에 대한 \\(95\\)% 신뢰구간을 구하여라\nanswer\n\\(\\hat{s.e.}(\\hat{\\beta}) = \\sqrt{\\frac{MSE}{\\sum^n_{i=1} x^2_i}} = \\sqrt{\\frac{54.5}{9103}} = 0.077\\)\n\\(95\\)% 신뢰구간: \\(\\hat{\\beta} \\pm t_{0.05/2}(10) \\times \\hat{s.e.}(\\hat{\\beta}) = 2 \\pm 2.23 \\times 0.077 = (1.818, 2.172)\\)\n\nround(qt(0.975,10),2)\n\n2.23"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-11",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-11",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(1)",
    "text": "(1)\n단순선형 회귀모형을 쓰고, 오차항에 대한 가정을 적으시오\nanswer\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\epsilon_i \\sim_{i.i.d.} N(0, \\sigma^2) , i=1,2,\\dots, 10\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-12",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-12",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(2)",
    "text": "(2)\n투입된 비료의 양과 감자의 수확량에 대한 회귀선을 구하시오\nanswer\n\\(S_{(xx)} = \\sum^n_{i=1} x^2_i - n(\\bar{x})^2 = 110 - 10 \\times 3^2 = 20\\)\n\\(S_{(yy)} = \\sum^n_{i=1} y^2_i - n(\\bar{y})^2 = 8712-10 \\times 29^2 = 302\\)\n\\(S_{(xy)} = \\sum^n_{i=1} x_i y_i - n(\\bar{x})(\\bar{y}) = 940-10 \\times 3 \\times 29 = 70\\)\n\\(\\to \\hat{\\beta_1} = \\frac{S_{(xy)}}{S_{(xy)}} = \\frac{70}{20} = 3.5, \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = 29-3.5 \\times 3 = 18.5\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-13",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-13",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(3)",
    "text": "(3)\n분산분석표를 작성하고, 유의수준 α = 0.05에서, 회귀직선에 대한 유의성 검정을 수행하시오.\nanswer\n\\(SSR = \\frac{S^2_{(xy)}}{S_{(xx)}} = \\frac{70^2}{20} = 245\\)\n\\(SSE = SST - SSR = 302 - 245 = 57\\)\n\n\n\n요인\n제곱합(SS).\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n\n\n\n\n회귀\n245\n1\n245\n34.39\n\n\n잔차\n57\n8\n7.125\n\n\n\n계\n302\n\n\n\n\n\n\n- 가설 : \\(H_0 : \\beta_1 = 0 \\text{ vs. } H_1 : \\beta_1 = 0\\)\n- 기각역 : \\(F_0 > F_{\\alpha}(1,8) = 5.32\\)\n- 결론: \\(F_0\\)값이 기각역에 속하므로 귀무가설을 기각할 수 있다. 즉 유의수준 \\(5\\)%에서 회귀직선이 유의하다고 할 수 있다.\n\nround(qf(0.95,1,8),2)\n\n5.32"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-14",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-14",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(4)",
    "text": "(4)\n오차항의 분산 \\(\\sigma^2\\)에 대한 추정치를 구하여라\nanswer\n\\(\\hat{\\sigma}^2 = MSE = 7.125\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-15",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-15",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(5)",
    "text": "(5)\n표본상관계수를 구하시오.\nanswer\n\\(r = \\frac{S_{(xy)}}{\\sqrt{S_{(xx)}S_{(yy)}}} = \\frac{70}{\\sqrt{20 \\times 302}} = 0.90\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-16",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-16",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(6)",
    "text": "(6)\n결정계수를 구하고, 결정계수의 의미를 설명하시오\nanswer\n\\(R^2 = \\frac{SSR}{SST} = \\frac{245}{302} = 0.81\\)\n회귀모형이 반응변수의 총변동의 \\(81\\)%를 설명하고 있다."
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-17",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-17",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(7)",
    "text": "(7)\n다음의 가설검정을 유의수준 \\(\\alpha = 0.05\\)에서 수행하시오.\n\\[H_0 : \\beta_1 = 2 \\text{ vs. } H_1 : \\beta_1 >2\\]\nanswer\n- 검정통계량 : \\(T = \\frac{\\hat{\\beta}_1 - \\beta^0_1}{\\hat{s.e.} (\\hat{\\beta}_1)} = \\frac{\\hat{\\beta}_1 - 2}{\\hat{s.e.}(\\hat{\\beta}_1)} \\sim_{H_0} t(8)\\)\n- \\(\\hat{s.e.}(\\hat{\\beta}_1) = \\sqrt{\\frac{MSE}{S_{(xx)}}} = \\sqrt{\\frac{7.125}{20}} = 0.597\\)\n- 검정통계량 관측값: \\(t_0 = \\frac{3.5-2}{0.597} = 2.513\\)\n- 기각역 : \\(t_0 > t_{0.05}(8) = 1.86\\)\n- 결론 : \\(t_0\\)값이 기각역에 속하므로 귀무가설을 기각할 수 있다. 즉 유의수준 \\(5\\)%에서 \\(\\beta_1\\)이 \\(2\\)보다 크다고 할 수 있다.\n\nround(qt(0.95,8),2) # 단측검정\n\n1.86"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-18",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-18",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(8)",
    "text": "(8)\n\\(3.5\\)kg의 비료를 사용했을 때 평균 감자 수확량과 하나의 개별 \\(y\\)값의 \\(90\\)% 신뢰구간을 각각 구하시오\nanswer\n- \\(\\hat{\\mu}_0 = \\hat{y}_0 = 18.5 + 3.5 x = 18.5 + 3.5 \\times 3.5 = 30.75(kg)\\)\n- \\(\\hat{s.e.}(E(\\hat{y|x = 3.5})) = \\sqrt{(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{(xx)}})\\hat{\\sigma}^2} = \\sqrt{(\\frac{1}{10} + \\frac{(3.5-3)^2}{20}) \\times 7.125} = 0.895\\)\n- \\(\\hat{s.e.}(\\hat{y}_0) = \\sqrt{(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{(xx)}})\\hat{\\sigma^2}} = \\sqrt{(1 + \\frac{1}{10} + \\frac{(3.5-3)^2}{20}) \\times 7.125} = 2.815\\)\n- 평균감자수확량의 \\(90\\)% 신뢰구간\n\\[\\hat{\\mu}_0 \\pm t_{0.1/2}(8)\\hat{s.e.}(E(\\hat{y|x = 3.5})) = 30.75 \\pm 1.86 \\times 0.895 = (29.085, 32.415)\\]\n- 개별 감자수확량의 \\(90\\)% 신뢰구간\n\\[\\hat{\\mu}_0 \\pm t_{0.1/2})(8) \\hat{s.e.}(\\hat{y}_0) = 30.75 \\pm 1.86 \\times 2.815 = (25.514, 35.986)\\]\n\nround(qt(0.95,8),2)\n\n1.86"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-20",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-20",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(1)",
    "text": "(1)\n적합결여검정을 수행하기 위한 가설을 적으시오.\nanswer\n\\(H_0 : E(y|x) = \\beta_0 + \\beta_1 x \\text{ vs. } H_1 : E(y|x) \\neq \\beta_0 + \\beta_1 x\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-21",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-21",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(2)",
    "text": "(2)\n가설검정을 위한 검정통계량은 무엇인가? 귀무가설이 사실이라고 가정할 때 검정통계량의 분포는 무엇인가?(증명 필요없음)\nanswer\n\\(F = \\frac{SSLF/(k-2)}{SSPE/(n-k)} \\sim_{H_0}, F(k-2,n-k)\\)\n- \\(SSPE = \\sum^k_{i=1} \\sum^{n_i}_{j=1} (y_{ij} - \\bar{y}_{i} )^2\\)\n- \\(SSLF = \\sum^k_{i=1}n_i (\\hat{y_i} - \\bar{y}_i)^2 = SSE - SSPE\\)\n- \\(\\bar{y}_i = \\sum^{n_i}_{j=1} y_{ij}/n_i\\)"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-22",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-22",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(3)",
    "text": "(3)\n(1)에 대한 가설 검정을 수행하시오\nanswer\n- \\(SSPE = (1.5^2 + \\dots + (-1)^2) = 15\\)\n- \\(SSLF = SSE - SSPE = 57-15=42\\)\n- 검정통계량의 관측값 : \\(F_0 = \\frac{42/3}{15/5} = \\frac{14}{3} = 4.67\\)\n\n$ k-2 = 5-2 = 3, n-k = 10-5 = 5$\n\n- 기각역 : \\(F_0 > F_{0.05}(3,5) = 5.41\\)\n- 결론: 검정통계량의 관측값이 기각역에 속하지 않기 떄문에 귀무가설을 기각할 수 없다. 즉, 유의수준 \\(5\\)%에서 회귀모형은 적절하지 않다고 할 수 있다.\n\nround(qf(0.95,3,5),2)\n\n5.41"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-24",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-24",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(1)",
    "text": "(1)\n가중회귀최소추정량(\\(WLSE\\))을 구하기 위한 가중오차제곱합(\\(Q\\))를 정의하여라.\nanswer\n\\(Q = \\sum^n_{i=1} w_i \\{ y_i - (\\beta_0 + \\beta_1 x_i) \\}^2\\)\nQuadratic Form"
  },
  {
    "objectID": "posts/rl/2023-02-22-rl-mid_term.html#section-25",
    "href": "posts/rl/2023-02-22-rl-mid_term.html#section-25",
    "title": "Advanced Regression Analysis Mid Term",
    "section": "(2)",
    "text": "(2)\n가중오차제곱합을 가장 작게 하는 WLSE를 구하여라.\nanswer\n가중최소제곱추정량(WLSE) : \\((\\hat{\\beta}_0,\\hat{\\beta}_1) = argmin_{\\beta_0,\\beta_1 \\in R} \\sum^n_{i=1}w_i \\{ y_i - (\\beta_0 + \\beta_1 x_i)\\}^2\\)\n\\(\\hat{\\beta}_1 = \\frac{\\sum w_i (x_i - \\bar{x}_w)(y_i - \\bar{y}_w)}{\\sum w_i(x_i - \\bar{x}_w)^2}\\)\n\\(\\hat{\\beta}_0 = \\bar{y}_w - \\hat{\\beta_1} \\bar{x}_w\\)\n단, \\(\\bar{x}_w = \\frac{\\sum w_i x_i}{\\sum w_i}, \\bar{y}_w = \\frac{\\sum w_i y_i}{\\sum w_i}\\)"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html",
    "href": "posts/rl/2022-09-21-rl_HW1.html",
    "title": "Regression HW 1",
    "section": "",
    "text": "고급회귀분석 과제, CH03, CH04\n과제1\n풀이 :\nR 이용하지 않고 직접 계산. (R로 단순 계산은 해도 됨)\n모든 문제에는 풀이가 있어야 함.\n풀이 없이 답만 있는 경우 ’0’점 처리.\n제출 방법 :\n직접 제출 (607호) 또는 스캔, 사진, tex 작업, 문서 작업 등 후 pdf로 변환 후 제출\npdf 아닌 경우 미제출 처리"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-2",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-2",
    "title": "Regression HW 1",
    "section": "(1)",
    "text": "(1)\n이 데이터의 산점도를 그리시오.\n\ndt <- data.frame(x = c(0.9,1.3,2.1,2.5,2.4,1.7,0.7,1.2,1.6),\n                 y = c(2.0,2.6,4.3,5.8,5.1,3.2,1.8,2.3,3.0))\ndt\n\n\n\nA data.frame: 9 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    0.92.0\n    1.32.6\n    2.14.3\n    2.55.8\n    2.45.1\n    1.73.2\n    0.71.8\n    1.22.3\n    1.63.0\n\n\n\n\nAnswer\n\nplot(y~x, \n     data = dt,\n     xlab = \"무게\",\n     ylab = \"에너지소모량\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\n\n\n\n\n\n양의 상관관계가 있어보인다.\n무게가 커질수록 에너지소모량도 큰 경향이 보이기 때문이다.\n우상향의 모양이라, 단순상관선형 적용해보면 되겠다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-3",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-3",
    "title": "Regression HW 1",
    "section": "(2)",
    "text": "(2)\n최소제곱법의 의한 회귀직선을 적합시키시오.\nAnswer\n\ndt1 <- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x),\n  y_bary = dt$y - mean(dt$y)) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndt1$x_barx2 <- dt1$x_barx^2\ndt1$y_bary2 <- dt1$y_bary^2\ndt1$xy <-dt1$x_barx * dt1$y_bary\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndt1\n\n\n\nA data.frame: 9 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2xy\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.92.0-0.7-1.34444440.491.8075309 0.94111111\n    21.32.6-0.3-0.74444440.090.5541975 0.22333333\n    32.14.3 0.5 0.95555560.250.9130864 0.47777778\n    42.55.8 0.9 2.45555560.816.0297531 2.21000000\n    52.45.1 0.8 1.75555560.643.0819753 1.40444444\n    61.73.2 0.1-0.14444440.010.0208642-0.01444444\n    70.71.8-0.9-1.54444440.812.3853086 1.39000000\n    81.22.3-0.4-1.04444440.161.0908642 0.41777778\n    91.63.0 0.0-0.34444440.000.1186420 0.00000000\n\n\n\n\n\n반올림 해주었다.\n\n\nround(colSums(dt1),3)\n\ni45x14.4y30.1x_barx0y_bary0x_barx23.26y_bary216.002xy7.05\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1 <- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\nbeta0 <- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\ncat(\"hat beta1 = \", beta1)\n\nhat beta0 =  -0.1156783hat beta1 =  2.162577\n\n\n\n\\(\\hat{y} = -0.1156783 + 2.162577x\\)의 모형으로 적합되었다.\n\n\nlm(y~x,dt)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nCoefficients:\n(Intercept)            x  \n    -0.1157       2.1626"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-4",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-4",
    "title": "Regression HW 1",
    "section": "(3)",
    "text": "(3)\n데이터의 산점도를 그리고 추정한 회귀직선을 (1)에서 그린 산점도 위에 그리시오.\nAnswer\n\nplot(y~x, \n     data = dt,\n     xlab = \"무게\",\n     ylab = \"에너지소모량\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\npar(new=TRUE)\nplot(-0.1156783 +  2.162577*x~x,\n     data = dt,\n     xlab = \"\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"blue\")\n\n\n\n\n\n추정한 회귀직선을 그려보니 오차가 클 것처럼 y와 \\(\\hat{y}\\)가 떨어진 값이 많아보인다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-5",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-5",
    "title": "Regression HW 1",
    "section": "(4)",
    "text": "(4)\n결정계수와 상관계수를 구하시오.\n\\(r_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\)\n\nrxy = colSums(dt1)[8]/sqrt(colSums(dt1)[6]*colSums(dt1)[7])\nrxy\n\nxy: 0.976090685311348\n\n\n\n상관계수는 약 98% 로, \\(x,y\\)간 높은 양의 상관관계가 있었다.\n\n\\(R^2 = \\frac{SSR}{SST} = r^2\\)\n\nSST = sum((dt$y - mean(dt$y))^2)\n\n\nSSR = sum( ( (-0.1156783 +  2.162577*dt$x)-mean(dt$y) )^2 )\n\n\nSSE = sum( ( dt$y-(-0.1156783 +  2.162577*dt$x))^2 )\n\n\ncat(\"SST = \", SST)\ncat(\"\\nSSR = \", SSR)\ncat(\"\\nSSE = \", SSE)\n\nSST =  16.00222\nSSR =  15.24617\nSSE =  0.7560566\n\n\n\nSSR/SST\n\n0.95275330164195\n\n\n\nrxy**2\n\nxy: 0.952753025951577\n\n\n\n결정계수는 약 95%로, 설명력도 높은 편이라고 말할 수 있지만, 결정계수는 다른 모델과 비교할때 언급되는 것이 적절하다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-6",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-6",
    "title": "Regression HW 1",
    "section": "(5)",
    "text": "(5)\n분산분석표를 작성하고 회귀직선의 유의 여부를 검정하시오 (유의수준 \\(α = 0.05\\) 사용).\n\nMSR = SSR/1\nMSE = SSE/7\n\n\ncat(\"MSR = \", MSR)\ncat(\"\\nMSE = \", MSE)\n\nMSR =  15.24617\nMSE =  0.1080081\n\n\n\nFvalue = MSR/MSE\n\n\ncat(\"F value = \",Fvalue)\n\nF value =  141.1577\n\n\n\ncat(\"p value = \",df(Fvalue,1,7))\n\np value =  1.614709e-07\n\n\n\n\n\n\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n15.24617\n15.24617\n141.1584\n1.614672e-07\n\n\nResiduals\n7\n0.7560522\n0.1080075\n\n\n\n\n\n\nF값은 141.1584, p value는 1.614672e-07가 나왔다.\n유의수준 5%에서 모형이 유의하다는 것을 알 수 있었다.\n\n\nanova(lm(y~x,dt))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x115.246165615.2461656141.15766.798033e-06\n    Residuals7 0.7560566 0.1080081      NA          NA"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-7",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-7",
    "title": "Regression HW 1",
    "section": "(6)",
    "text": "(6)\n\\(\\beta_0,\\beta_1\\) 에 대한 90% 신뢰구간을 구하시오.\nAnswer\n\\[\\hat{\\beta_0} \\pm t_{(\\alpha/2,n-2)}\\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}}\\]\n\ncat(\"Beta0 confidence level\",beta0 + qt(0.025, 7) * sqrt((MSE)*(1/9 + mean(dt$x)^2/sum((dt$x - mean(dt$x))^2))),\"~\",beta0 + qt(0.975, 7) * sqrt((MSE)*(1/9 + mean(dt$x)^2/sum((dt$x - mean(dt$x))^2))))\n\nBeta0 confidence level -0.8514415 ~ 0.620085\n\n\n\\[\\hat{\\beta_1} \\pm t_{(\\alpha/2,n-2)} \\sqrt{\\frac{MSE}{S_{xx}}}\\]\n\ncat(\"Beta1 confidence level\",beta1 + qt(0.025, 7) * sqrt((MSE)/sum((dt$x - mean(dt$x))^2)),'~',beta1 + qt(0.975, 7) * sqrt((MSE)/sum((dt$x - mean(dt$x))^2)))\n\nBeta1 confidence level 1.732168 ~ 2.592986\n\n\n\n\\(\\beta_0\\)의 신뢰구간은 0을 포함하였다.(\\(H_0 : \\beta_0=0\\) 채택)\n\\(\\beta_1\\)의 신뢰구간은 0을 포함하지 않았다.(\\(H_0 : \\beta_1=0\\) 기각)\n신뢰구간으로 신뢰구간에 0이 포함된 \\(\\beta_1\\) 계수만 유의미하다는 것을 알 수 있다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-8",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-8",
    "title": "Regression HW 1",
    "section": "(7)",
    "text": "(7)\n\\(H_0 :\\beta_1 =1\\) vs. \\(H_1 :\\beta_1 \\ne 1\\)의 가설검정을 유의수준 \\(α=0.1\\)에서 수행하시오.\n\\[\\text{t value} = \\frac{\\hat{\\beta_1} - 1}{s.e(\\hat{\\beta_1})}\\]\n\nTvalue = (beta1 - 1)/(sqrt((MSE/sum((dt$x - mean(dt$x))^2))))\n\n\nTvalue\n\n6.3870789106442\n\n\n\n-Tvalue\n\n-6.3870789106442\n\n\n\nqt(0.95,7)\n\n1.89457860509001\n\n\n\nqt(0.05,7)\n\n-1.89457860509001\n\n\n\n구힌 t value = 6.39가 유의수준 \\(\\alpha = 0.1\\) 에서의 t value = 1.89보다 크기 때문에 유의하다는 결과가 나와 귀무가설을 기각한다.\n따라서 \\(\\beta_1\\)은 1이 아니다.\n\nfigure로 표현\n\npar(mfrow=c(1,1))\nbasic <- seq(-3,3,by=0.01)\nplot(basic,dt(basic,df=7),type=\"l\",xlim=c(-8,8))\nabline(v=Tvalue,col=\"red\",lty=2)\nabline(v=qt(0.95,7),col=\"blue\",lty=2)\ntext(x=Tvalue, y=c(0.2), labels=c(\"tvalue\\n6.39\"), pos=4, col=\"black\")\ntext(x=qt(0.95,7), y=c(0.2), labels=c(\"t(0.05,7)\\n1.89\"), pos=4, col=\"black\")\nabline(v=-Tvalue,col=\"red\",lty=2)\nabline(v=qt(0.05,7),col=\"blue\",lty=2)"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-9",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-9",
    "title": "Regression HW 1",
    "section": "(8)",
    "text": "(8)\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측하시오. 이것은 무게가 1,000kg이 되는 차량의 에너지 소모량의 몇 배인가?\n\\[\\hat{\\mu}_0 = \\hat{\\beta_0} + \\hat{\\beta_1} x_0\\]\n\ncat(\"평균 에너지 소모량 = \",beta0 + beta1 * 3)\n\n평균 에너지 소모량 =  6.372052\n\n\n\ncat(\"무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측해보니 무게가 1,000kg이 되는 차량의 에너지 소모량의\",(beta0 + beta1 * 3)/(beta0 + beta1 * 1),\"배 였다.\")\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측해보니 무게가 1,000kg이 되는 차량의 에너지 소모량의 3.113028 배 였다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-10",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-10",
    "title": "Regression HW 1",
    "section": "(9)",
    "text": "(9)\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량과 하나의 개별 \\(y\\) 값의 90% 신뢰구간을 각각 구하시오.\nAnswer\n\ncat(\"무게가 3,000kg 이 되는 차량의 평균 에너지 소모량은\",beta0 + beta1 * 3,\"이다.\")\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량은 6.372052 이다.\n\n\n\\[\\hat{Var(\\hat{\\mu}_0}) = \\sigma (\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}})\\]\n\nsigma = MSE\n\n\nmean(dt$x)\n\n1.6\n\n\n\\(S_{xx}\\)\n\nsum((dt$x - mean(dt$x))^2)\n\n3.26\n\n\n\ncolSums(dt1)[6]\n\nx_barx2: 3.26\n\n\n\\(\\hat{Var(\\hat{\\mu_0})}\\)\n\nsqrt(sigma)*(1/9 + (3-1.6)^2/3.26)\n\n0.234106948837031\n\n\n\ncat(\"hat var(hat mu zero) = \",sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\nhat var(hat mu zero) =  0.2341069\n\n\n\\(\\hat{\\sigma}_{\\hat{\\mu_0}}\\)\n\nsqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n0.483845997024912\n\n\n\ncat(\"hat sigma(hat mu zero) = \", sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26)))\n\nhat sigma(hat mu zero) =  0.483846\n\n\n\\(\\hat{\\mu_0} \\pm t_{(\\alpha/2,(n-2))}\\hat{\\sigma_{\\hat{\\mu_0}}}\\)\n\n6.372052 + qt(0.95,7)*sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n7.28873627412184\n\n\n\n6.372052 - qt(0.95,7)*sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n5.45536772587816\n\n\n\ncat(\"개별 y값의 90% 신뢰구간은 (\",5.4553690631157,\"~\",7.2887349368843,\") 이다.\")\n\n개별 y값의 90% 신뢰구간은 ( 5.455369 ~ 7.288735 ) 이다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-11",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-11",
    "title": "Regression HW 1",
    "section": "(10)",
    "text": "(10)\n잔차 \\(e_i = y_i − \\hat{y}_i\\) 를 구하고 잔차의 합이 0 임을 확인하시오.\nAnswer\n\nepsilon = dt$y - (beta0 + beta1*dt$x)\n\n\nepsilon\n\n\n0.169359236537151-0.0956714383094752-0.1257327880027260.5092365371506480.0254942058623033-0.36070211315610.401874573960464-0.179413769597819-0.344444444444445\n\n\n\ncat(\"잔차의 합 = \",sum(epsilon))\n\n잔차의 합 =  8.881784e-16\n\n\n합이 0인 것을 확인했다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-12",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-12",
    "title": "Regression HW 1",
    "section": "(11)",
    "text": "(11)\n잔차들의 \\(x_i\\) 에 대한 가중합, \\(\\sum x_ie_i\\) 를 구하시오.\n\nsum(dt$x * epsilon)\n\n9.99200722162641e-16\n\n\n잔차들의 \\(x_i\\) 에 대한 가중합, \\(\\sum x_ie_i\\)이 0인 것을 확인했다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-13",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-13",
    "title": "Regression HW 1",
    "section": "(12)",
    "text": "(12)\n잔차들의 \\(\\hat{y}\\)에 대한 가중합 \\(\\sum \\hat{y}_ie_i\\), 를 구하시오.\n\nsum((beta0 + beta1*dt$x)*epsilon)\n\n1.74860126378462e-15\n\n\n잔차들의 \\(\\hat{y}\\)에 대한 가중합 \\(\\sum \\hat{y}_ie_i\\)이 0인 것을 확인했다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-14",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-14",
    "title": "Regression HW 1",
    "section": "(13)",
    "text": "(13)\n원점을 지나는 회귀직선을 구하시오.\nAnswer\n\\(\\hat{\\beta_1} = \\frac{\\sum(x_iy_i)}{\\sum(x_i^2)}\\)\n\nsum(dt$x * dt$y)/sum((dt$x^2))\n\n2.09923954372624\n\n\n\nbeta1_0 <- sum(dt$x * dt$y)/sum((dt$x^2))\n\n\ncat(\"hat beta1_0 = \", beta1_0)\n\nhat beta1_0 =  2.09924\n\n\n\n원점을 지나는 회귀직선은 \\(\\hat{y} = 2.09924x\\)의 모형으로 적합되었다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-15",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-15",
    "title": "Regression HW 1",
    "section": "(14)",
    "text": "(14)\n위 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간을 구하시오.\nAnswer\n\\[\\hat{\\beta_1} \\pm t_{(\\alpha/2,n-1)}\\frac{\\hat{\\sigma}}{\\sqrt{S_{xx}}}\\]\n\ncolSums(dt1)[6]\n\nx_barx2: 3.26\n\n\n\nSSR_0 = sum( ( (2.09924*dt$x) )^2 )\n\n\nSSE_0 = sum((dt$y - 2.09924*dt$x)^2)\n\n\nSST_0 = sum(dt$y^2)\n\n\nMSE_0 = SSE_0/8\n\n\nMSR_0 = SSR_0/1\n\n\nsigma_0 = sqrt(MSE_0)\n\n\nbeta1_0 + qt(0.95,8) * sigma_0/sqrt(3.26)\n\n2.41896448320474\n\n\n\nbeta1_0 - qt(0.95,8) * sigma_0/sqrt(3.26)\n\n1.77951460424773\n\n\n\ncat(\"원점을 지나는 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간은 (\",beta1_0 - qt(0.95,8) * sigma_0/sqrt(3.26),\"~\",beta1_0 + qt(0.95,8) * sigma_0/sqrt(3.26),\")이다.\")\n\n원점을 지나는 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간은 ( 1.779515 ~ 2.418964 )이다.\n\n\n\n\\(\\beta_1\\)의 신뢰구간은 0을 포함하지 않았다.(\\(H_0 : \\beta_1=0\\) 기각)\n신뢰구간으로 신뢰구간에 0이 포함된 \\(\\beta_1\\) 계수가 유의미하다는 것을 알 수 있다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-16",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-16",
    "title": "Regression HW 1",
    "section": "(15)",
    "text": "(15)\n원점을 지나는 회귀직선의 결정계수를 구하시오.\nAnswer\n\\(R^2 = \\frac{SSR}{SST} = r^2\\)\n\nSSR_0/SST_0\n\n0.993392179573841\n\n\n\ncat(\"원점을 지나는 회귀직선의 결정계수는 \", SSR_0/SST_0,\"로 약\",round(SSR_0/SST_0,2),\"% 였다.\")\n\n원점을 지나는 회귀직선의 결정계수는  0.9933922 로 약 0.99 % 였다."
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html#section-17",
    "href": "posts/rl/2022-09-21-rl_HW1.html#section-17",
    "title": "Regression HW 1",
    "section": "(16)",
    "text": "(16)\n원점을 포함한 회귀직선과 포함하지 않은 회귀직선의 결과를 비교하여라.\nAnswer\n\ncat(\"원점을 포함하지 않는 회귀직선의 결정계수는 \",SSR/SST,\"로, 원점을 포함하는 회귀직선의 결정계수인 \",SSR_0/SST_0,\"보다 작다.\")\n\n원점을 포함하지 않는 회귀직선의 결정계수는  0.9527533 로, 원점을 포함하는 회귀직선의 결정계수인  0.9933922 보다 작다.\n\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 평균재곱오차는 \",MSE,\"이며, 원점을 포함하는 회귀직선의 평균제곱오차는 \",MSE_0,\"이다. 원점을 포함하는 모형의 오차가 조금 더 작았다.\")\n\n원점을 포함하지 않는 회귀직선의 평균재곱오차는  0.1080081 이며, 원점을 포함하는 회귀직선의 평균제곱오차는  0.0963731 이다. 원점을 포함하는 모형의 오차가 조금 더 작았다.\n\n\n\nFvalue = MSR/MSE\n\n\nFvalue_0 = MSR_0 / MSE_0\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 F 값은 \",Fvalue,\"로, 원점을 포함하는 회귀직선의 F 값인 \",Fvalue_0,\"보다 작다. 따라서 원점을 포함한 모델이 회귀모형애 의해 설명되는 부분이 더 크며, 오차항에 기인된 부분이 더 작다.\")\n\n원점을 포함하지 않는 회귀직선의 F 값은  141.1577 로, 원점을 포함하는 회귀직선의 F 값인  1202.608 보다 작다. 따라서 원점을 포함한 모델이 회귀모형애 의해 설명되는 부분이 더 크며, 오차항에 기인된 부분이 더 작다.\n\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 p value 는 \",df(Fvalue,1,7),\"로, 원점을 포함하는 회귀직선의 p value인\",df(Fvalue_0,1,8),\"과 같이 p value이 충분히 작아 두 모형이 모두 유의함을 알 수 있었다.\")\n\n원점을 포함하지 않는 회귀직선의 p value 는  1.614709e-07 로, 원점을 포함하는 회귀직선의 p value인 1.728622e-12 과 같이 p value이 충분히 작아 두 모형이 모두 유의함을 알 수 있었다.\n\n\n\nplot(beta0 + beta1*x~x,\n     data = dt,\n     xlab = \"원점을 지나지 않는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"blue\")\nplot(beta1_0*x~x,\n     data = dt,\n     xlab = \"원점을 지나는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"red\")\n\n\n\n\n\n\n\n\n선형성 만족\n\n\nepsilon = dt$y - beta0 + beta1*dt$x\nepsilon_0 = dt$y - beta1_0*dt$x\n\n\nplot(epsilon,\n     xlab = \"원점을 지나지 않는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"blue\")\nplot(epsilon_0,\n     xlab = \"원점을 지나는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"red\")\n\n\n\n\n\n\n\n\n원점을 지나지 않는 모형이 한쪽 부호에 머무는 경향이 있어보인다.(독립성을 만족하지 않을 수 있다.)\n원점을 지나는 모형은 그러한 경향이 없고, 등분산성을 만족하는 것처럼 보인다.\n\n\nshapiro.test(beta0 + beta1*dt$x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  beta0 + beta1 * dt$x\nW = 0.95279, p-value = 0.7207\n\n\n\nshapiro.test(beta1_0*dt$x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  beta1_0 * dt$x\nW = 0.95279, p-value = 0.7207\n\n\n\n두 모형 모두 정규성 가정을 만족했다.\n\nANOVA table 비교\n\n\n\ny=beta0+beta1x\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n15.24617\n15.24617\n141.1584\n1.614672e-07\n\n\nResiduals\n7\n0.7560522\n0.1080075\n\n\n\n\n\n\n\n\ny=beta1x\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n115.89906559088\n115.89906559088\n1202.60806139735\n1.728622e-12\n\n\nResiduals\n8\n0.77098479088\n0.09637309886"
  },
  {
    "objectID": "posts/rl/2022-11-28-rl-CH13.html",
    "href": "posts/rl/2022-11-28-rl-CH13.html",
    "title": "고급회귀분석 실습 CH13",
    "section": "",
    "text": "chapter 13"
  },
  {
    "objectID": "posts/rl/2022-11-28-rl-CH13.html#example",
    "href": "posts/rl/2022-11-28-rl-CH13.html#example",
    "title": "고급회귀분석 실습 CH13",
    "section": "Example",
    "text": "Example\n\ndt <- data.frame(\n  y = c(17,26,21,30,22,1,12,19,4,16,\n        28,15,11,38,31,21,20,13,30,14),\n  x1 = c(151,92,175,31,104,277,210,120,290,238,\n         164,272,295,68,85,224,166,305,124,246),\n  x2 = factor(rep(c(0,1), each=10))\n)\n\n할 수 있는 경우의 수 1.\nx2 = factor(rep(c('M','F'), each=10))\n\ncharacter로 넣어도 factor로 인식한다.\n조심할 점 알파벳 순으로 숫자가 부여된다.\n따라서 F는 0, M은 1로 부여되었다.\n\n참고: 강의록은 F는 1, M은 0으로 부여되어 결과가 다름\n해석의 결과가 다르진 않지만 어떻게 해석하느냐가 달라짐\n\n\n\n\n\nx2 = factor(rep(c(0,1), each=10))\n\nM은 0, F는 1-> 강의록과 같음\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    yx1x2\n    <dbl><dbl><fct>\n\n\n    1171510\n    226 920\n    3211750\n    430 310\n    5221040\n    6 12770\n\n\n\n\n\ncontrasts(factor(dt$x2))\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1\n\n\n    00\n    11\n\n\n\n\n잘 부여되었나 확인 필요 - chracter일때는 이 함수를 사용해도 의미가 없어서 factor로 바꿔서 해줘야 한다.\n\nm <- lm(y~x1+x2, dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.834912   1.758659  19.239 5.64e-13 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx21          7.933953   1.414702   5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n1.\nx2 = factor(rep(c('M','F'), each=10))\n로 입력한 경우\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\\(x_2 = 0\\), F\n\\(x_2 = 1\\), M\n\\(x_2\\)는 0 아니면 1 인 지수함수\n\\(E(y|M) : \\beta_0 + \\beta_1x_1 + \\beta_2 = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1\\)\n2.\nx2 = factor(rep(c(0,1), each=10))\n로 입력한 경우\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\\(x_2 = 0\\), M\n\\(x_2 = 1\\), F\n\\(E(y|M) : \\beta_0 + \\beta_1x_1\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1+ \\beta_2 = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() +\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), \n                     values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\nm <- lm(y~x1+x2, dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.834912   1.758659  19.239 5.64e-13 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx21          7.933953   1.414702   5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\\(\\beta_2\\)가 유의함을 확인함(3.13e-05) - 남성과 여성이 차이가 있다.\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(m)[2], intercept = coef(m)[1], col= 'darkorange')+\n  geom_abline(slope = coef(m)[2], intercept = coef(m)[1]+coef(m)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n기울기 차이 고려 안해서 기울기는 같을 것\n\n교호작용\n\nm1 <- lm(y~x1*x2, dt)\n\n곱하기로 교호작용 표현\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1 * x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0463 -1.7591 -0.6232  1.9311  6.1102 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.656104   2.365392  14.229 1.68e-10 ***\nx1          -0.099858   0.012650  -7.894 6.59e-07 ***\nx21          8.313516   3.541379   2.348   0.0321 *  \nx1:x21      -0.002089   0.017766  -0.118   0.9078    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.218 on 16 degrees of freedom\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8803 \nF-statistic: 47.56 on 3 and 16 DF,  p-value: 3.405e-08\n\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\) - 성별, 성적, 성별*성적\n\\(M : x_2=0 \\to E(y|M) = \\beta_0+\\beta_1x_1\\)\n\\(F : x_2=1 \\to E(y|F) = \\beta_0 + \\beta_1x_1 + \\beta_2 + \\beta_3x_1 = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)x_1\\)\n\n식으로 봤을 때 절편, 기울기 모두 차이가 난다.\n절편 차이는 유의하다.\n기울기 차이는 거의 없다.\n교호작용은 없는 것으로 확인.\n\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(m1)[2], intercept = coef(m1)[1], col= 'darkorange')+\n  geom_abline(slope = coef(m1)[2]+coef(m1)[4], intercept = coef(m1)[1]+coef(m1)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\n책 예제\n\nlibrary(ISLR)\n\n\nhead(Carseats)\n\n\n\nA data.frame: 6 × 11\n\n    SalesCompPriceIncomeAdvertisingPopulationPriceShelveLocAgeEducationUrbanUS\n    <dbl><dbl><dbl><dbl><dbl><dbl><fct><dbl><dbl><fct><fct>\n\n\n    1 9.50138 7311276120Bad   4217YesYes\n    211.22111 4816260 83Good  6510YesYes\n    310.06113 3510269 80Medium5912YesYes\n    4 7.40117100 4466 97Medium5514YesYes\n    5 4.15141 64 3340128Bad   3813YesNo \n    610.8112411313501 72Bad   7816No Yes\n\n\n\n\n\ndim(Carseats)\n\n\n40011\n\n\n• Sales : 판매량 (단위: 1,000)\n• Price : 각 지점에서의 카시트 가격\n• ShelveLoc : 진열대의 등급 (Bad, Medium, Good)\n• Urban :도시 여부 (Yes, No)\n• US: 미국 여부 (Yes, No)\n\\(\\to\\) 세 개의 범주형 자료, 가변수 4개(3-2,2-1,2-1)\n\nfit <- lm(fit<-lm(Sales~Price+ShelveLoc+US, \n                  data=Carseats))\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = fit <- lm(Sales ~ Price + ShelveLoc + US, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1720 -1.2587 -0.0056  1.2815  4.7462 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     11.476347   0.498083  23.041  < 2e-16 ***\nPrice           -0.057825   0.003938 -14.683  < 2e-16 ***\nShelveLocGood    4.827167   0.277294  17.408  < 2e-16 ***\nShelveLocMedium  1.893360   0.227486   8.323 1.42e-15 ***\nUSYes            1.013071   0.195034   5.194 3.30e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.857 on 395 degrees of freedom\nMultiple R-squared:  0.5718,    Adjusted R-squared:  0.5675 \nF-statistic: 131.9 on 4 and 395 DF,  p-value: < 2.2e-16\n\n\n\ncontrasts(Carseats$ShelveLoc)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    GoodMedium\n\n\n    Bad00\n    Good10\n    Medium01\n\n\n\n\n알파벳 순으로 부여된 것 확인\n\\(y= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5\\)\n\nlibrary(car)\n\n\\(H_0 : \\beta_2 = \\beta_3 = 0\\)\n\\(\\beta_2 = 0, \\beta_3 = 0\\)\n\nC<-rbind(c(0,0,1,0,0,0),\n         c(0,0,0,1,0,0))\n\n\nlinearHypothesis(fit, C)\n\nERROR: Error in L %*% b: non-conformable arguments\n\n\n유의미한 결과 확인\n\n\n구간별 회귀분석\n\ndt <- data.frame(\n  y = c(377,249,355,475,139,452,440,257),\n  x1 = c(480,720,570,300,800,400,340,650)\n)\n\n\ndt$x2 = sapply(dt$x1, function(x) max(0, x-500))\n\n\nm <- lm(y ~ x1+x2, dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-22.765  29.765  18.068   4.068 -17.463  20.605 -15.117 -17.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 589.5447    60.4213   9.757 0.000192 ***\nx1           -0.3954     0.1492  -2.650 0.045432 *  \nx2           -0.3893     0.2310  -1.685 0.152774    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 24.49 on 5 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.9571 \nF-statistic: 79.06 on 2 and 5 DF,  p-value: 0.0001645\n\n\n\ndt2 <- rbind(dt[,2:3], c(500,0))\n\n\ndt2$y <- predict(m, newdata = dt2)\n\nthis is the predicted line of multiple linear regression\n\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') +\n  geom_line(color='darkorange',data = dt2, aes(x=x1, y=y))+\n  geom_vline(xintercept = 500, lty=2, col='red')+\n  theme_bw()"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html",
    "href": "posts/rl/2023-02-23-rl-final_term.html",
    "title": "Advanced Regression Analysis Final Term",
    "section": "",
    "text": "기말고사"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-1",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-1",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(1)",
    "text": "(1)\n\\(\\sum^n_{j=1} Var(\\hat{y}_j) = (p+1) \\sigma^2\\)\nanswer\n\\(\\hat{\\bf{y}}= X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y = Hy\\)\n\\(Var(X\\hat{\\beta}) = XVar(\\hat{\\beta})X^\\top\\)\n\\(Var(\\hat{\\beta}) = Var((X^\\top X)^{-1} X^\\top y) = (X^\\top X)^{-1} X^\\top Var(y)X(X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top X(X^\\top X)^{-1} \\sigma^2 = (X^\\top X )^{-1} \\sigma^2\\)\n\\(Var(\\hat{y}) = Var(X\\hat{\\beta}) = X(X^\\top X)^{-1} X^\\top \\sigma^2\\)\n\\(\\to Var(\\hat{y}) = HVar(y) H^\\top = HH\\sigma^2 = H\\sigma^2\\)\n\\(\\star H^\\top = H, H^2 = H, Var(y) = I_n \\sigma^2\\)\n\\(\\sum^{n}_{j=1} Var(\\hat{y}_j) = tr(Var(\\hat{y})) = tr(H\\sigma^2) = \\sigma^2 tr(H) = tr((X^\\top X)^{-1} X^\\top X)\\sigma^2 = tr(I_{p+1})\\sigma^2 = (p+1)\\sigma^2\\)\n\\(\\star tr(X (X^\\top X)^{-1} X^\\top)\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-2",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-2",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(2)",
    "text": "(2)\n\\(Cov(\\mathbf{e,y}) = \\sigma^2[I_n - X(X^\\top X)^{-1} X^\\top]\\)\nanswer\n\\(\\bf{e} = y - \\hat{y} = y - Hy = (I-H)y\\)\n\\(Cov(\\bf{e},y) = Cov((I-H)y,y) = (I-H)Var(y) = (I-H) \\sigma^2\\)\n\\(\\star Cov(Ax,y) = A Cov(X,Y)\\)\n\\(\\star Var(y) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-3",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-3",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(3)",
    "text": "(3)\n\\(Cov(\\mathbf{e,\\hat{y}}) = O_n\\)\nanswer\n\\(Cov(e(I - H)\\bf{y},Hy) = (I-H) Cov(y,y)H^\\top = (I-H)H\\sigma^2 = \\mathbb{O}_n \\sigma^2\\)\n\\(\\star Cov(Ax,By) = ACov(X,Y)B^\\top\\)\n\\(\\star H^\\top= H\\)\n\\(\\star H^2 = H\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-4",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-4",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(4)",
    "text": "(4)\n\\(Cov(\\mathbf{e,\\hat{\\beta}}) = O_{n \\times (p+1)}\\)\nanswer\n\\(Cov((I-H)\\bf{y},(X^\\top X)^{-1} y) = (I-H) Cov(y,y) X (X^\\top X)^{-1} = (I - X(X^\\top X)^{-1} X^\\top ) X (X^\\top X)^{-1} \\sigma^2 = \\{ X(X^\\top X)^{-1} - X(X^\\top X)^{-1} \\} \\sigma^2 = \\mathbb{O}_{n \\times ([+1)}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-5",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-5",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(5)",
    "text": "(5)\n\\(Cov(\\mathbf{\\epsilon, \\hat{\\beta}}) = \\sigma^2 X(X^\\top X)^{-1}\\)\nanswer\n\\(\\bf{\\hat{\\beta}} = (X^\\top X)^{-1} X^\\top y = (X^\\top X)^{-1} X^\\top (X\\beta + \\epsilon) = \\beta + (X^\\top X)^{-1} X^\\top \\epsilon\\)\n\\(Cov(\\bf{\\epsilon} , \\beta + (X^\\top X)^{-1} X^\\top \\epsilon ) = Cov(\\epsilon, \\beta) + Cov(\\epsilon, \\epsilon)X(X^\\top X)^{-1} = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\star Cov(\\epsilon, \\beta) = 0\\)\n\\(\\star Cov(\\epsilon, \\epsilon) = I_n \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-6",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-6",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(6)",
    "text": "(6)\n\\(\\mathbf{e^\\top y} = SSE\\)\nanswer\n\\(\\sum^{n}_{j=1} e_j y_j = \\bf{e^\\top y} = \\{ (I - H)y\\}^\\top y = y^\\top (I-H)y\\)\n\\(\\star \\bf{e}^\\top (e_1, \\dots , e_n) = (y-\\hat{y})^\\top\\)\n\\(\\star \\bf{y}^\\top = (y_1 , \\dots , y_n)\\)\n\\(SSE = \\sum^n_{j=1} (y_i - \\hat{y}_j)^2 = (\\bf{y} - \\hat{y} ) ^\\top ( y - \\hat{y} ) = e^\\top e = \\{ (I - H)y \\}^\\top \\{ (I - H)y \\} = y^\\top (I - H) (I-H)y = y^\\top (I - H)y\\)\n\\(\\star I - H_H+H^2 = I-H, H^2 = H\\)\n\\(\\therefore \\sum^{n}_{j=1} e_j y_j = SSE\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-7",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-7",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(7)",
    "text": "(7)\n\\(\\mathbf{e^\\top \\hat{y}}=0\\)\nanswer\n\\(\\sum^{n}_{j=1} \\bf{e_j \\hat{y}_j} = e^\\top \\hat{y} = y^\\top (I-H) Hy = y^\\top_{1\\times n} \\mathbb{O}_{n\\times n} y_{n \\times 1} = 0\\)\n\\(\\star H - H^2 = H - H = \\mathbb{O}_{n \\times n}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-8",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-8",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(8)",
    "text": "(8)\n\\(E(\\frac{SSE}{n-p-1}) = \\sigma^2\\)\nanswer\n정리 5.1\n\\(y \\sim N(\\mu,V)\\) 이면\n\\(E(y^\\top A y) = tr(AV) + \\mu^\\top A \\mu, cov(y,y^\\top A y) = 2VA\\mu\\)\n\\(\\frac{SSE}{\\sigma^2} = y^\\top \\frac{1}{\\sigma^2}(I-H)y\\)\n\\(B = \\frac{1}{\\sigma^2}(I-H)\\)\n\\(y \\sim N(X\\beta, I \\sigma^2)\\)\n\\(BV = \\frac{1}{\\sigma^2}(I-H)I\\sigma^2 = B, BV=B\\)\n\\(B, BV\\)는 멱등행렬\n\\(tr(BV) = tr(B) = tr(I-H) = tr(I) - tr(H) = n-p-1\\)\n\\(\\star\\) \\(X\\)가 \\(n \\times (p+1)\\) 행렬이고, \\(rank\\)가 \\(p+1\\), \\(\\therefore tr(H) = p+1\\)\n\\(\\mu = X B\\)\n\\(B = \\frac{1}{\\sigma^2}(I-H)\\)\n\\(B\\mu = \\frac{1}{\\sigma^2}(I-H) X \\beta = \\frac{1}{\\sigma^2}(X B - HXB) = 0\\)\n\\(\\star HX = X\\)\n\\(\\therefore \\mu^\\top B \\mu = 0\\)\n\\(E(\\frac{SSE}{\\sigma^2}) = n-p-1\\)\n\\(E(SSE) = (n-p-1)\\sigma^2\\)\n\\(\\therefore E(\\frac{SSE}{(n-p-1)}) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-10",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-10",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(1)",
    "text": "(1)\n행렬을 이용하여 중회귀모형을 정의하여라(\\(X,\\mathbf{y,\\beta,\\epsilon}\\)을 정확하게 표현)\nanswer\n\\(\\mathbf{y = X\\beta + \\epsilon, \\epsilon} \\sim N(X\\mathbf{\\beta}, I_n\\sigma^2)\\)\n\\(\\mathbf{y} = \\begin{pmatrix} 1 \\\\5\\\\0\\\\4\\\\4\\\\-1 \\end{pmatrix}, X = \\begin{pmatrix} 1&1&1\\\\1&2&1\\\\1&1&2\\\\1&3&1\\\\1&3&2\\\\1&3&3 \\end{pmatrix}, \\mathbf{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\end{pmatrix}, \\mathbf{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ \\epsilon_5 \\\\ \\epsilon_6\\end{pmatrix}\\)\n\n위에서 구한 \\(X,\\mathbf{y}\\)에 대하여 아래의 결과를 얻었을 때, 다음 물음에 답하여라.\n\n\\[(X^\\top X)^{-1} = \\begin{pmatrix} 1.52 & -0.35 & -0.36 \\\\ -0.35 & 0.23 & -0.09 \\\\ -0.36 & -0.09 & 0.34 \\end{pmatrix}, X^\\top \\mathbf{y} = \\begin{pmatrix} 13 \\\\ 32 \\\\ 15 \\end{pmatrix}, \\mathbf{y^\\top y} = 59\\]"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-11",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-11",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(2)",
    "text": "(2)\n\\(\\beta_0, \\beta_1, \\beta_2\\)를 추정하시오.\nanswer\n\\(\\mathbf{\\hat{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y} = \\begin{pmatrix} 3.16 \\\\ 1.46 \\\\ -2.46 \\end{pmatrix}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-12",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-12",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(3)",
    "text": "(3)\n분산분석표를 작성하고, 유의수준 \\(\\alpha=0.1\\)에서 회귀직선에 대한 유의성 검정을 수행하시오.\nanswer\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n\n\n\n\n회귀\n23.0769\n2\n11.5385\n4.4628\n\n\n잔차\n7.7564\n3\n2.5855\n\n\n\n계\n30.8333\n5\n\n\n\n\n\n\\(SST = \\mathbf{y^\\top y} - \\frac{1}{n}(\\sum y_i)^2 = 59 - 28.1667 = 30.8333\\)\n\\(SSR = SST - SSE\\)\n- \\(H_0: \\beta_1 = \\beta_2 = 0 \\text{ vs. } H_1 : \\text{ not } H_0\\)\n- 기각역: \\(F_0 \\ge F_{0.1}(2,3) = 5.46\\)\n- 결론: 기각역에 속하지 않으므로 \\(H_0\\) 기각 못함. 즉, 회귀모형은 유의수준 0.1에서 유의하지 않다."
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-13",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-13",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(4)",
    "text": "(4)\n결정계수(\\(R^2\\))와 수정된 결정계수(\\(R^2_{sdj}\\)) 구하시오.\nanswer\n\\(R^2 = \\frac{SSR}{SST} = 0.7484, R^2_{adj} = 1-\\frac{SSE/(n-p-1)}{SST/(n-1)} = 0.5807\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-14",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-14",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(5)",
    "text": "(5)\n오차항의 분산 \\(\\sigma^2\\)에 대한 추정치를 구하시오.\nanswer\n\\(\\hat{\\sigma}^2 = MSE = \\frac{SSE}{n-p-1} = 2.5855\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-15",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-15",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(6)",
    "text": "(6)\n\\(\\hat{\\beta}_2\\)의 의미를 설명하고, 가설 \\(H_0: \\beta_2 = 0, H_1 : \\beta_2 <0\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\nanswer\n\\(X_1\\)이 일정할 떄, \\(X_2\\)가 1단위 증가하면 \\(y\\)는 \\(\\hat{\\beta}_2\\)만큼 증가한다.\n가설검정\n- \\(Var(\\hat{\\beta}_2) = (X^\\top X)^{-1}_{(3,3)} \\sigma^2 = 0.34 \\sigma^2\\)\n- \\(\\hat{s.e.}(\\hat{\\beta}_2) = \\sqrt{0.34 \\times MSE } = \\sqrt{0.34 \\times 2.5855} = 0.9376\\)\n- \\(t_0 = \\frac{-2.46}{0.9376} = -2.62372\\)\n- 기각역: \\(t_0 < -t_{0.05}(3) = -2.35\\)\n- 결론: \\(H_0\\)기각 가능, 즉 0보다 작다고 할 수 있다.\n\nround(qt(0.05,3),2)\n\n-2.35"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-16",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-16",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(7)",
    "text": "(7)\n\\(\\beta_1\\)의 \\(90\\)% 신뢰구간을 구하시오.\nanswer\n\\(Var(\\hat{\\beta}_1) = (X^\\top X)^{-1}_{(2,2)} \\sigma^2 = 0.23 \\sigma^2\\)\n\\(\\hat{s.e.}(\\hat{\\beta}_1) = \\sqrt{0.23 \\times 2.5855} = 0.7711\\)\n\\(t_{0.5}(3) = 2.35\\)\n\\(\\hat{\\beta}_1 \\pm t_{0.05}(3)\\hat{s.e.}(\\hat{\\beta}_1) = (-0.3521,3.2721)\\)\n\nround(qt(0.95,3),2)\n\n2.35"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-17",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-17",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(8)",
    "text": "(8)\n\\(x_1 = x_2 = 2\\)에서 \\(E(y)\\)의 \\(95\\)% 신뢰구간을 구하시오.\nanswer\n\\(x_0 = \\begin{pmatrix} 1 \\\\ 2\\\\2\\end{pmatrix}\\)\n\\(\\hat{\\mu}_0 = x^\\top_0 \\mathbf{\\hat{\\beta}} = \\begin{pmatrix} 1 &2&2\\end{pmatrix}\\begin{pmatrix}3.16\\\\1.46\\\\-2.46\\end{pmatrix} = 1.16\\)\n\\(Var(\\hat{\\mu}_0) = x_0^\\top(X^\\top X)^{-1} x_0 \\sigma^2 = 0.24 \\sigma^2\\)\n\\(\\star x_0^\\top(X^\\top X)^{-1} x_0 = \\begin{pmatrix}1&2&2\\end{pmatrix}\\begin{pmatrix}1.52&-0.35&-0.36\\\\-0.35&0.23&-0.09\\\\-0.36&-0.09&0.34\\end{pmatrix} \\begin{pmatrix}1\\\\2\\\\2 \\end{pmatrix} = 0.24\\)\n\\(\\hat{s.e.}(\\hat{\\mu}_0) = \\sqrt{0.24 \\times 2.5855} = 0.7877\\)\n\\(1.16 \\pm t_{0.025}(3) \\hat{s.e.}(\\hat{\\mu}_0) = (-1.3449,3.6649)\\)\n\nround(qt(0.975,3),2)\n\n3.18"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-19",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-19",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(1)",
    "text": "(1)\n혈압을 예측하기 위하여 교호작용을 포함하는 중회귀모형을 정의하시오.(성별의 경우 남자=1)\nanswer\n\\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3x_{i1}x_{i2} + \\epsilon_i, \\epsilon_i, i=1,\\dots,10, \\epsilon_i \\sim_{i.i.d.}N(0,\\sigma^2)\\)\n\\(x_1\\) : 체중, \\(y\\): 혈압\n\\(x_2\\): 성별, \\(x_2 = \\begin{cases}1 & \\text{남자}\\\\0 & \\text{여자}\\end{cases}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-20",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-20",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(2)",
    "text": "(2)\n위의 모형을 행렬 형태로 표현하시오.(\\(X,\\mathbf{y,\\beta,\\epsilon}\\)을 정확하게 표현)\nanswer\n\\(\\mathbf{y} = X\\mathbf{\\beta + \\epsilon}, \\mathbf{\\epsilon} \\sim N(0,I_n\\sigma^2)\\)\n\\(\\mathbf{y} = \\begin{pmatrix}120\\\\130\\\\130\\\\155\\\\149\\\\150\\\\160\\\\125\\\\135\\\\140\\end{pmatrix}, X = \\begin{pmatrix}1& 70&1&70\\\\1&60&0&0\\\\1&88&1&88\\\\1&101&0&0\\\\1&80&0&0\\\\1&98&1&98\\\\1&90&0&0\\\\1&77&1&77\\\\1&65&0&0\\\\1&70&0&0 \\end{pmatrix}, \\mathbf{\\beta} = \\begin{pmatrix}\\beta_0\\\\ \\beta_1 \\\\ \\beta_2 \\end{pmatrix}, \\mathbf{\\epsilon} = \\begin{pmatrix}\\epsilon_1 \\\\ \\epsilon_2\\\\ \\epsilon_3\\\\ \\epsilon_4\\\\ \\epsilon_5\\\\ \\epsilon_6\\\\ \\epsilon_7\\\\ \\epsilon_8\\\\ \\epsilon_9\\\\ \\epsilon_{10} \\end{pmatrix}\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-21",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-21",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(3)",
    "text": "(3)\n위 모형의 적합 결과가 다음과 같다고 하자. 빈 칸을 채워 넣으시오. 마지막 칸에는 개별회귀계수에 대한 유의성검정을 위한 가설과, 검정결과(기각, 기각못함)를 적으시오.(풀이 있어야 함) 그리고 분산분석표를 이용하여 회귀모형의 유의성 검정을 하시오.(유의수준 \\(\\alpha = 0.05\\))\nanswer\n\\[\\text{추정}\\]\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd.Error\nt value\n검정결과\n\n\n\n\n절편\\(\\beta_0\\)\n\\(\\hat{\\beta}_0\\)=70.681\n10.779\n6.557\n기각\n\n\n체중\\(\\beta_1\\)\n\\(\\hat{\\beta}_1\\)=0.988\n0.1461\n6.763\n기각\n\n\n성별\\(\\beta_2\\)\n\\(\\hat{\\beta}_2\\)=-32.435\n15.830\n-2.049\n기각못함\n\n\n교호작용\\(\\beta_3\\)\n\\(\\hat{\\beta}_3\\)=0.138\n0.197\n0.702\n기각못함\n\n\n\n\\(t_0 = \\frac{\\hat{\\beta}_i}{\\hat{s.e.}(\\hat{\\beta}_i)}\\), 기각역 \\(|t_0| > t_{\\alpha/2}(n-p-1) = t_{0.025}(6) = 2.45\\)\n\\(H_0: \\beta_i = 0, H_1: \\beta_i \\neq 0\\)\n\nround(qt(0.975,6),2)\n\n2.45\n\n\n\\[\\text{분산분석표}\\]\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n\n\n\n\n회귀\n1578.14\n3\n526.0467\n42.5031\n\n\n잔차\n74.26\n6\n12.3767\n\n\n\n계\n1652.4\n9\n\n\n\n\n\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3 = 0 \\text{ vs. } H_1 \\text{ not } H_0\\)\n\\(F_0 > F_{\\alpha}(p,n-p-1) = F_{0.05}(3,6) = 4.76\\)\n이므로 \\(H_0\\)기각 가능, 즉 유의수준 0.05에서 회귀직선 유의"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-22",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-22",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(4)",
    "text": "(4)\n(3)의 결과를 보고, 결정계수(\\(R^2\\)) 와 수정된 결정계수(\\(R^2_{sdj}\\)) 구하시오.\nanswer\n\\(R^2 = \\frac{SSR}{SST} = 0.9551, R^2_{adj} = 1-\\frac{SSE/(n-p-1)}{SST/(n-1)} = 0.9326\\)\n\nround(1578.14/1652.4,4)\n\n0.9551\n\n\n\nround(1-(74.26/6)/(1652.4/9),4)\n\n0.9326"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-23",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-23",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(5)",
    "text": "(5)\n(3)번의 결과를 보고 모형 적합 결과를 설명하여라.(각 회귀계수의 의미와 유의성 검정 결과)\nanswer\n\\(E(y|\\text{남성}) = \\beta_0 + \\beta_1 x_1+\\beta_2+\\beta_3x_1 = (\\beta_0+\\beta_2) + (\\beta_1 + \\beta_3)x_1\\)\n\\(E(y|\\text{여성}) = \\beta_0 + \\beta_1x_1\\)\n\\(\\hat{\\beta}_2\\): 유의하지 않음: 남성과 여성의 절편 차이 없음\n\\(\\hat{\\beta}_3\\): 유의하지 않음: 남성과 여성의 기울기 차이 없음\nanswer"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-24",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-24",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(6)",
    "text": "(6)\n이번에는 교호작용을 제외한 모형을 적합해 보았다. 모형을 정의하여라.\nanswer\n\\(y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i , i=1,\\dots, 10, \\epsilon_i \\sim N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "posts/rl/2023-02-23-rl-final_term.html#section-25",
    "href": "posts/rl/2023-02-23-rl-final_term.html#section-25",
    "title": "Advanced Regression Analysis Final Term",
    "section": "(7)",
    "text": "(7)\n(7)번의 결과가 다음과 같을 때, 빈 칸을 채우고 적합 결과에 대해 설명하여라. (3)번 참고.\nanswer\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd.Error\nt value\n검정결과\n\n\n\n\n절편\n65.137\n7.059\n9.227\n기각\n\n\n체중\n1.064\n0.094\n11.264\n기각\n\n\n성별\n-21.482\n2.508\n-8.565\n기각\n\n\n\n\\(H_0: \\beta_i = 0 \\text{ vs. } H_1 : \\beta_i \\neq 0, i=1,2,3\\)\n기각역: \\(|t_0| > t_{\\alpha/2}(n-p-1) = t_{0.025}(7) = 2.36\\)\n\\(E(y|\\text{남성}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 = \\beta_0 + \\beta_2 + \\beta_1 x_1\\)\n\\(E(y|\\text{여성}) = \\beta_0 + \\beta_1x_1\\)\n\\(H_0:\\beta_2\\neq 0\\)을 기각할 수 있으므로, 성별에 따른 평균 혈압이 다르다고 할 수 있다.\n\nround(qt(0.975,7),2)\n\n2.36\n\n\n\ny <- c(120,130,130,155,149,150,160,125,135,140) # 혈압\nx1 <- c(70,60,88,101,80,98,90,77,65,70) # 체중\nx2 <- c(1,0,1,0,0,1,0,1,0,0) # 성별, 남, 여, 남, 여, 여, 남, 여, 남, 여, 여\ndf <- data.frame(y,x1,x2)\nanova(lm(y~x1+x2,df))\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11729.9696729.9695727.921850.001142928\n    x21739.4272739.4272428.283610.001101377\n    Residuals7183.0032 26.14331      NA         NA"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_CH03, CH04.html",
    "href": "posts/rl/2022-09-21-rl_CH03, CH04.html",
    "title": "고급회귀분석 실습 CH03, CH04",
    "section": "",
    "text": "chapter 3, chapter 4"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_CH03, CH04.html#data",
    "href": "posts/rl/2022-09-21-rl_CH03, CH04.html#data",
    "title": "고급회귀분석 실습 CH03, CH04",
    "section": "Data",
    "text": "Data\n\ndt <- data.frame(x = c(4,8,9,8,8,12,6,10,6,9),\n                 y = c(9,20,22,15,17,30,18,25,10,20))\ndt\n\n\n\nA data.frame: 10 × 2\n\n    xy\n    <dbl><dbl>\n\n\n     4 9\n     820\n     922\n     815\n     817\n    1230\n     618\n    1025\n     610\n     920\n\n\n\n\ncorrelation check\n\ncor(dt$x, dt$y)\n\n0.921812343945765\n\n\n산점도 확인\n\nplot(y~x, \n     data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 16,\n     cex  = 2,\n     col  = \"darkorange\")\n\n\n\n\n\npch 점 모양\ncex 점 크기\n양의상관관계 강하네,\n우상향이네, 단순상관선형 적용해보면 되겠다.\n\n\nggplot(dt, aes(x, y)) +\n  geom_point(col='steelblue', lwd=2) +\n  # geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1.2) +\n  xlab(\"광고료\")+ylab(\"총판매액\")+\n  # scale_x_continuous(breaks = seq(1,10))+\n  theme_bw() +\n  theme(axis.title = element_text(size = 14))\n\n\n\n\n\n적합\n\\(\\hat{ y} = \\widehat{(E(y|X=x))} = \\hat{\\beta_0} + \\hat{\\beta_1} * x\\)\n\\(H_0\\) : \\(\\beta_0\\) =0 vs \\(H_1\\) : \\(\\beta_0 \\ne 0\\)\n\\(H_0\\) : \\(\\beta_1 =0\\) vs \\(H_1\\) : \\(\\beta_1 \\ne 0\\)\n모형 적합을 한다 yhat을 찾는다. - 회귀분석을 한다. 평균 반응을 추정한다.\nlm linear model 사용\n\n## y = beta0 + beta1*x + epsilon\nmodel1 <- lm(y ~ x, dt)\n# lm(y ~ 0 + x, dt) beta0 없이 분석하고 싶을때\nmodel1\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nCoefficients:\n(Intercept)            x  \n     -2.270        2.609  \n\n\n설명변수 x 하나일때\n\nbeta0hat = -2.270\nbeta1hat = 2.609\n\n\nsummary(model1) \n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.600 -1.502  0.813  1.128  4.617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.2696     3.2123  -0.707 0.499926    \nx             2.6087     0.3878   6.726 0.000149 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.631 on 8 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.831 \nF-statistic: 45.24 on 1 and 8 DF,  p-value: 0.0001487\n\n\n\n모형의 유의성 검정 자체(f검정)\n개별 회귀계수에 대한 유의성검정(t검정)\nbeta1은 유의하지 않다\nbeta0는 유의하다\nf통계량은 45.24(msr/mse) p값 충분히 작아서 모형은 유의하다.\ny의 총 변동 중에 85%정도를 설명하고 있다.\nroot mse(RMSE) = 2.631\n\n\n6.726**2\n\n45.239076\n\n\n단순선형에서만 해당\n0.000149도 같음\n\nnames(model1)\n\n\n'coefficients''residuals''effects''rank''fitted.values''assign''qr''df.residual''xlevels''call''terms''model'\n\n\n\nmodel1$residuals # 보고 싶은 변수 입력해봐~\n\n10.83478260869565621.430.7913043478260874-3.65-1.660.9652173913043574.6173913043478281.182608695652179-3.3826086956521810-1.20869565217391\n\n\n\nmodel1$fitted.values  ##hat y\nmodel1$coefficients\n\n18.16521739130434218.6321.2086956521739418.6518.6629.0347826086957713.3826086956522823.8173913043478913.38260869565221021.2086956521739\n\n\n(Intercept)-2.2695652173913x2.60869565217391\n\n\n\nanova(model1)  ## 회귀모형의 유의성 검정\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1313.04348313.04347845.240340.0001486582\n    Residuals8 55.35652  6.919565      NA          NA\n\n\n\n\n\n설명변수의 개수가 x 자유도\n잔차의 자유도는 n-2\n\n\na <- summary(model1)\nls(a)\n\n\n'adj.r.squared''aliased''call''coefficients''cov.unscaled''df''fstatistic''r.squared''residuals''sigma''terms'\n\n\n\nsummary(model1)$coef   ## 회귀계수의 유의성 검정\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-2.2695653.212348-0.70651290.4999255886\n    x 2.6086960.387847 6.72609390.0001486582\n\n\n\n\n\nconfint(model1, level = 0.95)  ##회귀계수의 신뢰구간\n## beta +- t_alpha/2 (n-2) * se(beta)\nqt(0.025, 8)\nqt(0.975, 8)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-9.6772525.138122\n    x 1.7143193.503073\n\n\n\n\n-2.30600413520417\n\n\n2.30600413520417\n\n\n\nqt _ tquantile\n\n\n## y = beta1*x + epsilon\nmodel2 <- lm(y ~ 0 + x, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ 0 + x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0641 -1.5882  0.2638  1.4818  3.9359 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   2.3440     0.0976   24.02  1.8e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.556 on 9 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9829 \nF-statistic: 576.8 on 1 and 9 DF,  p-value: 1.798e-09\n\n\n\nintercept 없는 모습\nr squre가 두 번째가 높고,\np값도 훨씬 유의하게 나옴\n\n\nanova(model1)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1313.04348313.04347845.240340.0001486582\n    Residuals8 55.35652  6.919565      NA          NA\n\n\n\n\n\nanova(model2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x13769.18953769.1895576.81381.79763e-09\n    Residuals9  58.8105   6.5345      NA         NA\n\n\n\n\n\n###########\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"darkorange\")\nabline(model1, col='steelblue', lwd=2)\nabline(model2, col='green', lwd=2)\n\n\n\n\nmodel들이 기울기가 살짝씩 다르다\n\nco <- coef(model1)\n\n\nggplot(dt, aes(x, y)) +\n  geom_point(col='steelblue', lwd=1) +\n  geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1) +\n  xlab(\"광고료\")+ylab(\"총판매액\")+\n  theme_bw()+\n  theme(axis.title = element_text(size = 16))\n\n\n\n\n\n######## LSE 구하기\n# lm을 사용하지 않고 구할때\n\ndt1 <- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x), # x - x평균\n  y_bary = dt$y - mean(dt$y))  # y - y평균\n\n\ndt1$x_barx2 <- dt1$x_barx^2 # x 편차의 제곱\ndt1$y_bary2 <- dt1$y_bary^2 # y편차의 제곱\ndt1$xy <-dt1$x_barx * dt1$y_bary\n\n\ndt1\n\n\n\nA data.frame: 10 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2xy\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     1 4 9-4-9.616 92.1638.4\n     2 820 0 1.4 0  1.96 0.0\n     3 922 1 3.4 1 11.56 3.4\n     4 815 0-3.6 0 12.96 0.0\n     5 817 0-1.6 0  2.56 0.0\n     61230 411.416129.9645.6\n     7 618-2-0.6 4  0.36 1.2\n     81025 2 6.4 4 40.9612.8\n     9 610-2-8.6 4 73.9617.2\n    10 920 1 1.4 1  1.96 1.4\n\n\n\n\n\nround(colSums(dt1),3)\n\ni55x80y186x_barx0y_bary0x_barx246y_bary2368.4xy120\n\n\n\n### hat beta1 = S_xy / S_xx\n##hat beta0 = bar y - hat beta_1 * bar x\nbeta1 <- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\nbeta0 <- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\ncat(\"hat beta1 = \", beta1)\n\nhat beta0 =  -2.269565hat beta1 =  2.608696\n\n\n\n\n평균반응, 개별 y 추정\n구분할 수 있어야 한다\n신뢰구간 달라진다.\n\n## E(Y|x0) 평균반응\n## y = E(Y|x0) + epsilon 개별 y 추정\n# x0 = 4.5\nnew_dt <- data.frame(x = 4.5)\n\n\n# hat y0 = hat beta0 + hat beta1 * 4.5\n\npredict(model1, \n        newdata = new_dt,\n        interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695655.7982613.14087\n\n\n\n\nnew_data=new_df 정의 안 하면 fitted value가 나온다.\nconfidence는 평균반응\n\npredict(model1, newdata = new_dt, \n        interval = c(\"prediction\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695652.37912516.56001\n\n\n\n\nprediction은 개별 y 추정\n신뢰구간이 커진다. \\(\\to\\) 표준오차가 달라지기 때문\n\ndt_pred <- data.frame(\n  x = 1:12,\n  predict(model1, \n          newdata=data.frame(x=1:12), \n          interval=\"confidence\", level = 0.95))\ndt_pred\n\n\n\nA data.frame: 12 × 4\n\n    xfitlwrupr\n    <int><dbl><dbl><dbl>\n\n\n    1 1 0.3391304-6.2087835 6.887044\n    2 2 2.9478261-2.7509762 8.646628\n    3 3 5.5565217 0.690585410.422458\n    4 4 8.1652174 4.105889112.224546\n    5 510.7739130 7.475614014.072212\n    6 613.382608710.759780816.005437\n    7 715.991304313.874822318.107786\n    8 818.600000016.681775320.518225\n    9 921.208695719.092213623.325178\n    101023.817391321.194563426.440219\n    111126.426087023.127788029.724386\n    121229.034782624.975454333.094111\n\n\n\n\n\ndt_pred2 <- as.data.frame(predict(model1, \n                                  newdata=data.frame(x=1:12), \n                                  interval=\"prediction\", level = 0.95))\ndt_pred2\n\n\n\nA data.frame: 12 × 3\n\n    fitlwrupr\n    <dbl><dbl><dbl>\n\n\n    1 0.3391304-8.5867330 9.264994\n    2 2.9478261-5.375166611.270819\n    3 5.5565217-2.219929713.332973\n    4 8.1652174 0.866312815.464122\n    510.7739130 3.869230817.678595\n    613.3826087 6.773895719.991322\n    715.9913043 9.566714322.415894\n    818.600000012.237968324.962032\n    921.208695714.784105627.633286\n    1023.817391317.208678330.426104\n    1126.426087019.521404733.330769\n    1229.034782621.735878136.333687\n\n\n\n\n\nnames(dt_pred2)[2:3] <- c('plwr', 'pupr')\n\nplot 같이 그리게 데이터 합치기\n\ndt_pred3 <- cbind.data.frame(dt_pred, dt_pred2[,2:3])\n\n\nbarx <- mean(dt$x)\nbary <- mean(dt$y)\n\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"grey\",\n     ylim = c(min(dt_pred3$plwr), max(dt_pred3$pupr)))\nabline(model1, lwd = 5, col = \"darkorange\")\nlines(dt_pred3$x, dt_pred3$lwr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred3$x, dt_pred3$upr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred3$x, dt_pred3$plwr, col = \"dodgerblue\", lwd = 3, lty = 3)\nlines(dt_pred3$x, dt_pred3$pupr, col = \"dodgerblue\", lwd = 3, lty = 3)\n\nabline(h=bary,v=barx, lty=2, lwd=0.2, col='dark grey')\n\n\n\n\n\nggplot(dt_pred3, aes(x, fit)) +\n  geom_line(col='steelblue', lwd=2) +\n  xlab(\"\")+ylab(\"\")+\n  scale_x_continuous(breaks = seq(1,10))+\n  geom_line(aes(x, lwr), lty=2, lwd=1.5, col='darkorange') +\n  geom_line(aes(x, upr), lty=2, lwd=1.5, col='darkorange') +\n  geom_line(aes(x, plwr), lty=2, lwd=1.5, col='dodgerblue') +\n  geom_line(aes(x, pupr), lty=2, lwd=1.5, col='dodgerblue') +\n  geom_vline(xintercept = barx, lty=2, lwd=0.2, col='dark grey')+\n  geom_hline(yintercept = bary, lty=2, lwd=0.2, col='dark grey')+\n  theme_bw()\n\n\n\n\n\nbb <- summary(model1)$sigma * ( 1 + 1/10 +(dt$x - 8)^2/46)\ndt$ma95y <- model1$fitted + 2.306*bb\ndt$mi95y <- model1$fitted - 2.306*bb\n\n\nggplot(dt, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  geom_line(aes(x, mi95y), col = 'darkgrey', lty=2) +\n  geom_line(aes(x, ma95y), col = 'darkgrey', lty=2) +\n  theme_bw() +\n  theme(axis.title = element_blank())\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n잔차분석\n\n### epsilon : 선형성, 등분산성, 정규성, 독립성\n\n\ndt\ndt$yhat <- model1$fitted\n# fitted.values(model1) # y에 대한 추정값 구하기\ndt$resid <- model1$residuals\n# resid(model1)\n\n\n\nA data.frame: 10 × 4\n\n    xyma95ymi95y\n    <dbl><dbl><dbl><dbl>\n\n\n     4 916.94766-0.6172208\n     82025.2725411.9274568\n     92228.0131114.4042841\n     81525.2725411.9274568\n     81725.2725411.9274568\n    123037.8172220.2523444\n     61820.58263 6.1825918\n    102531.0174116.6173744\n     61020.58263 6.1825918\n     92028.0131114.4042841\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(resid ~ x, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\npar(mfrow=c(1,1))\n\n\n\n\n단순선형에서는 두 plot의 차이가 없다.\n\n선형성 만족\n등분산성 나름 만족\n정규성 아웃라이어 있는 거 같은데..\n독립성?\n\n\n# 독립성검정 : DW test\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\n## \ndwtest(model1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n# dwtest(model1, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho > 0\n# dwtest(model1, alternative = \"less\")  #H0 : uncorrelated vs H1 : rho < 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.3916\nalternative hypothesis: true autocorrelation is not 0\n\n\np 값 커서 기각할 수 없다.\n첫 번째꺼 주로 보기\n\n## 정규분포 (QQ plot)\nqqnorm(dt$resid, pch=16)\nqqline(dt$resid, col = 2)\n\n\n\n\n분위수분위수 그림 - 정규분포의 실제 - 어떤 분포의 이론적 분위수와 내가 가진 sample의 분위수 비교\n주로 꼬리쪽을 많이 본다. - 이 데이터의 경우 꼬리부분이 차이가 커 보임\n\nggplot(dt, aes(sample = resid)) + \n  stat_qq() + stat_qq_line() +\n  theme_bw()\n\n\n\n\n\n## 정규분포 검정 \nshapiro.test(dt$resid)  ##shapiro-wilk test\n#H0 : normal distributed vs H1 : not\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt$resid\nW = 0.92426, p-value = 0.3939\n\n\np값 작게 나오면 정규분포라고 하기 어렵다.\n\n정규성은 잔차를 넣어줬는데\nbptest는 model을 넣었다.\n\n\n## 등분산성 검정 \nbptest(model1) #Breusch–Pagan test\n# H0 : 등분산 vs H1 : 이분산 \n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 1.6727, df = 1, p-value = 0.1959\n\n\n\n\n책 예제\n\n# install.packages('UsingR')\nlibrary(UsingR)\n\nLoading required package: MASS\n\nLoading required package: HistData\n\nLoading required package: Hmisc\n\nLoading required package: lattice\n\nLoading required package: survival\n\nLoading required package: Formula\n\n\nAttaching package: ‘Hmisc’\n\n\nThe following objects are masked from ‘package:base’:\n\n    format.pval, units\n\n\n\nAttaching package: ‘UsingR’\n\n\nThe following object is masked from ‘package:survival’:\n\n    cancer\n\n\n\n\n\ndata(father.son)\n\n\nnames(father.son)\n\n\n'fheight''sheight'\n\n\n\nlm.fit<-lm(sheight~fheight, data=father.son)\n\n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = sheight ~ fheight, data = father.son)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8772 -1.5144 -0.0079  1.6285  8.9685 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.88660    1.83235   18.49   <2e-16 ***\nfheight      0.51409    0.02705   19.01   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.437 on 1076 degrees of freedom\nMultiple R-squared:  0.2513,    Adjusted R-squared:  0.2506 \nF-statistic: 361.2 on 1 and 1076 DF,  p-value: < 2.2e-16\n\n\n아버지의 키가 아들의 키의 25%만 설명\n\nplot(sheight~fheight, \n     data=father.son, \n     pch=16, cex=0.5,\n     xlab=\"father’s height (inches)\", \n     ylab=\"son’s height (inches)\")\nabline(lm.fit)\n\n\n\n\n\n\namazon<-read.csv(\"amazon.csv\")\nplot(High   ~Year  , amazon, pch=16)\n\nlm.fit<-lm(High~Year, data=amazon)\nsummary(lm.fit)\n\nconfint(lm.fit)\n\npar(mfrow=c(1,2))\nscatter.smooth(x=1:length(amazon$Year), y=residuals(lm.fit), xlab=\"Year\")\nscatter.smooth(x=predict(lm.fit), y=residuals(lm.fit), xlab=expression(hat(y)))\n\nlibrary(lmtest)\ndwtest(lm.fit)\n\n\nCall:\nlm(formula = High ~ Year, data = amazon)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3629 -0.5341  0.1479  0.4903  1.1412 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -330.21235   78.03319  -4.232 0.000725 ***\nYear           0.18088    0.03961   4.567 0.000371 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8001 on 15 degrees of freedom\nMultiple R-squared:  0.5816,    Adjusted R-squared:  0.5537 \nF-statistic: 20.85 on 1 and 15 DF,  p-value: 0.0003708\n\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-496.53615985-163.8885460\n    Year   0.09645429   0.2653104\n\n\n\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  lm.fit\nDW = 1.0487, p-value = 0.006864\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n양의 상관관계가 있다. - 시간순 index - 최근 관측 데이터에 영향 많이 받는 편"
  },
  {
    "objectID": "posts/rl/2022-11-14-rl_CH06, CH07.html",
    "href": "posts/rl/2022-11-14-rl_CH06, CH07.html",
    "title": "고급회귀분석 실습 CH06, CH07",
    "section": "",
    "text": "chapter 6, chapter 7\n\nlibrary(MASS)\n\n\ndata(Boston)\n\n\nhead(Boston)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00632182.3100.5386.57565.24.0900129615.3396.904.9824.0\n    20.02731 07.0700.4696.42178.94.9671224217.8396.909.1421.6\n    30.02729 07.0700.4697.18561.14.9671224217.8392.834.0334.7\n    40.03237 02.1800.4586.99845.86.0622322218.7394.632.9433.4\n    50.06905 02.1800.4587.14754.26.0622322218.7396.905.3336.2\n    60.02985 02.1800.4586.43058.76.0622322218.7394.125.2128.7\n\n\n\n\n\nnrow(Boston)\n\n506\n\n\n보스턴 집값 데이터 이 데이터는 보스턴 근교 지역의 집값 및 다른 정보를 포함한다.\nMASS 패키지를 설치하면 데이터를 로딩할 수 있다.\nB보스턴 근교 506개 지역에 대한 범죄율 (crim)등 14개의 변수로 구성\n• crim : 범죄율\n• zn: 25,000평방비트 기준 거지주 비율\n• indus: 비소매업종 점유 구역 비율\n• chas: 찰스강 인접 여부 (1=인접, 0=비인접)\n• nox: 일산화질소 농도 (천만개 당)\n• rm: 거주지의 평균 방 갯수 ***\n• age: 1940년 이전에 건축된 주택의 비율\n• dis: 보스턴 5대 사업지구와의 거리\n• rad: 고속도로 진입용이성 정도\n• tax: 재산세율 (10,000달러 당)\n• ptratio: 학생 대 교사 비율\n• black: 1000(B − 0.63)2, B: 아프리카계 미국인 비율\n• lstat : 저소득층 비율 ****\n• medv: 주택가격의 중앙값 (단위:1,000달러 당)\n\n데이터 세 변수만 가지고 산점도 그려보기\n음의 상관관계가 있는 것 같기도 하고~\n\n\npairs(Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))], pch=16, col='darkorange')\n\n\n\n\n\nlm(y ~ x_1 + x_2, data)\n\n\nfit_Boston<-lm(medv~rm+lstat, data=Boston)\n\n\nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\nR-squared:  0.6386\n\n집값 y의 총변동 중에 회귀모형이 63.86%정도 설명하고 있다.\n\nAdjusted R-squared:  0.6371\n\n설명변수가 증가하면 R-squared값이 증가하는 현상때문에 수정된 R-squared값이 필요\n모형 비교시 Adjusted R-squared가 나을 수도 있음\n\nrm,lstat의 p-value <2e-16\n\n\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n원하는 변수만 추출해내서 회귀분석 진행가능\n\n\ndt <- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    rmlstatmedv\n    <dbl><dbl><dbl>\n\n\n    16.5754.9824.0\n    26.4219.1421.6\n    37.1854.0334.7\n    46.9982.9433.4\n    57.1475.3336.2\n    66.4305.2128.7\n\n\n\n\n\n. 쓰면 설명변수 다 쓰겠다는 의미\n\n\nfit_Boston<-lm(medv~., data=dt)\n\n\\(\\hat{y} = -1.3583 + 5.0948*rm - 0.6424*lstat\\)\n\n저소득층이 변하지 않고 방이 하나 더 증가한다면 집값의 중앙값은 5.0948 정도 증가한다.\n방의 수는 변하지 않고 저소득층이 증가한다면 집값의 중앙값은 0.6424 정도 감소한다.\nSSR = 41308.84 = 20654.42 + 20654.42\n밑의 F 통계량이 아니라 summary 의 F 통계량을 봐야 한다.\n\n\nanova(fit_Boston)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    rm  120654.4220654.41622672.90398.266887e-95\n    lstat  1 6622.57 6622.56999215.75796.669365e-41\n    Residuals50315439.31   30.69445      NA          NA\n\n\n\n\n\\(var(\\hat{\\beta}) = (X^TX)^{-1} \\sigma^2\\)\n\nvcov(fit_Boston)  \n\n\n\nA matrix: 3 × 3 of type dbl\n\n    (Intercept)rmlstat\n\n\n    (Intercept)10.06683612-1.39248641-0.099178133\n    rm-1.39248641 0.19754958 0.011930670\n    lstat-0.09917813 0.01193067 0.001912441\n\n\n\n\n\nconfint(fit_Boston, level = 0.95)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-7.5919003 4.8753547\n    rm 4.2215504 5.9680255\n    lstat-0.7282772-0.5564395\n\n\n\n\n\ncoef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n(Intercept)4.87535465808389rm5.96802553290791lstat-0.556439501179164\n\n\n\ncoef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n(Intercept)-7.59190028183298rm4.22155043576519lstat-0.728277167309095\n\n\n평균반응, 개별 y 추정\n\\(E(Y|x_0), y = E(Y|x_0) + \\epsilon\\)\n\nnew_dt <- data.frame(rm=7, lstat=10)\n\n\\(\\hat{y}_0 = -1.3583 + 5.0948*7 - 0.6424*10\\)\n\npredict(fit_Boston, newdata = new_dt)\n\n1: 27.88165973604\n\n\n평균반응\n\npredict(fit_Boston, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  \n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816627.1734728.58985\n\n\n\n\n개별 y\n\npredict(fit_Boston, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  \n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816616.9737538.78957\n\n\n\n\n\nfit은 같고 lwr,upr만 달라진다.\n\n잔차분석\n\\(\\epsilon\\) : 선형성, 등분산성, 정규성, 독립성\nyhat <- fitted_values(fit_Boston)\n\nyhat <- fitted(fit_Boston)\n\n\nres <- resid(fit_Boston)\n\n잔차그림 - 대칭 아니다 - u 턴 커브 모형이다. - 애초에 산점도에서도 커브가 보였다. - 나중에 제곱텀 넣어볼 예정\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n제곱텀 필요해보인다.\n\n\nplot(res ~ dt$rm,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n제곱텀 필요해보인다.\n\n\nplot(res ~ dt$lstat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n독립성검정 : DW test\n\nlibrary(lmtest)\n\n\\(H_0 : \\text{ uncorrelated vs } H_1 : \\rho \\neq 0\\)\n\ndwtest(fit_Boston, alternative = \"two.sided\")  \n\n\n    Durbin-Watson test\n\ndata:  fit_Boston\nDW = 0.83421, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\np-value < 2.2e-16로 귀무가설 기각한다. 잔차는 독립이 아니다.\n\n잔차의 QQ plot\n정규성 확인 결과도 좋지 않음\n\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\n\n\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    rmlstatmedv\n    <dbl><dbl><dbl>\n\n\n    16.5754.9824.0\n    26.4219.1421.6\n    37.1854.0334.7\n    46.9982.9433.4\n    57.1475.3336.2\n    66.4305.2128.7\n\n\n\n\n\ndt$lstat2 <- (dt$lstat)^2\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 4\n\n    rmlstatmedvlstat2\n    <dbl><dbl><dbl><dbl>\n\n\n    16.5754.9824.024.8004\n    26.4219.1421.683.5396\n    37.1854.0334.716.2409\n    46.9982.9433.4 8.6436\n    57.1475.3336.228.4089\n    66.4305.2128.727.1441\n\n\n\n\n1번 방법\n\nfit_Boston2<-lm(medv~., data=dt)\n\n# 이렇게 쓰면 위에와 결과 같음\nfit_Boston2<-lm(medv~rm+lstat+lstat^2, data=Boston)\n2번 방법 변수 생성 없이 수행가능\n\nfit_Boston2<-lm(medv~rm+lstat+I(lstat^2), data=Boston)\n\n\nAdjusted R-squared:  0.7013이 63%에서 증가한 모습\n\n\nsummary(fit_Boston2)\n\n\nCall:\nlm(formula = medv ~ rm + lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1427  -3.2429  -0.4829   2.3607  27.0256 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.68964    3.13810   3.725 0.000217 ***\nrm           4.22727    0.41172  10.267  < 2e-16 ***\nlstat       -1.84863    0.12213 -15.136  < 2e-16 ***\nI(lstat^2)   0.03634    0.00348  10.443  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.027 on 502 degrees of freedom\nMultiple R-squared:  0.7031,    Adjusted R-squared:  0.7013 \nF-statistic: 396.2 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\nyhat2 <- fitted.values(fit_Boston2)\n\n\nres2 <- resid(fit_Boston2)\n\n\n곡선 느낌은 사라지고 outlier는 보이고 있다.\n\n\nplot(res2 ~ yhat2,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n꼬리가 죽은 모습\n\n\nqqnorm(res2, pch=16)\nqqline(res2, col = 2)\n\n\n\n\n\\(H_0 : \\text{ uncorrelated vs } H_1 : \\rho \\neq 0\\)\n\ndwtest(fit_Boston2, alternative = \"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit_Boston2\nDW = 0.84831, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n여전히 잔차가 독립이 아니고 기각한다.\n\n\nfit_Boston3 <- lm(medv~rm, data=Boston)\n\n\nfit_Boston4 <- lm(medv~lstat, data=Boston)\n\n\nsummary(fit_Boston3)\n\n\nCall:\nlm(formula = medv ~ rm, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -34.671      2.650  -13.08   <2e-16 ***\nrm             9.102      0.419   21.72   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\nsummary(fit_Boston4)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\nx1<-c(4,8,9,8,8,12,6,10,6,9)\nx2<-c(4,10,8,5,10,15,8,13,5,12)\ny<-c(9,20,22,15,17,30,18,25,10,20)\n\nFM\n\nfit<-lm(y~x1+x2)\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4575 -1.9100  0.3314  0.6388  3.2628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.6507     2.9075  -0.224   0.8293  \nx1            1.5515     0.6462   2.401   0.0474 *\nx2            0.7599     0.3968   1.915   0.0970 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.278 on 7 degrees of freedom\nMultiple R-squared:  0.9014,    Adjusted R-squared:  0.8732 \nF-statistic:    32 on 2 and 7 DF,  p-value: 0.0003011\n\n\n\nanova(fit)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11313.04348313.04347860.3231030.0001100467\n    x21 19.03040 19.030400 3.6671350.0970444465\n    Residuals7 36.32612  5.189446       NA          NA\n\n\n\n\ninstall.packages(\"car\")\n\nlibrary(car)\n\n\\(H_0 : T\\times\\beta = c\\)\n$b_1-b_2=0 (0,1,-1) $\n\\(H_0 : \\beta_1 = \\beta_2\\)\n\nlinearHypothesis(fit, c(0,1,-1), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1839.53514NA      NA       NA       NA\n    2736.32612 13.2090140.61837310.4574425\n\n\n\n\n\n제약조건, T matrix c(0,1,-1)\nc = 0\n\n\n3.209014 / (36.32612/7)\n\n0.618373170600108\n\n\n같다~\n\\(H_0 : \\beta_1 = 1\\)\n\nlinearHypothesis(fit, c(0,1,0), 1)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1840.10492NA      NA       NA       NA\n    2736.32612 13.7787970.72816960.4217136\n\n\n\n\n\n기각하지 못함\n\n\\(H_0 : \\beta_1 = \\beta_2 + 1\\)\n\nlinearHypothesis(fit, c(0,1,-1), 1)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1836.54865NA       NA        NA      NA\n    2736.32612 10.22252730.042880740.841845\n\n\n\n\n\nF0\n\n0.0428807391894599\n\n\n\\(H_0 : \\beta_1 = \\beta_2 + 1\\)\n\\(y = b_0 + b_1x_1 + b_2x_2 + \\epsilon = b_0+x_1 + b_2(x_1+x_2)+\\epsilon\\)\n\\(y-x_1 = b_0+b_2(x_1+x_2)+\\epsilon\\) : RM\n\ny1 <- y-x1\nz1 <- x1 + x2\n\n\nfit2 <- lm(y1~z1)\n\n\nsummary(fit2)\n\n\nCall:\nlm(formula = y1 ~ z1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5054 -1.9294  0.4236  0.6821  3.4473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.0014     2.2175  -0.452 0.663574    \nz1            0.6824     0.1242   5.493 0.000578 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.137 on 8 degrees of freedom\nMultiple R-squared:  0.7904,    Adjusted R-squared:  0.7642 \nF-statistic: 30.17 on 1 and 8 DF,  p-value: 0.0005785\n\n\n\nanova(fit2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\nFM\n\nanova(fit)  \n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11313.04348313.04347860.3231030.0001100467\n    x21 19.03040 19.030400 3.6671350.0970444465\n    Residuals7 36.32612  5.189446       NA          NA\n\n\n\n\nRM\n\nanova(fit2)  \n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\n\\(F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}\\)\nSSE_FM\n\nSSE_FM <- anova(fit)$Sum[3]  \n\nSSE_RM\n\nSSE_RM <- anova(fit2)$Sum[2]  \n\n\nF0 <- (SSE_RM-SSE_FM)/(SSE_FM/7)\nF0\n\n0.0428807391894599\n\n\n기각역 \\(F_{0.05}(1,7)\\)\n\nqf(0.95, 1, 7)\n\n5.59144785122073\n\n\n기각할 수 없다.\\(\\beta_1\\)은 \\(\\beta_2\\)에 1 더한 값과 같다."
  },
  {
    "objectID": "posts/rl/2022-12-08-rl-HW4.html",
    "href": "posts/rl/2022-12-08-rl-HW4.html",
    "title": "Regression HW 4",
    "section": "",
    "text": "고급회귀분석 과제, CH13\n네 번째 과제입니다.\n제출 기한 12월 8일\n행렬 계산은 R로 해도 됩니다.\n그리고, 계산 후에 R 함수(예:lm)를 이용하여 결과 확인하고, 본인의 풀이가 맞는지 결과 확인해보세요.\n제출 방법\n(pdf 아닌 문서는 미제출로 간주)\n주의사항"
  },
  {
    "objectID": "posts/rl/2022-12-08-rl-HW4.html#section",
    "href": "posts/rl/2022-12-08-rl-HW4.html#section",
    "title": "Regression HW 4",
    "section": "1.",
    "text": "1.\n\n\n\n일련번호\n\\(y\\)(총 소요시간)\n\\(x_1\\)(적성검사점수)\n\\(x_2\\)(성별)\n학력\n\n\n\n\n1\n17\n151\n남자\n대\n\n\n2\n26\n92\n남자\n고\n\n\n3\n21\n175\n남자\n대\n\n\n4\n30\n31\n남자\n고\n\n\n5\n22\n104\n남자\n고\n\n\n6\n1\n277\n남자\n대학원\n\n\n7\n12\n210\n남자\n대학원\n\n\n8\n19\n120\n남자\n대\n\n\n9\n4\n290\n남자\n대학원\n\n\n10\n16\n238\n남자\n대학원\n\n\n11\n28\n164\n여자\n대학원\n\n\n12\n15\n272\n여자\n대학원\n\n\n13\n11\n295\n여자\n대학원\n\n\n14\n38\n68\n여자\n고\n\n\n15\n31\n85\n여자\n대\n\n\n16\n21\n224\n여자\n대학원\n\n\n17\n20\n166\n여자\n대\n\n\n18\n13\n305\n여자\n대학원\n\n\n19\n30\n124\n여자\n대\n\n\n20\n14\n246\n여자\n대학원\n\n\n\n\ndf <- data.frame('y'=c(17,26,21,30,22,1,12,19,4,16,28,15,11,38,31,21,20,13,30,14),\n           'x1'=c(151,92,175,31,104,277,210,120,290,238,164,272,295,68,85,224,166,305,124,246),\n          'x2'=c('남자','남자','남자','남자','남자','남자','남자','남자','남자','남자','여자','여자','여자','여자','여자','여자','여자','여자','여자','여자'),\n          'edu'=c('대','고','대','고','고','대학원','대학원','대','대학원','대학원','대학원','대학원','대학원','고','대','대학원','대','대학원','대','대학원'))\n\n\n(1)\n\\(x_1\\)을 \\(x\\)로 놓고 각 성별에 대하여 회귀모형을 적합하시오. 그런 후 두 회귀직선의 기울기가 같은지에 대한 가설검정을 하시오(5장 p12 : 두 회귀모형의 검정 참고).\nanswer\n\nggplot(df, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() +\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), \n                     values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n남성, Model 1: \\(y_i = \\beta_{01} + \\beta_{11} x_{i1} + \\epsilon_i , \\epsilon_i \\sim i.i.d. N(0, \\sigma^2)\\)\n여성, Model 2: \\(y_i = \\beta_{02} + \\beta_{12} x_{i2} + \\epsilon_i , \\epsilon_i \\sim i.i.d. N(0, \\sigma^2)\\)\n남자\n\nplot(df$x1[df$x2==\"남자\"], df$y[df$x2==\"남자\"])\n\n\n\n\n\\(S_{yy}\\)\n\nsum((df$y[df$x2==\"남자\"] - (17+26+21+30+22+1+12+19+4+16)/10)**2)\n\n745.6\n\n\n\\(S_{xx}\\)\n\nsum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2)\n\n64705.6\n\n\n\\(S_{xy}\\)\n\nsum((df$y[df$x2==\"남자\"] - (17+26+21+30+22+1+12+19+4+16)/10) * \n    (df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10))\n\n-6461.4\n\n\n\\(\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\)\n\nbeta_men <- round(sum((df$y[df$x2==\"남자\"] - (17+26+21+30+22+1+12+19+4+16)/10) *\n    (df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)) / \n    sum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2),5)\nbeta_men\n\n-0.09986\n\n\n\\(\\hat{\\beta_0}\\)\n\nround((17+26+21+30+22+1+12+19+4+16)/10 - \n    round(sum((df$y[df$x2==\"남자\"] - (17+26+21+30+22+1+12+19+4+16)/10) *\n    (df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)) / \n    sum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2),5)*\n    (151+92+175+31+104+277+210+120+290+238)/10,5)\n\n33.65637\n\n\n\\(\\hat{y}_{men} = 33.65637 -0.09986 x_{men}\\)\n\nR code\n\nsummary(lm(df$y[df$x2==\"남자\"]~df$x1[df$x2==\"남자\"]))\n\n\nCall:\nlm(formula = df$y[df$x2 == \"남자\"] ~ df$x1[df$x2 == \"남자\"])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9953 -1.5008 -0.6915  1.0080  6.1102 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            33.65610    2.60379  12.926 1.21e-06 ***\ndf$x1[df$x2 == \"남자\"] -0.09986    0.01393  -7.171 9.51e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.542 on 8 degrees of freedom\nMultiple R-squared:  0.8654,    Adjusted R-squared:  0.8485 \nF-statistic: 51.43 on 1 and 8 DF,  p-value: 9.51e-05\n\n\n\nanswer\n\\(SSE_{men}\\)\n\nSSE_men <- sum((df$y[df$x2==\"남자\"] - (33.65637 - 0.09986*df$x1[df$x2==\"남자\"]))**2)\nSSE_men\n\n100.3747034298\n\n\n\\(MSE_{men} \\sim \\sigma^2_{men}\\)\n\nMSE_men <- sum((df$y[df$x2==\"남자\"] - (33.65637 - 0.09986*df$x1[df$x2==\"남자\"]))**2)/(10-2)\nMSE_men\n\n12.546837928725\n\n\n\nR code\n\nanova(lm(df$y[df$x2==\"남자\"]~df$x1[df$x2==\"남자\"]))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df$x1[df$x2 == \"남자\"]1645.2253645.2253051.425339.509826e-05\n    Residuals8100.3747 12.54684      NA          NA\n\n\n\n\n\nanswer\n여자\n\nplot(df$x1[df$x2==\"여자\"], df$y[df$x2==\"여자\"])\n\n\n\n\n\\(S_{yy}\\)\n\nsum((df$y[df$x2==\"여자\"] - (28+15+11+38+31+21+20+13+30+14)/10)**2)\n\n756.9\n\n\n\\(S_{xx}\\)\n\nsum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2)\n\n66542.9\n\n\n\\(S_{xy}\\)\n\nsum((df$y[df$x2==\"여자\"] - (28+15+11+38+31+21+20+13+30+14)/10) * \n    (df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10))\n\n-6783.9\n\n\n\\(\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\)\n\nbeta_women <- round(sum((df$y[df$x2==\"여자\"] - (28+15+11+38+31+21+20+13+30+14)/10) * \n    (df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10))/\n      sum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2),5)\nbeta_women\n\n-0.10195\n\n\n\\(\\hat{\\beta_0}\\)\n\nround((28+15+11+38+31+21+20+13+30+14)/10 - \n      round(sum((df$y[df$x2==\"여자\"] - (28+15+11+38+31+21+20+13+30+14)/10) * \n    (df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10))/\n      sum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2),5)*\n      (164+272+295+68+85+224+166+305+124+246)/10,5)\n\n41.97006\n\n\n\\(\\hat{y}_{women} = 41.97006 -0.10195 x_{women}\\)\n\nR code\n\nsummary(lm(df$y[df$x2==\"여자\"]~df$x1[df$x2==\"여자\"]))\n\n\nCall:\nlm(formula = df$y[df$x2 == \"여자\"] ~ df$x1[df$x2 == \"여자\"])\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.046 -1.952  0.716  2.060  2.963 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            41.96962    2.33998  17.936 9.57e-08 ***\ndf$x1[df$x2 == \"여자\"] -0.10195    0.01108  -9.205 1.57e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.857 on 8 degrees of freedom\nMultiple R-squared:  0.9137,    Adjusted R-squared:  0.9029 \nF-statistic: 84.73 on 1 and 8 DF,  p-value: 1.57e-05\n\n\n\nanswer\n\\(SSE_{women}\\)\n\nSSE_women <- sum((df$y[df$x2==\"여자\"] - (41.97006 - 0.10195*df$x1[df$x2==\"여자\"]))**2)\nSSE_women\n\n65.2965503775\n\n\n\\(MSE_{women} \\sim \\sigma^2_{women}\\)\n\nMSE_women <- sum((df$y[df$x2==\"여자\"] - (41.97006 - 0.10195*df$x1[df$x2==\"여자\"]))**2)/(10-2)\nMSE_women\n\n8.1620687971875\n\n\n\nR code\n\nanova(lm(df$y[df$x2==\"여자\"]~df$x1[df$x2==\"여자\"]))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df$x1[df$x2 == \"여자\"]1691.60345691.60345084.733841.56975e-05\n    Residuals8 65.29655  8.162069      NA         NA\n\n\n\n\n\nanswer\n가설\n\\(H_0: \\beta_{11} - \\beta_{12} = 0\\)\n\\(H_0: \\beta_{11} - \\beta_{12} \\neq 0\\)\n검정통계량 \\(t_0 = \\frac{\\hat{\\beta}_{11}- \\hat{\\beta}_{12}}{\\sqrt{\\hat{var}(\\hat{\\beta}_{11} - \\hat{\\beta}_{12})}}\\sim H_0, t(n_1 - n_2 - 4)\\)\n\\(var(\\hat{\\beta}_{11} - \\hat{\\beta}_{12})\\)\n\\(= var(\\hat{\\beta}_{11}) + var(\\hat{\\beta}_{12})\\)\n\\(= \\frac{\\sigma^2}{\\sum(x_{11} - \\bar{x}_{11})^2} + \\frac{\\sigma^2}{\\sum(x_{12} - \\bar{x}_{12})^2}\\)\n\\(\\hat{\\beta}_{11} - \\hat{\\beta}_{12}\\)\n\nbeta_men # CH13, p14 beta_1과 동일\nbeta_women # CH13 p1.4 beta1 + beta3 과 동일\n# beta11 - beta12 = \\beta3 = 0.0021\n\n-0.09986\n\n\n-0.10195\n\n\n\nbeta_men - beta_women\n\n0.00208999999999999\n\n\n\\(\\sigma^2 = MSE(FM) = \\frac{SSE_{FM}}{n_1 + n_2 - 1}\\)\n\\(\\star SSE_{FM} = SSE_1 + SSE_2\\)\n\nSSE_men\nSSE_women\nSSE_men + SSE_women # SSE_FM\n(SSE_men + SSE_women)/(10 +10-2)\n\n100.3747034298\n\n\n65.2965503775\n\n\n165.6712538073\n\n\n9.20395854485\n\n\n\\(\\sqrt{var(\\hat{\\beta}_{11} - \\hat{\\beta}_{12}))}\\)\n\nsqrt(MSE_men/sum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2) + \n    MSE_women/sum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2))\n# CG13 p,16 hat{=(s.e.)(beta_3)과 동일\n\n0.0177922812236426\n\n\n\\(t_0\\)\n\n(beta_men - beta_women)/ \n    sqrt(MSE_men/sum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2) + \n    MSE_women/sum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2))\n\n0.117466668479969\n\n\n\\(var(\\hat{\\beta}_{men} - \\hat{\\beta}_{women})\\)\n\nMSE_men/sum((df$x1[df$x2==\"남자\"] - (151+92+175+31+104+277+210+120+290+238)/10)**2) + \n    MSE_women/sum((df$x1[df$x2==\"여자\"] - (164+272+295+68+85+224+166+305+124+246)/10)**2)\n\n0.000316565271141184\n\n\n\nqt(0.025,16)\n\n-2.11990529922126\n\n\n기각역 : \\(|t_0| > t_{\\alpha/2}(n_1 + n_2 - 4) = 2.120\\)\n결론: 기각역에 속하지 않으므로 귀무가설 기각 못함.\n\\(H_0: \\beta_{11} - \\beta_{12} = 0\\) 채택\n따라서 성별간 두 기울기가 다르다고 할 수 없다.\n\nR code\n\nggplot(df, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(lm(df$y~df$x1*df$x2))[2], intercept = coef(lm(df$y~df$x1*df$x2))[1], col= 'darkorange')+\n  geom_abline(slope = coef(lm(df$y~df$x1*df$x2))[2], intercept = coef(lm(df$y~df$x1*df$x2))[1]+coef(lm(df$y~df$x1*df$x2))[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\nsummary(lm(df$y~df$x1*df$x2))\n\n\nCall:\nlm(formula = df$y ~ df$x1 * df$x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0463 -1.7591 -0.6232  1.9311  6.1102 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     33.656104   2.365392  14.229 1.68e-10 ***\ndf$x1           -0.099858   0.012650  -7.894 6.59e-07 ***\ndf$x2여자        8.313516   3.541379   2.348   0.0321 *  \ndf$x1:df$x2여자 -0.002089   0.017766  -0.118   0.9078    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.218 on 16 degrees of freedom\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8803 \nF-statistic: 47.56 on 3 and 16 DF,  p-value: 3.405e-08\n\n\n\\(\\beta_3 = -0.002089\\), 유의확률 $0.9078 $ 유의수준 \\(\\alpha=0.05\\)에서 기각할 수 없다.\n기각역: \\(|t_0|?t_{\\alpha/2}(n_1 + n_2 - 4) = t_{0.025}(16) = 2.120\\)\n기각역에 속하지 않으므로 \\(H_0\\) 기각 못함 \\(H_0: \\beta_{11} - \\beta_{12} = 0\\) 채택\n따라서 성별간 두 기울기가 다르다고 할 수 없다.\n\n\n(2)\n13장 강의노트 p.8 에 따르면 \\(x_1,x_2\\)을 설명변수로 했을 때 적합 결과는 \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2\\] \\[= 33.8349 − 0.1009x_1 + 7.9340x_2\\] 이었다. \\(\\beta_1\\) 의 추정값 \\(\\hat{\\beta}_1\\) 이 갖는 분산을 추정하고, \\(\\beta_1\\) 의 95% 신뢰구간을 구하시오.\nanswer\n\nbetahat_2 <- solve(t(matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3))%*%\n      matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3))%*%\n        t(matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3))%*%\n        matrix(df$y,nrow=20,ncol=1)\nbetahat_2\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    33.8349119\n    -0.1009177\n     7.9339526\n\n\n\n\n\ndf$sex <- ifelse(df$x2=='여자',1,0)\n\n\\(var(\\beta_1) = (x^\\top x)^{-1}_{(2,2)} \\sigma^2\\)\n\nt(matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3))%*%matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3)\n\n\n\nA matrix: 3 × 3 of type dbl\n\n      20  3637  10\n    36377960431949\n      10  1949  10\n\n\n\n\n\nsolve(t(matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3))%*%matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3))\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     0.31709536-1.286110e-03-0.0664325307\n    -0.00128611 7.619135e-06-0.0001988594\n    -0.06643253-1.988594e-04 0.2051902307\n\n\n\n\n\\((x^\\top x)^{-1}_{(2,2)}\\)\n\nsolve(t(matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3))%*%matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3))[2,2]\n\n7.61913469487271e-06\n\n\n\\(MSE \\sim \\sigma^2\\)\n\nsum((matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3)%*%betahat_2 - df$y)**2)/17\n\n9.75379176770424\n\n\n\\(var(\\beta_1) = (x^\\top x)^{-1}_{(2,2)} \\sigma^2\\)\n\nhat_var_b1 <- ((solve(t(matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3)) %*% matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3)))[2,2])*\n    sum((matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3)%*%betahat_2 - df$y)**2)/17\nhat_var_b1\n\n7.43154532638791e-05\n\n\n\nsqrt(hat_var_b1)\n\n0.00862064111675455\n\n\n\\(\\beta_1 \\pm t_{0.975}(17) se(\\hat{\\beta}_1)\\)\n\nbetahat_2[2] - qt(0.975,17) * sqrt(hat_var_b1)\n\n-0.119105687693037\n\n\n\nbetahat_2[2] + qt(0.975,17) * sqrt(hat_var_b1)\n\n-0.0827297618549584\n\n\n\nR code\n\nsummary(lm(df$y~matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3)))$coefficients\n\n\n\nA matrix: 3 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)33.83491191.758659173 19.2390395.635999e-13\n    matrix(c(rep(1, 20), df$x1, df$sex), nrow = 20, ncol = 3)2-0.10091770.008620641-11.7065221.468240e-09\n    matrix(c(rep(1, 20), df$x1, df$sex), nrow = 20, ncol = 3)3 7.93395261.414702366  5.6082133.134533e-05\n\n\n\n\n\nanova(lm(df$y~matrix(c(rep(1,20),df$x1,df$sex),nrow=20,ncol=3)))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    matrix(c(rep(1, 20), df$x1, df$sex), nrow = 20, ncol = 3) 21477.1355738.56777075.721093.419664e-09\n    Residuals17 165.8145  9.753792      NA          NA\n\n\n\n\n\nconfint(lm(df$y~matrix(c(rep(1,20),df$x1,rep(0,10),rep(1,10)),nrow=20,ncol=3)), level=0.95)\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)30.124465437.54535846\n    matrix(c(rep(1, 20), df$x1, rep(0, 10), rep(1, 10)), nrow = 20, ncol = 3)1        NA         NA\n    matrix(c(rep(1, 20), df$x1, rep(0, 10), rep(1, 10)), nrow = 20, ncol = 3)2-0.1191057-0.08272976\n    matrix(c(rep(1, 20), df$x1, rep(0, 10), rep(1, 10)), nrow = 20, ncol = 3)3 4.949191510.91871371\n\n\n\n\n\n\n(3)\n다음의 모형을 적합하시오. \\[y_i =\\beta_0 +\\beta_1x_{1i} +\\beta_2x_{2i} +\\beta_3x_{3i} +\\beta_4x_{4i} + \\epsilon_i\\]\n\\[x_{3i}=\\begin{cases} 1: & \\text{학력이 고졸} \\\\ 0 : &\\text{기타} \\end{cases}, x_{4i}= \\begin{cases} 1 : & \\text{학력이 대졸} \\\\ 0 : & \\text{기타}\\end{cases} \\]\nanswer\n\ndf$high <- ifelse(df$edu=='고',1,0)\n\n\ndf$univ <- ifelse(df$edu=='대',1,0)\n\n\\(\\hat{\\beta}\\)\n\nx_hu <- matrix(c(rep(c(1),20),df$x1,df$sex,df$high,df$univ),nrow=20,ncol=5)\n\n\nbetahat_hu <- solve(t(x_hu)%*%x_hu)%*%t(x_hu)%*%df$y\nbetahat_hu\n\n\n\nA matrix: 5 × 1 of type dbl\n\n    36.5226684\n    -0.1101234\n     7.8990531\n    -1.3758336\n    -2.4036479\n\n\n\n\n\\(y = 36.5226684 - 0.1101234 x_1 + 7.8990531 x_2 - 1.3758336 x_3 - 2.4036479 x_4\\)\n\nR code\n\nsummary(lm(y~x1+sex+high+univ,df))\n\n\nCall:\nlm(formula = y ~ x1 + sex + high + univ, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0185 -1.7038 -0.5386  1.6375  6.1526 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.52267    5.33138   6.851 5.50e-06 ***\nx1          -0.11012    0.01997  -5.516 5.93e-05 ***\nsex          7.89905    1.50447   5.250 9.79e-05 ***\nhigh        -1.37583    4.13333  -0.333    0.744    \nuniv        -2.40365    2.85796  -0.841    0.414    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.214 on 15 degrees of freedom\nMultiple R-squared:  0.9057,    Adjusted R-squared:  0.8806 \nF-statistic: 36.02 on 4 and 15 DF,  p-value: 1.587e-07\n\n\n\n\n\n(4)\n위의 모형에서 \\(\\beta_3\\)과 \\(\\beta_4\\)가 갖는 의미를 해석하시오.\n\\[y_i =\\beta_0 +\\beta_1x_{1i} +\\beta_2x_{2i} +\\beta_3x_{3i} +\\beta_4x_{4i} + \\epsilon_i\\]\n\\[x_{3i}=\\begin{cases} 1: & \\text{학력이 고졸} \\\\ 0 : &\\text{기타} \\end{cases}, x_{4i}= \\begin{cases} 1 : & \\text{학력이 대졸} \\\\ 0 : & \\text{기타}\\end{cases} \\]\nanswer\n\\(E(y) = \\beta_0 +\\beta_1x_{1} +\\beta_2x_{2} +\\beta_3x_{3} +\\beta_4x_{4}\\)\n학력이 고등학교 졸업\n\\(E(y|고) = (\\beta_0 +\\beta_3) +\\beta_1x_{1} +\\beta_2x_{2}\\)\n학력이 대학교 졸업\n\\(E(y|고) = (\\beta_0 +\\beta_4)+\\beta_1x_{1}+\\beta_2x_{2}\\)\n학력이 대학원 졸업\n\\(E(y|대학원) = \\beta_0 +\\beta_1x_{1}+\\beta_2x_{2}\\)\n\\(\\beta_3\\) = 학력이 고등학교 졸업일때의 평균 숙련 시간에서 학력이 대학원 졸업일때의 평균 숙련시간을 뺀 값이다.\n\\(\\beta_4\\) = 학력이 대학교 졸업일때의 평균 숙련 시간에서 학력이 대학원 졸업일때의 평균 숙련시간을 뺀 값이다.\n\n\n(5)\n위의 \\(\\beta_3\\)과 \\(\\beta_4\\)의 최소제곱추정값 \\(\\hat{\\beta}_3\\)과 $_4$가 갖는 각각의 분산을 추정하시오. \\(\\beta_3\\)과 \\(\\beta_4\\)의 95% 신뢰구간을 구하시오\nanswer\n\\(var(\\beta_3) = (x^\\top x)^{-1}_{(4,4)} \\sigma^2\\)\n\nt(x_hu)%*%x_hu\n\n\n\nA matrix: 5 × 5 of type dbl\n\n      20  3637  10  4  6\n    36377960431949295821\n      10  1949  10  1  3\n       4   295   1  4  0\n       6   821   3  0  6\n\n\n\n\n\nsolve(t(x_hu)%*%x_hu)\n\n\n\nA matrix: 5 × 5 of type dbl\n\n     2.751920464-0.009968093-0.231607148-1.958871839-1.272149536\n    -0.009968093 0.000038595 0.000397155 0.007022423 0.004488433\n    -0.231607148 0.000397155 0.219140617 0.147531812 0.067692796\n    -1.958871839 0.007022423 0.147531812 1.654085215 0.924204433\n    -1.272149536 0.004488433 0.067692796 0.924204433 0.790802611\n\n\n\n\n\\((x^\\top x)^{-1}_{(4,4)}\\)\n\nsolve(t(x_hu)%*%x_hu)[4,4]\n\n1.65408521513359\n\n\n\\(MSE \\sim \\sigma^2\\)\n\nsum((x_hu%*%betahat_hu - df$y)**2)\n\n154.929453525913\n\n\n\nsum((x_hu%*%betahat_hu - df$y)**2)/15\n\n10.3286302350608\n\n\n\\(var(\\hat{\\beta}_3) = (x^\\top x)^{-1}_{(4,4)} \\sigma^2\\)\n\nhat_var_hu_b3 <- (solve(t(x_hu)%*%x_hu)[4,4]%*% sum((x_hu%*%betahat_hu - df$y)**2)/15)[,]\nhat_var_hu_b3\n\n17.0844345643959\n\n\n\nsqrt(hat_var_hu_b3)\n\n4.133332138166\n\n\n\\(t_{\\alpha/2}(n-4-1)\\)\n\nqt(0.975,15)\n\n2.13144954555978\n\n\n\\(\\beta_3 \\pm t_{0.975}(15) se(\\hat{\\beta}_3)\\)\n\nbetahat_hu[4] - qt(0.975,15) * sqrt(hat_var_hu_b3)\n\n-10.1858224609978\n\n\n\nbetahat_hu[4] + qt(0.975,15) * sqrt(hat_var_hu_b3)\n\n7.43415535408522\n\n\n\\((x^\\top x)^{-1}_{(5,5)}\\)\n\nsolve(t(x_hu)%*%x_hu)[5,5]\n\n0.790802610709727\n\n\n\\(MSE \\sim \\sigma^2\\)\n\nsum((x_hu%*%betahat_hu - df$y)**2)/15\n\n10.3286302350608\n\n\n\\(var(\\beta_4) = (x^\\top x)^{-1}_{(5,5)} \\sigma^2\\)\n\nhat_var_hu_b4 <- (solve(t(x_hu)%*%x_hu)[5,5]%*% sum((x_hu%*%betahat_hu - df$y)**2)/15)[,]\nhat_var_hu_b4\n\n8.16790775494154\n\n\n\nsqrt(hat_var_hu_b4)\n\n2.85795517021201\n\n\n\\(\\beta_4 \\pm t_{0.975}(15) se(\\hat{\\beta}_4)\\)\n\nbetahat_hu[5] - qt(0.975,15) * sqrt(hat_var_hu_b4)\n\n-8.49523514036072\n\n\n\nbetahat_hu[5] + qt(0.975,15) * sqrt(hat_var_hu_b4)\n\n3.68793935719647\n\n\n\nR code\n\nsummary(lm(df$y~x_hu))$coefficients\n\n\n\nA matrix: 5 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)36.52266845.33137589 6.85051465.501546e-06\n    x_hu2-0.11012340.01996581-5.51559775.929593e-05\n    x_hu3 7.89905311.50446748 5.25039809.786601e-05\n    x_hu4-1.37583364.13333214-0.33286317.438444e-01\n    x_hu5-2.40364792.85795517-0.84103764.135338e-01\n\n\n\n\n\nanova(lm(df$y~x_hu))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x_hu 41488.0205372.0051436.016891.586767e-07\n    Residuals15 154.9295 10.32863      NA          NA\n\n\n\n\n\nconfint(lm(df$y~x_hu), level=0.95)\n\n\n\nA matrix: 6 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept) 25.159109747.88622717\n    x_hu1         NA         NA\n    x_hu2 -0.1526795-0.06756725\n    x_hu3  4.692356611.10574964\n    x_hu4-10.1858225 7.43415535\n    x_hu5 -8.4952351 3.68793936\n\n\n\n\n\n\n(6)\n만약 적성검사점수와 성별, 적성검사점수와 학력, 성별과 학력간에 교호작용이 있다면, 다음의 반응함수를 가정할 수 있다. \\[E(y)=\\beta_0 +\\beta_1x_1 +\\beta_2x_2 +\\beta_3x_3 +\\beta_4x_4\\] \\[+ \\beta_5x_1x_2 + \\beta_6x_1x_3 + \\beta_7x_1x_4 + \\beta_8x_2x_3 + \\beta_9x_2x_4\\] 데이터로부터 위의 반응함수를 적합하고, 다섯 개의 다음 가설을 유의수준 \\(\\alpha = 0.1\\)에서 검정하시오. \\(H_0 :\\beta_i =0, H_1 :\\beta_i \\neq 0(i=5,6,7,8,9)\\)\nanswer\n\nx_6 <- matrix(c(rep(c(1),20),df$x1,df$sex,df$high,df$univ,\n                df$x1*df$sex, df$x1*df$high, df$x1*df$univ, df$sex*df$high, df$sex*df$univ),nrow=20,ncol=10)\n\n\nx_6\n\n\n\nA matrix: 20 × 10 of type dbl\n\n    1151001  0  015100\n    1 92010  0 92  000\n    1175001  0  017500\n    1 31010  0 31  000\n    1104010  0104  000\n    1277000  0  0  000\n    1210000  0  0  000\n    1120001  0  012000\n    1290000  0  0  000\n    1238000  0  0  000\n    1164100164  0  000\n    1272100272  0  000\n    1295100295  0  000\n    1 68110 68 68  010\n    1 85101 85  0 8501\n    1224100224  0  000\n    1166101166  016601\n    1305100305  0  000\n    1124101124  012401\n    1246100246  0  000\n\n\n\n\n\nbetahat_6 <- solve(t(x_6)%*%x_6)%*%t(x_6)%*%df$y\nbetahat_6\n\n\n\nA matrix: 10 × 1 of type dbl\n\n    36.63696518\n    -0.11186981\n    12.27994350\n    -3.42748073\n    -6.74634061\n    -0.01528919\n     0.01659028\n     0.03861449\n     0.02924469\n    -4.10250464\n\n\n\n\n\\(\\hat{E}(y) = 36.63696518 - 0.11186981x_1 + 12.27994350x_2 - 3.42748073x_3 - 6.74634061x_4 - 0.01528919x_1 x_2 + 0.01659028x_1 x_3 + 0.03861449 x_1 x_4 + 0.02924469x_2 x_3 - 4.10250464x_2 x_4\\)\n\nt(x_6)%*%x_6\n\n\n\nA matrix: 10 × 10 of type dbl\n\n      20  3637  10  4  6  1949  295   821 1  3\n    363779604319492958214464032486511798368375\n      10  1949  10  1  3  1949   68   375 1  3\n       4   295   1  4  0    68  295     0 1  0\n       6   821   3  0  6   375    0   821 0  3\n    19494464031949 68375446403 4624 5015768375\n     295 24865  68295  0  462424865     068  0\n     821117983 375  0821 50157    0117983 0375\n       1    68   1  1  0    68   68     0 1  0\n       3   375   3  0  3   375    0   375 0  3\n\n\n\n\n\nsolve(t(x_6)%*%x_6)\n\n\n\nA matrix: 10 × 10 of type dbl\n\n     13.17371289-5.093089e-02-12.267241674-13.17371289-8.36632222 4.731945e-02 5.093089e-02 1.859418e-02  9.049519133 5.587008431\n     -0.05093089 2.007129e-04  0.047358588  0.05093089 0.03198551-1.864806e-04-2.007129e-04-7.327756e-05 -0.034677908-0.021032546\n    -12.26724167 4.735859e-02 15.889907134 12.26724167 6.05702973-6.112751e-02-4.735859e-02-5.585862e-03-11.733236632-7.260347528\n    -13.17371289 5.093089e-02 12.267241674 15.37525737 8.36632222-4.731945e-02-7.562090e-02-1.859418e-02 -9.572142583-5.587008431\n     -8.36632222 3.198551e-02  6.057029727  8.3663222210.98679372-2.278514e-02-3.198551e-02-4.736984e-02 -4.507640358-3.906316822\n      0.04731945-1.864806e-04 -0.061127507 -0.04731945-0.02278514 2.414928e-04 1.864806e-04 2.145159e-05  0.044705999 0.027035225\n      0.05093089-2.007129e-04 -0.047358588 -0.07562090-0.03198551 1.864806e-04 5.270126e-04 7.327756e-05  0.037179539 0.021032546\n      0.01859418-7.327756e-05 -0.005585862 -0.01859418-0.04736984 2.145159e-05 7.327756e-05 2.668358e-04  0.004127154 0.007485293\n      9.04951913-3.467791e-02-11.733236632 -9.57214258-4.50764036 4.470600e-02 3.717954e-02 4.127154e-03 10.045741217 5.421952259\n      5.58700843-2.103255e-02 -7.260347528 -5.58700843-3.90631682 2.703522e-02 2.103255e-02 7.485293e-03  5.421952259 4.226992806\n\n\n\n\n\\(MSE \\sim \\sigma^2\\)\n\nsum((x_6%*%betahat_6 - df$y)**2)\n\n128.60490936452\n\n\n\nsum((x_6%*%betahat_6 - df$y)**2)/10\n\n12.860490936452\n\n\n\\(H_0: \\beta_1 = 0 \\text{ vs } H_1: \\beta_i \\neq 0 (i=5,6,7,8,9)\\)\n\\(t_0 = \\frac{\\hat{\\beta}_i}{\\hat{s.e.}(\\hat{\\beta}_i)}\\sim H_0 , t(n-p-1)\\)\n\n\\(H_0 :\\beta_5 =0\\)\n\\(H_1 :\\beta_5 \\neq 0\\)\n\\((x^\\top x)^{-1}_{(6,6)}\\)\n\nsolve(t(x_6)%*%x_6)[6,6]\n\n0.000241492771933959\n\n\n\\(var(\\hat{\\beta}_5) = (x^\\top x)^{-1}_{(6,6)} \\sigma^2\\)\n\nhat_var_6_b5 <- (solve(t(x_6)%*%x_6)[6,6]*sum((x_6%*%betahat_6 - df$y)**2)/10)\nhat_var_6_b5\n\n0.00310571560467536\n\n\n\nsqrt(hat_var_6_b5)\n\n0.0557289476365323\n\n\n\\(t_0 = \\frac{\\beta_5}{\\sqrt{\\hat{var}(\\hat{\\beta}_5)}}\\)\n\nbetahat_6[6]/sqrt(hat_var_6_b5)\n\n-0.274349074823289\n\n\n\nqt(0.95,10)\n\n1.81246112281168\n\n\n\\(|t_0|<1.812\\)가 되어 귀무가설 \\(H_0 :\\beta_5 =0\\)은 기각할 수 없다.\n\n\\(H_0 :\\beta_6 =0\\)\n\\(H_1 :\\beta_6 \\neq 0\\)\n\\((x^\\top x)^{-1}_{(7,7)}\\)\n\nsolve(t(x_6)%*%x_6)[7,7]\n\n0.000527012620706128\n\n\n\\(var(\\beta_6) = (x^\\top x)^{-1}_{(7,7)} \\sigma^2\\)\n\nhat_var_6_b6 <- (solve(t(x_6)%*%x_6)[7,7]*sum((x_6%*%betahat_6 - df$y)**2)/10)\nhat_var_6_b6\n\n0.006777641031987\n\n\n\\(t_0 = \\frac{\\beta_6}{\\sqrt{\\hat{var}(\\hat{\\beta}_6)}}\\)\n\nbetahat_6[7]/sqrt(hat_var_6_b6)\n\n0.20151831465707\n\n\n\nqt(0.95,10)\n\n1.81246112281168\n\n\n\\(|t_0|<1.812\\)가 되어 귀무가설 \\(H_0 :\\beta_6 =0\\)은 기각할 수 없다.\n\n\\(H_0 :\\beta_7 =0\\)\n\\(H_1 :\\beta_7 \\neq 0\\)\n\\((x^\\top x)^{-1}_{(8,8)}\\)\n\nsolve(t(x_6)%*%x_6)[8,8]\n\n0.000266835833441736\n\n\n\\(var(\\beta_7) = (x^\\top x)^{-1}_{(8,8)} \\sigma^2\\)\n\nhat_var_6_b7 <- (solve(t(x_6)%*%x_6)[8,8]*sum((x_6%*%betahat_6 - df$y)**2)/10)\nhat_var_6_b7\n\n0.00343163981749807\n\n\n\\(t_0 = \\frac{\\beta_7}{\\sqrt{\\hat{var}(\\hat{\\beta}_7)}}\\)\n\nbetahat_6[8]\n\n0.0386144912979298\n\n\n\nbetahat_6[8]/sqrt(hat_var_6_b7)\n\n0.659173088855045\n\n\n\nqt(0.95,10)\n\n1.81246112281168\n\n\n\\(|t_0|<1.812\\)가 되어 귀무가설 \\(H_0 :\\beta_7 =0\\)은 기각할 수 없다.\n\n\\(H_0 :\\beta_8 =0\\)\n\\(H_1 :\\beta_8 \\neq 0\\)\n\\((x^\\top x)^{-1}_{(9,9)}\\)\n\nsolve(t(x_6)%*%x_6)[9,9]\n\n10.0457412166377\n\n\n\\(var(\\beta_8) = (x^\\top x)^{-1}_{(9,9)} \\sigma^2\\)\n\nhat_var_6_b8 <- (solve(t(x_6)%*%x_6)[9,9]*sum((x_6%*%betahat_6 - df$y)**2)/10)\nhat_var_6_b8\n\n129.193163866512\n\n\n\\(t_0 = \\frac{\\beta_8}{\\sqrt{\\hat{var}(\\hat{\\beta}_8)}}\\)\n\nbetahat_6[9]/sqrt(hat_var_6_b8)\n\n0.00257292605071365\n\n\n\nqt(0.95,10)\n\n1.81246112281168\n\n\n\\(|t_0|<1.812\\)가 되어 귀무가설 \\(H_0 :\\beta_8 =0\\)은 기각할 수 없다.\n\n\\(H_0 :\\beta_9 =0\\)\n\\(H_1 :\\beta_9 \\neq 0\\)\n\\(t_0 = \\frac{\\beta_9}{\\sqrt{\\hat{var}(\\hat{\\beta}_9)}}\\)\n\\((x^\\top x)^{-1}_{(10,10)}\\)\n\nsolve(t(x_6)%*%x_6)[10,10]\n\n4.22699280590833\n\n\n\\(var(\\beta_{10}) = (x^\\top x)^{-1}_{(10,10)} \\sigma^2\\)\n\nhat_var_6_b9 <- (solve(t(x_6)%*%x_6)[10,10]*sum((x_6%*%betahat_6 - df$y)**2)/10)\nhat_var_6_b9\n\n54.3612026688321\n\n\n\nbetahat_6[10]/sqrt(hat_var_6_b9)\n\n-0.55642233331273\n\n\n\nqt(0.95,10)\n\n1.81246112281168\n\n\n\\(|t_0|<1.812\\)가 되어 귀무가설 \\(H_0 :\\beta_9 =0\\)은 기각할 수 없다.\n\nR code\n\nsummary(lm(y ~ x1 + sex + high + univ + x1*sex + x1*high + x1*univ + sex*high + sex*univ,df))\n\n\nCall:\nlm(formula = y ~ x1 + sex + high + univ + x1 * sex + x1 * high + \n    x1 * univ + sex * high + sex * univ, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6490 -1.4326 -0.1288  0.8918  5.9881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 36.63697   13.01616   2.815   0.0183 *\nx1          -0.11187    0.05081  -2.202   0.0523 .\nsex         12.27994   14.29517   0.859   0.4104  \nhigh        -3.42748   14.06177  -0.244   0.8124  \nuniv        -6.74634   11.88678  -0.568   0.5829  \nx1:sex      -0.01529    0.05573  -0.274   0.7894  \nx1:high      0.01659    0.08233   0.202   0.8443  \nx1:univ      0.03861    0.05858   0.659   0.5247  \nsex:high     0.02924   11.36632   0.003   0.9980  \nsex:univ    -4.10250    7.37300  -0.556   0.5902  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.586 on 10 degrees of freedom\nMultiple R-squared:  0.9217,    Adjusted R-squared:  0.8513 \nF-statistic: 13.08 on 9 and 10 DF,  p-value: 0.0001985\n\n\n\nanova(lm(y ~ x1 + sex + high + univ + x1*sex + x1*high + x1*univ + sex*high + sex*univ,df))\n\n\n\nA anova: 10 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1 11170.35873151170.358731591.004203282.444210e-06\n    sex 1 306.7768085 306.776808523.854206666.380709e-04\n    high 1   3.5791086   3.5791086 0.278302646.093205e-01\n    univ 1   7.3058980   7.3058980 0.568088574.683946e-01\n    x1:sex 1   0.6267948   0.6267948 0.048738028.297162e-01\n    x1:high 1   0.1408619   0.1408619 0.010953079.187174e-01\n    x1:univ 1  12.5166525  12.5166525 0.973263983.471356e-01\n    sex:high 1   9.0585522   9.0585522 0.704370634.209294e-01\n    sex:univ 1   3.9816828   3.9816828 0.309605815.901555e-01\n    Residuals10 128.6049094  12.8604909         NA          NA\n\n\n\n\n\n\n(7)\n위의 반응함수에 대하여 회귀분석을 통한 분산분석표를 작성하고, 회귀변동의 유의성을 \\(F\\)-검정하시오. 이때 \\(F\\) -검정의 귀무가설을 \\(\\beta_i\\) 들로 표현하시오.\nanswer\n\\(SST\\)\n\nSST <- sum((df$y - (17+26+21+30+22+1+12+19+4+16+28+15+11+38+31+21+20+13+30+14)/20)**2)\nSST\n\n1642.95\n\n\n\\(SSR\\)\n\nSSR_7 <- sum(((17+26+21+30+22+1+12+19+4+16+28+15+11+38+31+21+20+13+30+14)/20 - x_6%*%betahat_6)**2)\nSSR_7\n\n1514.34509063554\n\n\n\\(MSR\\)\n\nMSR_7 <- SSR_7/9\nMSR_7\n\n168.260565626172\n\n\n\\(SSE\\)\n\nSSE_7 <- SST - SSR_7\nSSE_7\n\n128.604909364456\n\n\n\\(MSE\\)\n\nMSE_7 <- SSE_7/10\nMSE_7\n\n12.8604909364456\n\n\n\\(F_0\\)\n\nF0_7 <- MSR_7 / MSE_7\nF0_7\n\n13.0835258512048\n\n\n\nqf(0.95,9,10)\n\n3.02038294702137\n\n\n\n\n\n\n제곱합\n자유도\n평균제곱합\nF_0\nF_{0.05}(9,10)\n\n\n\n\n회귀\n1514.35\n9\n168.26\n13.084\n3.0204\n\n\n잔차\n128.6\n10\n12.86\n\n\n\n\n합계\n1642.95\n19\n\n\n\n\n\n\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0\\)\n\\(H_1 : \\text{not } H_0\\)\n\\(|F_0|>3.0204\\)가 되어 귀무가설 \\(H_0 : \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0\\)은 기각한다.\n따라서 유의수준 0.05에서 회귀모형이 유의함을 알 수 있다.\n\nR code\n\nround(anova(lm(y ~ x1 + sex + high + univ + x1*sex + x1*high + x1*univ + sex*high + sex*univ,df)),4)\n\n\n\nA anova: 10 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    x1 11170.35871170.358791.00420.0000\n    sex 1 306.7768 306.776823.85420.0006\n    high 1   3.5791   3.5791 0.27830.6093\n    univ 1   7.3059   7.3059 0.56810.4684\n    x1:sex 1   0.6268   0.6268 0.04870.8297\n    x1:high 1   0.1409   0.1409 0.01100.9187\n    x1:univ 1  12.5167  12.5167 0.97330.3471\n    sex:high 1   9.0586   9.0586 0.70440.4209\n    sex:univ 1   3.9817   3.9817 0.30960.5902\n    Residuals10 128.6049  12.8605     NA    NA"
  },
  {
    "objectID": "posts/rl/2022-12-08-rl-HW4.html#section-8",
    "href": "posts/rl/2022-12-08-rl-HW4.html#section-8",
    "title": "Regression HW 4",
    "section": "2.",
    "text": "2.\n아래의 데이터에 대하여 다음 물음에 답하시오.\n\n\n\n\\(x\\)\n1\n2\n2\n3\n4\n5\n5\n6\n7\n\n\n\n\n\\(y\\)\n2.0\n3.2\n3.2\n4.1\n5.2\n7.0\n7.4\n9.7\n11.5\n\n\n\n\ndf2 <- data.frame('y'=c(2.0,3.2,3.2,4.1,5.2,7.0,7.4,9.7,11.5),'x1'=c(1,2,2,3,4,5,5,6,7))\n\n\n(1)\n\\(x\\)와 \\(y\\)에 대한 산점도를 그려보고, 어떤 \\(x\\)의 값(\\(x_w\\))에서 구간을 두 개로 나누면 적절한지 논하시오.\nanswer\n\nplot(df2$x1,df2$y)\n\n\n\n\n산점도를 그려보면 \\(x = 4\\)를 기준으로 기울기가 달라지는 것으로 보인다.\n\n\n(2)\n위의 (1)에서 얻은 점을 경계로 구간별 단순선형회귀선을 추정하시오. 사용되는 모형은 \\[x_{2i} = \\begin{cases} 1 :& \\text{만약 }x_{1i} > x_w \\text{ 이면 }\\\\ 0 : &\\text{만약 } x_{1i} \\le x_w \\text{ 이면 } \\end{cases}\\] 과 같다.\nanswer\n\nplot(df2$x1,df2$y)\nabline(v=4,col=\"red\",lty=2)\n\n\n\n\n\ndf2$x2 <- sapply(df2$x1, function(x) max(0, x-4))\n\n\nx_sp <- matrix(c(rep(1,9),df2$x1,df2$x2),nrow=9,ncol=3)\n\n\nx_sp\n\n\n\nA matrix: 9 × 3 of type dbl\n\n    110\n    120\n    120\n    130\n    140\n    151\n    151\n    162\n    173\n\n\n\n\n\nbetahat_sp <- solve(t(x_sp)%*%x_sp)%*%t(x_sp)%*%df2$y\nbetahat_sp\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    1.071429\n    1.023469\n    1.119388\n\n\n\n\n\nR code\n\nmodel_1 <- lm(y ~ x1+x2, df2)\n\n\nsummary(model_1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30816 -0.09388  0.03469  0.08163  0.24898 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.07143    0.19695   5.440 0.001602 ** \nx1           1.02347    0.06973  14.677 6.28e-06 ***\nx2           1.11939    0.12937   8.652 0.000131 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1823 on 6 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9968 \nF-statistic:  1242 on 2 and 6 DF,  p-value: 1.4e-08\n\n\n\ndf2_2 <- rbind(df2[,2:3], c(4,0))\n\n\ndf2_2$y <- predict(model_1, newdata = df2_2)\n\nthis is the predicted line of multiple linear regression\n\nggplot(data = df2, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') +\n  geom_line(color='darkorange',data = df2_2, aes(x=x1, y=y))+\n  geom_vline(xintercept = 4, lty=2, col='red')+\n  theme_bw()\n\n\n\n\n\n\n(3)\n위의 (2)에 있는 모형에서 \\(\\beta_2\\) 의 90% 신뢰구간을 구하고, 그 의미를 해석하시오.\nanswer\n\nt(x_sp)%*%x_sp\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     9 35 7\n    3516943\n     7 4315\n\n\n\n\n\nsolve(t(x_sp)%*%x_sp)\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     1.1666667-0.3809524 0.5476190\n    -0.3809524 0.1462585-0.2414966\n     0.5476190-0.2414966 0.5034014\n\n\n\n\n\\((x^\\top x)^{-1}_{(3,3)}\\)\n\nsolve(t(x_sp)%*%x_sp)[3,3]\n\n0.503401360544218\n\n\n\\(MSE \\sim \\sigma^2\\)\n\nSSE_sp <- sum((df2$y - x_sp%*%betahat_sp)**2)\nSSE_sp\n\n0.199489795918367\n\n\n\nMSE_sp <- SSE_sp/(9-2-1)\nMSE_sp\n\n0.0332482993197279\n\n\n\\(var(\\beta_2) = (x^\\top x)^{-1}_{(3,3)} \\sigma^2\\)\n\nhat_var_b2_sp <- solve(t(x_sp)%*%x_sp)[3,3] * MSE_sp\nhat_var_b2_sp\n\n0.0167372391133324\n\n\n\nsqrt(hat_var_b2_sp)\n\n0.129372482056009\n\n\n\\(\\beta_2 \\pm t_{0.95}(6) se(\\hat{\\beta}_2)\\)\n\nbetahat_sp[3] - qt(0.95,6) * sqrt(hat_var_b2_sp)\n\n0.867993699129489\n\n\n\nbetahat_sp[3] + qt(0.95,6) * sqrt(hat_var_b2_sp)\n\n1.3707818110746\n\n\n\\(\\beta_2\\)는 기울기의 차이이다.\n90% 시뢰구긴이 0을 포함하지 않고 모두 양수이므로, \\(\\beta_2 \\neq 0\\)이라고 90% 확신할 수 있다.\n즉 구간별 회귀직선의 기울기는 다르다고 할 수 있다.(유의수준 0.1)\n\nR code\n\nsummary(model_1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30816 -0.09388  0.03469  0.08163  0.24898 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.07143    0.19695   5.440 0.001602 ** \nx1           1.02347    0.06973  14.677 6.28e-06 ***\nx2           1.11939    0.12937   8.652 0.000131 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1823 on 6 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9968 \nF-statistic:  1242 on 2 and 6 DF,  p-value: 1.4e-08\n\n\n\nanova(model_1)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1180.086940780.08694072408.753004.798337e-09\n    x21 2.4891251 2.4891251  74.864731.313702e-04\n    Residuals6 0.1994898 0.0332483        NA          NA\n\n\n\n\n\nconfint(model_1, level=0.90)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)0.68871731.454140\n    x10.88796341.158975\n    x20.86799371.370782"
  },
  {
    "objectID": "posts/ml/index.html",
    "href": "posts/ml/index.html",
    "title": "Special Topics in Machine Learning",
    "section": "",
    "text": "Those are posts of Special Topics in Machine Learning."
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "교수님 풀이, 안 건들임"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#이미지자료분석",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#이미지자료분석",
    "title": "Assignment 1",
    "section": "1. 이미지자료분석",
    "text": "1. 이미지자료분석\n아래를 이용하여 MNIST_SAMPLE 이미지 자료를 다운로드 받고 dls오브젝트를 만들어라.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\ndls = ImageDataLoaders.from_folder(path,suffle=False) \n\n\ndls.show_batch()\n\n\n\n\n(1) cnn_learner를 이용하여 lrnr 오브젝트를 생성하라. - arch 는 resnet34 로 설정할 것 - metrics 는 error_rate 로 설정할 것\n(풀이)\n\nlrnr = cnn_learner(dls, arch = resnet34, metrics=error_rate)\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n(2) fine_tune 을 이용하여 lrnr 오브젝트를 학습하라.\n(풀이)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.282870\n      0.150136\n      0.049068\n      00:05\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.042991\n      0.017522\n      0.006379\n      00:05\n    \n  \n\n\n\n(3) 아래를 이용하여 X,y를 만들어라.\nX,y = dls.one_batch()\nX,y의 shape을 조사하라. X에는 몇개의 이미지가 있는가? 이미지의 size는 얼마인가?\n(풀이)\n\nX,y = dls.one_batch()\nX.shape\n\ntorch.Size([64, 3, 28, 28])\n\n\nX에는 64개의 이미지가 있고 크기는 (28,28) 이다.\n(4) 아래의 코드를 이용하여 X의 두번째 이미지가 어떠한 숫자를 의미하는지 확인하라. (그림보고 3인지 7인지 확인하여 답을 쓸 것)\nshow_image(X[0])\n그리고 show_image가 정의된 파일의 경로를 확인하고 show_image가 python 내장함수 인지, torch에서 지원하는 함수인지 fastai에서 지원하는 함수인지 파악하라.\n(풀이)\n\nshow_image(X[1]) # 두번째 이미지 \n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image?\n\n\nSignature:\nshow_image(\n    im,\n    ax=None,\n    figsize=None,\n    title=None,\n    ctx=None,\n    cmap=None,\n    norm=None,\n    *,\n    aspect=None,\n    interpolation=None,\n    alpha=None,\n    vmin=None,\n    vmax=None,\n    origin=None,\n    extent=None,\n    interpolation_stage=None,\n    filternorm=True,\n    filterrad=4.0,\n    resample=None,\n    url=None,\n    data=None,\n    **kwargs,\n)\nDocstring: Show a PIL or PyTorch image on `ax`.\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/torch_core.py\nType:      function\n\n\n\n\n\nfastai에서 지원하는 함수\n\n(5) lrnr 오브젝트를 이용하여 AI가 X[0]을 어떤 값으로 판단하는지 확인하라. 올바르게 판단하였는가? 올바르게 판단했다면 몇 프로의 확신으로 판단하였는가? <– 문제가 의도한 것과 다르게 만들어졌어요\n(풀이)\n\nshow_image(X[0]) # 첫번째 이미지\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nlrnr.model(X[0].reshape(1,3,28,28))\n\nTensorBase([[ 3.4148, -5.0356]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nimport numpy as np\na=np.exp(3.4148)\nb=np.exp(-5.0356)\nprint('3일확률',a/(a+b))\nprint('7일확률',b/(a+b))\n\n3일확률 0.9997862308347155\n7일확률 0.0002137691652844868\n\n\n\n원래문제의도: lrnr.predict(X[0].to(\"cpu\"))"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#추천시스템",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#추천시스템",
    "title": "Assignment 1",
    "section": "2. 추천시스템",
    "text": "2. 추천시스템\n아래를 이용하여 rcmd_anal.csv 를 다운로드 받고 dls오브젝트를 만들어라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n(1) 73번 유저가 먹은 아이템 및 평점을 출력하는 코드를 작성하라. 이를 기반으로 73번 유저가 어떠한 취향인지 파악하라.\n(풀이)\n\ndf.query('user == 73')\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      720\n      73\n      20\n      3.733853\n      홍차10\n    \n    \n      721\n      73\n      18\n      3.975004\n      홍차8\n    \n    \n      722\n      73\n      9\n      1.119541\n      커피9\n    \n    \n      723\n      73\n      13\n      3.840801\n      홍차3\n    \n    \n      724\n      73\n      2\n      0.943742\n      커피2\n    \n    \n      725\n      73\n      4\n      1.152405\n      커피4\n    \n    \n      726\n      73\n      1\n      0.887292\n      커피1\n    \n    \n      727\n      73\n      7\n      0.947641\n      커피7\n    \n    \n      728\n      73\n      6\n      0.868370\n      커피6\n    \n    \n      729\n      73\n      17\n      3.873590\n      홍차7\n    \n  \n\n\n\n\n\n홍차를 선호\n\n(2) dls와 lrnr 오브젝트를 생성하고 lrnr 오브젝트를 학습하라.\n(풀이)\n\ndls = CollabDataLoaders.from_df(df)\nlrnr = collab_learner(dls,y_range=(0,5))\n\n\nlrnr.fit(50)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.337114\n      2.258755\n      00:00\n    \n    \n      1\n      2.328897\n      2.254714\n      00:00\n    \n    \n      2\n      2.320246\n      2.237874\n      00:00\n    \n    \n      3\n      2.300545\n      2.191783\n      00:00\n    \n    \n      4\n      2.265857\n      2.104007\n      00:00\n    \n    \n      5\n      2.207397\n      1.966761\n      00:00\n    \n    \n      6\n      2.123599\n      1.783263\n      00:00\n    \n    \n      7\n      2.008980\n      1.562448\n      00:00\n    \n    \n      8\n      1.865242\n      1.317642\n      00:00\n    \n    \n      9\n      1.697832\n      1.068948\n      00:00\n    \n    \n      10\n      1.515044\n      0.833239\n      00:00\n    \n    \n      11\n      1.326496\n      0.625003\n      00:00\n    \n    \n      12\n      1.139156\n      0.453686\n      00:00\n    \n    \n      13\n      0.962462\n      0.320953\n      00:00\n    \n    \n      14\n      0.802481\n      0.223124\n      00:00\n    \n    \n      15\n      0.662327\n      0.155420\n      00:00\n    \n    \n      16\n      0.542384\n      0.110662\n      00:00\n    \n    \n      17\n      0.442099\n      0.082435\n      00:00\n    \n    \n      18\n      0.359706\n      0.064858\n      00:00\n    \n    \n      19\n      0.292656\n      0.054441\n      00:00\n    \n    \n      20\n      0.238817\n      0.048325\n      00:00\n    \n    \n      21\n      0.195901\n      0.045092\n      00:00\n    \n    \n      22\n      0.161955\n      0.043386\n      00:00\n    \n    \n      23\n      0.135049\n      0.042616\n      00:00\n    \n    \n      24\n      0.113653\n      0.042549\n      00:00\n    \n    \n      25\n      0.096877\n      0.042678\n      00:00\n    \n    \n      26\n      0.083618\n      0.043010\n      00:00\n    \n    \n      27\n      0.073081\n      0.043308\n      00:00\n    \n    \n      28\n      0.064768\n      0.043905\n      00:00\n    \n    \n      29\n      0.058133\n      0.044605\n      00:00\n    \n    \n      30\n      0.053050\n      0.044990\n      00:00\n    \n    \n      31\n      0.048904\n      0.045569\n      00:00\n    \n    \n      32\n      0.045665\n      0.045833\n      00:00\n    \n    \n      33\n      0.043033\n      0.045906\n      00:00\n    \n    \n      34\n      0.040883\n      0.046624\n      00:00\n    \n    \n      35\n      0.039263\n      0.046878\n      00:00\n    \n    \n      36\n      0.037608\n      0.047040\n      00:00\n    \n    \n      37\n      0.036450\n      0.047146\n      00:00\n    \n    \n      38\n      0.035638\n      0.047335\n      00:00\n    \n    \n      39\n      0.034883\n      0.047623\n      00:00\n    \n    \n      40\n      0.034177\n      0.048048\n      00:00\n    \n    \n      41\n      0.033486\n      0.047836\n      00:00\n    \n    \n      42\n      0.033047\n      0.048263\n      00:00\n    \n    \n      43\n      0.032634\n      0.048296\n      00:00\n    \n    \n      44\n      0.032165\n      0.048577\n      00:00\n    \n    \n      45\n      0.031884\n      0.048578\n      00:00\n    \n    \n      46\n      0.031517\n      0.048725\n      00:00\n    \n    \n      47\n      0.031158\n      0.048977\n      00:00\n    \n    \n      48\n      0.030711\n      0.048955\n      00:00\n    \n    \n      49\n      0.030465\n      0.049127\n      00:00\n    \n  \n\n\n\n(3) 아래와 같은 데이터 프레임을 생성하고 df_new 에 저장하라.\n\n#collapse\nimport IPython \n_html='<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>user</th>\\n      <th>item</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>73</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>73</td>\\n      <td>2</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>73</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>73</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>73</td>\\n      <td>5</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>73</td>\\n      <td>6</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>73</td>\\n      <td>7</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>73</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>73</td>\\n      <td>9</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>73</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>73</td>\\n      <td>11</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>73</td>\\n      <td>12</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>73</td>\\n      <td>13</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>73</td>\\n      <td>14</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>73</td>\\n      <td>15</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>73</td>\\n      <td>16</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>73</td>\\n      <td>17</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>73</td>\\n      <td>18</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>73</td>\\n      <td>19</td>\\n    </tr>\\n    <tr>\\n      <th>19</th>\\n      <td>73</td>\\n      <td>20</td>\\n    </tr>\\n  </tbody>\\n</table>'\nIPython.display.HTML(_html)\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      73\n      1\n    \n    \n      1\n      73\n      2\n    \n    \n      2\n      73\n      3\n    \n    \n      3\n      73\n      4\n    \n    \n      4\n      73\n      5\n    \n    \n      5\n      73\n      6\n    \n    \n      6\n      73\n      7\n    \n    \n      7\n      73\n      8\n    \n    \n      8\n      73\n      9\n    \n    \n      9\n      73\n      10\n    \n    \n      10\n      73\n      11\n    \n    \n      11\n      73\n      12\n    \n    \n      12\n      73\n      13\n    \n    \n      13\n      73\n      14\n    \n    \n      14\n      73\n      15\n    \n    \n      15\n      73\n      16\n    \n    \n      16\n      73\n      17\n    \n    \n      17\n      73\n      18\n    \n    \n      18\n      73\n      19\n    \n    \n      19\n      73\n      20\n    \n  \n\n\n\n(풀이)\n\ndf_new=pd.DataFrame({'user':[73]*20,'item':range(1,21)})\ndf_new\n\n\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      73\n      1\n    \n    \n      1\n      73\n      2\n    \n    \n      2\n      73\n      3\n    \n    \n      3\n      73\n      4\n    \n    \n      4\n      73\n      5\n    \n    \n      5\n      73\n      6\n    \n    \n      6\n      73\n      7\n    \n    \n      7\n      73\n      8\n    \n    \n      8\n      73\n      9\n    \n    \n      9\n      73\n      10\n    \n    \n      10\n      73\n      11\n    \n    \n      11\n      73\n      12\n    \n    \n      12\n      73\n      13\n    \n    \n      13\n      73\n      14\n    \n    \n      14\n      73\n      15\n    \n    \n      15\n      73\n      16\n    \n    \n      16\n      73\n      17\n    \n    \n      17\n      73\n      18\n    \n    \n      18\n      73\n      19\n    \n    \n      19\n      73\n      20\n    \n  \n\n\n\n\n(4) 아래의 코드를 이용하여 73번 유저의 취향을 파악하라. 73번 유저가 커피3, 커피5를 먹는다면 얼마정도의 평점을 줄 것이라 예측되는가?\n_dl = dls.test_dl(df_new)\nlrnr.get_preds(dl=_dl)\n(풀이)\n\n_dl = dls.test_dl(df_new)\nlrnr.get_preds(dl=_dl)\n\n\n\n\n\n\n\n\n(tensor([0.9698, 1.0314, 1.0191, 1.0177, 1.0122, 0.9323, 1.0513, 1.0184, 1.0316,\n         0.9842, 3.8255, 3.9591, 3.8640, 3.8937, 3.9437, 3.8947, 3.8272, 3.9503,\n         3.8117, 3.8603]),\n None)\n\n\n\n커피3: 1.0191, 커피5: 1.0122"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#시퀀스자료분석",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#시퀀스자료분석",
    "title": "Assignment 1",
    "section": "3. 시퀀스자료분석",
    "text": "3. 시퀀스자료분석\n아래를 이용하여 자료를 다운로드 받아라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-19-human_numbers_100.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      text\n    \n  \n  \n    \n      0\n      0\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1\n      1\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      2\n      2\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      3\n      3\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      4\n      4\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1995\n      1995\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1996\n      1996\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1997\n      1997\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1998\n      1998\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1999\n      1999\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n  \n\n2000 rows × 2 columns\n\n\n\n(1) TextDataLoaders.from_df을 이용하여 dls오브젝트를 만들어라. - is_lm = True 로 설정할 것 - seq_len = 5 로 설정할 것\n(풀이)\n\ndls = TextDataLoaders.from_df(df,is_lm=True,seq_len=5,text_col='text')\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two ,\n      one , two , three\n    \n    \n      1\n      hundred xxbos one , two\n      xxbos one , two ,\n    \n    \n      2\n      one hundred xxbos one ,\n      hundred xxbos one , two\n    \n    \n      3\n      , one hundred xxbos one\n      one hundred xxbos one ,\n    \n    \n      4\n      nine , one hundred xxbos\n      , one hundred xxbos one\n    \n    \n      5\n      ninety nine , one hundred\n      nine , one hundred xxbos\n    \n    \n      6\n      , ninety nine , one\n      ninety nine , one hundred\n    \n    \n      7\n      eight , ninety nine ,\n      , ninety nine , one\n    \n    \n      8\n      ninety eight , ninety nine\n      eight , ninety nine ,\n    \n  \n\n\n\n(2) lrnr 오브젝트를 만들어라. - arch = AWD_LSTM 이용 - metrics = accuracy 이용\n(풀이)\n\nlrnr = language_model_learner(dls, arch= AWD_LSTM, metrics=accuracy)\n\n(3) lrnr오브젝트에서 fine_tune(3) 메소드를 이용하여 모형을 학습하라.\n(풀이)\n\nlrnr.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.534681\n      0.168856\n      0.977650\n      00:49\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.018749\n      0.003256\n      0.999205\n      00:54\n    \n    \n      1\n      0.001580\n      0.002430\n      0.999324\n      00:54\n    \n    \n      2\n      0.000651\n      0.002244\n      0.999315\n      00:54\n    \n  \n\n\n\n(4) ‘one , two ,’ 이후에 이어질 50개의 단어를 생성하라.\n(풀이)\n\nlrnr.predict('one, two,', n_words=50) \n\n\n\n\n\n\n\n\n'one , two , three , four , five , six , seven , eight , nine , ten , eleven , twelve , thirteen , fourteen , fifteen , sixteen , seventeen , eighteen , nineteen , twenty , twenty one , twenty two , twenty three , twenty four , twenty five'\n\n\n(5) ‘twenty , twenty one ,’ 이후에 이어질 50개의 단어를 생성하라.\n(풀이)\n\nlrnr.predict('twenty, twenty one,', n_words=50) \n\n\n\n\n\n\n\n\n'twenty , twenty one , twenty two , twenty three , twenty four , twenty five , twenty six , twenty seven , twenty eight , twenty nine , thirty , thirty one , thirty two , thirty three , thirty four , thirty five , thirty six , thirty seven , thirty eight ,'"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#리눅스명령어",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#리눅스명령어",
    "title": "Assignment 1",
    "section": "4. 리눅스명령어",
    "text": "4. 리눅스명령어\nCollab 에서 (혹은 리눅스기반 서버에서) 아래의 명령어를 순서대로 실행해보라.\n!ls\n!ls -a \n!ls .\n!ls .. \n!ls sample\n!mkdir asdf \n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n!cp 2022-09-08-rcmd_anal.csv ./asdf \n!ls ./asdf \n!rm 2022-09-08-rcmd_anal.csv \n!rm -rf asdf \n각 명령들이 무엇을 의미하는지 간단히 서술하라.\n(풀이)\n!ls - 현재디렉토리 파일+폴더 출력 - !ls . 와 같음 - !ls ./ 와 같음\n!ls -a - 현재디렉토리 파일+폴더 출력, 숨겨진 항목까지 출력\n!ls . - 현재디렉토리 파일+폴더 출력 - !ls 와 같음 - !ls ./ 와 같음\n!ls .. - 현재디렉토리보다 상위디렉토리의 파일+폴더 출력\n!ls sample - 현재디렉토리에 sample 디렉토리 출력 - !ls ./sample 과 같음\n!mkdir asdf - 현재디렉토리에 asdf 폴더 생성 - !mkdir ./asdf 와 같음\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv - url에 있는 파일 다운로드하여 현재디렉토리에 저장\n!cp 2022-09-08-rcmd_anal.csv ./asdf - 2022-09-08-rcmd_anal.csv 파일을 ./asdf 로 복사\n!ls ./asdf - 현재디렉토리에서 asdf 디렉토리의 내용출력 - !ls asdf 와 같음\n!rm 2022-09-08-rcmd_anal.csv - 현재 디렉토리에서 2022-09-08-rcmd_anal.csv 파일삭제; - rm ./2022-09-08-rcmd_anal.csv 와 같음\n!rm -rf asdf - 현재 디렉토리에서 asdf 삭제 (asdf 폴더내에 파일이 존재하면 파일도 같이 삭제) - r은 recursively, f는 force의 약자"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#appendix-ipynb---html-변환",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#appendix-ipynb---html-변환",
    "title": "Assignment 1",
    "section": "Appendix: ipynb -> html 변환",
    "text": "Appendix: ipynb -> html 변환\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-x3HQLeyrS7GLh70Dv_54Yg"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html",
    "href": "posts/ml/2022-11-09-ml-10w.html",
    "title": "RNN (10주차)",
    "section": "",
    "text": "기계학습 특강 (10주차) 11월9일 [순환신경망– abc예제, abdc예제, abcde예제, AbAcAd예제]"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#import",
    "href": "posts/ml/2022-11-09-ml-10w.html#import",
    "title": "RNN (10주차)",
    "section": "import",
    "text": "import\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n선행학습\n\nfor i in '123':\n    print(i)\n\n1\n2\n3\n\n\niterable object?\n\na = 3\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\na__iter__()\n\nNameError: name 'a__iter__' is not defined\n\n\n\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\n이게 없어\n\na = '3'\nfor i in a:\n    print(i)\n\n3\n\n\n\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n이제 있음\n\na = [1,2,3]\nfor i in a:\n    print(i)\n\n1\n2\n3\n\n\n\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na.__iter__()\n\n<list_iterator at 0x7f0f0e2b8bd0>\n\n\n\naa = iter(a)\n\n\naa.__next__()\n\n1\n\n\n\naa.__next__()\n\n2\n\n\n\naa.__next__()\n\n3\n\n\n\naa.__next__()\n\nStopIteration: \n\n\nStopIteration iter 끝내는 옵션\n\na = range(3)\nfor i in a:\n    print(i)\n\n0\n1\n2\n\n\n\naa = a.__iter__()\n\n\naa.__next__()\n\n0\n\n\n\naa.__next__()\n\n1\n\n\n\naa.__next__()\n\n2\n\n\n\naa.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#예비학습-net.parameters의-의미",
    "href": "posts/ml/2022-11-09-ml-10w.html#예비학습-net.parameters의-의미",
    "title": "RNN (10주차)",
    "section": "예비학습: net.parameters()의 의미",
    "text": "예비학습: net.parameters()의 의미\n9월27일 강의노트 중 “net.parameters()의 의미?”를 설명한다.\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[0.7520]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([-0.8206], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n\nnet.parameters() ## generator = \n\n<generator object Module.parameters at 0x7f6e8ac0f9d0>\n\n\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 무슨의미?\n\nfor param in net.parameters():\n    print(param)\n\nParameter containing:\ntensor([[0.7520]], requires_grad=True)\nParameter containing:\ntensor([-0.8206], requires_grad=True)\n\n\n- 그냥 이건 이런느낌인데?\n\nfor param in [net.weight,net.bias]:\n    print(param)\n\nParameter containing:\ntensor([[0.7520]], requires_grad=True)\nParameter containing:\ntensor([-0.8206], requires_grad=True)\n\n\n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트같은 iterable object로 만드는 함수라 이해할 수 있다.\nyhat = net(x)\n꼭 이런 식으로 정의할 필요는 없다\n- 응용예제1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022_backup/master/_notebooks/2022-09-22-regression.csv\") \nx=torch.tensor(df.x).float().reshape(100,1)\ny=torch.tensor(df.y).float().reshape(100,1)\n\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10) ## 이렇게 전달하면 됩니당!!\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = b+ w*x \n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n- 응용예제2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022_backup/master/_notebooks/2022-09-22-regression.csv\") \nx = torch.tensor(df.x).float().reshape(100,1)\ny = torch.tensor(df.y).float().reshape(100,1)\nX = torch.concat([torch.ones_like(x),x],axis=1)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) # What은 iterable 하지 않지만 [What]은 iterable 함\n\n\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = X@What \n    ## step2 \n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\n스스로 학습 (중간고사 대체과제)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하고자한다.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라.\n\nalpha0 = torch.tensor([0.5], requires_grad=True)\nalpha1 = torch.tensor([[0.5]], requires_grad=True)\nbeta0 = torch.tensor([0.7], requires_grad=True)\nbeta1 = torch.tensor([[0.7]], requires_grad=True)\n\n\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([alpha0,alpha1,beta0,beta1], lr=1/10)\n\n\nfor epoc in range(30):\n    ## 1\n    yhat = alpha0 + beta0 + alpha1*x + beta1*x \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nprint(alpha0+beta0)\n\ntensor([3.1279], grad_fn=<AddBackward0>)\n\n\n\n3.14 근처\n\n\nprint(alpha1+beta1)\n\ntensor([[6.0170]], grad_fn=<AddBackward0>)\n\n\n\n6.28 근처"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#define-some-funtions",
    "href": "posts/ml/2022-11-09-ml-10w.html#define-some-funtions",
    "title": "RNN (10주차)",
    "section": "Define some funtions",
    "text": "Define some funtions\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsoft = torch.nn.Softmax(dim=1)"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#exam2-abc",
    "href": "posts/ml/2022-11-09-ml-10w.html#exam2-abc",
    "title": "RNN (10주차)",
    "section": "Exam2: abc",
    "text": "Exam2: abc\n\ndata\n\ntxt = list('abc')*100\ntxt[:10]\n\n['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'a', 'b'], ['b', 'c', 'a', 'b', 'c'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=3,embedding_dim=1),\n    torch.nn.Tanh(),\n    #===#\n    torch.nn.Linear(in_features=1,out_features=3)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nnet[0] = embedding\nnet[1] = tanh\nnet[2] = linear\n- 결과해석\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nplt.plot(hidden[:9],'--o')\n\n\n\n\n\na blue\nb orange\nc green\n\n\nplt.plot(net(x).data[:9],'--o')\n\n\n\n\n\nplt.plot(yhat[:9],'--o')\n\n\n\n\n\n억지로 맞추고있긴한데 파라메터가 부족해보인다.\n\n- 결과시각화1\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\n학습이 제대로 되었다면 0의 결과가 나오지 않지\nnet진행될때 거의 w만 곱한 결과가 나오는 것인데 0이 hidden에서 나오면 w만 곱해도 변화가 없잖아? 그래서 주황색 선이 나온 거고\n\n- 첫번째 그림 \\(\\to\\) 두번빼 그림\n\nnet[2].weight,net[2].bias\n\n(Parameter containing:\n tensor([[-4.6804],\n         [ 0.3071],\n         [ 5.2894]], requires_grad=True),\n Parameter containing:\n tensor([-1.5440,  0.9143, -1.3970], requires_grad=True))\n\n\n\nhidden[:9], (net[-1].weight.data).T, net[-1].bias.data\n\n(tensor([[-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896]]),\n tensor([[-4.6804,  0.3071,  5.2894]]),\n tensor([-1.5440,  0.9143, -1.3970]))\n\n\n\nhidden[:9].shape\n\ntorch.Size([9, 1])\n\n\n\nnet[-1].weight.data.shape\n\ntorch.Size([3, 1])\n\n\n\n(net[-1].weight.data.T).shape\n\ntorch.Size([1, 3])\n\n\n\nhidden[:9]@(net[-1].weight.data).T + net[-1].bias.data\n\ntensor([[-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312]])\n\n\n\n(파랑,주황,초록) 순서로 그려짐\n파랑 = hidden * (-4.6804) + (-1.5440)\n주황 = hidden * (0.3071) + (0.9143)\n초록 = hidden * (5.2894) + (-1.3970)\n\n- 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. - weight: 파랑과 초록을 구분하는 역할을 함 - weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 ’b’라고 나올 확률을 학습시킨다. \\(\\to\\) 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0)\n\n참고: torch.nn.Linear()의 비밀? - 사실 \\({\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf W}\\)와 \\({\\boldsymbol b}\\)가 저장되는게 아니다. - \\({\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf A}\\)와 \\({\\boldsymbol b}\\)가 저장된다. - \\({\\bf W} = {\\bf A}^T\\) 인 관계에 있으므로 l1.weight 가 우리가 생각하는 \\({\\bf W}\\) 로 해석하려면 사실 transpose를 취해줘야 한다.\n왜 이렇게..? - 계산의 효율성 때문 (numpy의 구조를 알아야함) - \\({\\boldsymbol x}\\), \\({\\boldsymbol y}\\) 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음)\n잠깐 딴소리!!\n(예시1)\n\n_arr = np.array(range(4)).reshape(2,2)\n_arr\n\narray([[0, 1],\n       [2, 3]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시2)\n\n_arr = np.array(range(6)).reshape(3,2)\n_arr\n\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시3)\n\n_arr = np.array(range(6)).reshape(2,3)\n_arr\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n_arr.strides\n\n(24, 8)\n\n\n\n아래로 한칸 = 24칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n_arr = np.array(range(4),dtype=np.int8).reshape(2,2)\n_arr\n\narray([[0, 1],\n       [2, 3]], dtype=int8)\n\n\n\n_arr.strides\n\n(2, 1)\n\n\n\n아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n\n_arr = np.array(range(4),dtype=np.float64).reshape(2,2)\n_arr\n\narray([[0., 1.],\n       [2., 3.]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n_arr = np.array(range(4),dtype=np.float32).reshape(2,2)\n_arr\n\narray([[0., 1.],\n       [2., 3.]], dtype=float32)\n\n\n\n_arr.strides\n\n(8, 4)\n\n\n진짜 참고..\n\n1바이트 = 8비트\n1바이트는 2^8=256 의 정보 표현\nnp.int8은 8비트로 정수를 저장한다는 의미\n\n\n2**8\n\n256\n\n\n\nprint(np.array(55,dtype=np.int8))\nprint(np.array(127,dtype=np.int8))\nprint(np.array(300,dtype=np.int8)) # overflow \n\n55\n127\n44\n\n\n딴소리 끝!!\nweight의 transfose가 저장되는 이유 끝!\n혼자 크다고 인식하면 바꾸는…\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([299, 7])\n\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(7), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#exam3-abcd",
    "href": "posts/ml/2022-11-09-ml-10w.html#exam3-abcd",
    "title": "RNN (10주차)",
    "section": "Exam3: abcd",
    "text": "Exam3: abcd\n\ndata\n\ntxt = list('abcd')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'a'], ['b', 'c', 'd', 'a', 'b'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet[0].weight.data = torch.tensor([[-0.3333],[-2.5000],[5.0000],[0.3333]])\n\nnet[-1].weight.data = torch.tensor([[1.5000],[-6.0000],[-2.0000],[6.0000]])\nnet[-1].bias.data = torch.tensor([0.1500, -2.0000,  0.1500, -2.000])\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 9])\n\n\n\nplt.matshow(combined[:15],vmin=-15,vmax=15,cmap='bwr')\nplt.xticks(range(9), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n맞춘게 아니야. 0 근처인게 있잖아 hidden에서\n\n\n두개의 은닉노드를 이용한 풀이 – 깔끔한 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\nhidden layer 2장만으로 4가지 state 표현"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#exam4-abcde-스스로-공부",
    "href": "posts/ml/2022-11-09-ml-10w.html#exam4-abcde-스스로-공부",
    "title": "RNN (10주차)",
    "section": "Exam4: abcde (스스로 공부)",
    "text": "Exam4: abcde (스스로 공부)\n\ndata\n주어진 자료가 다음과 같다고 하자.\n\ntxt = list('abcde')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'a'])\n\n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)\n\n\n3개의 은닉노드를 이용한 풀이\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. - 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. (\\(2^1\\)) - 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. (\\(2^2\\)) - 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음. (\\(2^3\\))\n\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0]))\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5,embedding_dim=3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=3,out_features=5)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([499, 13])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-10w.html#exam5-abacad",
    "href": "posts/ml/2022-11-09-ml-10w.html#exam5-abacad",
    "title": "RNN (10주차)",
    "section": "Exam5: AbAcAd",
    "text": "Exam5: AbAcAd\n\ndata\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\n\n두개의 은닉노드를 이용한 풀이 – 실패\n- 데이터정리\n\nmapping = {'A':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 2, 0]), tensor([1, 0, 2, 0, 3]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([599, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\nA 뒤는 a,b,c 3개 중 하나, 따라서 1/3 의 확률이겠네 하고 학습해버렸다.\nobservation 마다 봐야하는 x가 달라(수가 )\n실패\n\n- 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다.\n\n\n\nX\ny\n\n\n\n\nA,b\nA\n\n\nb,A\nc\n\n\nA,c\nA\n\n\nc,A\nd\n\n\nA,d\nA\n\n\nd,A\nb\n\n\nA,b\nA\n\n\nb,A\nc\n\n\n…\n…\n\n\n\n- 순진한 접근방식의 비판: - 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조 - 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. - 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름.\n이것에 대한 해결책은 순환신경망이다.\n\n\n순환망을 위하여 data 다시정리\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx[:8],y[:8]\n\n(tensor([0, 1, 0, 2, 0, 3, 0, 1]), tensor([1, 0, 2, 0, 3, 0, 1, 0]))\n\n\n- 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n숫자형태의 벡터형태라고 가정하고 학습하는 순환신경망\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))\n\n\n\n\n실패했던 풀이의 재구현1\n- 방금 실패한 풀이\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n- Tanh까지만 클래스로 바꾸어서 구현 - 클래스를 이용하는 방법: https://guebin.github.io/DL2022/2022/11/01/(9주차)-11월1일.html#로지스틱-모형을-이용한-풀이\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self,x):\n        hidden = self.tanh(self.i2h(x))\n        return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회반복\n\nfor epoc in range(20): \n    ## 1 \n    ## 2 \n    hidden = hnet(x) \n    output = linr(hidden)\n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n\n실패했던 풀이의 재구현2\n- Tanh까지 구현한 클래스\n\n#\n# class Hnet(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n#         self.tanh = torch.nn.Tanh()\n#     def forward(self,x):\n#         hidden = self.tanh(self.i2h(x))\n#         return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회 반복\n\nx[0].shape\n\ntorch.Size([4])\n\n\n\nx[[0]].shape\n\ntorch.Size([1, 4])\n\n\n\nT = len(x) \nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n4개가 필요한 원핫인코딩과 달리 hidden layer는 2개만으로 4state 표현했다\n\n\n\n순환신경망의 아이디어\n\n모티브\n(예비생각1) \\({\\boldsymbol h}\\)에 대한 이해\n\\({\\boldsymbol h}\\)는 사실 문자열 ’abcd’들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) - (why1) h는 “학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태”라고 이해가능 - (why2) 실제로 예시를 살펴보면 그러했다.\n결론: 사실 \\({\\boldsymbol h}\\)는 잘 숙성되어있는 입력정보 \\({\\bf X}\\) 그 자체로 해석 할 수 있다.\n(예비생각2) 수백년전통을 이어가는 방법\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬\n* 기존방식 - \\(\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}\\)\n* 수백년 전통의 간장맛을 유지하는 방식\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3\\)\n\n* 수백년 전통의 간장맛을 유지하면서 조리를 한다면?\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n점점 맛있는 간장계란밥이 탄생함\n* 알고리즘의 편의상 아래와 같이 생각해도 무방\n\n\\(\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\), \\(\\text{간장}_0=\\text{맹물}\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n아이디어\n* 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로?\n\n\\(\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1\\)\n\\(\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2\\)\n\\(\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3\\)\n\n이제 우리가 배울것은 (1) “\\(\\text{콩물}_{t}\\)”와 “\\(\\text{간장}_{t-1}\\)”로 “\\(\\text{간장}_t\\)”를 숙성하는 방법 (2) “\\(\\text{간장}_t\\)”로 “\\(\\text{간장계란밥}_t\\)를 조리하는 방법이다\n즉 숙성담당 네트워크와 조리담당 네트워크를 각각 만들어 학습하면 된다.\n\n\n알고리즘\n세부적인 알고리즘 (\\(t=0,1,2,\\dots\\)에 대하여 한줄 한줄 쓴 알고리즘)\n\n\\(t=0\\)\n\n\\({\\boldsymbol h}_0=[[0,0]]\\) <– \\(\\text{간장}_0\\)은 맹물로 초기화\n\n\\(t=1\\)\n\n\\({\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\) - \\({\\boldsymbol x}_1\\): (1,4) - \\({\\bf W}_{ih}\\): (4,2) - \\({\\boldsymbol h}_0\\): (1,2) - \\({\\bf W}_{hh}\\): (2,2) - \\({\\boldsymbol b}_{ih}\\): (1,2) - \\({\\boldsymbol b}_{hh}\\): (1,2)\n\\({\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)\\)\n\n\\(t=2\\) <– 여기서부터는 \\(t=2\\)와 비슷\n\n\n좀 더 일반화된 알고리즘\n(ver1)\ninit \\(\\boldsymbol{h}_0\\)\nfor \\(t\\) in \\(1:T\\)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\)\n\\({\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)\\)\n\n(ver2)\ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden)) # $H_{t-1}$\n    output = linr(hidden)\n    yt_hat = soft(output)\n\n코드상으로는 \\(h_t\\)와 \\(h_{t-1}\\)의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n\n전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음\n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)\n\n\n\n순환신경망 구현1 – 성공\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f0f04614dd0>\n\n\n\n\n\n\n아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까..\n\n\n\n순환신경망 구현2 (with RNNCell) – 성공\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n(1) 숙성네트워크\n선언\n\nrnncell = torch.nn.RNNCell(4,2)\n\n가중치초기화 (순환신경망 구현1과 동일하도록)\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell()\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data \nrnncell.weight_hh.data = _rnncell.h2h.weight.data \nrnncell.bias_hh.data = _rnncell.h2h.bias.data \nrnncell.bias_ih.data = _rnncell.i2h.bias.data \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f0f0cfe77d0>\n\n\n\n\n\n\n\n순환신경망 구현3 (with RNN) – 성공\n(예비학습)\n- 아무리 생각해도 yhat구하려면 좀 귀찮음\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nsoft(cook(hidden))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2) \n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.bias_hh_l0.data = rnncell.bias_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data \nrnn.bias_ih_l0.data = rnncell.bias_ih.data \n\n\n_water\n\ntensor([[0., 0.]])\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\n똑같음!\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water)\n\n(tensor([[-0.2232,  0.9769],\n         [-0.9999, -0.9742],\n         [ 0.9154,  0.9992],\n         ...,\n         [ 0.9200,  0.9992],\n         [-0.9978, -0.0823],\n         [-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>))\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\ntorch.manual_seed(43052)\nrnn = torch.nn.RNN(4,2)\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4)\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters())) # 우리가 배울것: 숙성하는 방법 + 요리하는 방법 \n\n(4) 학습\n\nfor epoc in range(5000):\n    ## 1\n    _water = torch.zeros(1,2)\n    hidden, _ = rnn(x,_water)\n    output = cook(hidden)\n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nyhat = soft(cook(rnn(x,_water)[0]))\nyhat\n\ntensor([[1.9725e-02, 1.5469e-03, 8.2766e-01, 1.5106e-01],\n        [9.1875e-01, 1.6513e-04, 6.7703e-02, 1.3384e-02],\n        [2.0031e-02, 1.0659e-03, 8.5248e-01, 1.2642e-01],\n        ...,\n        [1.9640e-02, 1.3568e-03, 8.3705e-01, 1.4196e-01],\n        [9.9564e-01, 1.3114e-05, 3.5069e-03, 8.4108e-04],\n        [3.5473e-03, 1.5670e-01, 1.4102e-01, 6.9873e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[:15])\n\n<matplotlib.image.AxesImage at 0x7f0f159c0a10>"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html",
    "href": "posts/ml/2022-10-12-ml-6w.html",
    "title": "DNN (6주차)",
    "section": "",
    "text": "기계학습 특강 (6주차) 10월5일 [딥러닝의 기초 - 깊은신경망(2)– 시벤코정리, 신경망의표현, CPU vs GPU, 확률적경사하강법, 오버피팅]"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#imports",
    "href": "posts/ml/2022-10-12-ml-6w.html#imports",
    "title": "DNN (6주차)",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport torchvision\nfrom fastai.data.all import *\nimport matplotlib.pyplot as plt\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');\n\n\n#hide\ngraphviz.set_jupyter_format('png')\n\n'png'"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#시벤코정리",
    "href": "posts/ml/2022-10-12-ml-6w.html#시벤코정리",
    "title": "DNN (6주차)",
    "section": "시벤코정리",
    "text": "시벤코정리\n\n지난시간 논리전개\n- 아이디어: linear -> relu -> linear (-> sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다. - 아이디어의 실용성: 실제자료에서 꺾은선으로 표현되는 underlying은 몇개 없을 것 같음. 그건 맞는데 꺾이는 점을 많이 설정하면 얼추 비슷하게는 “근사” 시킬 수 있음. - 아이디어의 확장성: 이러한 논리전개는 X:(n,2)인 경우도 가능했음. (이 경우 꺾인선은 꺾인평면이 된다) - 아이디어에 해당하는 용어정리: 이 구조가 x->y 로 바로 가는 것이 아니라 x->(u1->v1)->(u2->v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. (이 용어는 이따가..)\n\n\n시벤코정리\nuniversal approximation thm: (범용근사정리,보편근사정리,시벤코정리), 1989\n\n하나의 은닉층을 가지는 “linear -> sigmoid -> linear” 꼴의 네트워크를 이용하여 세상에 존재하는 모든 (다차원) 연속함수를 원하는 정확도로 근사시킬 수 있다. (계수를 잘 추정한다면)\n\n- 사실 엄청 이해안되는 정리임. 왜냐햐면, - 그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아니야? - 요즘은 “linear -> sigmoid -> linear” 가 아니라 “linear -> relu -> linear” 조합으로 많이 쓰던데? - 요즘은 하나의 은닉층을 포함하는 네트워크는 잘 안쓰지 않나? 은닉층이 여러개일수록 좋다고 어디서 본 것 같은데?\n- 약간의 의구심이 있지만 아무튼 universal approximation thm에 따르면 우리는 아래와 같은 무기를 가진 꼴이 된다. - 우리의 무기: \\({\\bf X}: (n,p)\\) 꼴의 입력에서 \\({\\bf y}:(n,1)\\) 꼴의 출력으로 향하는 맵핑을 “linear -> relu -> linear”와 같은 네트워크를 이용해서 “근사”시킬 수 있다.\n(서연 필기) 한 층만 있어도 노드가 충분히 크면 은닉층 한 층으로 충분히 맞출 수 있다."
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#시벤코정리-proof",
    "href": "posts/ml/2022-10-12-ml-6w.html#시벤코정리-proof",
    "title": "DNN (6주차)",
    "section": "시벤코정리 proof",
    "text": "시벤코정리 proof\n\n그림으로 보는 증명과정\n- 데이터\n\nx = torch.linspace(-10,10,200).reshape(-1,1)\n\n- 아래와 같은 네트워크를 고려하자.\n\nl1 = torch.nn.Linear(in_features=1,out_features=2)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=2,out_features=1)\n\n- 직관1: \\(l_1\\),\\(l_2\\)의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다.\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+10.00,+10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,color='C2'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$')\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ \\\\l_1)(x)$')\n\n\n\n\n\n- 직관2: 아래들도 가능할듯?\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data,'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\n\n\n\n\n(서연 필기) 밑에 fig 다시 정의 안 해줬잖아. 그러니까 덮어쓴 거라 생각하면 돼\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+0.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nax[0].plot(x,l1(x).data,'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nfig\n\n\n\n\n- 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 \\((l_2\\circ a_1 \\circ l_1)(x)\\)의 결과로 주황색선 + 파란색선도 가능할 것 같다. \\(\\to\\) 실제로 가능함\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\n\nplt.plot(l2(a1(l1(x))).data)\n\n\n\n\n- 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 \\(h\\)를 만들 수 있다.\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n- 위와 같은 함수 \\(h\\)를 활성화함수로 하고 \\(m\\)개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- \\(h(x)\\)를 활성화함수로 가지는 네트워크를 설계하여 보자.\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) # activation 의 출력 \n\nforward 순전파\nbackward 역전파\n\na1=MyActivation()\n# a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation()\n\n\nplt.plot(x,a1(x)) \n\n\n\n\n히든레이어가 1개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1),\n            MyActivation(),\n            torch.nn.Linear(1,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 2개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2),\n            MyActivation(),\n            torch.nn.Linear(2,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 3개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 1024개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2048),\n            MyActivation(),\n            torch.nn.Linear(2048,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#시벤코정리-활용",
    "href": "posts/ml/2022-10-12-ml-6w.html#시벤코정리-활용",
    "title": "DNN (6주차)",
    "section": "시벤코정리 활용",
    "text": "시벤코정리 활용\n- 아래와 같이 하나의 은닉층을 가지고 있더라도 많은 노드수만 보장되면 매우 충분한 표현력을 가짐\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n예제1 (sin, exp)\n\ntorch.manual_seed(43052)\nx = torch.linspace(-10,10,200).reshape(-1,1)\nunderlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x)\neps = torch.randn(200).reshape(-1,1)*0.1\ny = underlying + eps \nplt.plot(x,y,'o',alpha=0.5)\nplt.plot(x,underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\nmseloss쓴 거 확인\n\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(x,underlying,lw=3)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제2 (스펙높아도 취업X)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nx = torch.tensor(df.x).reshape(-1,1).float()\ny = torch.tensor(df.y).reshape(-1,1).float()\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(df.x,df.underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\ntorch.manual_seed(43052)\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\nBCEloss쓴 거 확인\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(df.x,df.underlying,lw=3)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제3 (MNIST data with DNN)\n\n# 예비학습\n(예비학습1) Path\n\npath = untar_data(URLs.MNIST) \npath\n\nPath('/home/csy/.fastai/data/mnist_png')\n\n\n\npath 도 오브젝트임\npath 도 정보+기능이 있음\n\n- path의 정보\n\npath._str # 숨겨놓았네?\n\n'/home/csy/.fastai/data/mnist_png'\n\n\n- 기능1\npath는 객체,\n\npath.ls()\n\n(#2) [Path('/home/csy/.fastai/data/mnist_png/training'),Path('/home/csy/.fastai/data/mnist_png/testing')]\n\n\npath object의 list 보여주는 역할\n- 기능2\n\npath/'training'\n\nPath('/home/csy/.fastai/data/mnist_png/training')\n\n\n\npath/'testing'\n\nPath('/home/csy/.fastai/data/mnist_png/testing')\n\n\n- 기능1과 기능2의 결합\n\n(path/'training/3').ls()\n\n(#6131) [Path('/home/csy/.fastai/data/mnist_png/training/3/35407.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/26671.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/16171.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/15346.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/34710.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/48873.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/28796.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/15651.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/6894.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/37927.png')...]\n\n\n\n! ls /home/csy/.fastai/data/mnist_png\n\ntesting  training\n\n\n\n! ls /home/csy/.fastai/data/mnist_png/training\n\n0  1  2  3  4  5  6  7  8  9\n\n\n\n! ls /home/csy/.fastai/data/mnist_png/testing\n\n0  1  2  3  4  5  6  7  8  9\n\n\n\n‘/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’ 이 파일을 더블클릭하면 이미지가 보인단 말임\n\n(예비학습2) plt.imshow\n\nimgtsr = torch.tensor([[1.0,2],[2.0,4.0]])\nimgtsr\n\ntensor([[1., 2.],\n        [2., 4.]])\n\n\n\nplt.imshow(imgtsr,cmap='gray')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fa03ee40f10>\n\n\n\n\n\n\nimgtsr = torch.tensor([0.1,0.2,0.3,0.4]).reshape(2,2)\nimgtsr\n\ntensor([[0.1000, 0.2000],\n        [0.3000, 0.4000]])\n\n\n\nplt.imshow(imgtsr,cmap='gray')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fa03ebb70a0>\n\n\n\n\n\n(예비학습3) torchvision\n- ’/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’의 이미지파일을 torchvision.io.read_image 를 이용하여 텐서로 만듬\n!ls '/home/csy/.fastai/data/mnist_png/training/3'\n\nimgtsr = torchvision.io.read_image('/home/csy/.fastai/data/mnist_png/training/3/37912.png')\nimgtsr\n\ntensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66, 138,\n          149, 180, 138, 138,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  22, 162, 161, 228, 252, 252,\n          253, 252, 252, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 253, 252, 252, 252, 189,\n          184, 110, 119, 252, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  74, 161, 160,  77,  45,   4,\n            0,   0,  70, 252, 210,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,  22, 205, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0, 162, 253, 245,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           36, 219, 252, 139,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          222, 252, 202,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,\n          253, 252,  89,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 240,\n          253, 157,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 160, 253,\n          231,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 142, 252, 252,\n           42,  30,  78, 161,  36,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          185, 228, 252, 252, 168,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          253, 252, 252, 252, 116,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 179, 252,\n          253, 252, 252, 210,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22,\n          255, 253, 215,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  34,  89, 244,\n          253, 223,  98,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 123, 142, 234, 252, 252,\n          184,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 230, 253, 252, 252, 252, 168,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 126, 253, 252, 168,  43,   2,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n       dtype=torch.uint8)\n\n\n- 이 텐서는 (1,28,28)의 shape을 가짐\n\nimgtsr.shape\n\ntorch.Size([1, 28, 28])\n\n\n- imgtsr를 plt.imshow 로 시각화\n\nplt.imshow(imgtsr.reshape(28,28),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fa03e223370>\n\n\n\n\n\n\n진짜 숫자3이 있음\n\n\n\n# 데이터정리\n- 데이터정리\n\nthrees_fnames = (path/'training/3').ls()\nsevens_fnames = (path/'training/7').ls()\nlen(threes_fnames),len(sevens_fnames)\n\n(6131, 6265)\n\n\n\n6131, 1, 28, 28\n6265, 1, 28, 28\n\n\ntorch.stack([torchvision.io.read_image(str(threes_fnames[i])) for i in [0,1]]).shape\n\ntorch.Size([2, 1, 28, 28])\n\n\n\ntorch.stack([torchvision.io.read_image(str(fn)) for fn in threes_fnames]).shape\n\ntorch.Size([6131, 1, 28, 28])\n\n\n\ntorch.stack([torchvision.io.read_image(str(fn)) for fn in sevens_fnames]).shape\n\ntorch.Size([6265, 1, 28, 28])\n\n\n\nX3 = torch.stack([torchvision.io.read_image(str(threes_fnames[i])) for i in range(6131)])\nX7 = torch.stack([torchvision.io.read_image(str(sevens_fnames[i])) for i in range(6265)])\n\n\nX3.shape,X7.shape\n\n(torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28]))\n\n\n\nlen(threes_fnames) + len(sevens_fnames)\n\n12396\n\n\n\nX=torch.concat([X3,X7])\nX.shape\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nXnp = X.reshape(-1,1*28*28).float() # Xnp = X.reshape(-1,784).float()\nXnp.shape\n\ntorch.Size([12396, 784])\n\n\n\\(\\star\\) float형으로 바꿔주기\n\ny = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1) \ny.shape\n\ntorch.Size([12396, 1])\n\n\n\ny = torch.tensor([0.0]*len(threes_fnames) + [1.0]*len(sevens_fnames)).reshape(-1,1) \ny.shape\n\ntorch.Size([12396, 1])\n\n\n\nplt.plot(y,'o')\n\n\n\n\n\n“y=0”은 숫자3을 의미, “y=1”은 숫자7을 의미\n숫자3은 6131개, 숫자7은 6265개 있음\n\n\n\n# 학습\n- 네트워크의 설계\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1*28*28,out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\n\n\n\n\n\nfor epoc in range(200):\n    ## 1\n    yhat = net(Xnp) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\n\n\n\n\n\n대부분 잘 적합되었음"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#신경망의-표현-boldsymbol-x-to-hatboldsymbol-y-로-가는-과정을-그림으로-표현",
    "href": "posts/ml/2022-10-12-ml-6w.html#신경망의-표현-boldsymbol-x-to-hatboldsymbol-y-로-가는-과정을-그림으로-표현",
    "title": "DNN (6주차)",
    "section": "신경망의 표현 (\\({\\boldsymbol x} \\to \\hat{\\boldsymbol y}\\) 로 가는 과정을 그림으로 표현)",
    "text": "신경망의 표현 (\\({\\boldsymbol x} \\to \\hat{\\boldsymbol y}\\) 로 가는 과정을 그림으로 표현)\n\n예제1: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- 모든 observation과 가중치를 명시한 버전\n(표현1)\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xₙ\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xₙ*ŵ₁,    bias=False\" -> \"ŷₙ\"[label=\"sigmoid\"]\n\n    \".\" -> \"....................................\"[label=\"* ŵ₀\"]\n    \"..\" -> \"....................................\"[label=\"* ŵ₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₂\" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₂*ŵ₁,    bias=False\" -> \"ŷ₂\"[label=\"sigmoid\"]\n    \n    \"1  \" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₁\" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₁*ŵ₁,    bias=False\" -> \"ŷ₁\"[label=\"sigmoid\"]\n''')\n\n\n\n\n\n단점: 똑같은 그림의 반복이 너무 많음\n\n- observation 반복을 생략한 버전들\n(표현2) 모든 \\(i\\)에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다.\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xᵢ\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xᵢ*ŵ₁,    bias=False\" -> \"ŷᵢ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n(표현3) 그런데 (표현2)에서 아래와 같이 \\(x_i\\), \\(y_i\\) 대신에 간단히 \\(x\\), \\(y\\)로 쓰는 경우도 많음\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x*ŵ₁,    bias=False\" -> \"ŷ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n- 1을 생략한 버전들\n(표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음\n\n#collapse\ngv('''\n\"x\" -> \"x*ŵ₁,    bias=True\"[label=\"*ŵ₁\"] ;\n\"x*ŵ₁,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n(표현4의 수정) \\(\\hat{w}_1\\)대신에 \\(\\hat{w}\\)를 쓰는 것이 더 자연스러움\n\n#collapse\ngv('''\n\"x\" -> \"x*ŵ,    bias=True\"[label=\"*ŵ\"] ;\n\"x*ŵ,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n(표현5) 선형변환의 결과는 아래와 같이 \\(u\\)로 표현하기도 한다.\n\n#collapse\ngv('''\n\"x\" -> \"u\";\n\"u\" -> \"y\"[label=\"sigmoid\"] ''')\n\n\n\n\n\n다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요.\n\n\n\n예제2: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n참고: 코드로 표현\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n- 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자.\n\n(강의노트의 표현)\n\n#collapse\ngv('''\n\"x\" -> \" -x\"[label=\"*(-1)\"];\n\"x\" -> \" x\"[label=\"*1\"]\n\" x\" -> \"rlu(x)\"[label=\"relu\"] \n\" -x\" -> \"rlu(-x)\"[label=\"relu\"] \n\"rlu(x)\" -> \"u\"[label=\"*(-4.5)\"] \n\"rlu(-x)\" -> \"u\"[label=\"*(-9.0)\"] \n\"u\" -> \"sig(u)=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n(좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다.\n\n#collapse\ngv('''\n\"x\" -> \"u1[:,0]\"[label=\"*(-1)\"];\n\"x\" -> \"u1[:,1]\"[label=\"*1\"]\n\"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"] \n\"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"] \n\"v1[:,0]\" -> \"u2\"[label=\"*(-9.0)\"] \n\"v1[:,1]\" -> \"u2\"[label=\"*(-4.5)\"] \n\"u2\" -> \"v2=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n* Layer의 개념: \\({\\bf X}\\)에서 \\(\\hat{\\boldsymbol y}\\)로 가는 과정은 “선형변환+비선형변환”이 반복되는 구조이다. “선형변환+비선형변환”을 하나의 세트로 보면 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n이것을 다이어그램으로 표현한다면 아래와 같다.\n(선형+비선형을 하나의 Layer로 묶은 표현)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"u1[:,0]\"\n    \"X\" -> \"u1[:,1]\"\n    \"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -> \"u2\"\n    \"v1[:,1]\" -> \"u2\"\n    \"u2\" -> \"v2=yhat\"[label=\"sigmoid\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\nLayer를 세는 방법 - 정석: 학습가능한 파라메터가 몇층으로 있는지… - 일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음. - 위의 예제의 경우 number of layer = 2 이다.\n\n사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다..\n\nHidden Layer의 수를 세는 방법 - Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1 - 위의 예제의 경우 number of hidden layer = 1 이다.\n* node의 개념: \\(u\\to v\\)로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음.\n(노드의 개념이 포함된 그림)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat \"\n    \"node2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다.\n(“number of nodes = number of features”로 이해한 그림)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"feature1\"\n    \"X\" -> \"feature2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"feature1\" -> \"yhat \"\n    \"feature2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n\n다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다.\n\n\n\n예제3: \\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n(다이어그램표현)\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Input Layer\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node32\"\n    \"x2\" -> \"node32\"\n    \"..\" -> \"node32\"\n    \"x784\" -> \"node32\"\n\n\n    label = \"Hidden Layer: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    \"...\" -> \"yhat\"\n    \"node32\" -> \"yhat\"\n    \n    label = \"Outplut Layer: sigmoid\"\n}\n''')\n\n\n\n\n\nLayer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함\n\n- 위의 다이어그램에 대응하는 코드\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#cpu-vs-gpu",
    "href": "posts/ml/2022-10-12-ml-6w.html#cpu-vs-gpu",
    "title": "DNN (6주차)",
    "section": "CPU vs GPU",
    "text": "CPU vs GPU\n- 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음)\n\nGPU 사용방법\n- cpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \ny_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \nnet_cpu = torch.nn.Linear(1,1) \n\n연산되게끔 reshape으로 shape 변경해주세요\n\nx_cpu, y_cpu\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]))\n\n\n\nnet_cpu.weight, net_cpu.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n- gpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_gpu = x_cpu.to(\"cuda:0\")\ny_gpu = y_cpu.to(\"cuda:0\")\nnet_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") \n\ncpu있는 자체는 못 넣고\nnet_gpu = net_cpu.to(\"cuda:0\")\ncpu에 있는 net을 가져와서 정의해줘야 한다.\n\n\n_a = torch.nn.Linear(1,1)\n\n\n_a.weight, _a.bias\n\n(Parameter containing:\n tensor([[0.4074]], requires_grad=True),\n Parameter containing:\n tensor([-0.8885], requires_grad=True))\n\n\n\n_a.to(\"cuda:0\") \n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n_a.weight, _a.bias\n\n(Parameter containing:\n tensor([[0.4074]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8885], device='cuda:0', requires_grad=True))\n\n\n\n\nx_gpu, y_gpu\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'))\n\n\n\nnet_gpu.weight, net_gpu.bias\n\n(Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인\n\nx_cpu, y_cpu, net_cpu.weight, net_cpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n\nx_gpu, y_gpu, net_gpu.weight, net_gpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'),\n Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함\n(예시1)\n\nnet_cpu(x_cpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=<AddmmBackward0>)\n\n\n(예시2)\n\nnet_gpu(x_gpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9163]], device='cuda:0', grad_fn=<AddmmBackward0>)\n\n\n(예시3)\n\nnet_cpu(x_gpu) \n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시4)\n\nnet_gpu(x_cpu)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시5)\n\ntorch.mean((y_cpu-net_cpu(x_cpu))**2)\n\ntensor(1.2068, grad_fn=<MeanBackward0>)\n\n\n(예시6)\n\ntorch.mean((y_gpu-net_gpu(x_gpu))**2)\n\ntensor(1.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n\n\n(예시7)\n\ntorch.mean((y_gpu-net_cpu(x_cpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n(예시8)\n\ntorch.mean((y_cpu-net_gpu(x_gpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n둘다 cpu에 있던가 둘 다 gpu에 있던가\n\n\n시간측정 (예비학습)\n\nimport time \n\n\nt1 = time.time()\n\n\nt2 = time.time()\n\n\nt2-t1\n\n0.42019009590148926\n\n\n\n\nCPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\n- for문 준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5373966693878174\n\n\n\n\nGPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\n- for문돌릴준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.4511945247650146\n\n\n\n!! CPU가 더 빠르다?\n\n\n\nCPU vs GPU (20480)\n- CPU (20480)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n3.806452512741089\n\n\n- GPU (20480)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.308497667312622\n\n\n- 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문\n\n\nCPU vs GPU (204800)\n- CPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n65.33969926834106\n\n\n- GPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n2.077875852584839\n\n\n\n!nvidia-smi\n\nWed Oct 12 21:15:05 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n|  0%   41C    P8    28W / 420W |  12812MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A    482816      C   ...onda3/envs/csy/bin/python     4261MiB |\n|    0   N/A  N/A    580526      C   ...onda3/envs/csy/bin/python     3249MiB |\n|    0   N/A  N/A    605977      C   ...onda3/envs/csy/bin/python     2979MiB |\n|    0   N/A  N/A   1035719      C   ...onda3/envs/csy/bin/python     2321MiB |\n+-----------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#확률적경사하강법-배치-에폭",
    "href": "posts/ml/2022-10-12-ml-6w.html#확률적경사하강법-배치-에폭",
    "title": "DNN (6주차)",
    "section": "확률적경사하강법, 배치, 에폭",
    "text": "확률적경사하강법, 배치, 에폭\n\n좀 이상하지 않아요?\n- 우리가 쓰는 GPU: 다나와 PC견적 - GPU 메모리 끽해봐야 24GB\n- 우리가 분석하는 데이터: 빅데이터..?\n- 데이터의 크기가 커지는순간 X.to(\"cuda:0\"), y.to(\"cuda:0\") 쓰면 난리나겠는걸?\n\nx = torch.linspace(-10,10,100000).reshape(-1,1)\neps = torch.randn(100000).reshape(-1,1)\ny = x*2 + eps \n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n- 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까?\n\nplt.plot(x[::100],y[::100],'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n\n대충 이거만 가지고 적합해도 충분히 정확할것 같은데\n\n\n\nX,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?\n- 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나?\n- 아래의 알고리즘을 생각해보자. 1. 데이터를 반으로 나눈다. 2. 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. 3. yhat, loss, grad, update 수행 4. 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. 5. yhat, loss, grad, update 수행 6. 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. 7. 반복\n(서연 필기) 전체 다 올리면 경사하강법 부분적으로 올리면 확률적 경사하강법\n\n\n경사하강법, 확률적경사하강법, 미니배치 경사하강법\n10개의 샘플이 있다고 가정. \\(\\{(x_i,y_i)\\}_{i=1}^{10}\\)\n- ver1: 모든 샘플을 이용하여 slope 계산\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n…\n- ver2: 하나의 샘플만을 이용하여 slope 계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n(서연 필기) 불안해 - for 문도 많이 돌아야 해.\n- ver3: \\(m (\\leq n)\\) 개의 샘플을 이용하여 slope 계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n(서연 필기) 미니배치하고 남은 것도 계산된다.\n\n\n용어의 정리\n옛날\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n\nds, dl\n- ds\n\nx=torch.tensor(range(10)).float()#.reshape(-1,1)\ny=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1)\n\n\nds=torch.utils.data.TensorDataset(x,y)\nds\n\n<torch.utils.data.dataset.TensorDataset at 0x7fa03d24f940>\n\n\n\nds.tensors # 그냥 (x,y)의 튜플\n\n(tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]))\n\n\n- dl\n\ndl=torch.utils.data.DataLoader(ds,batch_size=3)\n#set(dir(dl)) & {'__iter__'}\n\n\nset(dir(dl)) & {'__iter__'}\n\n{'__iter__'}\n\n\ndir에 __iter__있으면 for문 쓰기 가능\n\nfor xx,yy in dl:\n    print(xx,yy)\n\ntensor([0., 1., 2.]) tensor([1., 1., 1.])\ntensor([3., 4., 5.]) tensor([1., 1., 0.])\ntensor([6., 7., 8.]) tensor([0., 0., 0.])\ntensor([9.]) tensor([0.])\n\n\n\n\nds, dl을 이용한 MNIST 구현\n- 데이터정리\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n- ds \\(\\to\\) dl\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048) \n\n\n12665/2048\n\n6.18408203125\n\n\n\ni = 0 \nfor xx,yy in dl: # 총 7번 돌아가는 for문 \n    print(i)\n    i=i+1\n\n0\n1\n2\n3\n4\n5\n6\n\n\n- 미니배치 안쓰는 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(70): \n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss= loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ntorch.sum((yhat>0.5) == y) / len(y) \n\ntensor(0.9981)\n\n\n\ntorch.mean(((yhat>0.5) == y)*1.0)\n\ntensor(0.9981)\n\n\n\nlen(y) / 2048\n\n6.18408203125\n\n\n- 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(10):\n    for xx,yy in dl: ## 7번\n        ## 1\n        #yhat = net(xx)\n        ## 2 \n        loss = loss_fn(net(xx),yy) \n        ## 3 \n        loss.backward() \n        ## 4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n(서연 필기)xx넣어서 학습 시키고 전체 X넣어서 확인\n\nlen(X)\n\n12665\n\n\n\ntorch.mean(((net(X)>0.5) == y)*1.0)\n\ntensor(0.9949)"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#오버피팅",
    "href": "posts/ml/2022-10-12-ml-6w.html#오버피팅",
    "title": "DNN (6주차)",
    "section": "오버피팅",
    "text": "오버피팅\n- 오버피팅이란? - 위키: In mathematical modeling, overfitting is “the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably”. - 제 개념: 데이터를 “데이터 = 언더라잉 + 오차”라고 생각할때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상.\n\n오버피팅 예시\n- \\(m\\)이 매우 클때 아래의 네트워크 거의 무엇이든 맞출수 있다고 보면 된다.\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n- 그런데 종종 맞추지 말아야 할 것들도 맞춘다.\nmodel: \\(y_i = (0\\times x_i) + \\epsilon_i\\), where \\(\\epsilon_i \\sim N(0,0.01^2)\\)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)\n\n\n\n\n\ny는 그냥 정규분포에서 생성한 오차이므로 \\(X \\to y\\) 로 항햐는 규칙따위는 없음\n\n\ntorch.manual_seed(1) \nnet=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)) \noptimizer= torch.optim.Adam(net.parameters())\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    yhat=net(x) \n    ## 2 \n    loss=loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizer.step()\n    net.zero_grad() \n\n\nplt.plot(x,y)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n우리는 데이터를 랜덤에서 뽑았는데, 데이터의 추세를 따라간다 \\(\\to\\) 오버피팅 (underlying이 아니라 오차항을 따라가고 있음)\n\n\n\n오버피팅이라는 뚜렷한 증거! (train / test)\n- 데이터의 분리하여 보자.\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nxtr = x[:80] \nytr = y[:80]\nxtest = x[80:]\nytest = y[80:]\nplt.plot(xtr,ytr)\nplt.plot(xtest,ytest)\nplt.title('train: blue / test: orange');\n\n\n\n\n- train만 학습\n\ntorch.manual_seed(1) \nnet1=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)) \noptimizr1= torch.optim.Adam(net1.parameters())\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    # net(xtr) \n    ## 2 \n    loss=loss_fn(net1(xtr),ytr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr1.step()\n    optimizr1.zero_grad() \n\n- training data로 학습한 net를 training data 에 적용\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net1(xtr).data,'--') # prediction (train) \n\n\n\n\n\ntrain에서는 잘 맞추는듯이 보인다.\n\n- training data로 학습한 net를 test data 에 적용\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net1(xtr).data,'--') # prediction (train) \nplt.plot(xtest,net1(xtest).data,'--') # prediction with unseen data (test) \n\n\n\n\n\ntrain은 그럭저럭 따라가지만 test에서는 엉망이다. \\(\\to\\) overfit"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-6w.html#숙제-해설-및-풀이는-여기참고",
    "href": "posts/ml/2022-10-12-ml-6w.html#숙제-해설-및-풀이는-여기참고",
    "title": "DNN (6주차)",
    "section": "숙제 (해설 및 풀이는 여기참고)",
    "text": "숙제 (해설 및 풀이는 여기참고)\n\n숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라\n\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,64)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,64)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n위에서 \\(a_1\\)은 relu를, \\(a_2\\)는 sigmoid를 의미한다.\n\n“y=0”은 숫자0을 의미하도록 하고 “y=1”은 숫자1을 의미하도록 설정하라.\n\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\n\n\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\n\n\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\n\n\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28).float()\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라.\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\nyhat.data\n\ntensor([[nan],\n        [nan],\n        [nan],\n        ...,\n        [nan],\n        [nan],\n        [nan]])\n\n\n학습이 잘 되지 않았다.\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!)\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nX = X/255\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr=torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래와 같은 수식을 이용하여 accuracy를 계산하라.\n\n\\(\\text{accuracy}=\\frac{1}{n}\\sum_{i=1}^n I(\\tilde{y}_i=y_i)\\) - \\(\\tilde{y}_i = \\begin{cases}  1 & \\hat{y}_i > 0.5 \\\\  0 & \\hat{y}_i \\leq 0.5 \\end{cases}\\) - \\(I(\\tilde{y}_i=y_i) = \\begin{cases} 1 & \\tilde{y}_i=y_i \\\\ 0 & \\tilde{y}_i \\neq y_i \\end{cases}\\)\n단, \\(n\\)은 0과 1을 의미하는 이미지의 수\n\nytilde = (yhat > 0.5) * 1\n\n\nytilde\n\ntensor([[0],\n        [0],\n        [0],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\n(ytilde == y) * 1\n\ntensor([[1],\n        [1],\n        [1],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\ntorch.sum((ytilde == y) * 1)\n\ntensor(12661)\n\n\n\ntorch.sum((ytilde == y) * 1)/len(y)\n\ntensor(0.9997)\n\n\n\nprint(\"accuraccy: \",torch.sum((ytilde == y) * 1)/len(y))\n\naccuraccy:  tensor(0.9997)\n\n\n\nprint(\"accuracy: \",((yhat>0.5) == y).sum() / len(y))\n\naccuracy:  tensor(0.9997)"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html",
    "href": "posts/ml/2022-11-21-ml-11w.html",
    "title": "RNN (11주차)",
    "section": "",
    "text": "기계학습 특강 (11주차) 11월16일 [순환신경망– abc예제, abdc예제, abcde예제, AbAcAd예제]"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#import",
    "href": "posts/ml/2022-11-21-ml-11w.html#import",
    "title": "RNN (11주차)",
    "section": "import",
    "text": "import\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#define-some-funtions",
    "href": "posts/ml/2022-11-21-ml-11w.html#define-some-funtions",
    "title": "RNN (11주차)",
    "section": "Define some funtions",
    "text": "Define some funtions\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#exam4-abacad-2",
    "href": "posts/ml/2022-11-21-ml-11w.html#exam4-abacad-2",
    "title": "RNN (11주차)",
    "section": "Exam4: AbAcAd (2)",
    "text": "Exam4: AbAcAd (2)\n\ndata\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))\n\n\n\n\n순환신경망 구현1 (손으로 직접구현) – 리뷰\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nx[[2]].shape\n\ntorch.Size([1, 4])\n\n\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1160dcaa50>\n\n\n\n\n\n\n\n순환신경망 구현2 (with RNNCell, hidden node 2)\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n구현1과 같은 초기값 (확인용)\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell() # 숙성담당 네트워크 \n\n\nrnncell = torch.nn.RNNCell(4,2)\n\n\ninput_size = 4\nhindden_size = 2\n\nrNNCell() 는 사실 torch.nn.RNNCell()와 같은 동작을 하도록 설계를 하였음.\n같은동작을 하는지 확인하기 위해서 동일한 초기상태에서 rNNCell()에 의하여 학습된 결과와 torch.nn.RNNCell()에 의하여 학습된 결과를 비교해보자.\n\nrnncell.weight_ih.shape\n\ntorch.Size([2, 4])\n\n\n\nrnncell.bias_ih.shape\n\ntorch.Size([2])\n\n\n\nrnncell.weight_hh.shape\n\ntorch.Size([2, 2])\n\n\n\nrnncell.bias_hh.shape \n\ntorch.Size([2])\n\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data\nrnncell.bias_ih.data = _rnncell.i2h.bias.data\nrnncell.weight_hh.data = _rnncell.h2h.weight.data\nrnncell.bias_hh.data = _rnncell.h2h.bias.data\n\n_rnncell 초기값을 rnncell에 넣어주기\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nhidden = torch.zeros(T,2) \n\n\n# t=0 \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\n# t=1~T \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat[:15].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123dd2650>\n\n\n\n\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11237d65d0>\n\n\n\n\n\n뒷부분갈수록 잘 맞음\n\n\n새로운 초기값\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\ntorch.nn.RNNCell(4,2)\n\nRNNCell(4, 2)\n\n\n앞 부분은 잘 맞지 않고 뒷부분은 잘 맞을 것!\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\nx -> h -> o -> yhat(softmax(o))\n네트워크만 학습된 상태, 따라서 hiddenlayer를 재구성해줘야 한다.\n\n\n\n순환신경망 구현3 (with RNN, hidden node 2) – 성공\n(예비학습)\n- 네트워크학습이후 yhat을 구하려면 번거로웠음\nhidden = torch.zeros(T,2) \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\nyhat = soft(cook(hidden))\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water), hidden\n\n((tensor([[-0.1271,  0.9965],\n          [-1.0000, -0.9987],\n          [ 0.9962,  1.0000],\n          ...,\n          [ 0.9962,  1.0000],\n          [-1.0000, -0.9897],\n          [ 0.9959,  1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[0.9959, 1.0000]], grad_fn=<SqueezeBackward1>)),\n tensor([[-0.2232,  0.9769],\n         [-0.9999, -0.9742],\n         [ 0.9154,  0.9992],\n         ...,\n         [ 0.9200,  0.9992],\n         [-0.9978, -0.0823],\n         [-0.9154,  0.9965]], grad_fn=<IndexPutBackward0>))\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[6.0688e-02, 4.2958e-01, 2.9885e-01, 2.1088e-01],\n        [9.9700e-01, 8.2096e-04, 1.3663e-03, 8.1039e-04],\n        [2.2782e-03, 3.3203e-01, 3.3260e-01, 3.3309e-01],\n        ...,\n        [2.2779e-03, 3.3203e-01, 3.3260e-01, 3.3309e-01],\n        [9.9692e-01, 8.4633e-04, 1.4012e-03, 8.3072e-04],\n        [2.2803e-03, 3.3206e-01, 3.3260e-01, 3.3306e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\nrnn = torch.nn.RNN(4,2)\n\n가중치초기화\n\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(4,2)\n\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,2) \nfor epoc in range(5000):\n    ## 1 \n    hidden,hT = rnn(x,_water)\n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n결과는 같을 것\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat.data[:15],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123908710>\n\n\n\n\n\n\n처음은 좀 틀렸음 ㅎㅎ\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11238cac90>\n\n\n\n\n\n\n뒤에는 잘맞음\n\n실전팁: _water 대신에 hT를 대입 (사실 큰 차이는 없음)\nhT는 이미 값이 저장되어 있잖아\n\nrnn(x[:6],_water),rnn(x[:6],hT)\n\n((tensor([[-0.9912, -0.9117],\n          [ 0.0698, -1.0000],\n          [-0.9927, -0.9682],\n          [ 0.5761, -1.0000],\n          [-0.9960, -0.0173],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)),\n (tensor([[-0.9713, -1.0000],\n          [ 0.0535, -1.0000],\n          [-0.9925, -0.9720],\n          [ 0.5759, -1.0000],\n          [-0.9960, -0.0180],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)))\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123881510>\n\n\n\n\n\n\n히든노드의 해석이 어려움.\n\nhidden layer->layer 더 있음 좋겠다\n\n\n순환신경망 구현4 (with RNN, hidden node 3) – 성공\n(1) 숙성네트워크~ (2) 조리네트워크\n\ntorch.manual_seed(2) #1 \nrnn = torch.nn.RNN(4,3) \ncook = torch.nn.Linear(3,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,3) \nfor epoc in range(5000):\n    ## 1\n    hidden,hT = rnn(x,_water) \n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123e41f10>\n\n\n\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123a3fd10>\n\n\n\n\n\n\n세번째 히든노드 = 대소문자(a/A)를 구분\n1,2 히든노드 = bcd를 구분"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#gpu-실험",
    "href": "posts/ml/2022-11-21-ml-11w.html#gpu-실험",
    "title": "RNN (11주차)",
    "section": "GPU 실험",
    "text": "GPU 실험\n\n20000 len + 20 hidden nodes\ncpu\n\nimport time\n\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n228.14399337768555\n\n\n왕 느려\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n6.0587098598480225\n\n\n\n왜 빠른지?\n\n\n\n20000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n28.451032400131226\n\n\nloss 미분할 때 시간 많이 잡음ㅁ\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.6839873790740967\n\n\n\n\n2000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n10.069071292877197\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.7792012691497803\n\n\n\n\n2000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n2.7720658779144287\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n0.2116107940673828\n\n\n\n\n2000 len + 5000 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n34.20541262626648\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n4.586683511734009\n\n\n\n\n2000 len + 5000 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n8.695780992507935\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n2.3646719455718994\n\n\n\n\n실험결과 요약\n\n\n\nlen\n# of hidden nodes\nbackward\ncpu\ngpu\nratio\n\n\n\n\n20000\n20\nO\n93.02\n3.26\n28.53\n\n\n20000\n20\nX\n18.85\n1.29\n14.61\n\n\n2000\n20\nO\n6.53\n0.75\n8.70\n\n\n2000\n20\nX\n1.25\n0.14\n8.93\n\n\n2000\n1000\nO\n58.99\n4.75\n12.41\n\n\n2000\n1000\nX\n13.16\n2.29\n5.74"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#exam5-abcabc",
    "href": "posts/ml/2022-11-21-ml-11w.html#exam5-abcabc",
    "title": "RNN (11주차)",
    "section": "Exam5: abcabC",
    "text": "Exam5: abcabC\n\ndata\n\ntxt = list('abcabC')*100\ntxt[:8]\n\n['a', 'b', 'c', 'a', 'b', 'C', 'a', 'b']\n\n\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])\n\n\n\n\nRNN\n\nbc\nbC\nb의 수준이 2개\nabc\namC\n문맥 고려해서 \\(\\to\\) hiddenlayer = 3\n\n\ntorch.manual_seed(43052) \nrnn = torch.nn.RNN(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+ list(linr.parameters()))\n\n\nrnn.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\n어차피 시각화하려면 cpu에 있어야해\n나중 기억!\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f116f95fd10>\n\n\n\n\n\n- 6000 epochs\n\n3: a일 확률\n4: b일 확률\n5: c일 확률\n6: C일 확률\n\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1162b1ad10>\n\n\n\n\n\n- 9000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11627385d0>\n\n\n\n\n\n- 12000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11626b9950>\n\n\n\n\n\n- 15000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-12:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11624ce950>\n\n\n\n\n\n\n15,000번 정도 하니 c와 C를 구분하는 모습\nhidden layer(0,1,2)의 색 순서에 따라 문맥상 다른 것을 알 수 있고 학습도 되는 모습을 볼 수 있다.\n\n\n\nLSTM\n- LSTM\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()))\n\n\nlstm.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f116245a750>\n\n\n\n\n\n\n하얀부분이 0 파란 부분이 -1 빨간 부분이 +1\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f11623e0a90>\n\n\n\n\n\n\nrnn에 비해 lstm은 조금 돌려도 어느정도 비교 잘 해낸다\n\n\n\nRNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n\n\n\n\nlstm이 rnn보다 이런 상황에서는 더 잘 학습해낸다.\nlinear 의 hiddenlayer로 구분되어 있다."
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#exam6-abcdabcd",
    "href": "posts/ml/2022-11-21-ml-11w.html#exam6-abcdabcd",
    "title": "RNN (11주차)",
    "section": "Exam6: abcdabcD",
    "text": "Exam6: abcdabcD\n\ndata\n\ntxt = list('abcdabcD')*100\ntxt[:8]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'D']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'c':2, 'd':3, 'D':4}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx=x.to(\"cuda:0\")\ny=y.to(\"cuda:0\")\n\n\n\nRNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 관찰1: LSTM이 확실히 장기기억에 강하다.\n- 관찰2: LSTM은 hidden에 0이 잘 나온다.\n\n사실 확실히 구분되는 특징을 판별할때는 -1,1 로 히든레이어 값들이 설정되면 명확하다.\n히든레이어에 -1~1사이의 값이 나온다면 애매한 판단이 내려지게 된다.\n그런데 이 애매한 판단이 어떻게 보면 문맥의 뉘앙스를 이해하는데 더 잘 맞다.\n그런데 RNN은 -1,1로 셋팅된 상황에서 -1~1로의 변화가 더디다는 것이 문제임."
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#lstm의-계산과정",
    "href": "posts/ml/2022-11-21-ml-11w.html#lstm의-계산과정",
    "title": "RNN (11주차)",
    "section": "LSTM의 계산과정",
    "text": "LSTM의 계산과정\n\ndata: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nab\naB\n로서 a의 수준이 2개로 나뉨 \\(\\to\\) hidden node = 2\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'B':2}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\n\n1 epoch ver1 (with torch.nn.LSTMCell)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt)\n    loss = loss / T\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n데이터 적으니까 cpu로 할 것임\n\n\nht,ct \n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))\n\n\n\nhidden node가 많고 len 이 클수록 GPU가 효율이 좋다\n\n\n\n1 epoch ver2 (완전 손으로 구현)\n\nt=0 \\(\\to\\) t=1\n- lstm_cell 을 이용한 계산 (결과비교용)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(1):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n이런결과를 어떻게 만드는걸까?\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n\n\\(i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})\\)\n\\(f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})\\)\n\\(g_t = \\tanh (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg})\\)\n\\(o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{hg})\\)\n\\(o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})\\)\n\\(c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\\)\n\\(h_t = o_t \\odot \\tanh (c_t)\\)\n\\(\\sigma = \\text{ Sigmoid }\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ }\\underrightarrow{sig} \\text{ }i_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\star \\text{ } \\underrightarrow{tanh} \\text{ }g_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ } \\triangleleft \\text{ }\\underrightarrow{sig} \\text{ }o_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t\\)\n\n위에 나온 W가 어떻게 계산되나\n\nweight_ih_l[k] – the learnable input-hidden weights of the \\(\\text{k}^{th}\\) layer \\((W_ii|W_if|W_ig|W_io)\\), of shape (4hidden_size, input_size) for \\(k = 0\\). Otherwise, the shape is (4hidden_size, num_directions * hidden_size). If proj_size > 0 was specified, the shape will be (4hidden_size, num_directions  proj_size) for \\(k > 0\\)\nweight_hh_l[k] – the learnable hidden-hidden weights of the \\(\\text{k}^{th}\\)layer \\((W_hi|W_hf|W_hg|W_ho)\\), of shape (4hidden_size, hidden_size). If proj_size > 0 was specified, the shape will be (4hidden_size, proj_size).\n- 직접계산\n\n\\(o_t\\) = output_gate\n\\(f_t\\) = forget_gate\n\\(i_t\\) = input_gate\n\n\nht = torch.zeros(1,2)\nct = torch.zeros(1,2)\n\n\n_ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n_ifgo\n\ntensor([[ 0.0137,  0.1495,  0.0879,  0.6436, -0.2615,  0.3974, -0.3506, -0.4550]],\n       grad_fn=<AddBackward0>)\n\n\n\\(\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft\\)각 두 개씩\n\ninput_gate = sig(_ifgo[:,0:2])\nforget_gate = sig(_ifgo[:,2:4])\ngt = tanh(_ifgo[:,4:6])\noutput_gate = sig(_ifgo[:,6:8])\n\n\nct = forget_gate * ct + input_gate * gt\nht = output_gate * tanh(ct)\n\n\nht,ct\n\n(tensor([[-0.0812,  0.1327]], grad_fn=<MulBackward0>),\n tensor([[-0.1991,  0.3563]], grad_fn=<AddBackward0>))\n\n\n\n\nt=0 \\(\\to\\) t=T\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        \n        ## lstm_cell step1: calculate _ifgo \n        _ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        input_gate = sig(_ifgo[:,0:2])\n        forget_gate = sig(_ifgo[:,2:4])\n        gt = tanh(_ifgo[:,4:6])\n        output_gate = sig(_ifgo[:,6:8])\n        ## lstm_cell step3: calculate ht,ct \n        ct = forget_gate * ct + input_gate * gt\n        ht = output_gate * tanh(ct)\n        \n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))\n\n\n\n\n\n1 epoch ver3 (with torch.nn.LSTM)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2)\nlinr = torch.nn.Linear(2,3) \n\n\nlstm = torch.nn.LSTM(3,2) \n\n\nlstm.weight_hh_l0.data = lstm_cell.weight_hh.data \nlstm.bias_hh_l0.data = lstm_cell.bias_hh.data \nlstm.weight_ih_l0.data = lstm_cell.weight_ih.data \nlstm.bias_ih_l0.data = lstm_cell.bias_ih.data \n\n\n초기화된 가중치값들로 덮어씌우기\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()), lr=0.1) \n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    # ## step2\n    # loss = loss_fn(output,y) \n    # ## step3\n    # loss.backward()\n    # ## step4 \n    # optimizr.step()\n    # optimizr.zero_grad() \n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.0975,  0.7134]], grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#lstm은-왜-강한가",
    "href": "posts/ml/2022-11-21-ml-11w.html#lstm은-왜-강한가",
    "title": "RNN (11주차)",
    "section": "LSTM은 왜 강한가?",
    "text": "LSTM은 왜 강한가?\n\ndata: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nn_words = 3\n\n\nmapping = {'a':0, 'b':1, 'B':2}\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:10],txt_y[:10]\n\n(['a', 'b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b'],\n ['b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b', 'a'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.],\n         ...,\n         [1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.]]),\n tensor([[0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.],\n         ...,\n         [0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.]]))\n\n\n\n\n1000 epoch\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(3,2) \nlinr = torch.nn.Linear(2,3) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1000): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    ## step2\n    loss = loss_fn(output,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\n\n시각화\n\nT = len(x)\ninput_gate = torch.zeros(T,2)\nforget_gate = torch.zeros(T,2)\noutput_gate = torch.zeros(T,2)\ng = torch.zeros(T,2)\ncell = torch.zeros(T,2)\nh = torch.zeros(T,2) \n\n\n변수를 담을 빈 셋 설정\n\n\nfor t in range(T): \n    ## 1: calculate _ifgo \n    _ifgo = x[[t]] @ lstm.weight_ih_l0.T + h[[t]] @ lstm.weight_hh_l0.T + lstm.bias_ih_l0 + lstm.bias_hh_l0 \n    ## 2: decompose _ifgo \n    input_gate[[t]] = sig(_ifgo[:,0:2])\n    forget_gate[[t]] = sig(_ifgo[:,2:4])\n    g[[t]] = tanh(_ifgo[:,4:6])\n    output_gate[[t]] = sig(_ifgo[:,6:8])\n    ## 3: calculate ht,ct \n    cell[[t]] = forget_gate[[t]] * cell[[t]] + input_gate[[t]] * g[[t]]\n    h[[t]] = output_gate[[t]] * tanh(cell[[t]])\n\n\ncombinded1 = torch.concat([input_gate,forget_gate,output_gate],axis=1)\ncombinded2 = torch.concat([g,cell,h,soft(output)],axis=1)\n\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n\n\n\n\n상단그림은 게이트의 값들만 시각화, 하단그림은 게이트 이외의 값들을 시각화\n\n\n\n시각화의 해석I\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\n\n\n\n\n- input_gate, forget_gate, output_gate는 모두 0~1 사이의 값을 가진다.\n- 이 값들은 각각 모두 \\({\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}, \\tanh({\\boldsymbol c}_t)\\)에 곱해진다. 따라서 input_gate, forget_gate, output_gate 는 gate의 역할로 비유가능하다. (1이면 통과, 0이면 차단)\n\ninput_gate: \\({\\boldsymbol g}_t\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\nforget_gate: \\({\\boldsymbol c}_{t-1}\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\noutput_gate: \\(\\tanh({\\boldsymbol c}_t)\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\n\n(서연 필기)\n\n값들이 0과 1사이의 값을 가진다\n파 -1 흰 0 빨 1\n0 곱하면 어떤 값이든 0이 되니까 차단한다 표현\n\n\n\n시각화의 해석II\n\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n- 결국 \\({\\boldsymbol g}_t\\to {\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\to \\hat{\\boldsymbol y}\\) 의 느낌이다. (\\({\\boldsymbol h}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_t\\)가 필요했고 \\({\\boldsymbol c}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_{t-1}\\)과 \\({\\boldsymbol g}_t\\)가 필요했음)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol c}_t) \\odot {\\boldsymbol o}_t\\)\n\\({\\boldsymbol c}_t ={\\boldsymbol c}_{t-1} \\odot {\\boldsymbol f}_t + {\\boldsymbol g}_{t} \\odot {\\boldsymbol i}_t\\)\n\n- \\({\\boldsymbol g}_t,{\\boldsymbol c}_t,{\\boldsymbol h}_t\\) 모두 \\({\\boldsymbol x}_t\\)의 정보를 숙성시켜 가지고 있는 느낌이 든다.\n- \\({\\boldsymbol g}_t\\) 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어 있다. (마치 RNN의 hidden node처럼!)\n\n\\(\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})\\)\n\n- \\({\\boldsymbol c}_t\\) 특징: \\({\\boldsymbol g}_t\\)와 매우 비슷하지만 약간 다른값을 가진다. 그래서 \\({\\boldsymbol g}_t\\)와는 달리 -1,1 이외의 값도 종종 등장.\n\nprint(\"first row: gt={}, ct={}\".format(g[-8].data, cell[-8].data))\nprint(\"second row: gt={}, ct={}\".format(g[-7].data, cell[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373])\n\n\n- \\({\\boldsymbol h}_t\\) 특징: (1) \\({\\boldsymbol c}_t\\)의 느낌이 있음 하지만 약간의 변형이 있음. (2) -1~1 사이에의 값을 훨씬 다양하게 가진다. (tanh때문)\n(서연 필기)\n\ncomparison of g,c part\n\n보니까 빨간 색은 1에 가까운 값, 파란색은 -1에 가까운 값들을 띄었다.\n그리고 연한 빨간색인 부분은 0.3592로 낮았고, g부분과 c부분이 열별로 보았을 때 달랐다\n\n\n\nprint(\"first row: gt={}, ct={}, ht={}\".format(g[-8].data, cell[-8].data,h[-8].data))\nprint(\"second row: gt={}, ct={}, ht={}\".format(g[-7].data, cell[-7].data,h[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984]), ht=tensor([ 0.7370, -0.3323])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373]), ht=tensor([ 0.0604, -0.6951])\n\n\n(서연 필기)\n\ncomparison of c,h part\n\nh는 c와 무관해보이지 않는다.\n단지 어떤 변형이 있는 것 같다.\n\n\n- 예전의문 해결\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문\n\n\n\nLSTM의 알고리즘 리뷰 I (수식위주)\n(step1) calculate \\({\\tt ifgo}\\)\n\\({\\tt ifgo} = {\\boldsymbol x}_t \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1} \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias\\)\n\\(=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias\\)\n참고: 위의 수식은 아래코드에 해당하는 부분\nifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n(step2) decompose \\({\\tt ifgo}\\) and get \\({\\boldsymbol i}_t\\), \\({\\boldsymbol f}_t\\), \\({\\boldsymbol g}_t\\), \\({\\boldsymbol o}_t\\)\n\\({\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )\\)\n\\({\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )\\)\n\\({\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )\\)\n\\({\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )\\)\n(step3) calculate \\({\\boldsymbol c}_t\\) and \\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)\\)\n\n\nLSTM의 알고리즘 리뷰 II (느낌위주)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ } \\triangleleft \\text{ }\\underrightarrow{sig} \\text{ }o_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t\\)\n\n이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n\n- 느낌1: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은 콩물에서 간장을 3차로 나누어 숙성하는 느낌이다.\n\n콩물: \\({\\boldsymbol x}_t\\)\n1차숙성: \\({\\boldsymbol g}_t\\)\n2차숙성: \\({\\boldsymbol c}_t\\)\n3차숙성: \\({\\boldsymbol h}_t\\)\n\n- 느낌2: \\({\\boldsymbol g}_t\\)에 대하여\n\n계산방법: \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)를 \\({\\bf W}_{ig}, {\\bf W}_{hg}\\)를 이용해 선형결합하고 \\(\\tanh\\)를 취한 결과\nRNN에서 간장을 만들던 그 수식에서 \\(h_t\\)를 \\(g_t\\)로 바꾼것\n크게 2가지의 의미를 가진다 (1) 과거와 현재의 결합 (2) 활성화함수 \\(\\tanh\\)를 적용\n\n(서연 필기)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ }\\underrightarrow{sig} \\text{ }i_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\star \\text{ } \\underrightarrow{tanh} \\text{ }g_t\\)\n를 풀어서 쓰면\n\\(\\tanh(x_t W_{ig} + h_{t-1} W_{hg} + bias)\\)\nRNN: \\(h_t = \\tanh(x_t W + h_{t-1} W + bias)\\)\nLSTM: \\(g_t = \\tanh(x_t W + h_{t-1} W + bias)\\) - 과거\\(h_{t-1}\\)와 현재\\(x_t\\)의 결합\n- 느낌3: \\({\\boldsymbol c}_t\\)에 대하여 (1)\n\n계산방법: \\({\\boldsymbol g}_{t}\\)와 \\({\\boldsymbol c}_{t-1}\\)를 요소별로 선택하고 더하는 과정\n\\(g_t\\)는 (1) 과거와 현재의 결합 (2) 활성화함수 tanh를 적용으로 나누어지는데 이중에서 (1) 과거와 현재의 정보를 결합하는 과정만 해당한다. 차이점은 요소별 선택 후 덧셈\n\n\\(g_t\\)는 선형 결합\n\n이러한 결합을 쓰는 이유? 게이트를 이용하여 과거와 현재의 정보를 제어 (일반적인 설명, 솔직히 내가 좋아하는 설명은 아님)\n\n(서연 필기)\n\\(c_t = g_t \\odot Input + c_{t-1} \\odot Forget\\)\n- 느낌4: \\({\\boldsymbol c}_t\\)에 대하여 (2) // \\({\\boldsymbol c}_t\\)는 왜 과거와 현재의 정보를 제어한다고 볼 수 있는가?\n\\(t=1\\) 시점 계산과정관찰\n\ninput_gate[1],g[1],forget_gate[1],cell[0]\n\n(tensor([0.9065, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.0014], grad_fn=<SelectBackward0>),\n tensor([ 0.3592, -0.9373], grad_fn=<SelectBackward0>))\n\n\n\\([0.9,1.0] \\odot {\\boldsymbol g}_t + [1.0,0.0] \\odot {\\boldsymbol c}_{t-1}\\)\n(서연 필기)\n여기서 곱은 element별 곱 - \\([0.9,1.0] \\odot g_t = [0.9,1.0]\\odot [g_1,g_2] = [0.9g_1(현재)_,1.0g_2(과거)]\\) - 여기서 0이 현재에 곱해지면 현재를 기억하지 않고 과거에 0이 곱해지면 과거를 기억하지 않도록 조정할 수 있음\n\\(\\star\\) gate없으면 조정 못 하나??\\(\\to\\) no, weigjht로도 조정할 수 있지 않을까?\n\nforget_gate는 \\(c_{t-1}\\)의 첫번째 원소는 기억하고, 두번째 원소는 잊으라고 말하고 있음 // forget_gate는 과거(\\(c_{t-1}\\))의 정보를 얼마나 잊을지 (= 얼마나 기억할지) 를 결정한다고 해석할 수 있다.\ninput_gate는 \\(g_{t}\\)의 첫번째 원소와 두번째 원소를 모두 기억하되 두번째 원소를 좀 더 중요하게 기억하라고 말하고 있음 // input_gate는 현재(\\(g_{t}\\))의 정보를 얼만큼 강하게 반영할지 결정한다.\n이 둘을 조합하면 \\({\\boldsymbol c}_t\\)가 현재와 과거의 정보중 어떠한 정보를 더 중시하면서 기억할지 결정한다고 볼 수 있다.\n\n\n이 설명은 제가 좀 싫어해요, 싫어하는 이유는 (1) “기억의 정도를 조절한다”와 “망각의 정도를 조절한다”는 사실 같은말임. 그래서 forget_gate의 용어가 모호함. (2) 기억과 망각을 조정하는 방식으로 꼭 gate의 개념을 사용해야 하는건 아님\n\n- 느낌5: \\({\\boldsymbol c}_t\\)에 대하여 (3)\n\n사실상 LSTM 알고리즘의 꽃이라 할 수 있음.\nLSTM은 long short term memory의 약자임. 기존의 RNN은 장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘 활용함.\nLSTM이 장기기억을 잘 활용하는 비법은 바로 \\({\\boldsymbol c}_t\\)에 있다.\n\n(서연필기) \\(c_t\\)로 과거, 현재 기억 조절할 수 있기 때문에\n- 느낌6: \\({\\boldsymbol h}_t\\)에 대하여 - 계산방법: \\(\\tanh({\\boldsymbol c}_t)\\)를 요소별로 선택\n데이터 다 가져와서 선택하는 방식\n- RNN, LSTM의 변수들 비교 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n과거정보\n현재정보\n과거와 현재의 결합방식\n활성화\n느낌\n비고\n\n\n\n\nRNN-\\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) W \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n간장\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM-\\({\\boldsymbol g}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\)W \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n1차간장\n\n\n\nLSTM-\\({\\boldsymbol c}_t\\)\n\\({\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol g}_t\\)\n\\(\\odot\\) W\\(\\to\\) \\(+\\)\nNone\n2차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\nLSTM-\\({\\boldsymbol h}_t\\)\nNone\n\\({\\boldsymbol c}_t\\)\nNone\n\\(\\tanh\\), \\(\\odot\\)\n3차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\n\n\nRNN은 기억할 과거정보가 \\({\\boldsymbol h}_{t-1}\\) 하나이지만 LSTM은 \\({\\boldsymbol c}_{t-1}\\), \\({\\boldsymbol h}_{t-1}\\) 2개이다.\n\n- 알고리즘리뷰 :\n\n콩물\\(x_t\\),과거3차간장\\(h_{t-1}\\) \\(\\overset{\\times,+,\\tanh}{\\longrightarrow}\\) 현재1차간장\\(g_t\\)\n현재1차간장\\(c_{t-1}\\), 과거2차간장 \\(\\overset{\\odot,+,\\tanh}{\\longrightarrow}\\) 현재2차간장\n현재2차간장\\(c_t\\) \\(\\overset{\\tanh,\\odot}{\\longrightarrow}\\) 현재3차간장\\(h_t\\)\n\n\n\nLSTM이 강한이유\n- LSTM이 장기기억에 유리함. 그 이유는 input, forget, output gate 들이 과거기억을 위한 역할을 하기 때문.\n\n비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭 LSTM같은 구조일 필요는 없음. (왜 3차간장을 만들때 tanh를 써야하는지? 게이트는 꼭3개이어야 하는지?)\n\n- 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다.\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문\n\n문잭적으로 이해 ->유리하다 칭함"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-11w.html#참고자료들",
    "href": "posts/ml/2022-11-21-ml-11w.html#참고자료들",
    "title": "RNN (11주차)",
    "section": "참고자료들",
    "text": "참고자료들\n\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\nhttps://arxiv.org/abs/1402.1128"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html",
    "href": "posts/ml/2022-11-30-12wk.html",
    "title": "RNN (12주차)",
    "section": "",
    "text": "순환신경망 minor topics"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-abcabc",
    "href": "posts/ml/2022-11-30-12wk.html#data-abcabc",
    "title": "RNN (12주차)",
    "section": "data: abcabC",
    "text": "data: abcabC\n\ntxt = list('abcabC')*100\ntxt[:8]\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#실험",
    "href": "posts/ml/2022-11-30-12wk.html#실험",
    "title": "RNN (12주차)",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험2\n\nHIDDEN = 4\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험3\n\nHIDDEN = 8\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,8))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment3: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#결론",
    "href": "posts/ml/2022-11-30-12wk.html#결론",
    "title": "RNN (12주차)",
    "section": "결론",
    "text": "결론\n- 노드수가 많으면 학습에 유리함\n(서연 필기) c/C를 맞추는 것(error)보다 확실한 규칙을 맞추는 것(underline)이 중요\\(\\to\\)오히려 맞추면 과적합으로 볼 수 있다 - 그래서 학습이 잘 되었으면 - 첫 칸 - 둘째 칸 - 셋쨰, 넷째 칸 - 이 순으로 predict 되었을 것"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-abcc",
    "href": "posts/ml/2022-11-30-12wk.html#data-abcc",
    "title": "RNN (12주차)",
    "section": "data: ab(c,C)",
    "text": "data: ab(c,C)\n\n# torch.manual_seed(43052)\n# txta = 'a'*50\n# txtb = 'b'*50\n# prob_upper = torch.bernoulli(torch.zeros(50)+0.5) \n# txtc = list(map(lambda x: 'c' if x==1 else 'C', prob_upper))\n# txt = ''.join([txta[i]+','+txtb[i]+','+txtc[i]+',' for i in range(50)]).split(',')[:-1]\n# txt_x = txt[:-1] \n# txt_y = txt[1:]\n# pd.DataFrame({'txt_x':txt_x,'txt_y':txt_y}).to_csv(\"2022-11-25-ab(c,C).csv\",index=False)\n\n\ndf= pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/IV.%20RNN/2022-11-25-ab(c%2CC).csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      txt_x\n      txt_y\n    \n  \n  \n    \n      0\n      a\n      b\n    \n    \n      1\n      b\n      c\n    \n    \n      2\n      c\n      a\n    \n    \n      3\n      a\n      b\n    \n    \n      4\n      b\n      c\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      144\n      a\n      b\n    \n    \n      145\n      b\n      C\n    \n    \n      146\n      C\n      a\n    \n    \n      147\n      a\n      b\n    \n    \n      148\n      b\n      c\n    \n  \n\n149 rows × 2 columns\n\n\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(df.txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(df.txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\")"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#실험-1",
    "href": "posts/ml/2022-11-30-12wk.html#실험-1",
    "title": "RNN (12주차)",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험2\n\nHIDDEN = 16\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#결론-1",
    "href": "posts/ml/2022-11-30-12wk.html#결론-1",
    "title": "RNN (12주차)",
    "section": "결론",
    "text": "결론\n- 노드수가 너무 많으면 오버피팅 경향도 있음"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-human-numbers-5",
    "href": "posts/ml/2022-11-30-12wk.html#data-human-numbers-5",
    "title": "RNN (12주차)",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#torch를-이용한-learn",
    "href": "posts/ml/2022-11-30-12wk.html#torch를-이용한-learn",
    "title": "RNN (12주차)",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\")\nfor epoc in range(50):\n    ## 1 \n    hidden, (hT,cT) =lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nplt.matshow(soft(output).data[-10:].to(\"cpu\"),cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f568441a1d0>"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#fastai-이용한-learn",
    "href": "posts/ml/2022-11-30-12wk.html#fastai-이용한-learn",
    "title": "RNN (12주차)",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy \ndls = DataLoaders(dl1,dl2) \n\nfastai 를 이용하여 class를 사용하기 위한 목차\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(50)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.722138\n      1.502271\n      00:00\n    \n    \n      1\n      1.611093\n      1.973368\n      00:00\n    \n    \n      2\n      1.734299\n      1.481888\n      00:00\n    \n    \n      3\n      1.669271\n      1.377668\n      00:00\n    \n    \n      4\n      1.608570\n      1.368541\n      00:00\n    \n    \n      5\n      1.566517\n      1.267919\n      00:00\n    \n    \n      6\n      1.521232\n      1.106543\n      00:00\n    \n    \n      7\n      1.465657\n      0.959904\n      00:00\n    \n    \n      8\n      1.404815\n      0.856123\n      00:00\n    \n    \n      9\n      1.344825\n      0.802936\n      00:00\n    \n    \n      10\n      1.290437\n      0.794831\n      00:00\n    \n    \n      11\n      1.244395\n      0.771966\n      00:00\n    \n    \n      12\n      1.203488\n      0.735865\n      00:00\n    \n    \n      13\n      1.165525\n      0.690032\n      00:00\n    \n    \n      14\n      1.129149\n      0.621654\n      00:00\n    \n    \n      15\n      1.092401\n      0.555875\n      00:00\n    \n    \n      16\n      1.055485\n      0.493046\n      00:00\n    \n    \n      17\n      1.018588\n      0.423167\n      00:00\n    \n    \n      18\n      0.981230\n      0.349703\n      00:00\n    \n    \n      19\n      0.943231\n      0.279531\n      00:00\n    \n    \n      20\n      0.904838\n      0.216544\n      00:00\n    \n    \n      21\n      0.866475\n      0.166756\n      00:00\n    \n    \n      22\n      0.828821\n      0.125583\n      00:00\n    \n    \n      23\n      0.792214\n      0.094763\n      00:00\n    \n    \n      24\n      0.757037\n      0.072662\n      00:00\n    \n    \n      25\n      0.723539\n      0.055544\n      00:00\n    \n    \n      26\n      0.691763\n      0.042442\n      00:00\n    \n    \n      27\n      0.661703\n      0.032804\n      00:00\n    \n    \n      28\n      0.633335\n      0.025908\n      00:00\n    \n    \n      29\n      0.606606\n      0.020872\n      00:00\n    \n    \n      30\n      0.581437\n      0.017020\n      00:00\n    \n    \n      31\n      0.557727\n      0.014002\n      00:00\n    \n    \n      32\n      0.535379\n      0.011625\n      00:00\n    \n    \n      33\n      0.514297\n      0.009755\n      00:00\n    \n    \n      34\n      0.494391\n      0.008293\n      00:00\n    \n    \n      35\n      0.475579\n      0.007180\n      00:00\n    \n    \n      36\n      0.457784\n      0.006386\n      00:00\n    \n    \n      37\n      0.440938\n      0.005807\n      00:00\n    \n    \n      38\n      0.424976\n      0.005199\n      00:00\n    \n    \n      39\n      0.409830\n      0.004525\n      00:00\n    \n    \n      40\n      0.395437\n      0.003926\n      00:00\n    \n    \n      41\n      0.381747\n      0.003398\n      00:00\n    \n    \n      42\n      0.368712\n      0.002977\n      00:00\n    \n    \n      43\n      0.356291\n      0.002673\n      00:00\n    \n    \n      44\n      0.344447\n      0.002432\n      00:00\n    \n    \n      45\n      0.333144\n      0.002230\n      00:00\n    \n    \n      46\n      0.322349\n      0.002058\n      00:00\n    \n    \n      47\n      0.312030\n      0.001911\n      00:00\n    \n    \n      48\n      0.302160\n      0.001785\n      00:00\n    \n    \n      49\n      0.292712\n      0.001678\n      00:00\n    \n  \n\n\n\n\nplt.matshow(soft(lrnr.model(x)[-10:]).data.to(\"cpu\"),cmap = 'bwr', vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f5687137bd0>"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-hihello",
    "href": "posts/ml/2022-11-30-12wk.html#data-hihello",
    "title": "RNN (12주차)",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트1-_water의-생략",
    "href": "posts/ml/2022-11-30-12wk.html#세트1-_water의-생략",
    "title": "RNN (12주차)",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,4).to(\"cuda:0\")\nlstm(x, (_water,_water))\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n\nx.shape\n\ntorch.Size([999, 7])\n\n\n999개, 구별되는 문자 7개"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트2-x.shape-l-h_in-or-lnh_in",
    "href": "posts/ml/2022-11-30-12wk.html#세트2-x.shape-l-h_in-or-lnh_in",
    "title": "RNN (12주차)",
    "section": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))",
    "text": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))\n- 파라메터 설명\n\n\\(L\\) = sequece length = 시계열의 길이 = 간장을 몇 년 전통으로 이어갈지\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{in}\\) = input_size = 시점을 고정하였을 경우 입력자료의 차원 = 입력시계열이 시점별로 몇개의 변수로 나타내어 지는지? = 만약에 원핫인코딩으로 단어를 정리하면 단어수를 의미함\n\n우리가 실습했던 거 모두 N이 1이었다 그래서 안 썼음 - 1일 때만 아래와 같이 여러 버전 가능\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드3: x의 차원은 사실 엄밀하게는 (\\(L\\),\\(N\\),\\(H_{in}\\)) 와 같다…\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x.reshape(999,1,7))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 코드4: batch_first=True옵션을 사용하여 lstm을 만든경우\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4,batch_first=True).to(\"cuda:0\")\n\n\nlstm(x.reshape(1,999,7))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563],\n          [-0.0786, -0.1430, -0.0250,  0.1189],\n          [-0.0300, -0.2256, -0.1324,  0.1439],\n          ...,\n          [-0.0723,  0.0620,  0.1913,  0.2015],\n          [-0.1155,  0.0746,  0.1747,  0.2938],\n          [-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "href": "posts/ml/2022-11-30-12wk.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "title": "RNN (12주차)",
    "section": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))\n- 파라메터 설명\n\n\\(D\\) = 2 if bidirectional=True otherwise 1 = 양방향이면 2, 단방향이면 1 (우리는 단방향만 배움)\nnum_layres = 중첩된 RNN일 경우 (우리는 중첩을 안시켰음)\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{out}\\) = 히든노드의 수\n\n- 코드5: x.shape = (\\(L\\),\\(1\\),\\(H_{in}\\)) \\(\\to\\) hidden.shape = (\\(1\\),\\(1\\),\\(H_{out}\\))\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,1,4).to(\"cuda:0\") \nlstm(x.reshape(999,1,7),(_water,_water))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 사실 _water.shape = (1,\\(H_{out}\\)) 에서 1은 observation의 차원을 의미하는게 아님 (그런데 대충 그렇게 생각해도 무방함)\n\n한 시점의 콩물에 대하여 양방향으로 간장을 만들면 _water.shape = (2,h)\n한 시점의 콩물에 대하여 3중첩으로 간장을 만들면 _water.shape = (3,h)\n한 시점의 콩물에 대하여 3중첩간장을 양방향으로 만들면 _water.shape = (6,h)"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-hihello-1",
    "href": "posts/ml/2022-11-30-12wk.html#data-hihello-1",
    "title": "RNN (12주차)",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트1-_water의-생략-1",
    "href": "posts/ml/2022-11-30-12wk.html#세트1-_water의-생략-1",
    "title": "RNN (12주차)",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\n_water = torch.zeros(1,4).to(\"cuda:0\")\nxt.shape, _water.shape\n\n(torch.Size([1, 7]), torch.Size([1, 4]))\n\n\n\nlstmcell(xt,(_water,_water))\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트2-xt.shape-nh_in-or-h_in",
    "href": "posts/ml/2022-11-30-12wk.html#세트2-xt.shape-nh_in-or-h_in",
    "title": "RNN (12주차)",
    "section": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))",
    "text": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))\nn: timeserie 개수, 1일 경우 생략 가능\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드3:\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\nxt.shape\n\ntorch.Size([7])\n\n\n\nlstmcell(xt)\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))\n\n\n(1,n)의 형태라면 괄호 하나 빼도 가능"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#세트3-hidden.shape-nh_out-or-h_out",
    "href": "posts/ml/2022-11-30-12wk.html#세트3-hidden.shape-nh_out-or-h_out",
    "title": "RNN (12주차)",
    "section": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))\n- 코드4: xt.shape = (\\(H_{out}\\)) \\(\\to\\) _water.shape = \\((H_{out})\\)\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\n_water = torch.zeros(4).to(\"cuda:0\")\nxt.shape,_water.shape\n\n(torch.Size([7]), torch.Size([4]))\n\n\n\nlstmcell(xt, (_water,_water))\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#똑같은-코드들-정리",
    "href": "posts/ml/2022-11-30-12wk.html#똑같은-코드들-정리",
    "title": "RNN (12주차)",
    "section": "똑같은 코드들 정리",
    "text": "똑같은 코드들 정리\n- 원래 1은 단순히 observation의 차원이 아니다. 즉 \\({\\bf X}_{n \\times p}\\)에서 \\(n\\)에 대응하는 차원으로 생각할 수 없다.\n- 그런데 (1) 단방향 (2) 조각내지 않은 시계열 (3) 중첩하지 않은 순환망에 한정하여서는 observation 처럼 생각해도 무방하다. <– 엄밀하게는 이게 위험한 생각임. 하지만 정식으로 모두 따지려면 너무 헷갈림"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#실제구현시-기억할-것",
    "href": "posts/ml/2022-11-30-12wk.html#실제구현시-기억할-것",
    "title": "RNN (12주차)",
    "section": "실제구현시 기억할 것",
    "text": "실제구현시 기억할 것\n- 현실적으로 (1)-(3)이 아닌 조건에서는 Cell 단위로 연산을 이용할 일이 없다. (느리거든요) // 그냥 이해용으로 구현\n- torch.nn.RNN 혹은 torch.nn.LSTM 으로 네트워크를 구성할시 _water의 dim을 명시할 일도 없다.\n- 오로지 고려해야 할 것은 입력시계열을 조각낼지 조각내지 않을지"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data",
    "href": "posts/ml/2022-11-30-12wk.html#data",
    "title": "RNN (12주차)",
    "section": "data",
    "text": "data\n\ntxt = list('hi!')*3 + list('hi?')*3"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#조각내지-않은-시계열",
    "href": "posts/ml/2022-11-30-12wk.html#조각내지-않은-시계열",
    "title": "RNN (12주차)",
    "section": "조각내지 않은 시계열",
    "text": "조각내지 않은 시계열\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f5686bc0b90>\n\n\n\n\n\n첫번째 stack은 hi!로 학습 두번째 stack은 hi?로 학습하여 결과가 이럼"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#조각난-시계열",
    "href": "posts/ml/2022-11-30-12wk.html#조각난-시계열",
    "title": "RNN (12주차)",
    "section": "조각난 시계열",
    "text": "조각난 시계열\n\ntxt1= txt[:9]\ntxt2= txt[9:]\n\n\ntxt1,txt2\n\n(['h', 'i', '!', 'h', 'i', '!', 'h', 'i', '!'],\n ['h', 'i', '?', 'h', 'i', '?', 'h', 'i', '?'])\n\n\n\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\n\n9에서 하나씩 빼서 x,y 만들었으니까 8\n\nx1.shape, y1.shape, x2.shape, y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape, yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(xx) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n첫번째 stzck 과 두번째 stack의 합\n  loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f5687aed450>\n\n\n\n\n\nxx로 학습한 것들인데 만약 x를 넣는다면?\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f5687accf50>\n\n\n\n\n\n\nhidden, _ = lstm(x1)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f5687add750>\n\n\n\n\n\n\nhidden, _ = lstm(x2)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f56820f78d0>\n\n\n\n\n\n\nhidden.shape\n\ntorch.Size([17, 10])\n\n\n\nlinr(hidden).shape\n\ntorch.Size([17, 4])\n\n\n- 조각난 시계열로 학습한 경우는 hi!에서 hi?로 바뀔 수 없다. 왜냐햐면 그러한 연결정보가 끊어져 있으니까"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#재미있는-실험",
    "href": "posts/ml/2022-11-30-12wk.html#재미있는-실험",
    "title": "RNN (12주차)",
    "section": "재미있는 실험",
    "text": "재미있는 실험\n- x1만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x1) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y1)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x2)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b701ba890>\n\n\n\n\n\n- x2만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x2) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y2)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x1)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b9809ef50>"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#data-human-numbers-5-1",
    "href": "posts/ml/2022-11-30-12wk.html#data-human-numbers-5-1",
    "title": "RNN (12주차)",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#fastai-이용한-learn-1",
    "href": "posts/ml/2022-11-30-12wk.html#fastai-이용한-learn-1",
    "title": "RNN (12주차)",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\n한 뭉치에 몇 개 있는지\ntorch.nn.LSTM(batxh_size)\n몇 개로 나눠져 있는지\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy \ndls = DataLoaders(dl1,dl2) \n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(43052)\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.762846\n      1.502211\n      00:00\n    \n    \n      1\n      1.631212\n      1.620583\n      00:00\n    \n    \n      2\n      1.627597\n      1.443686\n      00:00\n    \n    \n      3\n      1.580216\n      1.368762\n      00:00\n    \n    \n      4\n      1.536200\n      1.307310\n      00:00\n    \n    \n      5\n      1.496099\n      1.216339\n      00:00\n    \n    \n      6\n      1.453670\n      1.113821\n      00:00\n    \n    \n      7\n      1.408125\n      1.019931\n      00:00\n    \n    \n      8\n      1.361426\n      0.941434\n      00:00\n    \n    \n      9\n      1.315507\n      0.884034\n      00:00\n    \n  \n\n\n\n\nsoft(lrnr.model(x)).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.144, 0.145]], dtype=float32)"
  },
  {
    "objectID": "posts/ml/2022-11-30-12wk.html#torch를-이용한-learn-1",
    "href": "posts/ml/2022-11-30-12wk.html#torch를-이용한-learn-1",
    "title": "RNN (12주차)",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\noptim으로 adam 사용\n\nfor epoc in range(10):\n    ## 1 \n    hidden, _ = lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nhidden, _ = lstm(x)\noutput = linr(hidden) \nsoft(output).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.145, 0.145]], dtype=float32)"
  },
  {
    "objectID": "posts/ml/2022-12-13-final_seoyeon.html",
    "href": "posts/ml/2022-12-13-final_seoyeon.html",
    "title": "Finalterm",
    "section": "",
    "text": "기말고사"
  },
  {
    "objectID": "posts/ml/2022-12-13-final_seoyeon.html#covid19-tweets-to-텍스트생성-30점",
    "href": "posts/ml/2022-12-13-final_seoyeon.html#covid19-tweets-to-텍스트생성-30점",
    "title": "Finalterm",
    "section": "1. COVID19 tweets \\(\\to\\) 텍스트생성 (30점)",
    "text": "1. COVID19 tweets \\(\\to\\) 텍스트생성 (30점)\n아래의 코드를 이용하여 자료를 다운로드 하라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\ndf\n\n\n\n\n\n  \n    \n      \n      UserName\n      ScreenName\n      Location\n      TweetAt\n      OriginalTweet\n      Sentiment\n    \n  \n  \n    \n      0\n      3799\n      48751\n      London\n      16-03-2020\n      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n      Neutral\n    \n    \n      1\n      3800\n      48752\n      UK\n      16-03-2020\n      advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n      Positive\n    \n    \n      2\n      3801\n      48753\n      Vagabonds\n      16-03-2020\n      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n      Positive\n    \n    \n      3\n      3802\n      48754\n      NaN\n      16-03-2020\n      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n      Positive\n    \n    \n      4\n      3803\n      48755\n      NaN\n      16-03-2020\n      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n      Extremely Negative\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41152\n      44951\n      89903\n      Wellington City, New Zealand\n      14-04-2020\n      Airline pilots offering to stock supermarket shelves in #NZ lockdown #COVID-19 https://t.co/cz89uA0HNp\n      Neutral\n    \n    \n      41153\n      44952\n      89904\n      NaN\n      14-04-2020\n      Response to complaint not provided citing COVID-19 related delays. Yet prompt in rejecting policy before consumer TAT is over. Way to go ?\n      Extremely Negative\n    \n    \n      41154\n      44953\n      89905\n      NaN\n      14-04-2020\n      You know itÂs getting tough when @KameronWilds  is rationing toilet paper #coronavirus #toiletpaper @kroger martinsville, help us out!!\n      Positive\n    \n    \n      41155\n      44954\n      89906\n      NaN\n      14-04-2020\n      Is it wrong that the smell of hand sanitizer is starting to turn me on?\\r\\r\\n\\r\\r\\n#coronavirus #COVID19 #coronavirus\n      Neutral\n    \n    \n      41156\n      44955\n      89907\n      i love you so much || he/him\n      14-04-2020\n      @TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\n      Negative\n    \n  \n\n41157 rows × 6 columns\n\n\n\n(1) TextDataLoaders.from_df을 이용하여 dls오브젝트를 만들어라.\n\ntext_col=‘OriginalTweet’ 로 설정\nis_lm=True 로 설정\nseq_len=64 로 설정\n\n\n## 올바르게 dls를 생성하였을 경우 dls.show_batch()의 결과는 아래와 같음. \n\n\ndls = TextDataLoaders.from_df(df,is_lm=True,seq_len=64,text_col='OriginalTweet')\n\n\n\n\n\n\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj i 'd like to say i xxunk this up because of # covid_19 quarantine , but i work in a grocery store and thankfully i still have a job . \\r\\r\\n\\r\\r\\n i did have a blast making it . \\r\\r\\n\\r\\r\\n▁ # dinner # pasta # xxunk # xxunk ▁ https : / / t.co / xxunk xxbos @gavinnewsom gavinnewsom xxmaj gov .\n      xxmaj i 'd like to say i xxunk this up because of # covid_19 quarantine , but i work in a grocery store and thankfully i still have a job . \\r\\r\\n\\r\\r\\n i did have a blast making it . \\r\\r\\n\\r\\r\\n▁ # dinner # pasta # xxunk # xxunk ▁ https : / / t.co / xxunk xxbos @gavinnewsom gavinnewsom xxmaj gov . xxmaj\n    \n    \n      1\n      https : / / t.co / xxunk xxbos xxmaj wow @donaldjtrumpjr do you have a pair of these xxunk just popped up in my xxmaj facebook feed ! xxmaj buying them to support online shopping and xxmaj trump ! https : / / t.co / xxunk # coronavirus # trump2020 https : / / t.co / xxunk xxbos xxmaj if you or your loved\n      : / / t.co / xxunk xxbos xxmaj wow @donaldjtrumpjr do you have a pair of these xxunk just popped up in my xxmaj facebook feed ! xxmaj buying them to support online shopping and xxmaj trump ! https : / / t.co / xxunk # coronavirus # trump2020 https : / / t.co / xxunk xxbos xxmaj if you or your loved one\n    \n    \n      2\n      now looking at some sort of super gel . i thought i was gon na save money by cooking b xxrep 4 u t that money is going to online shopping xxrep 5 ? \\r\\r\\n▁ # coronavirus # life xxbos xxup covid - alert : xxmaj coronavirus xxmaj crashes xxmaj prices for xxmaj hotels , xxmaj rental xxmaj cars , and xxmaj women 's\n      looking at some sort of super gel . i thought i was gon na save money by cooking b xxrep 4 u t that money is going to online shopping xxrep 5 ? \\r\\r\\n▁ # coronavirus # life xxbos xxup covid - alert : xxmaj coronavirus xxmaj crashes xxmaj prices for xxmaj hotels , xxmaj rental xxmaj cars , and xxmaj women 's xxmaj\n    \n    \n      3\n      the window of a house in # xxmaj xxunk # xxmaj xxunk today while driving to supermarket . # happy # coronavirus # xxunk https : / / t.co / xxunk xxbos xxup covid 19 xxmaj requires xxmaj website xxmaj accessibility xxmaj xxunk for consumer - facing xxmaj businesses https : / / t.co / xxunk xxbos all these stores have sales cause of\n      window of a house in # xxmaj xxunk # xxmaj xxunk today while driving to supermarket . # happy # coronavirus # xxunk https : / / t.co / xxunk xxbos xxup covid 19 xxmaj requires xxmaj website xxmaj accessibility xxmaj xxunk for consumer - facing xxmaj businesses https : / / t.co / xxunk xxbos all these stores have sales cause of covid-19\n    \n    \n      4\n      xxmaj business xxmaj alive xxmaj during xxmaj epidemic \\r\\r\\n▁ # xxup prices # xxmaj business # epidemic # socialmediamarketing # covid_19 # coronavirus # seo # xxunk # xxunk # smo # xxup ppc \\r\\r\\n https : / / t.co / xxunk https : / / t.co / xxunk xxbos xxunk xxunk xxmaj prices vary from mask to mask , however it is a\n      business xxmaj alive xxmaj during xxmaj epidemic \\r\\r\\n▁ # xxup prices # xxmaj business # epidemic # socialmediamarketing # covid_19 # coronavirus # seo # xxunk # xxunk # smo # xxup ppc \\r\\r\\n https : / / t.co / xxunk https : / / t.co / xxunk xxbos xxunk xxunk xxmaj prices vary from mask to mask , however it is a necessary\n    \n    \n      5\n      # inflation , rising prices and declining purchasing power are already unavoidable due to # xxup covid-19 and low oil prices , with implications for political stability and elections in # xxmaj russia . xxunk https : / / t.co / xxunk xxbos xxmaj iâm thrilled that xxunk proactively applied to xxunk and has since been approved to temporarily reduce overage charges and expand\n      inflation , rising prices and declining purchasing power are already unavoidable due to # xxup covid-19 and low oil prices , with implications for political stability and elections in # xxmaj russia . xxunk https : / / t.co / xxunk xxbos xxmaj iâm thrilled that xxunk proactively applied to xxunk and has since been approved to temporarily reduce overage charges and expand internet\n    \n    \n      6\n      macron has imposed a two - week lockdown , declaring a xxunk on # coronavirus ; cancelled municipal xxunk have been ordered to stay home and will only be allowed out for âessential xxunk such as trips to the grocery store or pharmacy ! ? xxbos xxmaj we promise we 're not hoarding . xxmaj who could have know that donations into our #\n      has imposed a two - week lockdown , declaring a xxunk on # coronavirus ; cancelled municipal xxunk have been ordered to stay home and will only be allowed out for âessential xxunk such as trips to the grocery store or pharmacy ! ? xxbos xxmaj we promise we 're not hoarding . xxmaj who could have know that donations into our # xxunk\n    \n    \n      7\n      covid-19 . and i ( and other supermarket workers , and xxup nhs / hospital staff , and care workers ) will get little to nothing in return . xxbos \" # xxmaj travel xxmaj news : xxmaj xxunk xxmaj announces # xxmaj travel xxmaj industry 's xxmaj first and xxmaj only xxmaj daily xxmaj measure of # xxmaj consumer # xxmaj travel xxmaj\n      . and i ( and other supermarket workers , and xxup nhs / hospital staff , and care workers ) will get little to nothing in return . xxbos \" # xxmaj travel xxmaj news : xxmaj xxunk xxmaj announces # xxmaj travel xxmaj industry 's xxmaj first and xxmaj only xxmaj daily xxmaj measure of # xxmaj consumer # xxmaj travel xxmaj patterns\n    \n    \n      8\n      is wrong with people ? xxmaj stop hoarding , you brainless , selfish xxup scum xxrep 3 ! # xxup covidiot # xxup covid?19 # coronavirus https : / / t.co / xxunk xxbos xxmaj never thought i d hear an interview like this on xxmaj health xxmaj secretary pleading with people to stop panic buying food and telling former nurses amp doctors your\n      wrong with people ? xxmaj stop hoarding , you brainless , selfish xxup scum xxrep 3 ! # xxup covidiot # xxup covid?19 # coronavirus https : / / t.co / xxunk xxbos xxmaj never thought i d hear an interview like this on xxmaj health xxmaj secretary pleading with people to stop panic buying food and telling former nurses amp doctors your xxup\n    \n  \n\n\n\n(2) language_model_learner를 이용하여 오브젝트를 생성하라. lrnr.fine_tune(3,1e-1)을 이용하여 학습하라.\n\narch= AWD_LSTM 이용\nmetrics = [accuracy,perplexity]\n\n\nlrnr = language_model_learner(dls, arch= AWD_LSTM, metrics=[accuracy,perplexity] )\n\n\nlrnr.fine_tune(3,1e-1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.811512\n      4.503637\n      0.284446\n      90.345108\n      00:54\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      3.995195\n      3.870983\n      0.333415\n      47.989555\n      00:56\n    \n    \n      1\n      3.719837\n      3.669204\n      0.353983\n      39.220692\n      00:56\n    \n    \n      2\n      3.442434\n      3.596037\n      0.364176\n      36.453487\n      00:56\n    \n  \n\n\n\n(3) “the price of” 이후에 이어질 단어들을 생성하라. (n_words=20 으로 설정할 것)\n\n## 생성예시\n\n\n\n\n\n\n\n\n'the price of stuff increases in other states as a result of the # coronavirus pandemic . So it makes alternatives .'\n\n\n\nlrnr.predict('the price of', n_words=20) \n\n\n\n\n\n\n\n\n'the price of what we need is on Check out some Facebook websites Live with you and adapt to'"
  },
  {
    "objectID": "posts/ml/2022-12-13-final_seoyeon.html#covid19-tweets-to-분류-30점",
    "href": "posts/ml/2022-12-13-final_seoyeon.html#covid19-tweets-to-분류-30점",
    "title": "Finalterm",
    "section": "2. COVID19 tweets \\(\\to\\) 분류 (30점)",
    "text": "2. COVID19 tweets \\(\\to\\) 분류 (30점)\n아래의 코드를 이용하여 자료를 다운로드 하라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/Corona_NLP_train.csv',encoding=\"ISO-8859-1\")\ndf\n\n\n\n\n\n  \n    \n      \n      UserName\n      ScreenName\n      Location\n      TweetAt\n      OriginalTweet\n      Sentiment\n    \n  \n  \n    \n      0\n      3799\n      48751\n      London\n      16-03-2020\n      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n      Neutral\n    \n    \n      1\n      3800\n      48752\n      UK\n      16-03-2020\n      advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order\n      Positive\n    \n    \n      2\n      3801\n      48753\n      Vagabonds\n      16-03-2020\n      Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P\n      Positive\n    \n    \n      3\n      3802\n      48754\n      NaN\n      16-03-2020\n      My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\n      Positive\n    \n    \n      4\n      3803\n      48755\n      NaN\n      16-03-2020\n      Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n      Extremely Negative\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41152\n      44951\n      89903\n      Wellington City, New Zealand\n      14-04-2020\n      Airline pilots offering to stock supermarket shelves in #NZ lockdown #COVID-19 https://t.co/cz89uA0HNp\n      Neutral\n    \n    \n      41153\n      44952\n      89904\n      NaN\n      14-04-2020\n      Response to complaint not provided citing COVID-19 related delays. Yet prompt in rejecting policy before consumer TAT is over. Way to go ?\n      Extremely Negative\n    \n    \n      41154\n      44953\n      89905\n      NaN\n      14-04-2020\n      You know itÂs getting tough when @KameronWilds  is rationing toilet paper #coronavirus #toiletpaper @kroger martinsville, help us out!!\n      Positive\n    \n    \n      41155\n      44954\n      89906\n      NaN\n      14-04-2020\n      Is it wrong that the smell of hand sanitizer is starting to turn me on?\\r\\r\\n\\r\\r\\n#coronavirus #COVID19 #coronavirus\n      Neutral\n    \n    \n      41156\n      44955\n      89907\n      i love you so much || he/him\n      14-04-2020\n      @TartiiCat Well new/used Rift S are going for $700.00 on Amazon rn although the normal market price is usually $400.00 . Prices are really crazy right now for vr headsets since HL Alex was announced and it's only been worse with COVID-19. Up to you whethe\n      Negative\n    \n  \n\n41157 rows × 6 columns\n\n\n\n(1) TextDataLoaders.from_df을 이용하여 dls오브젝트를 만들어라.\n\ntext_col=’OriginalTweet’로 설정\nlabel_col=’Sentiment’로 설정\nseq_len=64 로 설정\n\n\n## 올바르게 dls를 생성하였을 경우 dls.show_batch()의 결과는 아래와 같음. \n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxrep 5 ? ? ? xxrep 7 ? ? ? xxrep 7 ? xxrep 4 ? xxrep 4 ? xxrep 11 ? ? ? xxrep 6 ? xxrep 4 ? , xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 3 ? ? ? ? ? xxrep 4 ? ? ? xxrep 3 ? , xxrep 4 ? ? ? ? ? xxrep 6 ? xxrep 3 ? xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? \\r\\r\\n▁ xxrep 5 ? xxrep 6 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 4 ? ? ? xxrep 4 ? xxrep 6 ? xxrep 4 ? xxrep 8 ? ? ? xxrep 6 ? ? ? xxrep 5 ? ? ? xxrep 3 ? xxrep 4 ? ? ? xxrep 7 ? xxrep 5 ? - xxrep 8 ? xxrep 5\n      Neutral\n    \n    \n      1\n      xxbos xxrep 5 ? xxrep 5 ? ? ? xxrep 6 ? xxrep 5 ? xxrep 3 ? ? ? xxrep 3 ? ? ? xxrep 4 ? xxrep 3 ? xxrep 3 ? xxrep 5 ? xxrep 10 ? xxrep 5 ? xxrep 5 ? xxrep 3 ? xxrep 5 ? ? ? xxrep 4 ? xxrep 7 ? xxrep 3 ? xxrep 3 ? \\r\\r\\n▁ # sindh government spokesman @murtazawahab1 terms # xxmaj quarantine facilities at # xxmaj xxunk border a joke . xxmaj watch the exclusive visuals of criminal negligence ? ? https : / / t.co / xxunk\n      Negative\n    \n    \n      2\n      xxbos xxunk xxup very xxup soon xxup the xxup food xxup will xxup be xxup their xxup leverage xxup to xxup control xxup people . xxup hungry xxup people xxup are xxup easy xxup to xxup lead xxup if xxup you xxup promise xxup them xxup food . xxup they xxup are xxup not xxup just xxup killing xxup people xxup with xxup this xxup covid-19 , xxup but xxup the xxup big xxup farmers , xxup processors , xxup and xxup the xxup endless xxup chain xxup of xxup supply xxup and xxup dema\n      Extremely Positive\n    \n    \n      3\n      xxbos xxmaj when xxmaj disneyland xxup reopens it will xxup feature a xxup harrowing xxup new xxup death - defying xxup xxunk -- xxup going to the xxup grocery xxup store & & xxup interacting w / xxup people w / in 6 ' when your xxup facemask xxup suddenly xxup slips , your xxup gloves xxup fall xxup off & & xxup you xxup forgot your xxup hand xxup sanitizer ! ! # xxmaj disneyland # xxunk # xxmaj covid_19 # xxup covid https : / / t.co / xxunk\n      Positive\n    \n    \n      4\n      xxbos @gavinnewsom @govmurphy https : / / t.co / xxunk \\r\\r\\n xxup this xxup is xxup why xxup the # xxup coronavirus xxup is xxup so xxup contagious , a xxup single xxup cough xxup can xxup spread xxup across a xxup supermarket xxup aisle xxup right xxup over xxup the xxup aisle xxup and xxup into xxup the xxup next xxup aisle , xxup gross xxrep 4 ! xxup what xxup took xxup you xxup so xxup long xxup to xxup sign xxup an xxup eo xxup making xxup people\n      Extremely Negative\n    \n    \n      5\n      xxbos # xxup xxunk : # xxup worldwide , xxup y' all xxup alright xxup out xxup there ? xxup the # xxup xxunk xxup is xxup indoors xxup xxunk ' xxup care xxup of # xxup xxunk xxup behind xxup the xxup scenes , xxup xxunk ' xxup this # xxup coronavirus xxup outbreak … supermarket xxup xxunk ' xxup daily : xxup xxunk ' xxup like # xxup xxunk xxup out xxup here … . xxup bought 20 xxup xxunk  https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      6\n      xxbos # xxunk ? \\r\\r\\n\\r\\r\\n xxup with xxup no xxup sports xxup in xxup our xxup lives i xxup wanna xxup provide xxup y all xxup with xxup quality xxup xxunk xxup with xxup lower xxup prices xxup than xxup retail xxup all xxup xxunk xxup now xxup are 40 $ xxup each xxup until xxup sports xxup returns xxup due xxup to # xxmaj covid_19 \\r\\r\\n\\r\\r\\n xxup dms xxup are xxup always xxup open xxrep 4 ! xxup hmu xxrep 3 ! https : / / t.co / xxunk\n      Extremely Negative\n    \n    \n      7\n      xxbos # xxup cbd xxup can not xxup cure xxup the # xxup coronavirus xxup but xxup it xxup can \\r\\r\\n▁ ? xxup cure xxup coronavirus xxup symptoms \\r\\r\\n▁ ? xxup ease xxup your xxup anxiety \\r\\r\\n▁ ? xxup boost xxup immune xxup system \\r\\r\\n▁ ? xxup act xxup as a xxup natural xxup painkiller ! \\r\\r\\n * prices have been reduced at this difficult time to help everyone ? ? \\r\\r\\n https : / / t.co / wrlhyzizaa # keep safe https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      8\n      xxbos xxmaj what # coronavirus taught us : \\r\\r\\n 1 . xxmaj to stay at home and be with family . \\r\\r\\n 2 . xxmaj to eat home made , healthy , food . \\r\\r\\n 3 . xxmaj to maintain hygiene . \\r\\r\\n 4 . xxmaj to meditate . \\r\\r\\n 5 . xxmaj to give up junk food . \\r\\r\\n 6 . xxmaj to avoid unnecessary travel . \\r\\r\\n 7 . xxmaj to stockup groceries on time . \\r\\r\\n 8 . xxmaj help your spouse in daily xxunk .\n      Positive\n    \n  \n\n\n\n\ndls = TextDataLoaders.from_df(df,text_col='OriginalTweet',label_col='Sentiment',seq_len=64)\n\n\n\n\n\n\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxrep 5 ? ? ? xxrep 7 ? ? ? xxrep 7 ? xxrep 4 ? xxrep 4 ? xxrep 11 ? ? ? xxrep 6 ? xxrep 4 ? , xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 3 ? ? ? ? ? xxrep 4 ? ? ? xxrep 3 ? , xxrep 4 ? ? ? ? ? xxrep 6 ? xxrep 3 ? xxrep 3 ? xxrep 3 ? ? ? xxrep 3 ? \\r\\r\\n▁ xxrep 5 ? xxrep 6 ? ? ? xxrep 3 ? xxrep 4 ? xxrep 4 ? ? ? xxrep 4 ? xxrep 6 ? xxrep 4 ? xxrep 8 ? ? ? xxrep 6 ? ? ? xxrep 5 ? ? ? xxrep 3 ? xxrep 4 ? ? ? xxrep 7 ? xxrep 5 ? - xxrep 8 ? xxrep 5\n      Neutral\n    \n    \n      1\n      xxbos xxrep 5 ? : xxrep 4 ? xxrep 6 ? ? xxrep 8 ? \\r\\r\\n . \\r\\r\\n ? ? xxrep 6 ? xxrep 4 ? ? ? 500 xxrep 5 ? xxrep 5 ? ? ? xxrep 5 ? xxrep 7 ? xxrep 4 ? xxrep 7 ? ? ? xxrep 3 ? xxrep 7 ? xxrep 4 ? xxrep 6 ? xxrep 9 ? \" xxrep 6 ? \" xxrep 4 ? xxrep 7 ? xxrep 4 ? xxrep 6 ? xxrep 7 ? xxrep 5 ? xxrep 6 ? xxrep 3 ? xxrep 7 ? ? ? xxrep 5 ? .. \\r\\r\\n▁ # xxrep 3 ? _ xxrep 4 ? _ xxrep 6 ? _ xxrep 4 ? _ xxrep 3 ? https : / / t.co / xxunk\n      Neutral\n    \n    \n      2\n      xxbos xxup ask xxup your xxup self xxup what xxup do xxup you xxup think xxup is xxup going xxup to xxup happen xxup the xxup time xxup to xxup wake xxup up xxup is xxup now xxup do xxup you xxup think xxup food xxup going xxup to xxup be xxup xxunk xxup on xxup shop xxup shelfs .. no \\r\\r\\n xxup do xxup you xxup think xxup food xxup rise xxup in xxup price .. yes \\r\\r\\n xxup i m xxup going xxup to xxup stock xxup up xxup as xxup much i xxup can \\r\\r\\n xxup food xxup ladies xxup gentleman xxup is xxup most xxup valuable xxup asset \\r\\r\\n▁ # xxmaj coronavirus # xxup covid19 https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      3\n      xxbos # coronavirus xxmaj people , xxup stop xxup panic xxup buying , xxup you xxup do xxup not xxup need xxup to xxup stock xxup up xxup on xxup food xxup like xxup you xxup wo n't xxup see xxup the xxup sun xxup for xxup the xxup rest xxup of xxup your xxup life , xxup no xxup one xxup told xxup us xxup to xxup stock xxup up xxup on xxup toilet xxup paper , xxup no xxup one xxup told xxup us xxup to xxup stock xxup up xxup on xxup hand xxup sanitizer , xxup just xxup regular xxup soap xxup works xxup better . ( 1 )\n      Positive\n    \n    \n      4\n      xxbos xxup keep xxup your xxup home xxup safe & & xxup clean \\r\\r\\n xxmaj the xxmaj best xxmaj way to xxmaj avoid the # xxmaj coronavirus is in xxmaj clean xxmaj home \\r\\r\\n xxmaj absolutely xxmaj outstanding xxmaj cleaning @ xxmaj awesome xxmaj rates \\r\\r\\n xxmaj prices : 2 xxmaj hours 2 xxmaj maids $ 75 + \\r\\r\\n xxmaj serving xxmaj las # xxmaj vegas , # xxmaj summerlin , # xxmaj xxunk xxmaj city & & xxmaj more \\r\\r\\n https : / / t.co / xxunk \\r\\r\\n ( xxunk - xxunk \\r\\r\\n▁ # xxup xxunk # xxup xxunk # xxup xxunk https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      5\n      xxbos # coronavirus # xxmaj kits xxrep 3 ! # xxmaj masks ( 3 ply ) , # handsanitizer ( 75 % ) # xxunk , # xxmaj xxunk , # xxunk . xxmaj order xxmaj now xxrep 3 ! \\r\\r\\n ( # xxmaj families # xxmaj corporations \\r\\r\\n▁ # xxmaj entrepreneurs # hypebeast \\r\\r\\n▁ # xxup ap # xxunk # xxmaj cnn # xxup cdc # newyork # xxmaj atlanta # stlouis # xxmaj denver # xxup xxunk \\r\\r\\n▁ # xxmaj mayors # xxup xxunk ) \\r\\r\\n xxmaj visit : \\r\\r\\n https : / / t.co / xxunk https : / / t.co / xxunk\n      Neutral\n    \n    \n      6\n      xxbos # xxup lda xxmaj city xxmaj lahore xxmaj residential xxmaj files xxmaj prices xxmaj update \\r\\r\\n xxup lda xxmaj city xxmaj lahore 5 xxmaj marla xxunk xxmaj lacs \\r\\r\\n xxup lda xxmaj city xxmaj lahore 10 xxmaj marla xxunk xxmaj lacs \\r\\r\\n xxup lda xxmaj city xxmaj lahore 1 xxmaj xxunk xxunk xxmaj lacs \\r\\r\\n note : next xxmaj ballot will be xxmaj held on 18th xxmaj april 2020 \\r\\r\\n\\r\\r\\n xxmaj xxunk xxmaj xxunk \\r\\r\\n xxunk xxrep 3 3 41 xxrep 3 7 16 \\r\\r\\n\\r\\r\\n▁ # pandemic \\r\\r\\n▁ # coronavirusinpakistan \\r\\r\\n▁ # covid_19 https : / / t.co / xxunk\n      Neutral\n    \n    \n      7\n      xxbos xxmaj so glad i donât : \\r\\r\\n xxmaj live in a big city . \\r\\r\\n xxmaj rely on takeout / dine out . \\r\\r\\n xxmaj rely on delivery . \\r\\r\\n xxmaj rely on transport . \\r\\r\\n\\r\\r\\n xxmaj so glad i xxup do : \\r\\r\\n xxmaj have 6 month stock of food . \\r\\r\\n xxmaj own guns / ammo . \\r\\r\\n xxmaj have most family / friends within blocks . \\r\\r\\n xxmaj live in a small town with woods . \\r\\r\\n xxmaj have a big truck / fam car . \\r\\r\\n▁ # coronavirus https : / / t.co / xxunk\n      Extremely Positive\n    \n    \n      8\n      xxbos xxmaj in the wake of xxup covid 19 , xxmaj lets xxmaj share xxmaj burden . \\r\\r\\n\\r\\r\\n xxmaj enjoy xxmaj flat 10 % xxmaj off + xxmaj free xxmaj shipping and xxmaj xxunk . ? ? \\r\\r\\n\\r\\r\\n xxmaj hurry and shop our xxmaj products at xxup flat 10 % xxup off ! \\r\\r\\n\\r\\r\\n * term and xxmaj conditions xxmaj apply \\r\\r\\n https : / / t.co / xxunk \\r\\r\\n▁ # xxunk # sale # trackers # xxmaj xxunk # xxmaj fleet # xxmaj covid # xxmaj shopping # xxmaj tracking https : / / t.co / xxunk\n      Extremely Positive\n    \n  \n\n\n\n(2) text_classifier_learner를 이용하여 오브젝트를 생성하라. lrnr.fine_tune(5,1e-2)을 이용하여 학습하라.\n\narch= AWD_LSTM 이용\nmetrics = accuracy 이용\n\n\nlrnr = text_classifier_learner(dls,AWD_LSTM,metrics=accuracy).to_fp16()\n\n\nlrnr.fine_tune(5,1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.451519\n      1.370851\n      0.389625\n      00:08\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.158805\n      1.060165\n      0.558012\n      00:15\n    \n    \n      1\n      0.940539\n      0.825643\n      0.677317\n      00:16\n    \n    \n      2\n      0.786679\n      0.702226\n      0.732839\n      00:16\n    \n    \n      3\n      0.656008\n      0.682333\n      0.743288\n      00:16\n    \n    \n      4\n      0.593655\n      0.661963\n      0.758596\n      00:16\n    \n  \n\n\n\n(3) 아래의 텍스트에 대한 분류결과를 확인하라.\n\n“the government’s approach to the pendemic has been a complete disaster”\n“the new vaccines hold the promise of a quick return to economic growth”\n\nhint “the government’s approach to the pendemic has been a complete disaster” 에 대하여서는 부정으로, “the new vaccines hold the promise of a quick return to economic growth”에 대하여서는 긍정으로 예측되어야 적절하다.\nnegative\n\nlrnr.predict(\"the government's approach to the pendemic has been a complete disaster\") \n\n\n\n\n\n\n\n\n('Extremely Negative',\n tensor(0),\n tensor([8.7544e-01, 3.0173e-06, 1.2416e-01, 7.9043e-05, 3.0868e-04]))\n\n\npositive\n\nlrnr.predict(\"the new vaccines hold the promise of a quick return to economic growth\") \n\n\n\n\n\n\n\n\n('Extremely Positive',\n tensor(1),\n tensor([1.2628e-06, 9.2473e-01, 5.2453e-05, 4.5041e-05, 7.5169e-02]))"
  },
  {
    "objectID": "posts/ml/2022-12-13-final_seoyeon.html#human-numbers-5-40점",
    "href": "posts/ml/2022-12-13-final_seoyeon.html#human-numbers-5-40점",
    "title": "Finalterm",
    "section": "3. human numbers 5 (40점)",
    "text": "3. human numbers 5 (40점)\n아래와 같은 데이터가 있다고 하자.\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[:5], txt_y[:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\nx[:5],y[:5]\n\n(tensor([[0., 1., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0.],\n         [0., 0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 1., 0., 0.]], device='cuda:0'),\n tensor([[1., 0., 0., 0., 0., 0.],\n         [0., 0., 1., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 1., 0., 0.],\n         [1., 0., 0., 0., 0., 0.]], device='cuda:0'))\n\n\n(1) torch.nn.RNNCell()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(12345)\nrnncell=torch.nn.RNNCell(6,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,6).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()))\n\n\nT = len(x) \nfor epoc in range(500): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,20).to(\"cuda:0\") \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\") \nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nyhat = soft(linr(hidden))\n\n\nplt.matshow(yhat[-20:].data.to(\"cpu\"),cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f7fee9c2c50>\n\n\n\n\n\n(2) torch.nn.RNN()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(12345)\nrnn = torch.nn.RNN(6,20).to(\"cuda:0\")\nlinr=torch.nn.Linear(20,6).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()))\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\")\nfor epoc in range(8000):\n    ## 1 \n    hidden,hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\n\n\nplt.matshow(yhat.data[-20:].to(\"cpu\"),cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f8083af6990>\n\n\n\n\n\n(3) torch.nn.LSTMCell()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(12345) \nlstmcell = torch.nn.LSTMCell(6,4).to(\"cuda:0\") \nlinr = torch.nn.Linear(4,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(5000):\n    ## 1\n    hidden = []\n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[1., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:20],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','one','two','three','four','five']);\n\n\n\n\n(4) torch.nn.LSTM()을 이용하여 다음단어를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(12345) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(5000):\n    ## 1 \n    hidden, (hT,cT) =lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nplt.matshow(soft(output).data[-20:].to(\"cpu\"),cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(6),labels=[',','one','two','three','four','five']);\n\n\n\n\n\n참고: https://guebin.github.io/DL2022/posts/2022-11-29-13wk-2-final.html 의 1번풀이를 참고하세요"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html",
    "href": "posts/ml/2022-09-21-ml_3w.html",
    "title": "DNN (3주차)",
    "section": "",
    "text": "기계학습 특강 (3주차) 9월21일 [회귀분석, 선형모형, 손실함수, 경사하강법]"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#imports",
    "href": "posts/ml/2022-09-21-ml_3w.html#imports",
    "title": "DNN (3주차)",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#로드맵",
    "href": "posts/ml/2022-09-21-ml_3w.html#로드맵",
    "title": "DNN (3주차)",
    "section": "로드맵",
    "text": "로드맵\n- 회귀분석 \\(\\to\\) 로지스틱 \\(\\to\\) 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN)\n- 강의계획서"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#ref",
    "href": "posts/ml/2022-09-21-ml_3w.html#ref",
    "title": "DNN (3주차)",
    "section": "ref",
    "text": "ref\n- 넘파이 문법이 약하다면? (reshape, concatenate, stack)\n\nreshape: 아래 링크의 넘파이공부 2단계 reshape 참고\n\nhttps://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html\n\nconcatenate, stack: 아래 링크의 넘파이공부 4단계 참고\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#회귀모형-소개",
    "href": "posts/ml/2022-09-21-ml_3w.html#회귀모형-소개",
    "title": "DNN (3주차)",
    "section": "회귀모형 소개",
    "text": "회귀모형 소개\n- model: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n- model: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix} \\quad = \\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#회귀모형에서-데이터-생성",
    "href": "posts/ml/2022-09-21-ml_3w.html#회귀모형에서-데이터-생성",
    "title": "DNN (3주차)",
    "section": "회귀모형에서 데이터 생성",
    "text": "회귀모형에서 데이터 생성\n\n\ntorch.manual_seed(1)\n_r = torch.randn(100).sort() # 두번쨰는 index~\n\n\ntype(_r)\n\ntorch.return_types.sort\n\n\n\n_r[0]\n\ntensor([-3.3312, -2.5832, -2.2456, -2.1021, -1.6095, -1.6091, -1.5256, -1.4782,\n        -1.4465, -1.1608, -1.1334, -1.0703, -1.0373, -1.0233, -1.0055, -0.9962,\n        -0.9823, -0.9798, -0.9276, -0.9274, -0.8743, -0.8696, -0.8313, -0.8138,\n        -0.7981, -0.7773, -0.7735, -0.7502, -0.7479, -0.7121, -0.7040, -0.6970,\n        -0.6629, -0.6540, -0.6540, -0.6446, -0.6298, -0.6200, -0.6177, -0.6092,\n        -0.5962, -0.5601, -0.5065, -0.4757, -0.4610, -0.4370, -0.2515, -0.2223,\n        -0.2106, -0.1860, -0.1853, -0.1759, -0.1578, -0.1316, -0.1110, -0.1010,\n        -0.1002, -0.0721, -0.0288, -0.0255, -0.0075,  0.0103,  0.0457,  0.0612,\n         0.0663,  0.0998,  0.1374,  0.1530,  0.1578,  0.1938,  0.1991,  0.1991,\n         0.2284,  0.2444,  0.2927,  0.3037,  0.3434,  0.3956,  0.4415,  0.4676,\n         0.5451,  0.6155,  0.6995,  0.7317,  0.7626,  0.8073,  0.8539,  0.8657,\n         0.9386,  1.1017,  1.1120,  1.1651,  1.3851,  1.5392,  1.5748,  1.6734,\n         1.6871,  1.8793,  2.0154,  2.3571])\n\n\n\n_r[1]\n\ntensor([62, 90, 26, 92,  3,  7,  0, 94, 27, 17, 95, 98, 45, 65, 67, 74, 79,  6,\n        86, 48, 99, 61, 75, 85, 30, 10, 35,  1, 63,  8, 72, 16, 22,  2, 82, 59,\n        47, 93, 29,  5, 66, 77, 80, 39, 76, 51, 11, 12, 68, 58, 73, 25, 42, 31,\n        40, 96,  4, 33, 43, 64, 69, 71, 37, 28, 50, 81, 56, 38, 34, 89, 36, 19,\n        14, 21, 41,  9, 97, 78, 53, 15, 49, 88, 18, 83, 52, 23, 91, 20, 57, 24,\n        87, 54, 84, 60, 46, 70, 13, 32, 55, 44])\n\n\n\na,_ = _r[0],_r[1]\n\n\na\n\ntensor([-3.3312, -2.5832, -2.2456, -2.1021, -1.6095, -1.6091, -1.5256, -1.4782,\n        -1.4465, -1.1608, -1.1334, -1.0703, -1.0373, -1.0233, -1.0055, -0.9962,\n        -0.9823, -0.9798, -0.9276, -0.9274, -0.8743, -0.8696, -0.8313, -0.8138,\n        -0.7981, -0.7773, -0.7735, -0.7502, -0.7479, -0.7121, -0.7040, -0.6970,\n        -0.6629, -0.6540, -0.6540, -0.6446, -0.6298, -0.6200, -0.6177, -0.6092,\n        -0.5962, -0.5601, -0.5065, -0.4757, -0.4610, -0.4370, -0.2515, -0.2223,\n        -0.2106, -0.1860, -0.1853, -0.1759, -0.1578, -0.1316, -0.1110, -0.1010,\n        -0.1002, -0.0721, -0.0288, -0.0255, -0.0075,  0.0103,  0.0457,  0.0612,\n         0.0663,  0.0998,  0.1374,  0.1530,  0.1578,  0.1938,  0.1991,  0.1991,\n         0.2284,  0.2444,  0.2927,  0.3037,  0.3434,  0.3956,  0.4415,  0.4676,\n         0.5451,  0.6155,  0.6995,  0.7317,  0.7626,  0.8073,  0.8539,  0.8657,\n         0.9386,  1.1017,  1.1120,  1.1651,  1.3851,  1.5392,  1.5748,  1.6734,\n         1.6871,  1.8793,  2.0154,  2.3571])\n\n\n\n_ones = torch.ones(100)\n\n\nX = torch.stack([_ones,a]).T\n\n\n#같아요 _X = torch.stack([_ones,a],axis=1)\n\n\nϵ = torch.randn(100)*0.5\n\n\nx=4*2.5+ϵ\n\n\nx\n\ntensor([10.1027, 10.1526, 10.2678,  9.7844, 10.0786, 10.6270, 10.6638,  9.7523,\n         9.0098, 10.8993, 10.0509, 10.1700,  9.6777,  9.8565, 11.6606,  9.7990,\n         9.8485,  9.1191, 10.3174,  9.5978,  9.4814,  9.4665,  9.8957,  9.8922,\n        11.1476, 10.3375, 10.8567,  9.1029,  9.2396, 10.4598,  9.7258,  9.8264,\n        10.2365,  9.7857, 10.2757,  9.2263, 10.3787,  9.7966,  9.9361, 10.1402,\n        10.8730, 10.9275,  9.6468, 11.2785, 10.3853,  9.4630,  9.8992,  9.7199,\n         9.6880,  9.5114, 10.4374, 10.4936, 10.1252,  9.6035, 10.2616, 10.6118,\n         9.7983,  9.5204,  9.9974,  9.9606,  9.8054,  9.9602, 10.3802,  9.4987,\n         9.5180, 10.0708,  9.9182,  9.8209,  9.9703,  8.7540, 10.1211, 10.1442,\n        10.0516, 10.5502,  9.8292, 10.4737, 10.3112,  9.7759,  9.8572, 10.1940,\n        10.2575,  9.0763,  8.5416,  9.7163,  9.9647, 10.1735,  9.6732, 10.7793,\n        10.2000, 11.2211,  9.8091, 10.2163,  8.9914, 10.2118, 10.2865,  9.1019,\n         9.8469,  9.7899, 10.1414, 10.1821])\n\n\n\nW = torch.tensor([2.5,4])\n\n\nW.shape\n\ntorch.Size([2])\n\n\n곱하지지 않았어야하지만 곱해짐..!\n\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\ntorch.manual_seed(43052)\nones= torch.ones(100)\nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1)\nW = torch.tensor([2.5,4])\nϵ = torch.randn(100)*0.5\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.5+4*x,'--')\n\n\n\n\nblue를 observe한 상태에서 orange를 measure함\n학습이 된 상태: prediction을 제시할 수 있는 상태\nunderline function을 아는 상태는 w0와 w1을 아는 상태라고 할 수 있다.\n\\(x_{new}\\)가 주어졌을때 underline function과 얼마나 떨어져 있나 보면 되니까"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#회귀모형에서-학습이란",
    "href": "posts/ml/2022-09-21-ml_3w.html#회귀모형에서-학습이란",
    "title": "DNN (3주차)",
    "section": "회귀모형에서 학습이란?",
    "text": "회귀모형에서 학습이란?\n- 파란점만 주어졌을때, 주황색 점선을 추정하는것. 좀 더 정확하게 말하면 given data로 \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)를 최대한 \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는것.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\n\\(\\hat{y} = x \\hat{W}\\)\n- 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다.\n적당한?\n\nplt.plot(x,y,'o')\n\n\n\n\n- 시도: \\((\\hat{w}_0,\\hat{w}_1)=(-5,10)\\)을 선택하여 선을 그려보고 적당한지 판단.\n\nplt.plot(x,y,'o')\nplt.plot(x,-5+x*10,'--')\n\n\n\n\n\n\\(\\hat{y}_i=-5 +10 x_i\\) 와 같이 \\(y_i\\)의 값을 적합시키겠다는 의미\n\n- 벡터표현으로 주황색점선을 계산\n\nWhat = torch.tensor([-5.0,10.0])\n\n\nX.shape\n\ntorch.Size([100, 2])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'--')\n\n\n\n\ndata를 보고 architecture를 설계하는 modeling과정"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "href": "posts/ml/2022-09-21-ml_3w.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "title": "DNN (3주차)",
    "section": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)",
    "text": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)\n- 이론적으로 추론 <- 회귀분석시간에 배운것\n- 컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법) <- 우리가 오늘 파이토치로 실습해볼 내용.\n- 전략: 아래와 같은 3단계 전략을 취한다.\n\nstage1: 아무 점선이나 그어본다..\nstage2: stage1에서 그은 점선보다 더 좋은 점선으로 바꾼다.\nstage3: stage1 - 2 를 반복한다.\n\n\nStage1: 첫번째 점선 – 임의의 선을 일단 그어보자\n- \\(\\hat{w}_0=-5, \\hat{w}_1 = 10\\) 으로 설정하고 (왜? 그냥) 임의의 선을 그어보자.\n\n처음에는 \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}=\\begin{bmatrix} -5 \\\\ 10 \\end{bmatrix}\\) 를 대입해서 주황색 점선을 적당히 그려보자는 의미\n끝에 requires_grad=True는 나중에 미분을 위한 것\n\n\nWhat = torch.tensor([-5.0,10.0])\nWhat\n\ntensor([-5., 10.])\n\n\ntensor에서 tf.variable로 출력할떄롸 같은 결과임\n\nWhat = torch.tensor([-5.0,10.0],requires_grad=True)\nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n꼬리표가 생겼다.\n\nWhat.detach()\n\ntensor([-5., 10.])\n\n\n\nWhat.data\n\ntensor([-5., 10.])\n\n\n꼬리표가 사라졌다.\n꼬리표 있어도 계산은 되지만, matplot에서는 오류..\n그려보자!\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\n\n\n\n\n\n\nStage2: 첫번째 수정 – 최초의 점선에 대한 ‘적당한 정도’를 판단하고 더 ’적당한’ 점선으로 업데이트 한다.\n- ’적당한 정도’를 판단하기 위한 장치: loss function 도입!\n\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2\\)\n\\(=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n\nloss = torch.sum((y - X@What)**2)\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n- loss 함수의 특징 - \\(y_i \\approx \\hat{y}_i\\) 일수록 loss값이 작다. - \\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0,\\hat{w}_1)\\)을 잘 찍으면 loss값이 작다. - (중요) 주황색 점선이 ‘적당할 수록’ loss값이 작다.\n- 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. - 궁극적으로는 아예 모든 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾으면 좋겠다. (stage2에서 할일은 아님)\n- 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. - 적당해보이는 주황색 선을 찾자 \\(\\to\\) \\(loss(w_0,w_1)\\)를 최소로하는 \\((w_0,w_1)\\)의 값(정의역 set)을 찾자.\n- 수정된 목표: \\(loss(w_0,w_1)\\)를 최소로 하는 \\((w_0,w_1)\\)을 구하라. - 단순한 수학문제가 되었다. 마치 \\(loss(w)=w^2-2w+3\\) 을 최소화하는 \\(w\\)를 찾으라는 것과 같음. - 즉 “적당한 선으로 업데이트 하라 = 파라메터(\\(W\\))를 학습 하라 = 손실함수를 최소화 하라”\n- 우리의 무기: 경사하강법, 벡터미분\n\n\nStage2를 위한 경사하강법 복습\n경사하강법 아이디어 (1차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접선) <– 미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다.\n(서연필기) 접선 음수면 오른쪽으로 가고 접선 양수면 왼쪽으로 가쟈~\n경사하강법 아이디어 (2차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접평면) <– 편미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다.\n(서연필기) x,y 다르게 정의하는~편미분\nloss를 줄이도록 \\({\\bf W}\\)를 개선하는 방법\n- \\(수정값 \\leftarrow 원래값 - 기울어진크기(=미분계수) \\times \\alpha\\)\n\n여기에서 \\(\\alpha\\)는 전체적인 보폭의 크기를 결정한다. 즉 \\(\\alpha\\)값이 클수록 한번의 update에 움직이는 양이 크다.\n\n- \\({\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)\n(서연필기) 미분계수 반대로 움직이기 위해 마이너스(-) 취해주자\n(서연필기) 알파자체가 음수면 방향이 바뀌니까 양수!\n\n마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라.\n\\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1):\\) 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라.\n\\(\\alpha\\)의 의미: 전체적인 보폭의 속도를 조절, \\(\\alpha\\)가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다.\n\n\n- 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다.\n- 경사하강법으로 loss를 줄이기 위해서는 \\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (loss.backward()로 하면된다)\n\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n\nWhat.grad\n\n\nloss.backward()\n\n\nWhat.grad\n\ntensor([-1342.2523,  1188.9307])\n\n\n(서연필기) What.grad의 결과 값이 생겼다!\n\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n(서연필기) loss 계산할때 What에있는 꼬리표가 따라와서 loss에도 꼬리표가 붙었다.\n\nloss.backward()의 의미: loss를 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!\n\n\nloss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2)\n# 이었고 \nWhat=torch.tensor([-5.0,10.0],requires_grad=True)\n# 이므로 결국 What으로 미분하라는 의미. \n# 미분한 식이 나오는 것이 아니고, \n# 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. \n- 위에서 loss.backward()의 과정은 미분을 활용하여 \\((-5,10)\\)에서의 순간기울기를 구했다는 의미임.\n\nWhat,What.grad\n\n(tensor([-5., 10.], requires_grad=True), tensor([-1342.2523,  1188.9307]))\n\n\n- (-5,10)에서 loss의 순간기울기 값은 What.grad로 확인가능하다.\n\n이것이 의미하는건 \\((-5,10)\\)에서의 \\(loss(w_0,w_1)\\)의 순간기울기가 \\((-1342.2523, 1188.9307)\\) 이라는 의미\n\n- (확인1) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 손계산으로 검증하여 보자.\n\n\\(loss(w_0,w_1)=({\\bf y}-\\hat{\\bf y})^\\top ({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf XW})^\\top ({\\bf y}-{\\bf XW})\\)\n\\(\\frac{\\partial}{\\partial {\\bf W} }loss(w_0,w_1)=-2{\\bf X}^\\top {\\bf y}+2{\\bf X}^\\top {\\bf X W}\\)\n\n\n- 2 * X.T @ y + 2 * X.T @ X @ What\n\ntensor([-1342.2522,  1188.9305], grad_fn=<AddBackward0>)\n\n\n- (확인2) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 편미분을 간단히 구현하여 검증하여 보자.\n\n\\(\\frac{\\partial}{\\partial {\\bf W} } loss(w_0,w_1)=\\begin{bmatrix}\\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\end{bmatrix}loss(w_0,w_1) =\\begin{bmatrix}\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\end{bmatrix}\\)\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\n스칼라일때\nh = 0.01\n(loss(w+h) - loss(w)) / h\n\n_lossfn = lambda w0,w1: torch.sum((y-w0-w1*x)**2)\n_lossfn(-5,10)\n\ntensor(8587.6875)\n\n\n\nh=0.001\n(_lossfn(-5+h,10) - _lossfn(-5,10))/h,  (_lossfn(-5,10+h) - _lossfn(-5,10))/h\n\n(tensor(-1341.7968), tensor(1190.4297))\n\n\n-5,10에서의 편미분한 순간기울기\n\n약간 오차가 있지만 얼추비슷 \\(\\to\\) 잘 계산했다는 소리임\n\n(서연필기) 꼭 정확하진 않지!\n- 수정전, 수정하는폭, 수정후의 값은 차례로 아래와 같다.\n\nalpha=0.001 \nprint('수정전: ' + str(What.data)) # What 에서 미분꼬리표를 떼고 싶다면? What.data or What.detach()\nprint('수정하는폭: ' +str(-alpha * What.grad))\nprint('수정후: ' +str(What.data-alpha * What.grad))\nprint('*참값: (2.5,4)' )\n\n수정전: tensor([-5., 10.])\n수정하는폭: tensor([ 1.3423, -1.1889])\n수정후: tensor([-3.6577,  8.8111])\n*참값: (2.5,4)\n\n\n- Wbefore, Wafter 계산\n\nWbefore = What.data\nWafter = What.data- alpha * What.grad\nWbefore, Wafter\n\n(tensor([-5., 10.]), tensor([-3.6577,  8.8111]))\n\n\ndata쓰는지 grad 쓰는지 명확히\n- Wbefore, Wafter의 시각화\n\nplt.plot(x,y,'o')\nplt.plot(x,X@Wbefore,'--')\nplt.plot(x,X@Wafter,'--')\n\n\n\n\n\n\n\nStage3: Learn (=estimate \\(\\bf\\hat{W})\\)\n- 이 과정은 Stage1,2를 반복하면 된다.\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True) #\n\n\nalpha=0.001 \nfor epoc in range(30): ## 30번 반복합니다!! \n    yhat=X@What \n    loss=torch.sum((y-yhat)**2)\n    loss.backward() \n    What.data = What.data-alpha * What.grad\n    What.grad=None\n\n(서연필기) What.grad=None 해주는 이유는 grad가 미분을 누적하기 때문에 막아주기 위해서\n\n원래 철자는 epoch이 맞아요\n\n- 반복결과는?! (최종적으로 구해지는 What의 값은?!) - 참고로 true\n\nWhat.data ## true인 (2.5,4)와 상당히 비슷함\n\ntensor([2.4290, 4.0144])\n\n\n- 반복결과를 시각화하면?\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "href": "posts/ml/2022-09-21-ml_3w.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "title": "DNN (3주차)",
    "section": "파라메터의 학습과정 음미 (학습과정 모니터링)",
    "text": "파라메터의 학습과정 음미 (학습과정 모니터링)\n\n학습과정의 기록\n- 기록을 해보자.\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.detach().tolist())\n    What.grad=None\n\n(서연필기) list 그대로 받으니까 꼬리표 삭제\n- \\(\\hat{y}\\) 관찰 (epoch=3, epoch=10, epoch=15)\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[2],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[9],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[14],'--')\n\n\n\n\n\nlen(yhat_history[0])\n\n100\n\n\n- \\(\\hat{\\bf W}\\) 관찰\n\nWhat_history\n\n[[-3.657747745513916, 8.81106948852539],\n [-2.554811716079712, 7.861191749572754],\n [-1.649186372756958, 7.101552963256836],\n [-0.9060714244842529, 6.49347448348999],\n [-0.29667866230010986, 6.006272315979004],\n [0.2027742564678192, 5.615575313568115],\n [0.6119104623794556, 5.302003383636475],\n [0.9469034671783447, 5.050129413604736],\n [1.2210699319839478, 4.847657680511475],\n [1.4453645944595337, 4.684779167175293],\n [1.6287915706634521, 4.553659439086914],\n [1.778746247291565, 4.448036193847656],\n [1.90129816532135, 4.3628973960876465],\n [2.0014259815216064, 4.294229507446289],\n [2.0832109451293945, 4.238814353942871],\n [2.149996757507324, 4.194070339202881],\n [2.204521894454956, 4.157923698425293],\n [2.249027729034424, 4.128708839416504],\n [2.285348415374756, 4.105085849761963],\n [2.31498384475708, 4.0859761238098145],\n [2.339160442352295, 4.070511341094971],\n [2.3588807582855225, 4.057991027832031],\n [2.3749637603759766, 4.0478515625],\n [2.3880786895751953, 4.039637088775635],\n [2.3987717628479004, 4.032979965209961],\n [2.40748929977417, 4.027583599090576],\n [2.414595603942871, 4.023208141326904],\n [2.4203879833221436, 4.019659042358398],\n [2.4251089096069336, 4.016779899597168],\n [2.4289560317993164, 4.014443874359131]]\n\n\n- loss 관찰\n\nplt.plot(loss_history)\n\n\n\n\n\n\n학습과정을 animation으로 시각화\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n- 왼쪽에는 \\((x_i,y_i)\\) and \\((x_i,\\hat{y}_i)\\) 을 그리고 오른쪽에는 \\(loss(w_0,w_1)\\) 을 그릴것임\n\nfig = plt.figure()\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n\n\n\n- 왼쪽그림!\n\nax1.plot(x,y,'o')\nline, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요해요..\n\n\nfig\n\n\n\n\n- 오른쪽 그림1: \\(loss(w_0,w_1)\\)\n\n_w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n_w1 = np.arange(-6, 11, 0.5)\nw1,w0 = np.meshgrid(_w1,_w0)\nlss=w0*0\nfor i in range(len(_w0)):\n    for j in range(len(_w1)):\n        lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\nax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \nax2.azim = 40  ## 3d plot의 view 조절 \nax2.dist = 8   ## 3d plot의 view 조절 \nax2.elev = 5   ## 3d plot의 view 조절 \n\n\nfig\n\n\n\n\n- 오른쪽 그림2: \\((w_0,w_1)=(2.5,4)\\) 와 \\(loss(2.5,4)\\) 값 <- loss 함수가 최소가 되는 값 (이거 진짜야? ㅋㅋ)\n\nax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f65aca1f040>\n\n\n\nfig\n\n\n\n\n- 오른쪽 그림3: \\((w_0,w_1)=(-3.66, 8.81)\\) 와 \\(loss(-3.66,8.81)\\) 값\n\nWhat_history[0]\n\n[-3.657747745513916, 8.81106948852539]\n\n\n\nax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='grey') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f65ac8728b0>\n\n\n\nfig\n\n\n\n\n- 애니메이션\n\ndef animate(epoc):\n    line.set_ydata(yhat_history[epoc])\n    ax2.scatter(What_history[epoc][0],What_history[epoc][1],loss_history[epoc],color='grey')\n    return line\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 함수로 만들자..\n\ndef show_lrpr(data,history):\n    x,y = data \n    loss_history,yhat_history,What_history = history \n    \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,y,'o')\n    line, = ax1.plot(x,yhat_history[0]) \n    ## ax2: 오른쪽그림 \n    _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n    _w1 = np.arange(-6, 11, 0.5)\n    w1,w0 = np.meshgrid(_w1,_w0)\n    lss=w0*0\n    for i in range(len(_w0)):\n        for j in range(len(_w1)):\n            lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\n    ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \n    ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n    ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='b') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n    ax2.azim = 40  ## 3d plot의 view 조절 \n    ax2.dist = 8   ## 3d plot의 view 조절 \n    ax2.elev = 5   ## 3d plot의 view 조절 \n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(서연필기) 알파의 정도에 따라 학습 속도가 달라져.."
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#alpha에-대하여-alpha는-학습률",
    "href": "posts/ml/2022-09-21-ml_3w.html#alpha에-대하여-alpha는-학습률",
    "title": "DNN (3주차)",
    "section": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)",
    "text": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)\n\n(1) \\(\\alpha=0.0001\\): \\(\\alpha\\) 가 너무 작다면? \\(\\to\\) 비효율적이다.\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(2) \\(\\alpha=0.0083\\): \\(\\alpha\\)가 너무 크다면? \\(\\to\\) 다른의미에서 비효율적이다 + 위험하다..\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0083\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(3) \\(\\alpha=0.0085\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0085\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad.data; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(서연필기) 최솟값보다 오히려 커지는 경향이 나와버림\n\n\n(4) \\(\\alpha=0.01\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.01\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3w.html#숙제",
    "href": "posts/ml/2022-09-21-ml_3w.html#숙제",
    "title": "DNN (3주차)",
    "section": "숙제",
    "text": "숙제\n- 학습률(\\(\\alpha\\))를 조정하며 실습해보고 스크린샷 제출\n\n(1) \\(\\alpha=0.0015\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.015\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(2) \\(\\alpha=0.0038\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0038\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html",
    "href": "posts/ml/2022-12-07-13wk.html",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "",
    "text": "copy"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#틀린이유",
    "href": "posts/ml/2022-12-07-13wk.html#틀린이유",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "틀린이유",
    "text": "틀린이유\n\nid(a)\n\n139660748276272\n\n\n\nid(b)\n\n139660748276272\n\n\n실제로는 a,b가 저장된 메모리 주소가 동일함"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1",
    "href": "posts/ml/2022-12-07-13wk.html#예제1",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1",
    "text": "예제1\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\n\n여기에서 a,b,c는 모두 같은 value를 가진다.\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3, 4]\n\n\n\nc\n\n[1, 2, 3, 4]\n\n\n하지만 그 id까지 같은 것은 아니다.\n\nid(a), id(b), id(c)\n\n(139660748305984, 139660748305984, 139660748286224)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2",
    "href": "posts/ml/2022-12-07-13wk.html#예제2",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2",
    "text": "예제2\n\na=[1,2,3] \nb=a \na=[1,2,3]+[4] \n\na를 다시 정의한 것이라 보면 될 듯\n\na,b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n\nid(a), id(b)\n\n(139660748313296, 139660748290960)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1-1",
    "href": "posts/ml/2022-12-07-13wk.html#예제1-1",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1",
    "text": "예제1\n\na=1+2021\nid(a)\n\n139660748450224\n\n\n일단 할당\n\nb=2023-1\nid(b)\n\n139660748450768\n\n\n독립적으로 오브젝트 만들었으니 id가 다르지\n\nid(2022)\n\n139660748450832"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2-이제-다-이해했다고-생각했는데..",
    "href": "posts/ml/2022-12-07-13wk.html#예제2-이제-다-이해했다고-생각했는데..",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2: 이제 다 이해했다고 생각했는데..",
    "text": "예제2: 이제 다 이해했다고 생각했는데..\n\na=1+2 \nid(a)\n\n7402432\n\n\n\nb=4-1\nid(b)\n\n7402432\n\n\n이게 왜 똑같지..?\n(해설) 파이썬의 경우 효율성을 위해서 -5~256까지의 정수를 미리 저장해둠.\n\nid(3)\n\n7402432"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1-2",
    "href": "posts/ml/2022-12-07-13wk.html#예제1-2",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1",
    "text": "예제1\n\n아래의 예제를 살펴보자. (참조를 제대로 이해했다면 아래의 예제는 자연스럽게 이해가능)\n\n\nl1 = [3, [66,55,44]]\nl2 = l1\n\n\nid(l1),id(l2)\n\n(139660748249920, 139660748249920)\n\n\n\nl1[0]=4\n\n\nl1\n\n[4, [66, 55, 44]]\n\n\n\nl2\n\n[4, [66, 55, 44]]\n\n\n\nl2.append(5)\nl2\n\n[4, [66, 55, 44], 5]\n\n\n\nl1\n\n[4, [66, 55, 44], 5]"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2-r과-같이-를-쓰고-싶다면",
    "href": "posts/ml/2022-12-07-13wk.html#예제2-r과-같이-를-쓰고-싶다면",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2: R과 같이 = 를 쓰고 싶다면?",
    "text": "예제2: R과 같이 = 를 쓰고 싶다면?\n\nl1 = [3, [66,55,44]]\nl2 = l1.copy()\n\n\nid(l1),id(l2) ## 드디어 주소가 달라졌다.\n\n(139660748530944, 139660748530784)\n\n\n주소 달라짐!!\n\nl1[0]=100\n\n\nl1\n\n[100, [66, 55, 44]]\n\n\n\nl2\n\n[3, [66, 55, 44]]"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제3-이제-다-이해했다고-생각했는데..",
    "href": "posts/ml/2022-12-07-13wk.html#예제3-이제-다-이해했다고-생각했는데..",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제3: 이제 다 이해했다고 생각했는데..",
    "text": "예제3: 이제 다 이해했다고 생각했는데..\n\n이제 다 이해했다고 생각했는데..\n\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nid(l1),id(l2)\n\n(139660807804960, 139660807804560)\n\n\n\nl1[1].append(33)\n\n\nl1\n\n[3, [66, 55, 44, 33]]\n\n\n\nl2\n\n[3, [66, 55, 44, 33]]\n\n\n왜 또 참조한것마냥 l1과 l2가 같이 바뀌고 있지?"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1-3",
    "href": "posts/ml/2022-12-07-13wk.html#예제1-3",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1",
    "text": "예제1\n\na=2222\nb=2222\n\n\nid(a),id(b)\n\n(139660739479536, 139660739478192)\n\n\n메모리 상황\n\n2222라는 오브젝트가 어떤공간(id(a): 139753545300880)에 생성되고 그 공간에 a라는 라벨이 붙음\n2222라는 오브젝트가 어떤공간(id(b): 139753545300880)에 생성되고 그 공간에 b라는 라벨이 붙음\n\n즉 -5~256 이외의 2개의 메모리 공간을 추가적으로 사용"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2-1",
    "href": "posts/ml/2022-12-07-13wk.html#예제2-1",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2",
    "text": "예제2\n\na=[1,2,2222]\nb=[1,2,2222]\n\n메모리 공간에 1,2는 이미 저장되어 있어서 생성할 필요 없고 2022만 메모리 공간에 생성하면 된다.\n\nid(a), [id(a[0]),id(a[1]),id(a[2])] # a=[1,2,2222]\n\n(139660739560064, [7402368, 7402400, 139660739480272])\n\n\n\nid(b), [id(b[0]),id(b[1]),id(b[2])] # b=[1,2,2222] \n\n(139660739560864, [7402368, 7402400, 139660739479312])\n\n\n\na.append(4)\n\n\na\n\n[1, 2, 2222, 4]\n\n\n\nb\n\n[1, 2, 2222]\n\n\n메모리상황\n\n-5~256까지의 숫자는 미리 메모리에 저장되어 있다.(이터닝) 이중에서 1은 id(a[0]): 7394656, 2는 id(a[1]): 7394688에 저장되어있음.\n2222가 공간 id(a[2]): 139753178093776에서 만들어진다.\n어떠한 리스트오브젝트가 공간 id(a): 139753182327904에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 a라는 포스트잇을 붙인다.\n2222가 공간 id(a)[2]: 139753178095568에서 만들어진다.\n어떠한 리스트오브젝트가 공간 id(b): 139753173818656에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 b라는 포스트잇을 붙인다.\na라는 포스트잇이 붙은 공간으로 이동하여 원소에 4를 추가시킨다.\n\n즉 -5~256이외에 4개의 메모리 공간을 추가사용 (a,b,a의 2222,b의 2222)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제3",
    "href": "posts/ml/2022-12-07-13wk.html#예제3",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제3",
    "text": "예제3\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139660748288960, [7402432, 139660739562784])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139660739539504, [7402432, 139660739562784])\n\n\n메모리상황\n\n-5~256까지의 숫자가 메모리에 저장되어 있다.\n저장된 숫자중 66,55,44를 묶어서 리스트로 구성하고 이 리스트를 공간 id(l1[1]): 139753183707216에 저장.\n숫자 3과 공간 id(l1[1]): 139753183707216에 저장된 리스트 [66,55,44]를 하나로 묶어서 새로운 리스트를 구성하고 이를 공간 id(l1): 139753183437040에 저장. 공간 id(l1): 139753183437040에 l1이라는 포스트잇 생성.\n공간 id(l2): 139753182311120에 l1의 원소들을 모아서 새로운 리스트를 구성함. 공간 id(l2): 139753182311120에 l2라는 포스트잇 생성.\n\n\nl1[0] = 7777\nl1,l2\n\n([7777, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139660748288960, [139660739478544, 139660739562784])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139660739539504, [7402432, 139660739562784])\n\n\n\nl1[0]은 원래 공간 7394720와 binding 되어 있었음.\n\n그런데 7777이라는 새로운 오브젝트가 공간 id(l1): 139753178092080에 생성되고 l1[0]이 공간 139753178092080와 다시 binding 됨."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제4",
    "href": "posts/ml/2022-12-07-13wk.html#예제4",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제4",
    "text": "예제4\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1.append(7777)\n\n\nl1,l2\n\n([3, [66, 55, 44], 7777], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1]), id(l1[2])]\n\n(139660739540064, [7402432, 139660748258672, 139660739477968])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139660748282608, [7402432, 139660748258672])\n\n\n\n예제3, 예제4를 통하여 리스트가 가변형객체라는 것을 확인할 수 있다. 예제3의 경우 l1이 저장되어있던 메모리공간의 내용물이 [3,[66,55,44]] 에서 [7777,[66,55,44]] 로 바뀌었다. 예제4의 경우 l1이 저장되어있던 메모리공간의 내용물이 [3,[66,55,44]] 에서 [3,[66,55,44],7777] 로 바뀌었다."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제5-우리를-힘들게-했던-그-예제.",
    "href": "posts/ml/2022-12-07-13wk.html#예제5-우리를-힘들게-했던-그-예제.",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제5: 우리를 힘들게 했던 그 예제.",
    "text": "예제5: 우리를 힘들게 했던 그 예제.\n(시점1)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nl1,l2\n\n([3, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139660748600336, [7402432, 139660748255792])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139660748237712, [7402432, 139660748255792])\n\n\n(시점2)\n\nl1[1].append(7777)\n\nl1[1]의 묶음방식이 저장된 공간에 7777 추가할 거야\n\nl1,l2\n\n([3, [66, 55, 44, 7777]], [3, [66, 55, 44, 7777]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139660748600336, [7402432, 139660748255792])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139660748237712, [7402432, 139660748255792])\n\n\n해설: 사실 시점1에서 메모리 주소상황을 잘 이해했다면 신기한 일이 아니다. .copy()는 l1과 l2의 주소만 다르게 만들 뿐 내용물인 l1[0],l1[1]는 동일하니까."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제6-신임교수최규빈이영미",
    "href": "posts/ml/2022-12-07-13wk.html#예제6-신임교수최규빈이영미",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제6: 신임교수=[‘최규빈’,‘이영미’]",
    "text": "예제6: 신임교수=[‘최규빈’,‘이영미’]\n- 최규빈, 이영미는 신임교수임\n\n신임교수 = ['최규빈','이영미']\n\n\nid(신임교수), id('최규빈'), id('이영미')\n\n(139660748511984, 139660748209680, 139660748209200)\n\n\n- 신임교수를 누군가는 막내들이라고 부르기도 함.\n\n막내들 = 신임교수 \n\n참조\n\nid(막내들), id(신임교수)\n\n(139660748511984, 139660748511984)\n\n\n“막내들”이라는 단어와 “신임교수”라는 단어는 사실 같은 말임\n여기까지 참조 설명\n- 새로운 교수 “박혜원”이 뽑혔음.\n\n신임교수.append(\"박혜원\")\n\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 전북대 통계학과에서 R특강팀을 구성하여 방학중 R교육을 실시하고자함. 특강팀은 우선 신임교수들로 구성.\n\nR특강팀 = 신임교수.copy()\nR특강팀 \n\n['최규빈', '이영미', '박혜원']\n\n\n- R특강팀에 최혜미교수님 추가. (그렇지만 최혜미교수님이 막내는 아니야.. // 참조와 shallow copy의 차이점)\n\nR특강팀.append(\"최혜미\") \n\n\nR특강팀, 신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원', '최혜미'], ['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- R특강팀에서 양성준 교수를 추가하여 파이썬 특강팀을 구성\n\n파이썬특강팀 = [R특강팀, \"양성준\"]\n파이썬특강팀\n\n[['최규빈', '이영미', '박혜원', '최혜미'], '양성준']\n\n\n- 이영미교수는 다른 일이 많아서 R특강 팀에서 제외됨. (그럼 자연히 파이썬에서도 제외됨!!)\n\nR특강팀.remove(\"이영미\")\n\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])\n\n\n하지만 이영미교수는 여전히 신임교수이면서 막내들임\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 새로운 교수로 “손흥민”이 임용됨.\n\n막내들.append(\"손흥민\")\n\n\n막내들, 신임교수\n\n(['최규빈', '이영미', '박혜원', '손흥민'], ['최규빈', '이영미', '박혜원', '손흥민'])\n\n\n- 그렇다고 해서 손흥민 교수가 바로 R이나 파이썬 특강팀에 자동소속되는건 아님\nshallow copy로 id 주소가 각각 할당되었기 때문\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1-motivation-example",
    "href": "posts/ml/2022-12-07-13wk.html#예제1-motivation-example",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1: Motivation example",
    "text": "예제1: Motivation example\n- 아래의 상황을 다시 생각해보자.\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = 파이썬특강팀.copy()\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '최혜미']])\n\n\n이슈: 이영미교수가 파이썬특강에서 제외되면서 ADSP특강팀에서도 제외되었음. 그런데 사실 이영미교수가 파이썬특강팀에서만 제외되길 원한 것이지 ADSP특강팀에서 제외되길 원한게 아닐수도 있음.\n해결: Deep copy의 사용\n\nimport copy\n\n패키지 필요\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = copy.deepcopy(파이썬특강팀)\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '이영미', '최혜미']])\n\n\n\ncopy?\n\n\n\nType:        module\nString form: <module 'copy' from '/home/csy/anaconda3/envs/py37/lib/python3.7/copy.py'>\nFile:        ~/anaconda3/envs/py37/lib/python3.7/copy.py\nDocstring:  \nGeneric (shallow and deep) copying operations.\nInterface summary:\n        import copy\n        x = copy.copy(y)        # make a shallow copy of y\n        x = copy.deepcopy(y)    # make a deep copy of y\nFor module specific errors, copy.Error is raised.\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\nPython's deep copy operation avoids these problems by:\n a) keeping a table of objects already copied during the current\n    copying pass\n b) letting user-defined classes override the copying operation or the\n    set of components copied\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor array, nor\nany similar types.\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2-2",
    "href": "posts/ml/2022-12-07-13wk.html#예제2-2",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2",
    "text": "예제2\n- deepcopy\n\nl1 = [3,[66,[55,44]]] \nl2 = copy.deepcopy(l1)\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 139660748278352\nl2: 139660748540176\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 139660748278352 [7402432, 139660739509776]\nl2: 139660748540176 [7402432, 139660739569056]\n\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 139660748278352 [7402432, [7404448, 139660748244224]]\nl2: 139660748540176 [7402432, [7404448, 139660739567776]]\n\n\n묶음 방식이 달라지면서 다른 주소가 할당된 모습\n- 비교를 위한 shallow copy\n\nl1 = [3,[66,[55,44]]] \nl2 = l1.copy()\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 139660739568976\nl2: 139660748318112\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 139660739568976 [7402432, 139660807428768]\nl2: 139660748318112 [7402432, 139660807428768]\n\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 139660739568976 [7402432, [7404448, 139660748313776]]\nl2: 139660748318112 [7402432, [7404448, 139660748313776]]\n\n\n- 비교를 위한 참조\n\nl1 = [3,[66,[55,44]]] \nl2 = l1\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 139660748238432\nl2: 139660748238432\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 139660748238432 [7402432, 139660748240128]\nl2: 139660748238432 [7402432, 139660748240128]\n\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 139660748238432 [7402432, [7404448, 139660748531664]]\nl2: 139660748238432 [7402432, [7404448, 139660748531664]]\n\n\n주소 다 똑같아"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제1-4",
    "href": "posts/ml/2022-12-07-13wk.html#예제1-4",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제1",
    "text": "예제1\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]]\nl2= l1.copy() \nl1[-1].append(33)\n\n\nl1,l2\n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44, 33]])\n\n\nshallow copy 썼으니까 같이 추가된 모습"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제2-3",
    "href": "posts/ml/2022-12-07-13wk.html#예제2-3",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제2",
    "text": "예제2\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \n\n\nl1,l2\n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44]])\n\n\n\nid(l1),id(l2)\n\n(139660748276032, 139660748245744)\n\n\nl1에 재할당한 것이라 생각하면 된다."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제3-1",
    "href": "posts/ml/2022-12-07-13wk.html#예제3-1",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제3",
    "text": "예제3\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\n\n\nl1,l2\n\n([3, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1),id(l2)\n\n(139660807804240, 139660748282048)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제4-1",
    "href": "posts/ml/2022-12-07-13wk.html#예제4-1",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제4",
    "text": "예제4\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n(잘못된 상상) 아래의 코드와 결과가 같을거야!!\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \n# l1[-1] = l1[-1]+[33] \n# l1[-1].remove(33)\nl1[-1].append(33)\n\n\nl1,l2 \n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44, 33]])\n\n\n(하지만 현실은)\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nl1,l2\n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44]])\n\n\n재할당의 개념이 있었기 때문에 주소가 다른게 할당된 것이고, 결국 다른 결과가 나온다."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#예제5",
    "href": "posts/ml/2022-12-07-13wk.html#예제5",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "예제5",
    "text": "예제5\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nl1,l2\n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44, 33]])\n\n\n\nid(l1),id(l2)\n\n(139660739562464, 139660739561344)\n\n\n주소는 다르지만 재할당의 개념이 없어!\nl1에 append 하는 식으로 되어서 묶음 방식이 같이 움직인다.\n\n??? 예제4랑 예제5는 같은코드가 아니었음!!! a += [1] 는 새로운 오브젝트를 만드는게 아니고, 기존의 오브젝트를 변형하는 스타일의 코드였음! (마치 append 메소드처럼)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#motivation-example",
    "href": "posts/ml/2022-12-07-13wk.html#motivation-example",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "Motivation example",
    "text": "Motivation example\n- 우리는 이제 아래의 내용은 마스터함\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1].append(33)이랑 같은거..\n\n\nl1,l2\n\n([3, [66, 55, 44, 33]], [3, [66, 55, 44, 33]])\n\n\n- 아래의 결과를 한번 예측해볼까?\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\nl2[1] += (33,)\n\n이번엔 튜플로\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44, 33)])"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#해설",
    "href": "posts/ml/2022-12-07-13wk.html#해설",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "해설",
    "text": "해설\n(시점1)\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\n\n이번엔 리스트 안에 튜플을 넣어봄\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 139660739586080\nl2: 139660807832512\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 139660739586080 [7402432, 139660748091792]\nl2: 139660807832512 [7402432, 139660748091792]\n\n\n(시점2)\n\nl2[1] += (33,)\n\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44, 33)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 139660739586080\nl2: 139660807832512\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 139660739586080 [7402432, 139660748091792]\nl2: 139660807832512 [7402432, 139660739627824]\n\n\n주소 139753182280032에 있는 값을 바꾸고 싶지만 불변형이라 못바꿈 \\(\\to\\) 그냥 새로 만들자. 그래서 그걸 139753174874064에 저장하자.\n튜플의 묶음 방식은 불변형! 정의: 값을 변환할 수 없는 오브젝트다~ 새로 오브젝트 만들어 저장함.\n리스트 : 값을 바꿀 수 있는 오브젝트다, 묶음 방식을 바꿀 수 있다."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#차원의-실체",
    "href": "posts/ml/2022-12-07-13wk.html#차원의-실체",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "2차원의 실체",
    "text": "2차원의 실체\n- 2차원 array a,b를 선언하자.\n\na = np.array([[11,22,33,44]]).reshape(2,2)\nb = np.array([[11,22,33,44,55,66]]).reshape(2,3)\nc = np.array([11,22,33,44]).reshape(4,1)\nd = np.array([11,22,33,44])\n\n- a,b,c,d 속성비교\n\na.shape, b.shape, c.shape, d.shape ## 차원 \n\n((2, 2), (2, 3), (4, 1), (4,))\n\n\n\na.strides, b.strides, c.strides, d.strides ## 차원이랑 관련이 있어보임.. + 8의 배수 \n\n((16, 8), (24, 8), (8, 8), (8,))\n\n\n- strides는 무엇?\n\nstrides: (다음 행으로 가기위해서 JUMP해야하는 메모리 공간수, 다음 열로 가기위해서 JUMP해야하는 메모리 공간수)\n\n- 사실 a,b,c,d 는 모두 1차원으로 저장되어있음. (중첩된 리스트꼴이 아니라)\nshape이나 strides 등의 옵션으로 1차원이 아니게 보여지는 것 뿐"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#참조",
    "href": "posts/ml/2022-12-07-13wk.html#참조",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "참조",
    "text": "참조\n- a를 선언, b는 a의 참조\n\na=np.array([[1,2],[3,4]])\nb=a ## 참조 \n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n- a의 shape을 바꾸어보자 \\(\\to\\) b도 같이 바뀐다\n\na.shape = (4,)\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3, 4])\n\n\n\nid(a),id(b)\n\n(139660739644368, 139660739644368)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#view",
    "href": "posts/ml/2022-12-07-13wk.html#view",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "view",
    "text": "view\n- a를 선언, b는 a의 view\n\na=np.array([[1,2],[3,4]]) \nb=a.view() ## shallow copy 라고 부르기도 한다. \n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n\na.shape= (4,1)\n\n\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nid(a), id(b)\n\n(139660382911248, 139660382911344)\n\n\nview가 shallow copy 같은 이유 껍데기 주소만 복사해옴, 공간에 대한 주소는 같지 않음\n- 그런데..\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n- 출생의 비밀\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n\nb.base\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\n? 이거 바뀐 a아니야?\n\n\nid(b.base), id(a)\n\n(139660382911248, 139660382911248)\n\n\n- View - b가 a의 뷰라는 의미는, b가 a를 소스로하여 만들어진 오브젝트란 의미이다. - 따라서 이때 b.base는 a가 된다. - b는 자체적으로 데이터를 가지고 있지 않으며 a와 공유한다. - 이러한 의미에서 view를 shallow copy 라고 부른다. (stride, shape과 같은 껍데기만 새로 생성, base는 유지)\nnote1 원본 ndarray의 일 경우는 .base가 None으로 나온다.\n\na.base\n\n\nprint(a.base)\n\nNone\n\n\nnote2 b.base의 shpae과 b의 shape은 아무 관련없다.\n\nb.shape\n\n(2, 2)\n\n\n\nb.base.shape\n\n(4, 1)"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#copy",
    "href": "posts/ml/2022-12-07-13wk.html#copy",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "copy",
    "text": "copy\n- a를 선언, b는 a의 copy\n\na=np.array([[1,2],[3,4]])\nb=a.copy() # 껍데기를 새로 생성 (strides, shape) + base도 새로생성 \n\n\nid(a),id(b)\n\n(139660382910288, 139660382910000)\n\n\n- a의 shape을 바꿔도 b에는 적용되지 않음\n\na.shape = (4,1)\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- 그리고 a[0]의 값을 바꿔도 b에는 적용되지 않음.\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- b의 출생을 조사해보니..\n\na.base,b.base\n\n(None, None)\n\n\n출생의 비밀은 없었다. 둘다 원본.\n- .view() 는 껍데기만, .copy() 는 껍데기 + base 까지 새로생성\n\nAppendix: .copy의 한계(?)\n\na=np.array([1,[1,2]],dtype='O')\na\n\narray([1, list([1, 2])], dtype=object)\n\n\n\nb=a.copy()\n\n\nb\n\narray([1, list([1, 2])], dtype=object)\n\n\n\na[0]=222\n\n\na\n\narray([222, list([1, 2])], dtype=object)\n\n\n\nb\n\narray([1, list([1, 2])], dtype=object)\n\n\n\na[1][0]=333\n\n\na\n\narray([222, list([333, 2])], dtype=object)\n\n\n\nb\n\narray([1, list([333, 2])], dtype=object)\n\n\n해결책: 더 깊은 복사\n\nimport copy \n\n\na=np.array([1,[1,2]],dtype='O')\nb=copy.deepcopy(a)\n\n\na\n\narray([1, list([1, 2])], dtype=object)\n\n\n\nb\n\narray([1, list([1, 2])], dtype=object)\n\n\n\na[0]=100\n\n\na,b\n\n(array([100, list([1, 2])], dtype=object),\n array([1, list([1, 2])], dtype=object))\n\n\n\na[1][0]=200\n\n\na,b\n\n(array([100, list([200, 2])], dtype=object),\n array([1, list([1, 2])], dtype=object))\n\n\n- 중간요약\n\n사실 .copy()는 온전한 deep copy 가 아니라 level 2 deep copy 이다.\n따라서 .copy()는 base의 정보를 shallow copy 한다 (level 1 deep copy 한다.)\n그래서 base가 다시 중첩구조를 가지는 경우는 온전한 deep-copy가 수행되지 않는다.\n그런데 일반적으로 넘파이를 이용할때 자주 사용하는 데이터 구조인 행렬, 텐서등은 base가 중첩구조를 가지지 않는다. (1차원 array로만 저장되어 있음)\n따라서 행렬, 텐서에 한정하면 .copy()는 온전한 deep copy라고 이해해도 무방하다.\n\n행/열 같지 않으면 numpy쓰면 힘들걸..\n모든 데이터 구조가 2차원까지로 정리 된다."
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#별명-뷰-카피",
    "href": "posts/ml/2022-12-07-13wk.html#별명-뷰-카피",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "별명, 뷰, 카피",
    "text": "별명, 뷰, 카피\n- test 함수 작성\n\ndef test(a,b): \n    if id(a) == id(b): \n        print(\"별명\")\n    elif id(a) == id(b.base) or id(a.base)==id(b): \n        print(\"뷰\")\n    elif (id(a.base)!=id(None) and id(b.base)!=id(None)) and id(a.base) == id(b.base):\n        print(\"공통의 base를 가짐\")\n    else: \n        print(\"카피, 혹은 아무 관련없는 오브젝트\") \n\n- 잘 동작하나?\n(테스트1)\n\na=np.array([1,2,3,4])\nb=a\n\n\ntest(a,b)\n\n별명\n\n\n참조\n(테스트2)\n\na=np.array([1,2,3,4])\nb=a.view()\n\n\ntest(a,b)\n\n뷰\n\n\n(테스트3)\n\na=np.array([1,2,3,4])\nb=a.view()\nc=a.view()\n\n\ntest(b,c)\n\n공통의 base를 가짐\n\n\n\ntest(a,b)\n\n뷰\n\n\n\ntest(a,c)\n\n뷰\n\n\n(테스트4)\n\na=np.array([1,2,3,4])\nb=a.copy()\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트"
  },
  {
    "objectID": "posts/ml/2022-12-07-13wk.html#결론",
    "href": "posts/ml/2022-12-07-13wk.html#결론",
    "title": "A1: 깊은복사와 얕은복사 (12주차)",
    "section": "결론",
    "text": "결론\n- 우리가 사용했던 어떠한 것들이 뷰가 나올지 카피가 나올지 사실 잘 모른다. (그래서 원리를 이해해도 대응할 방법이 사실없음)\n\n예시1\n\na=np.array([1,2,3,4])\nb=a[:3]\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3])\n\n\n\ntest(a,b)\n\n뷰\n\n\n\nc=a[[0,1,2]]\nc\n\narray([1, 2, 3])\n\n\n\ntest(a,c)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\nd = a[3]\n\n\ntest(a,d)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\n\n예시2\n\na=np.array([[1,2],[3,4]])\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb=a.flatten()\nc=a.ravel()\nd=a.reshape(-1)\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\ntest(a,c)\n\n뷰\n\n\n\ntest(a,d)\n\n뷰\n\n\n\ntest(c,d)\n\n공통의 base를 가짐\n\n\n\ntest(b,c)\n\n카피, 혹은 아무 관련없는 오브젝트"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html",
    "href": "posts/ml/2022-12-21-Extra-2.html",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "",
    "text": "GAN– MNIST 3/7"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#생성모형이란-쉬운-설명",
    "href": "posts/ml/2022-12-21-Extra-2.html#생성모형이란-쉬운-설명",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "생성모형이란? (쉬운 설명)",
    "text": "생성모형이란? (쉬운 설명)\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#gan의-응용분야",
    "href": "posts/ml/2022-12-21-Extra-2.html#gan의-응용분야",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "GAN의 응용분야",
    "text": "GAN의 응용분야\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면?\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요….."
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#생성모형이란-통계학과-버전의-설명",
    "href": "posts/ml/2022-12-21-Extra-2.html#생성모형이란-통계학과-버전의-설명",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "생성모형이란? 통계학과 버전의 설명",
    "text": "생성모형이란? 통계학과 버전의 설명\n\n제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다."
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#gan의-원리",
    "href": "posts/ml/2022-12-21-Extra-2.html#gan의-원리",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "GAN의 원리",
    "text": "GAN의 원리\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n위조범: 더 정교하게 만들자..\n경찰: 더 판별능력을 업그레이드 하자!\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#import",
    "href": "posts/ml/2022-12-21-Extra-2.html#import",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "import",
    "text": "import\n\nimport torch \nfrom fastai.vision.all import *\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#data",
    "href": "posts/ml/2022-12-21-Extra-2.html#data",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "data",
    "text": "data\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\nthrees = (path/'train'/'3').ls()\n\n\nX = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255\n\n\nX.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\nplt.imshow(X[0])\n\n<matplotlib.image.AxesImage at 0x7f36568083d0>\n\n\n\n\n\n- MLP를 이용해 학습하기 위해 X의 차원을 변경\n\nX=X.reshape(6131,28*28)\nX.shape\n\ntorch.Size([6131, 784])"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#위조지폐범의-설계-noise-to-가짜이미지를-만들어-내는-네트워크를-만들자.",
    "href": "posts/ml/2022-12-21-Extra-2.html#위조지폐범의-설계-noise-to-가짜이미지를-만들어-내는-네트워크를-만들자.",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "위조지폐범의 설계: noise \\(\\to\\) 가짜이미지를 만들어 내는 네트워크를 만들자.",
    "text": "위조지폐범의 설계: noise \\(\\to\\) 가짜이미지를 만들어 내는 네트워크를 만들자.\n- 네트워크의 입력? 적당한 벡터, 혹은 매트릭스에 노이즈(랜덤으로 채운 어떠한 숫자들)를 채운것\n- 네트워크의 출력? (28,28)의 텐서, 784의 벡터\n\nnet1 = torch.nn.Sequential(torch.nn.Linear(in_features=28, out_features=64),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64, out_features=64), \n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64, out_features=784),\n                           torch.nn.Sigmoid()) ## 마지막의 시그모이드는 출력이 0~1사이로 나오게 하기 위함 \ncounterfeiter = net1"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#경찰의-설계-진짜이미지는-1-가짜이미지는-0으로-판별하는-dnn을-만들자.",
    "href": "posts/ml/2022-12-21-Extra-2.html#경찰의-설계-진짜이미지는-1-가짜이미지는-0으로-판별하는-dnn을-만들자.",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "경찰의 설계: 진짜이미지는 1, 가짜이미지는 0으로 판별하는 DNN을 만들자.",
    "text": "경찰의 설계: 진짜이미지는 1, 가짜이미지는 0으로 판별하는 DNN을 만들자.\n- 네트워크의 입력? (28,28)의 텐서, 혹은 784의 벡터\n- 네트워크의 출력? yhat (y는 0 or 1)\n\nnet2 = torch.nn.Sequential(torch.nn.Linear(in_features=784,out_features=64),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64,out_features=28),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=28,out_features=1),\n                           torch.nn.Sigmoid()\n                           )\npolice = net2"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#스토리전개",
    "href": "posts/ml/2022-12-21-Extra-2.html#스토리전개",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "스토리전개",
    "text": "스토리전개\n- 아래는 진짜이미지\n\nrealimage=X[0].reshape(28,28)\nplt.imshow(realimage)\n\n<matplotlib.image.AxesImage at 0x7f365676c850>\n\n\n\n\n\n- 위와 같은 진짜 이미지를 경찰이 봤음 \\(\\to\\) yhat이 나오겠죠?\n\npolicehat_from_realimage = police(realimage.reshape(-1))\npolicehat_from_realimage\n\ntensor([0.5015], grad_fn=<SigmoidBackward0>)\n\n\n\n진짜 이미지일수록 policehat_from_realimage \\(\\approx\\) 1 이어야 함\n하지만 그렇지 못함 (배운것이 없는 무능한 경찰)\n\n- 이번에는 가짜이미지를 경찰이 봤다고 생각해보자.\n(step1) 랜덤으로 아무숫자나 28개를 생성한다.\n\nerr= torch.randn(28)\nerr\n\ntensor([ 3.0166, -1.1472, -0.0645,  1.1425, -0.5632, -1.7625,  0.4353,  0.2066,\n        -0.4214,  0.2895, -1.7030, -0.8060,  0.4684,  0.5643, -0.2158,  0.9998,\n        -1.0958, -1.5119, -1.5331,  1.0283,  0.0254,  0.6410,  0.8624,  1.0544,\n        -0.6116,  0.5087, -0.0657, -0.7712])\n\n\n(step2) 위조범은 err를 입력으로 받고 가짜이미지를 만든다.\n\ncouterfeiter_output = counterfeiter(err)\nfakeimage=couterfeiter_output.reshape(28,28)\nplt.imshow(fakeimage.detach())\n\n<matplotlib.image.AxesImage at 0x7f36563d4550>\n\n\n\n\n\n\n누가봐도 가짜자료임\n위조범의 실력이 형편없음\n\n(step3) 위조범이 생성한 이미지를 경찰한테 넘긴다.\n\npolicehat_from_fakeimage = police(couterfeiter_output)\n#policehat_from_fakeimage = police(fakeimage.detach().reshape(-1))\npolicehat_from_fakeimage\n\ntensor([0.5014], grad_fn=<SigmoidBackward0>)\n\n\n- 경찰의 실력도 형편없고 위조범의 실력도 형편없다."
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#경찰네트워크의-실력을-향상시키자.",
    "href": "posts/ml/2022-12-21-Extra-2.html#경찰네트워크의-실력을-향상시키자.",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "경찰네트워크의 실력을 향상시키자.",
    "text": "경찰네트워크의 실력을 향상시키자.\n- 데이터 정리 - 원래 \\(n=6131\\)개의 이미지 자료가 있음. 이를 \\({\\bf X}\\)라고 하자. 따라서 \\({\\bf X}\\)의 차원은 (6131,784). - 위조범이 만든 가짜자료를 원래 자료와 같은 숫자인 6131개 만듬. 이 가짜자료를 \\(\\tilde{\\bf X}\\)라고 하자. 따라서 \\(\\tilde{\\bf X}\\)의 차원은 (6131,784). - 진짜자료는 1, 가짜자료는 0으로 라벨링.\n\nX.shape\n\ntorch.Size([6131, 784])\n\n\n\nerr= torch.randn(6131,28)\ncounterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n\n\nreal_label=torch.tensor([[1.0]]*6131) ## y=1 \nfake_label=torch.tensor([[0.0]]*6131) ## y=0\n\n- step1: yhat, 경찰의 예측\n\npolicehat_from_realimage = police(X) \npolicehat_from_fakeimage = police(counterfeiter_output) \n\n- step2: 손실함수? 경찰의 미덕은 (1) 가짜를 가짜라고 하고 (2) 진짜를 진짜라 해야한다.\n\nloss_fn = torch.nn.BCELoss() \n\n\nloss_of_police =\\\nloss_fn(policehat_from_fakeimage,fake_label)+\\\nloss_fn(policehat_from_realimage,real_label)\n\nloss_of_police\n\ntensor(1.3793, grad_fn=<AddBackward0>)\n\n\n- step3~4는 미분이후 업데이트\n- 옵티마이저를 설계하자.\n\noptimizer_of_police = torch.optim.Adam(police.parameters())\n\n- for 문을 돌리자.\n\nfor i in range(50): \n    ## 1 yhat \n    policehat_from_realimage = police(X) \n    \n    #policehat_from_fakeimage = police(Xitlde)\n    err= torch.randn(6131,28)\n    counterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n    policehat_from_fakeimage= police(counterfeiter_output)\n    \n    ## 2 loss \n    loss_of_police =\\\n    loss_fn(policehat_from_fakeimage,fake_label)+\\\n    loss_fn(policehat_from_realimage,real_label)\n    \n    ## 3 back propagation \n    loss_of_police.backward()\n    \n    ## 4 update\n    optimizer_of_police.step()\n    optimizer_of_police.zero_grad()\n\n- 훈련된 경찰의 성능을 살펴보자.\n\npolice(counterfeiter_output)\n\ntensor([[0.0022],\n        [0.0022],\n        [0.0023],\n        ...,\n        [0.0022],\n        [0.0022],\n        [0.0022]], grad_fn=<SigmoidBackward0>)\n\n\n\npolice(X)\n\ntensor([[0.9996],\n        [0.9991],\n        [0.9994],\n        ...,\n        [0.9992],\n        [0.9902],\n        [0.9917]], grad_fn=<SigmoidBackward0>)\n\n\n- 우수한 경찰 (비록 위조범의 수준이 낮긴하지만)"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#위조범네트워크의-성능을-향상시키자.",
    "href": "posts/ml/2022-12-21-Extra-2.html#위조범네트워크의-성능을-향상시키자.",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "위조범네트워크의 성능을 향상시키자.",
    "text": "위조범네트워크의 성능을 향상시키자.\n- 자료구조: X는 임의의 에러이미지, net(X)는 fakeimage\n\nerr=torch.randn(6131,28) \ncounterfeiter_output= counterfeiter(err) \n\n- 손실함수: 잘 훈련된 경찰조차도 잘못된 판단을 내릴만큼 가짜지폐를 잘 만들면 위조범의 실력이 우수하다 볼 수 있음\n\npolicehat_from_fakeimage = police(counterfeiter_output) \nloss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label) ## 가짜이미지를 보고 경찰이 진짜라고 믿으면 위조범의 실력이 좋은것임  \n\n- 옵티마이저\n\noptimizer_of_counterfeiter = torch.optim.Adam(counterfeiter.parameters())\n\n- 학습\n\nfor i in range(50): \n    ## 1 \n    err=torch.randn(6131,28) \n    counterfeiter_output= counterfeiter(err)  \n    policehat_from_fakeimage = police(counterfeiter_output) \n    ## 2 \n    loss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label)\n    ## 3 \n    loss_of_counterfeiter.backward()\n    ## 4 \n    optimizer_of_counterfeiter.step()\n    optimizer_of_counterfeiter.zero_grad()\n\n- 위조범의 실력향상을 감상해보자.\n\nplt.imshow(counterfeiter_output[0].reshape(28,28).data)\n\n<matplotlib.image.AxesImage at 0x7f36562f7410>"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-2.html#두-적대적-네트워크를-경쟁시키자.",
    "href": "posts/ml/2022-12-21-Extra-2.html#두-적대적-네트워크를-경쟁시키자.",
    "title": "Extra-2: 생성모형(GAN)",
    "section": "두 적대적 네트워크를 경쟁시키자.",
    "text": "두 적대적 네트워크를 경쟁시키자.\n\nfor k in range(100): \n    for i in range(50): \n        ## 1 yhat \n        policehat_from_realimage = police(X) \n    \n        #policehat_from_fakeimage = police(Xitlde)\n        err= torch.randn(6131,28)\n        counterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n        policehat_from_fakeimage= police(counterfeiter_output)\n    \n        ## 2 loss \n        loss_of_police =\\\n        loss_fn(policehat_from_fakeimage,fake_label)+\\\n        loss_fn(policehat_from_realimage,real_label)\n    \n        ## 3 back propagation \n        loss_of_police.backward()\n    \n        ## 4 update\n        optimizer_of_police.step()\n        optimizer_of_police.zero_grad()\n        \n    for i in range(50): \n        ## 1 \n        err=torch.randn(6131,28) \n        counterfeiter_output= counterfeiter(err)  \n        policehat_from_fakeimage = police(counterfeiter_output) \n        ## 2 \n        loss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label)\n        ## 3 \n        loss_of_counterfeiter.backward()\n        ## 4 \n        optimizer_of_counterfeiter.step()\n        optimizer_of_counterfeiter.zero_grad()        \n\n- 위조범의 최종적 실력향상감상\n\nplt.imshow(counterfeiter_output[0].reshape(28,28).data)\n\n<matplotlib.image.AxesImage at 0x7f365528e7d0>"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_2.html",
    "href": "posts/ml/2022-10-26-ml_8w_2.html",
    "title": "CNN (8주차) 2",
    "section": "",
    "text": "기계학습 특강 (8주차) 10월26일–(2) [이미지자료분석 - Transfer Learning, CAM (설명가능한 인공지능모형, XAI)]"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_2.html#imports",
    "href": "posts/ml/2022-10-26-ml_8w_2.html#imports",
    "title": "CNN (8주차) 2",
    "section": "imports",
    "text": "imports\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_2.html#transfer-learning",
    "href": "posts/ml/2022-10-26-ml_8w_2.html#transfer-learning",
    "title": "CNN (8주차) 2",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls()\n\n(#3) [Path('/home/csy/.fastai/data/cifar10/train'),Path('/home/csy/.fastai/data/cifar10/labels.txt'),Path('/home/csy/.fastai/data/cifar10/test')]\n\n\n\n!ls '/home/csy/.fastai/data/cifar10/train'\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\n수제네트워크\n\ndls\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test') \n\n\n_X,_y = dls.one_batch()\n_X.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\n!ls '/home/csy/.fastai/data/cifar10/train' # 10개의 클래스\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ndls.show_batch()\n\n\nlrnr 생성\n\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(3,128,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\nnet는 cpu에 있고 X는 gpu에 있으니 cpu로 불러오자\n\n#net1(_X.to(\"cpu\")).shape\n\n\nnet = torch.nn.Sequential(\n    net1, \n    torch.nn.Linear(25088,10)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy) \n\n\nnet.to(\"cuda:0\")\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n  (1): Linear(in_features=25088, out_features=10, bias=True)\n)\n\n\n\n학습\n\n\nX,y=dls.one_batch()\n\n\nlrnr.model(X).shape\n\ntorch.Size([64, 10])\n\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.266191\n      1.226523\n      0.572100\n      00:05\n    \n    \n      1\n      1.128916\n      1.124115\n      0.609800\n      00:05\n    \n    \n      2\n      1.025027\n      1.076060\n      0.629600\n      00:05\n    \n    \n      3\n      0.956499\n      1.071469\n      0.636600\n      00:05\n    \n    \n      4\n      0.852002\n      1.033129\n      0.650600\n      00:05\n    \n    \n      5\n      0.811420\n      1.071609\n      0.641600\n      00:05\n    \n    \n      6\n      0.735469\n      1.074108\n      0.648300\n      00:04\n    \n    \n      7\n      0.703909\n      1.094982\n      0.648800\n      00:04\n    \n    \n      8\n      0.623525\n      1.132971\n      0.645000\n      00:05\n    \n    \n      9\n      0.589313\n      1.157667\n      0.637900\n      00:05\n    \n  \n\n\n\n\n이게 생각보다 잘 안맞아요.. 70넘기 힘듬\n\n\n\n전이학습 (남이 만든 네트워크)\n\nlrnr 생성\n\n학습되어 있는 파라메터까지 같이 가져오기\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n\\(k=1000\\) 즉 1000개의 물체를 구분하는 모형임\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy)\n\n\n학습\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.813206\n      0.955131\n      0.677300\n      00:21\n    \n    \n      1\n      0.636926\n      0.719258\n      0.760700\n      00:21\n    \n    \n      2\n      0.538001\n      0.802607\n      0.765500\n      00:21\n    \n    \n      3\n      0.446174\n      0.591965\n      0.804200\n      00:20\n    \n    \n      4\n      0.339985\n      0.677038\n      0.786200\n      00:20\n    \n    \n      5\n      0.283703\n      0.664880\n      0.797400\n      00:21\n    \n    \n      6\n      0.221962\n      0.734830\n      0.787000\n      00:21\n    \n    \n      7\n      0.183193\n      0.720297\n      0.798000\n      00:21\n    \n    \n      8\n      0.160181\n      0.785769\n      0.790900\n      00:21\n    \n    \n      9\n      0.144745\n      0.745676\n      0.804400\n      00:21\n    \n  \n\n\n\n\nCIFAR10을 맞추기 위한 네트워크가 아님에도 불구하고 상당히 잘맞음\n일반인이 거의 밑바닥에서 설계하는것보다 전이학습을 이용하는 것이 효율적일 경우가 많다.\n\n\n\n전이학습 다른 구현: 순수 fastai 이용\n- 예전코드 복습\n\npath = untar_data(URLs.PETS)/'images'\n\n\nfiles= get_image_files(path)\n\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nlrnr = cnn_learner(dls,resnet34,metrics=accuracy)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.204861\n      0.011182\n      0.995940\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.056896\n      0.009584\n      0.996617\n      00:44\n    \n  \n\n\n\n- 사실 위의 코드가 transfer learning 이었음.\n\n#collapse_output\nlrnr.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)\n\n\n\nXAI(설명가능한 인공지능)\n딥러닝 연구의 네가지 축 - step 1. 아키텍처 - 최근 연구 특징 : 비전문가 + 블랙박스(안 보이는 의미) - 설명가능한 딥러닝에 대한 요구 - step 2. 손실함수 - step 3. 미분계산 - step 4. 옵티마이저"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_2.html#cam",
    "href": "posts/ml/2022-10-26-ml_8w_2.html#cam",
    "title": "CNN (8주차) 2",
    "section": "CAM",
    "text": "CAM\n\nCAM이란?\n\nref: http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf\n\n- Class Activation Mapping (CAM)은 설명가능한 인공지능모형 (eXplainable Artificial Intelligence, XAI) 중 하나로 CNN의 판단근거를 시각화하는 기술\n\n\n학습에 사용할 데이터 Load\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath.ls()\n\n(#7393) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\n\nfiles= get_image_files(path)\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\n\n구현0단계– 예비학습\n\n# 하나의 이미지 선택\n\nximg = PILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0]\nx,x.shape\n\n(TensorImage([[[[0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                ...,\n                [0.8745, 0.8784, 0.8824,  ..., 0.8902, 0.8863, 0.8824],\n                [0.9059, 0.8980, 0.8902,  ..., 0.8824, 0.8863, 0.8824],\n                [0.8863, 0.8863, 0.8824,  ..., 0.8784, 0.8863, 0.8863]],\n \n               [[0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                ...,\n                [0.8784, 0.8824, 0.8863,  ..., 0.8745, 0.8667, 0.8588],\n                [0.9098, 0.9020, 0.8902,  ..., 0.8745, 0.8706, 0.8627],\n                [0.8902, 0.8902, 0.8784,  ..., 0.8784, 0.8745, 0.8706]],\n \n               [[0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                ...,\n                [0.8863, 0.8902, 0.8980,  ..., 0.8784, 0.8706, 0.8667],\n                [0.9176, 0.9137, 0.9059,  ..., 0.8745, 0.8706, 0.8667],\n                [0.8980, 0.9020, 0.8980,  ..., 0.8745, 0.8706, 0.8667]]]],\n             device='cuda:0'),\n torch.Size([1, 3, 512, 512]))\n\n\n\n\n# AP layer\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) \n\n\nX = torch.arange(48).reshape(1,3,4,4)*1.0 \nX\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(X)\n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\nX[0,0,...].mean(),X[0,1,...].mean(),X[0,2,...].mean()\n\n(tensor(7.5000), tensor(23.5000), tensor(39.5000))\n\n\n\n\n# torch.einsum\n(예시1)\n\ntsr = torch.arange(12).reshape(4,3)\ntsr\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n\ntorch.einsum('ij->ji',tsr)\n\ntensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])\n\n\n(예시2)\n\ntsr1 = torch.arange(12).reshape(4,3).float()\ntsr2 = torch.arange(15).reshape(3,5).float()\n\n\ntsr1 @ tsr2\n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n\ntorch.einsum('ij,jk -> ik',tsr1,tsr2) \n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n(예시3)\n\nx.to(\"cpu\").shape\n\ntorch.Size([1, 3, 512, 512])\n\n\ntorch,einsum을 사용하여 shape을 아래로 변경\n\ntorch.einsum('ocij -> ijc',x.to(\"cpu\")).shape\n\ntorch.Size([512, 512, 3])\n\n\n\nplt.imshow(torch.einsum('ocij -> ijc',x.to(\"cpu\")))\n\n<matplotlib.image.AxesImage at 0x7fe60eea0e50>\n\n\n\n\n\n\n\n\n구현1단계– 이미지분류 잘하는 네트워크 선택\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\nlrnr = cnn_learner(dls,resnet34,metrics=accuracy)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.180252\n      0.032132\n      0.989851\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.053625\n      0.008279\n      0.997970\n      00:44\n    \n  \n\n\n\n\n\n구현2단계– 네트워크의 끝 부분 수정\n- 모형의 분해\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\nnet1이 2d part, net1이 1d part\n- net2를 좀더 살펴보자.\n\nnet2\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nprint(net1(_X).shape)\nprint(net2[0](net1(_X)).shape)\nprint(net2[1](net2[0](net1(_X))).shape)\nprint(net2[2](net2[1](net2[0](net1(_X)))).shape)\n\ntorch.Size([64, 512, 16, 16])\ntorch.Size([64, 1024, 1, 1])\ntorch.Size([64, 1024])\ntorch.Size([64, 1024])\n\n\n- net2를 아래와 같이 수정하고 재학습하자 (왜?)\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -> (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -> (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -> (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy) # loss_fn??\n\n\nlrnr2.loss_func, lrnr.loss_func ## 알아서 기존의 loss function으로 잘 들어가 있음. \n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5) # net2를 수정해서 accuracy가 안좋아지긴 했는데 그래도 쓸만함 \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.240225\n      0.521585\n      0.826793\n      00:44\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.139931\n      0.159443\n      0.940460\n      00:44\n    \n    \n      1\n      0.123673\n      0.396028\n      0.864682\n      00:44\n    \n    \n      2\n      0.094375\n      0.136513\n      0.952639\n      00:44\n    \n    \n      3\n      0.052172\n      0.057100\n      0.977673\n      00:44\n    \n    \n      4\n      0.028230\n      0.041083\n      0.985792\n      00:44\n    \n  \n\n\n\n\n\n구현3단계– 수정된 net2에서 Linear와 AP의 순서를 바꿈\n- 1개의 observation을 고정하였을 경우 출력과정 상상\n\nximg = PILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nprint(net1(x).shape)\nprint(net2[0](net1(x)).shape)\nprint(net2[1](net2[0](net1(x))).shape)\nprint(net2[2](net2[1](net2[0](net1(x)))).shape)\n\ntorch.Size([1, 512, 16, 16])\ntorch.Size([1, 512, 1, 1])\ntorch.Size([1, 512])\ntorch.Size([1, 2])\n\n\n- 최종결과 확인\n\nnet(x)\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n아마 모델 달라서 값이 다른 것일까..!\n\ndls.vocab\n\n['cat', 'dog']\n\n\n\nnet(x)에서 뒤쪽의 값이 클수록 ’dog’를 의미한다.\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n- 아래와 같이 순서를 바꿔서 한번 계산해보고 싶다. (왜???..)\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기에서 (1,512,16,16) -> (1,2,16,16) 로 가는 선형변환을 적용하는 방법? (16,16) each pixel에 대하여 (512 \\(\\to\\) 2)로 가는 변환을 수행\n\n- 통찰: 이 경우 특이하게도 레이어의 순서를 바꿨을때 출력이 동일함 (선형변환하고 평균내거나 평균내고 선형변환하는건 같으니까)\n\n_x =torch.tensor([1,2,3.14,4]).reshape(4,1)\n_x \n\ntensor([[1.0000],\n        [2.0000],\n        [3.1400],\n        [4.0000]])\n\n\n\n_l1 = torch.nn.Linear(1,1,bias=False)\n_l1(_x).mean() # _x -> 선형변환 -> 평균 \n\ntensor(-0.2621, grad_fn=<MeanBackward0>)\n\n\n\n_l1(_x.mean().reshape(1,1)) # _x -> 평균 -> 선형변환\n\ntensor([[-0.2621]], grad_fn=<MmBackward0>)\n\n\n- 구현해보자.\n\nnet2[2].weight.shape,net1(x).shape\n\n(torch.Size([2, 512]), torch.Size([1, 512, 16, 16]))\n\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\nnet2[0](why)\n\nTensorImage([[[[-6.7946]],\n\n              [[ 8.0881]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nnet(x)\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n\n잠깐 멈추고 생각\n- 이미지\n\nximg\n\n\n\n\n- 네트워크의 결과\n\nnet2(net1(x))\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n-9.0358 << 9.0926 이므로 ’ximg’는 높은 확률로 개라는 뜻이다.\n\n내거에서는 9.0926이 10.2985\n- 아래의 네트워크를 관찰\n\\[\\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}} = [-9.0358,9.0926]\\]\n\nnet2[0](why)\n\nTensorImage([[[[-6.7946]],\n\n              [[ 8.0881]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n더 파고들어서 분석해보자.\n\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n(why[0,0,:,:]).mean(), (why[0,1,:,:]).mean()\n\n(TensorImage(-6.7946, device='cuda:0', grad_fn=<AliasBackward0>),\n TensorImage(8.0881, device='cuda:0', grad_fn=<AliasBackward0>))\n\n\nwhy[0,0,:,:]\n\n#collapse_output\n(why[0,0,:,:]).to(torch.int64)\n\nTensorImage([[   0,    0,    0,    0,    0,    0,    0,   -1,   -1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,   -9,  -18,  -18,   -9,   -2,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -8,  -31,  -51,  -44,  -25,   -8,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -16,  -50,  -82,  -73,  -45,  -14,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,   -1,  -21,  -59,  -98, -111,  -63,  -18,\n                -1,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -17,  -53,  -94, -100,  -60,  -18,\n                -2,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -10,  -37,  -65,  -66,  -40,  -13,\n                -2,   -1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -6,  -25,  -43,  -34,  -16,   -4,\n                -2,   -1,   -1,    0,    0],\n             [   0,    0,    0,    0,    0,   -5,  -17,  -22,  -15,   -4,   -1,\n                -1,   -1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -4,  -11,  -11,   -7,   -1,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,   -1,   -2,   -3,   -2,    0,    0,    0,\n                 0,    1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,    0,    1,    2,    1,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -3,    0,    1,    4,    3,    0,\n                -1,    0,    0,    0,    0],\n             [   0,   -1,    0,    0,   -1,   -2,   -1,    1,    3,    2,   -1,\n                -2,   -1,    0,    0,    0],\n             [  -1,   -1,    0,   -1,   -1,   -5,   -3,    0,    0,    0,   -2,\n                -2,   -2,   -1,   -1,   -1],\n             [  -1,   -1,    0,    0,   -1,   -7,   -5,    0,   -1,   -1,   -1,\n                -1,   -2,   -1,   -1,   -1]], device='cuda:0')\n\n\n\n이 값들의 평균은 -9.0358 이다. (이 값이 클수록 이 그림이 고양이라는 의미 = 이 값이 작을수록 이 그림이 고양이가 아니라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 작은값이 있어서 -9.0358이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 작은 값들은 ximg가 고양이가 아니라고 판단하는 근거가 된다.\n\nwhy[0,1,:,:]\n\n#collapse_output\n(why[0,1,:,:]).to(torch.int64)\n\nTensorImage([[  0,   0,   0,   0,   0,   0,   0,   2,   2,   1,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   1,  11,  21,  21,  11,   3,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   9,  35,  61,  53,  30,  11,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  19,  58,  95,  87,  54,  17,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,  24,  68, 114, 132,  75,  22,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,  20,  61, 109, 118,  72,  21,   2,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  12,  43,  75,  77,  46,  15,   3,   1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   7,  28,  50,  39,  18,   5,   2,   2,\n                1,   0,   0],\n             [  0,   0,   0,   0,   0,   6,  19,  26,  17,   5,   1,   1,   1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   5,  12,  13,   9,   2,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   2,   3,   3,   1,   0,   0,   0,  -1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   1,   1,  -1,  -2,  -1,   1,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   4,   0,  -1,  -5,  -3,   0,   1,   0,\n                0,   0,   0],\n             [  0,   1,   0,   0,   1,   3,   1,  -1,  -3,  -3,   1,   2,   1,\n                0,   0,   1],\n             [  1,   1,   1,   1,   1,   6,   4,   0,   0,   0,   2,   3,   2,\n                1,   1,   1],\n             [  1,   1,   1,   1,   1,   8,   6,   1,   1,   1,   1,   1,   2,\n                1,   1,   1]], device='cuda:0')\n\n\n\n이 값들의 평균은 9.0926 이다. (이 값이 클수록 이 그림이 강아지라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 값들이 있어서 9.0926이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 큰 값들은 결국 ximg를 강아지라고 판단하는 근거가 된다.\n\n- 시각화\n\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n<matplotlib.image.AxesImage at 0x7fe61ad76350>\n\n\n\n\n\n\nmagma = 검은색 < 보라색 < 빨간색 < 노란색\n왼쪽그림의 검은 부분은 고양이가 아니라는 근거, 오른쪽그림의 노란부분은 강아지라는 근거\n\n- why_cat, why_dog를 (16,16) \\(\\to\\) (512,512) 로 resize\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n<matplotlib.image.AxesImage at 0x7fe616821f10>\n\n\n\n\n\n- 겹쳐그리기\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7fe5eac7af10>\n\n\n\n\n\n- 하니이미지 시각화\n\n!wget https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg\n\n--2022-11-02 23:41:24--  https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg\nResolving github.com (github.com)... 20.200.245.247\nConnecting to github.com (github.com)|20.200.245.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘2022-09-06-hani01.jpeg’\n\n2022-09-06-hani01.j     [ <=>                ] 134.25K  --.-KB/s    in 0.02s   \n\n2022-11-02 23:41:24 (8.09 MB/s) - ‘2022-09-06-hani01.jpeg’ saved [137470]\n\n\n\n\n#\n#!wget https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg?raw=true\nximg= PILImage.create('2022-09-07-dogs.jpeg')\nx= first(dls.test_dl([ximg]))[0]\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7fe61ae94190>\n\n\n\n\n\n- 하니이미지 시각화 with prob\n\nsftmax=torch.nn.Softmax(dim=1)\n\n\nsftmax(net(x))\n\nTensorImage([[1.1767e-09, 1.0000e+00]], device='cuda:0',\n            grad_fn=<AliasBackward0>)\n\n\n\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n구현4단계– CAM 시각화\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=25\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=50\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=75\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html",
    "href": "posts/ml/2022-12-21-Extra-1.html",
    "title": "Extra-1: 추천시스템",
    "section": "",
    "text": "추천시스템"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#주절주절-intro",
    "href": "posts/ml/2022-12-21-Extra-1.html#주절주절-intro",
    "title": "Extra-1: 추천시스템",
    "section": "주절주절 intro",
    "text": "주절주절 intro\n- Data\n\ndf_view = pd.read_csv('2022-12-21-rcmdsolo.csv',index_col=0)\ndf_view \n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- 데이터를 이해할때 필요한 가정들 – 내맘대로 한 설정임.\n\n(옥순,영자,정숙)은 (영식,영철,영호)와 성격이 잘 맞고 (영숙,순자,현숙)은 (광수,상철,영수)와 성격이 잘맞음\n((옥순,영자,정숙),(영식,영철,영호))은 MBTI가 I로 시작하고 ((영숙,순자,현숙),(광수,상철,영수))는 MBTI가 E로 시작한다.\n\n- 목표: NaN 을 추론\n- 수동추론:\n\n(옥순,영호)이 만난다면? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? \\(\\to\\) 4.0 정도?\n(정숙,영식)조합은? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? + 정숙은 다 잘맞던데..? \\(\\to\\) 4.8 정도?\n(현숙,영식)조합은? \\(\\to\\) 현숙은 E성향인데 영식은 I성향이므로 잘 안맞을 것임 + 현숙은 원래 좀 눈이 높음 \\(\\to\\) 0.25 정도?\n\n- 좀 더 체계적인 추론\n사람들이 가지고 있는 성향들을 두 개의 숫자로 표현하자.\n\n옥순의 성향 = (I성향,E성향) = (1.9, 0.0)\n영식의 성향 = (I성향,E성향) = (2.0, 0.1)\n현숙의 성향 = (I성향,E성향) = (0.0, 1.5)\n\n(1) 옥순과 영식의 궁합 \\(\\approx\\) 옥순의I성향\\(\\times\\)영식의I성향 \\(+\\) 옥순의E성향\\(\\times\\)영식의E성향 // 적합\n\na1= np.array([1.9,0.0]).reshape(2,1) # a1은 옥순의 성향, col-vec으로 선언하자. \nb1= np.array([2.0,0.1]).reshape(2,1) # b1은 영식의 성향, col-vec으로 선언하자.\n(a1*b1).sum()\n\n3.8\n\n\n(2) 현숙과 영식의 궁합 \\(\\approx\\) 현숙의I성향\\(\\times\\)영식의I성향 \\(+\\) 현숙의E성향\\(\\times\\)영식의E성향 // 예측\n\na6= np.array([0.0,1.5]).reshape(2,1)\n(a6*b1).sum()\n\n0.15000000000000002\n\n\n\n그럴듯함..\n\n- 모델링\n아래가 같음을 관찰하라. (차원만 다름)\n\n(a1*b1).sum(), a1.T@b1\n\n(3.8, array([[3.8]]))\n\n\n\n(a6*b1).sum(), a6.T@b1\n\n(0.15000000000000002, array([[0.15]]))\n\n\n만약에 여자의성향, 남자의성향을 적당한 매트릭스로 정리할 수 있다면 궁합매트릭스를 만들 수 있음\n\na1= np.array([1.9,0.0]).reshape(2,1)\na2= np.array([2.0,0.1]).reshape(2,1)\na3= np.array([2.5,1.0]).reshape(2,1)\na4= np.array([0.1,1.9]).reshape(2,1)\na5= np.array([0.2,2.1]).reshape(2,1)\na6= np.array([0.0,1.5]).reshape(2,1)\nA = np.concatenate([a1,a2,a3,a4,a5,a6],axis=1)\nA\n\narray([[1.9, 2. , 2.5, 0.1, 0.2, 0. ],\n       [0. , 0.1, 1. , 1.9, 2.1, 1.5]])\n\n\n\nb1= np.array([2.0,0.1]).reshape(2,1)\nb2= np.array([1.9,0.2]).reshape(2,1)\nb3= np.array([1.8,0.3]).reshape(2,1)\nb4= np.array([0.3,2.1]).reshape(2,1)\nb5= np.array([0.2,2.0]).reshape(2,1)\nb6= np.array([0.1,1.9]).reshape(2,1)\nB = np.concatenate([b1,b2,b3,b4,b5,b6],axis=1)\nB\n\narray([[2. , 1.9, 1.8, 0.3, 0.2, 0.1],\n       [0.1, 0.2, 0.3, 2.1, 2. , 1.9]])\n\n\n\nA.T@B\n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\na1.T@b1, a2.T@b2, a3.T@b1\n\n(array([[3.8]]), array([[3.82]]), array([[5.1]]))\n\n\n결국 모형은 아래와 같다.\n\\[\\text{궁합매트릭스} = {\\bf A}^\\top {\\bf B} + \\text{오차}\\]\n- 학습전략: 아래의 매트릭스중에서 어떤값은 관측하였고 어떤값은 관측하지 못함 \\(\\to\\) 관측한 값들만 대충 비슷하게 하면 되는거 아니야?\n\nA.T@B \n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- 자료를 아래와 같이 정리한다면?\n\ndf = pd.DataFrame([(f,m,df_view.loc[f,m]) for f in df_view.index for m in df_view.columns if not np.isnan(df_view.loc[f,m])])\ndf.columns = ['X1','X2','y']\ndf\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n    \n      5\n      영자\n      영호\n      3.7\n    \n    \n      6\n      영자\n      광수\n      0.5\n    \n    \n      7\n      영자\n      영수\n      0.2\n    \n    \n      8\n      정숙\n      영철\n      4.9\n    \n    \n      9\n      정숙\n      영호\n      4.7\n    \n    \n      10\n      정숙\n      상철\n      1.2\n    \n    \n      11\n      정숙\n      영수\n      1.3\n    \n    \n      12\n      영숙\n      영식\n      0.6\n    \n    \n      13\n      영숙\n      영철\n      0.2\n    \n    \n      14\n      영숙\n      광수\n      4.1\n    \n    \n      15\n      영숙\n      상철\n      4.3\n    \n    \n      16\n      순자\n      영식\n      0.7\n    \n    \n      17\n      순자\n      영철\n      0.9\n    \n    \n      18\n      순자\n      광수\n      4.2\n    \n    \n      19\n      순자\n      영수\n      3.9\n    \n    \n      20\n      현숙\n      영철\n      0.2\n    \n    \n      21\n      현숙\n      영호\n      0.3\n    \n    \n      22\n      현숙\n      상철\n      3.5\n    \n    \n      23\n      현숙\n      영수\n      3.4\n    \n  \n\n\n\n\n\nmapp1 = {k[1]:k[0] for k in enumerate(df.X1.unique())}\nmapp2 = {k[1]:k[0] for k in enumerate(df.X2.unique())}\nmapp1,mapp2\n\n({'옥순': 0, '영자': 1, '정숙': 2, '영숙': 3, '순자': 4, '현숙': 5},\n {'영식': 0, '영철': 1, '광수': 2, '상철': 3, '영호': 4, '영수': 5})\n\n\n\nX1 = torch.tensor(list(map(lambda name: mapp1[name], df.X1)))\nX2 = torch.tensor(list(map(lambda name: mapp2[name], df.X2)))\nX1 = torch.nn.functional.one_hot(X1).float()\nX2 = torch.nn.functional.one_hot(X2).float()\ny = torch.tensor(df.y).float()\n\n- yhat을 구하는 과정..\n\nl1 = torch.nn.Linear(in_features=6,out_features=2) \nl2 = torch.nn.Linear(in_features=6,out_features=2)\n\n\nl1(X1) # 옥순~현숙의 성향들 \n\ntensor([[ 0.5101,  0.1558],\n        [ 0.5101,  0.1558],\n        [ 0.5101,  0.1558],\n        [ 0.5101,  0.1558],\n        [ 0.1860, -0.1332],\n        [ 0.1860, -0.1332],\n        [ 0.1860, -0.1332],\n        [ 0.1860, -0.1332],\n        [ 0.5840, -0.5035],\n        [ 0.5840, -0.5035],\n        [ 0.5840, -0.5035],\n        [ 0.5840, -0.5035],\n        [ 0.0356, -0.3052],\n        [ 0.0356, -0.3052],\n        [ 0.0356, -0.3052],\n        [ 0.0356, -0.3052],\n        [ 0.5411, -0.3360],\n        [ 0.5411, -0.3360],\n        [ 0.5411, -0.3360],\n        [ 0.5411, -0.3360],\n        [ 0.0738,  0.1614],\n        [ 0.0738,  0.1614],\n        [ 0.0738,  0.1614],\n        [ 0.0738,  0.1614]], grad_fn=<AddmmBackward0>)\n\n\n\nl2(X2) # 영식~영수의 성향들 \n\ntensor([[ 0.1939,  0.0405],\n        [ 0.1073, -0.2484],\n        [-0.0011, -0.2328],\n        [-0.1622, -0.1059],\n        [ 0.1939,  0.0405],\n        [-0.1635, -0.0460],\n        [-0.0011, -0.2328],\n        [-0.1319,  0.1783],\n        [ 0.1073, -0.2484],\n        [-0.1635, -0.0460],\n        [-0.1622, -0.1059],\n        [-0.1319,  0.1783],\n        [ 0.1939,  0.0405],\n        [ 0.1073, -0.2484],\n        [-0.0011, -0.2328],\n        [-0.1622, -0.1059],\n        [ 0.1939,  0.0405],\n        [ 0.1073, -0.2484],\n        [-0.0011, -0.2328],\n        [-0.1319,  0.1783],\n        [ 0.1073, -0.2484],\n        [-0.1635, -0.0460],\n        [-0.1622, -0.1059],\n        [-0.1319,  0.1783]], grad_fn=<AddmmBackward0>)\n\n\n- 몇개의 관측치만 생각해보자..\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n\n\n\n(l1(X1)[0]*l2(X2)[0]).sum() # (옥순의성향 * 영식의성향).sum()\n\ntensor(0.1052, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 3.9 이어야 한다.\n\n\n(l1(X1)[1]*l2(X2)[1]).sum() # (옥순의성향 * 영철의성향).sum()\n\ntensor(0.0160, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 4.1 이어야 한다.\n\n- yhat을 구하면!\n\nyhat = (l1(X1) * l2(X2)).sum(axis=1) # (l1(X1) * l2(X2)).sum(1)와 결과가 같음 \nyhat\n\ntensor([ 0.1052,  0.0160, -0.0369, -0.0992,  0.0307, -0.0243,  0.0308, -0.0483,\n         0.1877, -0.0724, -0.0414, -0.1668, -0.0054,  0.0796,  0.0710,  0.0265,\n         0.0913,  0.1415,  0.0776, -0.1313, -0.0322, -0.0195, -0.0291,  0.0190],\n       grad_fn=<SumBackward1>)\n\n\n\nyhat[:2],y[:2] # 이 값들이 비슷해야 하는데..\n\n(tensor([0.1052, 0.0160], grad_fn=<SliceBackward0>), tensor([3.9000, 4.1000]))\n\n\n- 0~5 까지의 범위로 고정되어 있으니까 아래와 같이 해도 되겠음..\n\nsig = torch.nn.Sigmoid()\n\n\nyhat = sig((l1(X1) * l2(X2)).sum(axis=1))*5 # (l1(X1) * l2(X2)).sum(1)와 결과가 같음 \nyhat\n\ntensor([2.6314, 2.5200, 2.4539, 2.3760, 2.5383, 2.4696, 2.5385, 2.4397, 2.7340,\n        2.4096, 2.4482, 2.2920, 2.4932, 2.5995, 2.5887, 2.5332, 2.6140, 2.6766,\n        2.5970, 2.3361, 2.4598, 2.4756, 2.4637, 2.5238],\n       grad_fn=<MulBackward0>)\n\n\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(3.2296, grad_fn=<MeanBackward0>)"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#torch를-이용한-학습",
    "href": "posts/ml/2022-12-21-Extra-1.html#torch를-이용한-학습",
    "title": "Extra-1: 추천시스템",
    "section": "torch를 이용한 학습",
    "text": "torch를 이용한 학습\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(6,2) \nl2 = torch.nn.Linear(6,2)\nsig = torch.nn.Sigmoid() \n\n\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(list(l1.parameters())+list(l2.parameters()))\n\n\nfor epoc in range(5000):\n    ## 1 \n    feature1 = l1(X1)\n    feature2 = l2(X2) \n    matching_score = (feature1*feature2).sum(axis=1) \n    yhat = sig(matching_score)*5 # 만약에 1~3점이라면 \"1+sig(matching_score)*2\" 와 같이 하면 되었을듯 \n    ## 2 \n    loss = loss_fn(yhat,y)    \n    ## 3 \n    loss.backward()    \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat\n\ntensor([3.9382, 4.0624, 0.4665, 0.3353, 4.5038, 3.6975, 0.3562, 0.3558, 4.8614,\n        4.7208, 1.1813, 1.3158, 0.4606, 0.3573, 4.1288, 4.2734, 0.8611, 0.7347,\n        4.0493, 4.0464, 0.1810, 0.3124, 3.5031, 3.3948],\n       grad_fn=<MulBackward0>)\n\n\n\ny\n\ntensor([3.9000, 4.1000, 0.5000, 0.3000, 4.5000, 3.7000, 0.5000, 0.2000, 4.9000,\n        4.7000, 1.2000, 1.3000, 0.6000, 0.2000, 4.1000, 4.3000, 0.7000, 0.9000,\n        4.2000, 3.9000, 0.2000, 0.3000, 3.5000, 3.4000])\n\n\n\nl1(X1) # 두번째 칼럼이 I 성향 점수로 \"해석\"된다\n\ntensor([[-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201]], grad_fn=<AddmmBackward0>)\n\n\n\n포인트: 여성출연자중, 정숙은 대체로 잘 맞춰주고 현숙은 그렇지 않았음.. \\(\\to\\) 그러한 가중치가 잘 드러남!!"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#fastai를-이용한-학습",
    "href": "posts/ml/2022-12-21-Extra-1.html#fastai를-이용한-학습",
    "title": "Extra-1: 추천시스템",
    "section": "fastai를 이용한 학습",
    "text": "fastai를 이용한 학습\n(1) dls\n\ndf.head() # 앞단계 전처리의 산물\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n\n\n\ndls = CollabDataLoaders.from_df(df,bs=2,valid_pct=2/24)\n\n(2) lrnr 생성\n\nlrnr = collab_learner(dls,n_factors=2,y_range=(0,5))\n\n(3) 학습\n\nlrnr.fit(30,lr=0.05)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.591074\n      3.904783\n      00:00\n    \n    \n      1\n      3.101654\n      3.466946\n      00:00\n    \n    \n      2\n      2.374579\n      1.997278\n      00:00\n    \n    \n      3\n      1.698134\n      0.770927\n      00:00\n    \n    \n      4\n      1.231943\n      0.426845\n      00:00\n    \n    \n      5\n      0.926442\n      0.370695\n      00:00\n    \n    \n      6\n      0.712243\n      0.409414\n      00:00\n    \n    \n      7\n      0.555685\n      0.388226\n      00:00\n    \n    \n      8\n      0.438495\n      0.432698\n      00:00\n    \n    \n      9\n      0.349056\n      0.497208\n      00:00\n    \n    \n      10\n      0.280163\n      0.439011\n      00:00\n    \n    \n      11\n      0.225661\n      0.401682\n      00:00\n    \n    \n      12\n      0.182180\n      0.352246\n      00:00\n    \n    \n      13\n      0.147650\n      0.385491\n      00:00\n    \n    \n      14\n      0.119871\n      0.335851\n      00:00\n    \n    \n      15\n      0.097699\n      0.352764\n      00:00\n    \n    \n      16\n      0.079746\n      0.317207\n      00:00\n    \n    \n      17\n      0.065122\n      0.334073\n      00:00\n    \n    \n      18\n      0.053214\n      0.318105\n      00:00\n    \n    \n      19\n      0.043502\n      0.307008\n      00:00\n    \n    \n      20\n      0.035621\n      0.297896\n      00:00\n    \n    \n      21\n      0.029218\n      0.296005\n      00:00\n    \n    \n      22\n      0.024133\n      0.273959\n      00:00\n    \n    \n      23\n      0.019993\n      0.212600\n      00:00\n    \n    \n      24\n      0.016710\n      0.216026\n      00:00\n    \n    \n      25\n      0.014002\n      0.240446\n      00:00\n    \n    \n      26\n      0.012028\n      0.239832\n      00:00\n    \n    \n      27\n      0.011325\n      0.132652\n      00:00\n    \n    \n      28\n      0.011786\n      0.154043\n      00:00\n    \n    \n      29\n      0.010474\n      0.395495\n      00:00\n    \n  \n\n\n\n(4) 예측\n적합값 확인\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n      y_pred\n    \n  \n  \n    \n      0\n      2.0\n      2.0\n      4.3\n      3.425303\n    \n    \n      1\n      4.0\n      1.0\n      0.5\n      0.660918\n    \n  \n\n\n\n(옥순의 궁합)\n\ndf_new = pd.DataFrame({'X1':['옥순']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      옥순\n      영식\n    \n    \n      1\n      옥순\n      영철\n    \n    \n      2\n      옥순\n      영호\n    \n    \n      3\n      옥순\n      광수\n    \n    \n      4\n      옥순\n      상철\n    \n    \n      5\n      옥순\n      영수\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([3.9011, 4.1288, 3.6021, 0.6609, 0.3678, 0.5451]), None)\n\n\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n(정숙의 궁합)\n\ndf_new = pd.DataFrame({'X1':['정숙']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      정숙\n      영식\n    \n    \n      1\n      정숙\n      영철\n    \n    \n      2\n      정숙\n      영호\n    \n    \n      3\n      정숙\n      광수\n    \n    \n      4\n      정숙\n      상철\n    \n    \n      5\n      정숙\n      영수\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([4.8320, 4.8423, 4.6991, 1.5996, 1.1552, 1.2886]), None)\n\n\n높으면 잘 맞는다,\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- Appedix: fastai 구조공부..\n\nlrnr.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(7, 2)\n  (i_weight): Embedding(7, 2)\n  (u_bias): Embedding(7, 1)\n  (i_bias): Embedding(7, 1)\n)\n\n\n\nlrnr.model.forward??\n\n\nSignature: lrnr.model.forward(x)\nDocstring:\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.\nSource:   \n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/collab.py\nType:      method\n\n\n\n\n\nbias를 제외하면 우리가 짠 모형과 같음!"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#data",
    "href": "posts/ml/2022-12-21-Extra-1.html#data",
    "title": "Extra-1: 추천시스템",
    "section": "data",
    "text": "data\n- 예전에 살펴본 예제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n- 기억을 살리기 위해서..\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_view.csv')\ndf_view\n\n\n\n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#모형",
    "href": "posts/ml/2022-12-21-Extra-1.html#모형",
    "title": "Extra-1: 추천시스템",
    "section": "모형",
    "text": "모형\n(편의상 바이어스를 제외하면)\n- 특징벡터:\n\n유저1의 취향 = [커피를 좋아하는 정도, 홍차를 좋아하는 정도]\n아이템1의 특징 = [커피의 특징, 홍차인 특징]\n\n- 평점\n\n유저1이 아이템1을 먹었을경우 평점: 유저1의 취향과 아이템1의 특징의 내적 = (유저1의 취향 \\(\\odot\\) 아이템1의 특징).sum()"
  },
  {
    "objectID": "posts/ml/2022-12-21-Extra-1.html#학습",
    "href": "posts/ml/2022-12-21-Extra-1.html#학습",
    "title": "Extra-1: 추천시스템",
    "section": "학습",
    "text": "학습\n(1) dls\n\ndls = CollabDataLoaders.from_df(df)\n\n\ndls.items\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      961\n      97\n      4\n      1.085834\n      커피4\n    \n    \n      320\n      33\n      4\n      3.994618\n      커피4\n    \n    \n      50\n      6\n      20\n      1.018980\n      홍차10\n    \n    \n      743\n      75\n      13\n      3.745429\n      홍차3\n    \n    \n      369\n      37\n      3\n      4.010463\n      커피3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      132\n      14\n      4\n      3.826174\n      커피4\n    \n    \n      613\n      62\n      5\n      1.257438\n      커피5\n    \n    \n      543\n      55\n      20\n      4.140480\n      홍차10\n    \n    \n      351\n      36\n      9\n      4.057546\n      커피9\n    \n    \n      432\n      44\n      11\n      1.374712\n      홍차1\n    \n  \n\n800 rows × 4 columns\n\n\n\n(2) lrnr\n\nlrnr = collab_learner(dls,n_factors=2) # 교재에는 y_range 를 설정하도록 되어있지만 설정 안해도 적합에는 크게 상관없음..\n\n(3) fit\n\nlrnr.fit(10,0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.737049\n      2.797844\n      00:00\n    \n    \n      1\n      3.731511\n      2.010181\n      00:00\n    \n    \n      2\n      2.508080\n      0.507782\n      00:00\n    \n    \n      3\n      1.719157\n      0.202478\n      00:00\n    \n    \n      4\n      1.232710\n      0.094146\n      00:00\n    \n    \n      5\n      0.906034\n      0.071345\n      00:00\n    \n    \n      6\n      0.680708\n      0.064701\n      00:00\n    \n    \n      7\n      0.520706\n      0.060424\n      00:00\n    \n    \n      8\n      0.404349\n      0.059761\n      00:00\n    \n    \n      9\n      0.318644\n      0.064408\n      00:00\n    \n  \n\n\n\n(4) predict\n(적합된 값 확인)\n\nlrnr.show_results() # 누를때마다 결과다름\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      60.0\n      14.0\n      4.000825\n      3.871396\n    \n    \n      1\n      44.0\n      1.0\n      3.906349\n      3.825386\n    \n    \n      2\n      67.0\n      20.0\n      4.144467\n      3.897682\n    \n    \n      3\n      22.0\n      4.0\n      4.192549\n      3.861295\n    \n    \n      4\n      22.0\n      5.0\n      3.576439\n      3.764175\n    \n    \n      5\n      20.0\n      5.0\n      4.457733\n      3.844410\n    \n    \n      6\n      96.0\n      2.0\n      1.066144\n      1.056824\n    \n    \n      7\n      28.0\n      7.0\n      3.968184\n      4.046108\n    \n    \n      8\n      14.0\n      11.0\n      0.829723\n      0.991227\n    \n  \n\n\n\n(예측값)\n\ndf_new = pd.DataFrame({'user':[1,1,1,1], 'item':[9,10,11,12]})\ndf_new\n\n\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      1\n      9\n    \n    \n      1\n      1\n      10\n    \n    \n      2\n      1\n      11\n    \n    \n      3\n      1\n      12\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([3.9138, 3.9825, 0.9447, 0.8223]), None)"
  },
  {
    "objectID": "posts/ml/2022-09-07-ml.html",
    "href": "posts/ml/2022-09-07-ml.html",
    "title": "Intro",
    "section": "",
    "text": "기계학습 특강 (1주차) 9월7일\n\n딥러닝 프레임워크\n딥러닝 슈퍼스타: 힌튼, 얀르쿤, 요수아 벤지오, 응\n요수아벤지오가 티아노를 만들었음.\n\n2010 티아노 (요수아벤지오)\n2013 caffe\n2015 체이너\n2015 텐서플로우 (구글)\n2015 케라스 (프랑소와 숄레)\n2016 파이토치 (페이스북)\n\n2017 티아노 개발 중지 선언 - 사실 개인 개발자나 대학연구팀이 구글과 메타(페이스북)을 이기는 건 불가능 - 사실상 텐서플로우와 파이토치가 양분해야하는게 맞는데? - 케라스가 안죽어..\n\n\n케라스의 시대\n케라스의 시대가 왔다 왜?? 쓰기가 쉬워요..\n누가 쓰기쉽냐\n\n비 컴퓨터공학 출신\n딥러닝의 딥도 모르는 사람\n\n–> 인공지능구현을 거의 엑셀다루는 수준으로 내림..\n–> 잘 모르겠지만 이렇게 이미지 넣으면 개 고양이 구분해줍니다!\n다른 언어는 어떠냐\n\n텐서플로우: 이건 진짜 못된 언어임 (높은 수준의 파이썬 숙련도가 필요, 컴공위주로 만들어졌음)\n파이토치: 프로그래밍 숙련도는 높지 않은데, 딥러닝의 알고리즘이 머리속에 있어야 함\n\n케라스가 압도..\n결말: 구글이 케라스를 삽니다.\n\n\n케라스와 파이토치의 시대\n그런데 지금은 파이토치랑 케라스(텐서플로우)가 비등비등해요,\n그런데 점점 파이토치를 쓰는 추세 왜?"
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html",
    "href": "posts/ml/2022-12-23-Extra-3.html",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "",
    "text": "벡터미분, 역전파와 기울기 소멸\n기울기 소멸: loss 를 W로 미분했더니 그 값이 거의 0ㅇ이 나오는 현상 \\(\\to\\) update가 거의 이루어지지 않음\n이유 - W -> loss인 함수는 W에 어떤한 선형변환 \\(\\to\\) 비선형변환 \\(\\to\\) 선형변환 \\(\\to\\) 비선형변환 \\(\\to \\dots\\) \\(\\to\\) loss 의 과정으로 해석 가능 - 즉 loss는 W의 합성의 합섭ㅇ의… 합성함수로 해석가능 - loss 를 W로 미분한 값은 각 변환단ㅇ계에서 정의되는 함수의 도함수를 모두 곱한 것과 같음(체인룰) - 체인 중에서도 하나라도 0이 나오면 곱한 값은 0 이다. - 체인이 길수록 하나라도 0이 나오는 경우가 많음\n왜 깊은 신경망일수록 기울기 소멸이 빈번한가? - 체인이 길기 때문에.\n왜 순환신경망일수록 기울기 소멸이 빈번할까>? - 체인이 길기 때문에."
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#예시-2021-빅데이터분석-중간고사-문제-2-b",
    "href": "posts/ml/2022-12-23-Extra-3.html#예시-2021-빅데이터분석-중간고사-문제-2-b",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "예시: 2021 빅데이터분석 중간고사 문제 2-(b)",
    "text": "예시: 2021 빅데이터분석 중간고사 문제 2-(b)\n- 미분계수를 계산하는 문제였음..\n\nhttps://guebin.github.io/BDA2021/2021/11/09/mid.html\n\n- 체인룰을 이용하여 미분계수를 계산하여 보자.\n\nones= torch.ones(5)\nx = torch.tensor([11.0,12.0,13.0,14.0,15.0])\nX = torch.vstack([ones,x]).T\ny = torch.tensor([17.7,18.5,21.2,23.6,24.2])\n\n\nW = torch.tensor([3.0,3.0]) \n\n\nu = X@W \nv = y-u \nloss = v.T @ v \n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642814471/work/aten/src/ATen/native/TensorShape.cpp:3277.)\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\nloss\n\ntensor(2212.1799)\n\n\n- \\(\\frac{\\partial}{\\partial\\bf W}loss\\) 의 계산\n\\(\\frac{\\partial }{\\partial \\bf W}loss = \\left({\\bf X}^\\top \\right) \\left(-{\\bf I} \\right) \\left(2{\\bf v}\\right)\\)\n\nX.T @ -torch.eye(5) @ (2*v) \n\ntensor([ 209.6000, 2748.6001])\n\n\n- 참고로 중간고사 답은\n\nX.T @ -torch.eye(5)@ (2*v) / 5 \n\ntensor([ 41.9200, 549.7200])\n\n\n입니다.\n- 확인\n\n_W = torch.tensor([3.0,3.0],requires_grad=True) \n\n\n_loss = (y-X@_W).T @ (y-X@_W)\n\n\n_loss.backward()\n\n\n_W.grad.data\n\ntensor([ 209.6000, 2748.6001])\n\n\n- \\(\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}\\) 임을 확인하라.\n\nv\n\ntensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000])\n\n\n\n_v= torch.tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000],requires_grad=True)\n\n\n_loss = _v.T @ _v \n\n\n_loss.backward() \n\n\n_v.grad.data, v \n\n(tensor([-36.6000, -41.0000, -41.6000, -42.8000, -47.6000]),\n tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))\n\n\n- \\(\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top\\) 의 계산\n\n_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n_u\n\ntensor([36., 39., 42., 45., 48.], requires_grad=True)\n\n\n\n_v = y - _u ### 이전의 _v와 또다른 임시 _v \n\n\n(_v.T).backward()\n\nRuntimeError: grad can be implicitly created only for scalar outputs\n\n\n\n사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음\n\n그런데 \\(\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)\\) 이므로\n조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다.\n\n\\(\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top\\)의 결과를 저장할 매트릭스를 만든다. 적당히 A라고 만들자.\n_u 하나를 임시로 만든다. 그리고 \\(v_1\\)을 _u로 미분하고 그 결과를 A의 첫번째 칼럼에 기록한다.\n_u를 또하나 임시로 만들고 \\(v_2\\)를 _u로 미분한뒤 그 결과를 A의 두번째 칼럼에 기록한다.\n(1)-(2)와 같은 작업을 \\(v_5\\)까지 반복한다.\n\n(0)을 수행\n\nA = torch.zeros((5,5))\nA\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n(1)을 수행\n\nu,v \n\n(tensor([36., 39., 42., 45., 48.]),\n tensor([-18.3000, -20.5000, -20.8000, -21.4000, -23.8000]))\n\n\n\n_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\nv1 = (y-_u)[0]\n\n\n이때 \\(v_1=g(f({\\bf u}))\\)와 같이 표현할 수 있다. 여기에서 \\(f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top\\), 그리고 \\(g((v_1,\\dots,v_n)^\\top)=v_1\\) 라고 생각한다. 즉 \\(f\\)는 벡터 뺄셈을 수행하는 함수이고, \\(g\\)는 프로젝션 함수이다. 즉 \\(f:\\mathbb{R}^5 \\to \\mathbb{R}^5\\)인 함수이고, \\(g:\\mathbb{R}^5 \\to \\mathbb{R}\\)인 함수이다.\n\n여기서 \\(v_1\\)은 꼬리표로서 selection 작성되어 있음\n\nv1.backward()\n\n\n_u.grad.data\n\ntensor([-1., -0., -0., -0., -0.])\n\n\n\nA[:,0]= _u.grad.data\n\nA의 첫번째 칼럼에 이것을 넣어주세요\n\nA\n\ntensor([[-1.,  0.,  0.,  0.,  0.],\n        [-0.,  0.,  0.,  0.,  0.],\n        [-0.,  0.,  0.,  0.,  0.],\n        [-0.,  0.,  0.,  0.,  0.],\n        [-0.,  0.,  0.,  0.,  0.]])\n\n\n(2)를 수행\n\n_u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\nv2 = (y-_u)[1]\n\n\nv2.backward()\n\n\n_u.grad.data\n\ntensor([-0., -1., -0., -0., -0.])\n\n\n\nA[:,1]= _u.grad.data\nA\n\ntensor([[-1., -0.,  0.,  0.,  0.],\n        [-0., -1.,  0.,  0.,  0.],\n        [-0., -0.,  0.,  0.,  0.],\n        [-0., -0.,  0.,  0.,  0.],\n        [-0., -0.,  0.,  0.,  0.]])\n\n\n(3)을 수행 // 그냥 (1)~(2)도 새로 수행하자.\n\nfor i in range(5): \n    _u = torch.tensor([36., 39., 42., 45., 48.],requires_grad=True)\n    _v = (y-_u)[i]\n    _v.backward()\n    A[:,i]= _u.grad.data\n\n\nA\n\ntensor([[-1., -0., -0., -0., -0.],\n        [-0., -1., -0., -0., -0.],\n        [-0., -0., -1., -0., -0.],\n        [-0., -0., -0., -1., -0.],\n        [-0., -0., -0., -0., -1.]])\n\n\n\n이론적인 결과인 \\(-{\\bf I}\\)와 일치한다.\n\n- \\(\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top\\)의 계산\n\\(\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)\\)\n\nB = torch.zeros((2,5))\nB\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nW\n\ntensor([3., 3.])\n\n\n\n_W = torch.tensor([3., 3.],requires_grad=True)\n_W\n\ntensor([3., 3.], requires_grad=True)\n\n\n\nfor i in range(5): \n    _W = torch.tensor([3., 3.],requires_grad=True)\n    _u = (X@_W)[i]\n    _u.backward()\n    B[:,i]= _W.grad.data\n\n\nB # X의 트랜스포즈\n\ntensor([[ 1.,  1.,  1.,  1.,  1.],\n        [11., 12., 13., 14., 15.]])\n\n\n\nX\n\ntensor([[ 1., 11.],\n        [ 1., 12.],\n        [ 1., 13.],\n        [ 1., 14.],\n        [ 1., 15.]])\n\n\n\n이론적인 결과와 일치"
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#잠깐-생각해보자..",
    "href": "posts/ml/2022-12-23-Extra-3.html#잠깐-생각해보자..",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "잠깐 생각해보자..",
    "text": "잠깐 생각해보자..\n- 결국 위의 예제에 한정하여 임의의 \\({\\bf \\hat{W}}\\)에 대한 \\(\\frac{\\partial}{\\partial {\\bf \\hat W}}loss\\)는 아래와 같이 계산할 수 있다.\n\n(단계1) \\(2{\\bf v}\\)를 계산하고\n(단계2) (단계1)의 결과 앞에 \\(-{\\bf I}\\)를 곱하고\n(단계3) (단계2)의 결과 앞에 \\({\\bf X}^\\top\\)를 곱한다.\n\n- step1에서 \\({\\bf v}\\)는 어떻게 알지?\n\nX \\(\\to\\) u=X@W \\(\\to\\) v = y-u\n그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것 아니었나?\nstep1: yhat, step2: loss, step3: derivate, step4: update\n\n- (중요) step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을 다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한 방법을 통하여 이용하여 보자.\n\nbackprogation 알고리즘 모티브\n- 아래와 같이 함수의 변환을 아키텍처로 이해하자. (함수의입력=레이어의입력, 함수의출력=레이어의출력)\n\n\\({\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})\\)\n\n- 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다. (\\({\\bf X} \\to {\\bf \\hat y} \\to loss\\)가 아니라 \\({\\bf W} \\to loss({\\bf W})\\)로 생각해보세요)\n\n\\({\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss\\)\n\n- 그렇다면 아래와 같은 사실을 관찰할 수 있다.\n\n(단계1) \\(2{\\bf v}\\)는 function of \\({\\bf v}\\)이고, \\({\\bf v}\\)는 l3의 입력 (혹은 l2의 출력)\n(단계2) \\(-{\\bf I}\\)는 function of \\({\\bf u}\\)이고, \\({\\bf u}\\)는 l2의 입력 (혹은 l1의 출력)\n(단계3) 마찬가지의 논리로 \\({\\bf X}^\\top\\)는 function of \\({\\bf W}\\)로 해석할 수 있다.\n\n- 요약: \\(2{\\bf v},-{\\bf I}, {\\bf X}^\\top\\)와 같은 핵심적인 값들이 사실 각 층의 입/출력 값들의 함수꼴로 표현가능하다. \\(\\to\\) 각 층의 입/출력 값들을 모두 기록하면 미분계산을 유리하게 할 수 있다.\n\n문득의문: 각 층의 입출력값 \\({\\bf v}, {\\bf u}, {\\bf W}\\)로 부터 \\(2{\\bf v}, -{\\bf I}, {\\bf X}^\\top\\) 를 만들어내는 방법을 모른다면 헛수고 아닌가?\n의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, …) 정도가 전부임. 따라서 변환규칙은 미리 계산할 수 있음.\n\n- 결국\n(1) 순전파를 하면서 입출력값을 모두 저장하고\n(2) 그에 대응하는 층별 미분계수값 \\(2{\\bf v}, -{\\bf I}, {\\bf X}^\\top\\) 를 구하고\n(3) 층별미분계수값을 다시 곱하면 (그러니까 \\({\\bf X}^\\top (-{\\bf I}) 2{\\bf v}\\) 를 계산) 된다.\n\n\nbackpropagation\n(1) 순전파를 계산하고 각 층별 입출력 값을 기록\n\nyhat = net(X)\nloss = loss_fn(yhat,y)\n\n(2) 역전파를 수행하여 손실함수의 미분값을 계산\n\nloss.backward()\n\n- 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU 메모리..\n- 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용)\ngpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한 코어숫자)\n\n아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 // net.to(\"cuda:0\")\n순전파 계산: 중간 계산결과를 모두 GPU메모리에 저장 (순전파 계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를 계산하기 위한 대비) // net(X)\n오차 및 손실함수 계산: loss = loss_fn(yhat,y)\n역전파 계산: 순전파단계에서 저장된 계산결과를 활용하여 손실함수의 미분값을 계산 // loss.backward()\n다음 순전파 계산: 이전값은 삭제하고 새로운 중간계산결과를 GPU메모리에 올림\n반복."
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#some-comments",
    "href": "posts/ml/2022-12-23-Extra-3.html#some-comments",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "some comments",
    "text": "some comments\n- 역전파기법은 체인룰 + \\(\\alpha\\) 이다. - 미분 계산을 하기 위함인데 여기서 파라메터 업데이트 필요하지\n- 오차역전파기법이라는 용어를 쓰는 사람도 있다.\n- 이미 훈련한 네트워크에 입력 \\(X\\)를 넣어 결과값만 확인하고 싶을 경우 순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. - 예) 개/고양이 확인 등"
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#고요속의-외침",
    "href": "posts/ml/2022-12-23-Extra-3.html#고요속의-외침",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "고요속의 외침",
    "text": "고요속의 외침\n- https://www.youtube.com/watch?v=ouitOnaDtFY\n- 중간에 한명이라도 잘못 말한다면.."
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#정의",
    "href": "posts/ml/2022-12-23-Extra-3.html#정의",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "정의",
    "text": "정의\n- In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation."
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#이해",
    "href": "posts/ml/2022-12-23-Extra-3.html#이해",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "이해",
    "text": "이해\n- 당연한것 아닌가?\n\n그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의 기울기를 통하여 업데이트 하는 방식\n역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + \\(\\alpha\\)). 구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을 각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다.\n0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인 기울기가 나올 것이라고 생각하는것 자체가 이상함)\n\n0 전달되면 업데이트 안 되잖아\n\nimport numpy as np \n\n\ngrads = np.random.uniform(low=-2,high=2,size=100) \ngrads\n\narray([ 1.27649548, -1.3435985 , -1.98858349,  1.98266931,  0.74704939,\n       -0.4831162 ,  1.41216797, -1.20423568,  0.66637324,  1.18085266,\n        0.23356958, -0.39851281,  0.2357107 , -0.28592461, -0.96001359,\n       -1.6276445 , -0.75687257,  0.48770524,  0.02553662,  0.18478814,\n        1.28850714,  1.41398052, -0.33970936,  0.36829707,  1.85991256,\n        0.02243541, -0.68804507,  0.63659842, -1.13905311,  0.94093391,\n        1.58809026, -0.25106013, -0.14446307,  1.31747003,  1.52190566,\n       -0.44264824, -1.95722305, -0.77865942, -0.3350363 , -0.53638126,\n       -0.77254936, -1.22118632,  0.6137345 ,  0.89975951, -1.70293244,\n        1.42661365, -0.43175558, -1.30904545, -0.53912915,  1.51725173,\n       -1.83965849,  1.00143736, -0.67129779,  0.36061957, -1.68850939,\n       -0.4272909 , -0.34715325,  1.72387253, -0.98340508, -1.60825385,\n        1.64523373, -0.79036932,  1.82578785, -0.53592773, -0.61384056,\n        0.9689625 ,  1.27971335,  0.51555469, -0.53425795,  0.38883373,\n       -0.28595978, -1.93730647, -1.94581503, -0.48984819, -1.21831701,\n       -1.25965989,  1.79542393,  1.2637913 , -0.93178556, -0.61210568,\n        1.23775906, -1.80601708,  1.40265496, -0.59602715,  1.44638486,\n       -1.71721283,  1.58345756, -1.03992841,  1.10167726,  1.13332066,\n       -0.76344022, -0.7246539 ,  0.27256115, -1.95501872, -0.65922909,\n        0.78715854, -0.29077574, -0.45110518, -1.64836114,  1.91692815])\n\n\n\ngrads.prod()\n\n-4.089052412862103e-11\n\n\n\n기울기가 소멸함\n\n\ngrads = np.random.uniform(low=-5,high=5,size=100) \ngrads.prod()\n\n-1.6059047739389678e+16\n\n\n\n기울기가 폭발함.\n\n\ngrads = np.random.uniform(low=-1,high=3.5,size=100) \ngrads.prod()\n\n13058.879478242436\n\n\n- 도깨비: 기울기가 소멸하기도 하고 터지기도 한다."
  },
  {
    "objectID": "posts/ml/2022-12-23-Extra-3.html#해결책-기울기-소멸에-대한-해결책",
    "href": "posts/ml/2022-12-23-Extra-3.html#해결책-기울기-소멸에-대한-해결책",
    "title": "Extra-3: 딥러닝의 기초 (5)",
    "section": "해결책 (기울기 소멸에 대한 해결책)",
    "text": "해결책 (기울기 소멸에 대한 해결책)\n- Multi-level hierarchy\n\n여러층을 쪼개서 학습하자 \\(\\to\\) 어떻게? 사전학습, 층벼학습\n기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임.\n결국 입력자료를 바꾼뒤에 학습하는 형태\n\n- Gradient clipping\n\n너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)\n\n- Faster hardware\n\nGPU를 중심으로 한 테크닉\n근본적인 문제해결책은 아니라는 힌튼의 비판\nCPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도?\n\n- Residual Networks, LSTM\n\n아키텍처를 변경하는 방법\n\n- Other activation functions\n\n렐루의 개발\n렐루가 음수는 아예 0, 양수는 기울기 확실하니까\n\n- 배치정규화\n\n어쩌다보니 되는것.\n배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기 소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로 빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한 언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에 언급했음)\n\n- 기울기를 안구하면 안되나?\n\n베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) \\(\\to\\) GPU를 어떻게 쓰지? \\(\\to\\) 느리다"
  },
  {
    "objectID": "posts/ml/2022-12-08-13wk.html",
    "href": "posts/ml/2022-12-08-13wk.html",
    "title": "RNN (13주차)",
    "section": "",
    "text": "IMDB자료의 분석 (텍스트생성과 감성분류), 잡담"
  },
  {
    "objectID": "posts/ml/2022-12-08-13wk.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "href": "posts/ml/2022-12-08-13wk.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "title": "RNN (13주차)",
    "section": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석",
    "text": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석\n- 순환신경망은 순서가 있는 (말이 좀 애매하지만 아무튼 이렇게 많이 표현해요) 자료를 분석할때 사용할 수 있다. 순서가 있는 자료는 대표적으로 시계열자료과 텍스트자료가 있다.\n- 그래서 언뜻 생각하면 텍스트마이닝이나 시계열분석과 내용이 비슷할 것 같지만 사실 그렇지 않다.\n\n텍스트마이닝의 토픽: 단어를 어떻게 숫자로 잘 만들지, 토픽모델 // 자잘하고 실용적인 느낌? 공학적임..\n\n시계열분석의 토픽: 예측(forecasting)과 신뢰구간, 변화점과 관련한 연구 (detection/test), 정상/비정상시계열모형 (ARIMA, GARCH), Cointegration Test, // 느낌이 좀 거창해.. 경제와 관련 많음.\n순환신경망의 토픽(재작년까지): 텍스트생성, 텍스트분류 + 시계열 자료의 예측, 단어의 숫자화 … 텍스트마이닝과 시계열분석의 거의 모든 토픽에 관여함\n순환신경망의 토픽(작년부터?): 딥러닝의 거의 모든 영역에 관여하기 시작함 (심지어 요즘 이미지 분석도 순환망으로 합니다)\n\n\nhttps://youtu.be/thsXGOkcGGg"
  },
  {
    "objectID": "posts/ml/2022-12-08-13wk.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "href": "posts/ml/2022-12-08-13wk.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "title": "RNN (13주차)",
    "section": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?",
    "text": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?\n- 과거기준(텍스트생성, 텍스트분류, 시계열자료예측 등에만 순환망이 이용되었을 때): 학부수준에서 순수 RNN만 알아도 충분했던 것 같음. LSTM이나 GRU는 석사수준?\n- 현재기준: 석사기준 LSTM 같은건 기본이고 어텐션, 트랜스포머등에 대한 개념도 잘 알고 있어야 함. (학부는 잘 모르겠네..)\n- 내 생각: 결국 아키텍처는 근데 유행이라 아키텍처는 한번 따라하면서 이해해보고 핵심 아이디어만 이해하면 된다고 생각함. 즉 LSTM 같은 특정모형의 아키텍처를 달달 외울필요는 없다, 수식써있는거 보고 이해하면 그만임. (수식정도를 이해할 능력은 필요한게.. 코드를 짤때 옵션을 이해할 수는 있어야하니까)\n- 망상: 나중에는 순환신경망이 거의 모든 딥러닝 방법의 base가 되지 않을까?"
  },
  {
    "objectID": "posts/ml/2022-12-08-13wk.html#잡담3-fastai-pytorch-lightning",
    "href": "posts/ml/2022-12-08-13wk.html#잡담3-fastai-pytorch-lightning",
    "title": "RNN (13주차)",
    "section": "잡담3: fastai, pytorch lightning",
    "text": "잡담3: fastai, pytorch lightning\n- 비 컴퓨터공학 출신이 쓰기에는 fastai가 좀 더 쓰기 편한건 사실\n- pytorch lightning은 fastai보다 쓰기 어렵지만 (진짜 약간의 클래스관련 지식이 필요함, 솔직히 별로 어렵진 않아요) 좀 더 순수 파이토치에 가깝고 따라서 코드를 뜯어보기 편리하다.\n- 과거의 생각\n\n전문가: pytorch + fastai // pytorch + pytorch lightning (컴공출신)\n비 전문가: 순수 fastai\n\n- 요즘 생각\n\n모두: pytorch + pytorch lightning\n특정한경우: 순수 fastai <– 모형이 구현되어 있다면 fastai가 좋긴 좋아.. 그런데 모형의 구현속도가 못따라감"
  },
  {
    "objectID": "posts/ml/2022-12-08-13wk.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "href": "posts/ml/2022-12-08-13wk.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "title": "RNN (13주차)",
    "section": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)",
    "text": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)\n- 능력1: 코드이해력 (= 구현능력 = 코드 베끼는 능력)\n\n이미지분석? 해봤음. 텍스트자료? 해봤음. 시계열? 해봤음. 등등등등? 다 해본적 있음. 어떤 원리인지 정확하게 몰라도 다 해본적 있고 그래서 일할 수 있음!!\n돌아가는 코드 최대한 많이 모아놓으세요. torch, fastai, pytorch lightning, tensorflow, keras 등등\n\n- 능력2: 최신트렌드를 파악할 수 있는 힘 (= 논문이해력)\n\n공부, 공부, 공부… A to Z 까지 수식 다 뜯어보고 코드 다 뜯어보면서 집요하게 공부해야함. (LSTM에서 했던것 처럼!) 물론 차근차근 알려주면 수업이 있다면 좋겠지 그런데 보통은 적당히 두리뭉실하게 설명하지 detail 하게 설명하는 수업은 잘 없음. (지루하거든요)\n수식이나 코드중 하나라도 볼 줄 모르면 능력2를 얻는것 자체가 불가능."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html",
    "href": "posts/ml/2022-09-14-ml_2w.html",
    "title": "DNN (2주차)",
    "section": "",
    "text": "기계학습 특강 (2주차) 9월14일 [추천시스템, 텍스트분석, GAN]"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#imports",
    "href": "posts/ml/2022-09-14-ml_2w.html#imports",
    "title": "DNN (2주차)",
    "section": "imports",
    "text": "imports\n\n#\nfrom fastai.collab import * ## 추천시스템\nfrom fastai.text.all import * ## 텍스트분석 \nfrom fastai.vision.all import *  ## GAN (이미지분석) \nfrom fastai.vision.gan import * ## GAN (이미지생성)\n\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#이미지-자료분석-실습-지난시간-복습",
    "href": "posts/ml/2022-09-14-ml_2w.html#이미지-자료분석-실습-지난시간-복습",
    "title": "DNN (2주차)",
    "section": "이미지 자료분석 실습 (지난시간 복습)",
    "text": "이미지 자료분석 실습 (지난시간 복습)\n\n1단계: 데이터의 정리\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath.ls()\n\n(#7393) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\nevery files’ list\n\nfnames = get_image_files(path)\n\n\nfnames\n\n(#7390) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\nimage files’ list\n\nImageDataLoaders.from_name_func??\n\n\nSignature:\nImageDataLoaders.from_name_func(\n    path,\n    fnames,\n    label_func,\n    valid_pct=0.2,\n    seed=None,\n    item_tfms=None,\n    batch_tfms=None,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    def from_name_func(cls, path, fnames, label_func, **kwargs):\n        \"Create from the name attrs of `fnames` in `path`s with `label_func`\"\n        if sys.platform == 'win32' and isinstance(label_func, types.LambdaType) and label_func.__name__ == '<lambda>':\n            # https://medium.com/@jwnx/multiprocessing-serialization-in-python-with-pickle-9844f6fa1812\n            raise ValueError(\"label_func couldn't be lambda function on Windows\")\n        f = using_attr(label_func, 'name')\n        return cls.from_path_func(path, fnames, f, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/data.py\nType:      method\n\n\n\n\ndef f(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\nf(x) = x+ 1\nlambda x : x+1\n\nfnames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nf = lambda fname: 'cat' if fname[0].isupper() else 'dog'\n\n\nf('s')\n\n'dog'\n\n\n\nf('D')\n\n'cat'\n\n\ndls = ImageDataLoaders.from_name_func(\n    path, \n    fnames,\n    lambda fname: 'cat' if fname[0].isupper() else 'dog'\n    item_tfms=Resize(224)) \n\ndls = ImageDataLoaders.from_name_func(\n    path, \n    fnames,\n    f, # f대신 (lambda fname: 'cat' if fname[0].isupper() else 'dog') 를 넣어도 가능\n    item_tfms=Resize(224)) # 사이즈가 달라서 통일\n\n\ndls.show_batch()\n\n\n\n\n\n\n2단계: lrnr 오브젝트 생성\n\ncnn_learner??\n\n\nSignature:\ncnn_learner(\n    dls,\n    arch,\n    normalize=True,\n    n_out=None,\n    pretrained=True,\n    config=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=None,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    cut=None,\n    n_in=3,\n    init=<function kaiming_normal_ at 0x7fcbc439a8b0>,\n    custom_head=None,\n    concat_pool=True,\n    lin_ftrs=None,\n    ps=0.5,\n    first_bn=True,\n    bn_final=False,\n    lin_first=False,\n    y_range=None,\n)\nSource:   \n@delegates(create_cnn_model)\ndef cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None,\n                # learner args\n                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n                # other model args\n                **kwargs):\n    \"Build a convnet style learner from `dls` and `arch`\"\n    if config:\n        warnings.warn('config param is deprecated. Pass your args directly to cnn_learner.')\n        kwargs = {**config, **kwargs}\n    meta = model_meta.get(arch, _default_meta)\n    if normalize: _add_norm(dls, meta, pretrained)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    model = create_cnn_model(arch, n_out, pretrained=pretrained, **kwargs)\n    splitter=ifnone(splitter, meta['split'])\n    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n                   moms=moms)\n    if pretrained: learn.freeze()\n    # keep track of args for loggers\n    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n    return learn\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/learner.py\nType:      function\n\n\n\n\n!cat ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/learner.py\n이 코드로 존재하는 함수의 정의 확인 가능\n어디 소속된 함수인지 확인 하기 위해\nfastai에 소속된 cnn_leaner,따라서 fastai를 import해야 나타나지.\n\nlrnr = cnn_learner(dls,resnet34,metrics=error_rate)\n\n\nlrnr.dls.show_batch()\n\n\n\n\n\nid(lrnr.dls)\n\n140510181797744\n\n\n\nid(dls)\n\n140510181797744\n\n\n주소가 같다. 같은 역할\nlrnr에 dls가 소속되어 있다고 생각(?) - 포스트잇을 위에 덧붙인다 생각\n\n\n3단계: lrnr.학습()\n학습하는 fine_tune 이외에 여러가지 있음 - fine_tune 학습된 일부는 유지하고 바꿀 부분만 학습시키는 법: transfer learning\nfor exampel: cnn의 1d에서는 끝에만 학습\n\nlrnr.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.155603\n      0.014394\n      0.006766\n      00:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.051081\n      0.008424\n      0.002706\n      00:11\n    \n  \n\n\n\n\nfine_tune()은 모든 가중치를 학습하는 것이 아니라 일부만 학습하는 것임.\nfine_tune()이외이 방법으로 학습할 수도 있음.\n\n\n\n4단계: lrnr.예측()\n(방법1) lrnr.predict() 함수를 이용\n\nlrnr.predict('2022-09-07-dogs.jpeg') # 방법1-1\n#lrnr.predict(PILImage.create('2022-09-07-dogs.jpeg')) # 방법1-2\n#lrnr.predict(path.ls()[0]) # 방법1-3\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.2932e-05, 9.9998e-01]))\n\n\n컴퓨터가 이해하기 쉬운 방법인 1-2번째 방법\n\nlrnr.predict(PILImage.create('2022-09-07-dogs.jpeg'))\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.2932e-05, 9.9998e-01]))\n\n\n\nlrnr.predict(path.ls()[1])\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.4945e-08, 1.0000e+00]))\n\n\n\ndir(lrnr.model)\ndirectory에 _call_있으면 함수처럼 사용 가능\n\n(방법2) lrnr.model(X) 를 이용: X의 shape이 (?,3,224,224)의 형태의 텐서이어야함\n\ntype(dls.one_batch())\n\ntuple\n\n\n끝에 괄호로 묶여 있으면 tuple\n\nX,y = dls.one_batch() # 방법2\nlrnr.model(X[0:1]) \n\nTensorBase([[-8.3588,  7.0462]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nX[1].shape\n\ntorch.Size([3, 224, 224])\n\n\n\nX[:2].shape\n\ntorch.Size([2, 3, 224, 224])\n\n\n\nX.shape\n\ntorch.Size([64, 3, 224, 224])\n\n\nimage의 사이즈 224 * 224 - 3개의 채널 - 64개 - image, 입력\n\ny.shape\n\ntorch.Size([64])\n\n\n\n예측값\n\n\ny[:3]\n\nTensorCategory([1, 1, 0], device='cuda:0')\n\n\nlrnr.model(X[0])\n오류 뜬다. - torch.Size([3, 224, 224]) - shape을 - torch.Size([?, 3, 224, 224]) - 이런 식으로 만들어주자, 입력\n\nlrnr.model(X[:3])\n\nTensorBase([[ -8.3605,   7.0472],\n        [ -4.4236,   5.1110],\n        [ 14.0977, -13.0582]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nlrnr.model(X)\n\nTensorBase([[ -8.3595,   7.0465],\n        [ -4.4246,   5.1111],\n        [ 14.0959, -13.0577],\n        [ -6.4868,   7.2289],\n        [ -3.4974,   2.0202],\n        [ -7.1135,   6.2276],\n        [ -4.2407,   2.9429],\n        [ -7.0260,   6.4789],\n        [ -6.5011,   5.1029],\n        [ -7.4927,   4.9038],\n        [ -5.7292,   5.0113],\n        [ -9.6244,   5.7399],\n        [ -6.8247,   3.4742],\n        [ 17.2742, -12.6829],\n        [ -4.0548,   2.6589],\n        [ 16.3894, -14.2360],\n        [ -3.8864,   5.6632],\n        [ -5.1192,   6.0355],\n        [ 11.3016, -13.4798],\n        [ -8.1850,   7.5925],\n        [  8.3147,  -5.9946],\n        [ -8.0415,   8.4349],\n        [ -9.6461,   8.3790],\n        [ -5.4923,   5.8070],\n        [ 12.1504,  -9.3661],\n        [ -7.7945,   6.7907],\n        [ -5.0291,   3.4955],\n        [ 13.8045, -11.3889],\n        [ -4.5400,   5.1561],\n        [ 16.5360, -13.3928],\n        [ -4.0467,   3.3478],\n        [ -5.8401,   7.2492],\n        [  6.9878,  -4.8408],\n        [ -8.0189,   6.0578],\n        [ -7.7578,   4.7063],\n        [ -5.0351,   4.5309],\n        [  6.0511,  -4.1623],\n        [ -8.4919,   8.1300],\n        [ -5.9893,   5.8341],\n        [ -7.0671,   6.2901],\n        [ 17.0369, -13.7746],\n        [ -6.7633,   5.5232],\n        [ -7.3533,   7.6700],\n        [ -8.3923,   6.6368],\n        [ 13.2212, -10.2649],\n        [ 14.7573, -11.7938],\n        [ -6.6409,   5.6934],\n        [ -6.5882,   4.9800],\n        [ -5.2839,   5.3899],\n        [ -5.7066,   4.9765],\n        [ -5.8099,   3.8355],\n        [ -8.5055,   7.2022],\n        [ -8.7006,   4.5980],\n        [ -5.4901,   4.5288],\n        [ -7.6612,   7.1533],\n        [ 15.9380, -16.2778],\n        [  7.9763,  -7.1954],\n        [ 13.4158, -10.9864],\n        [ -4.9234,   2.9219],\n        [ -4.0274,   4.1298],\n        [ 16.8217, -16.0985],\n        [ -8.6418,   7.1085],\n        [ -5.9216,   6.0076],\n        [ -5.3720,   3.9876]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\\(y\\) : 왼쪽이 크면 0, 오른쪽이 크면 1 - 둘다 음수인 건 없네? - 왼쪽이 양수면 0 오른쪽이 양수면 1로 생각 가능하겠다."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#프로그래밍-과정",
    "href": "posts/ml/2022-09-14-ml_2w.html#프로그래밍-과정",
    "title": "DNN (2주차)",
    "section": "프로그래밍 과정",
    "text": "프로그래밍 과정\n\n프로그래밍 과정 overview\n- overview\n\ndls 오브젝트 생성\nlrnr 오브젝트 생성\nlrnr.학습()\nlrnr.예측()\n\n\n\n이미지분석, 추천시스템, 텍스트분석, GAN 분석과정 비교\n- 비교\n\n\n\n\n\n\n\n\n\n\n\n이미지분석(CNN)\n추천시스템\n텍스트분석\nGAN\n\n\n\n\n1단계\nImageDataLoaders\nCollabDataLoaders\nTextDataLoaders\nDataBlock -> dls\n\n\n2단계\ncnn_learner()\ncollab_learner()\nlanguage_model_learner()\nGANLearner.wgan()\n\n\n3단계\nlrnr.fine_tune(1)\nlrnr.fit()\nlrnr.fit()\nlrnr.fit()\n\n\n4단계\nlrnr.predict(), lrnr.model(X)\nlrnr.model(X)\nlrnr.predict()"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#추천시스템-실습",
    "href": "posts/ml/2022-09-14-ml_2w.html#추천시스템-실습",
    "title": "DNN (2주차)",
    "section": "추천시스템 실습",
    "text": "추천시스템 실습\n\n1단계\ngithub에서 해당 파일의 raw click하여 주소 가져오기\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv\n위와 같이 wget사용하면 주소의 data 바로 다운 가능\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv')\ndf_view\n\n\n\n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns\n\n\n\n컴퓨터가 좋아하는 타입은 아님\n\nrow0 - row49 에 해당하는 유저는 커피를 선호\nrow50 - row99 에 해당하는 유저는 홍차를 선호\n\n위의 자료는 비효율적, tidy data로 바꿔주자, 아래와 같이 정리함으로써 저장할 data도 줄어든다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n\n컴퓨터는 이러한 형태를 더 분석하기 좋아한다.\n\n!cat 파일명\ndata 도 확인 가능하다\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n!cat Real_estate_valuation_data_set.csv\n\n\ndf.item.unique(),df.user.unique()\n# 유저는 1~100 으로 아이템은 1~20으로 번호가 매겨져 있음 \n\n(array([15,  1, 11,  5,  4, 14,  6, 20, 12, 17,  8,  9, 13, 19, 18, 16,  2,\n         3, 10,  7]),\n array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n         92,  93,  94,  95,  96,  97,  98,  99, 100]))\n\n\nitem, user 번호 확인\n\n\nCollabDataLoaders.from_df??\n\n\n\nSignature:\nCollabDataLoaders.from_df(\n    ratings,\n    valid_pct=0.2,\n    user_name=None,\n    item_name=None,\n    rating_name=None,\n    seed=None,\n    path='.',\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @delegates(DataLoaders.from_dblock)\n    @classmethod\n    def from_df(cls, ratings, valid_pct=0.2, user_name=None, item_name=None, rating_name=None, seed=None, path='.', **kwargs):\n        \"Create a `DataLoaders` suitable for collaborative filtering from `ratings`.\"\n        user_name   = ifnone(user_name,   ratings.columns[0])\n        item_name   = ifnone(item_name,   ratings.columns[1])\n        rating_name = ifnone(rating_name, ratings.columns[2])\n        cat_names = [user_name,item_name]\n        splits = RandomSplitter(valid_pct=valid_pct, seed=seed)(range_of(ratings))\n        to = TabularCollab(ratings, [Categorify], cat_names, y_names=[rating_name], y_block=TransformBlock(), splits=splits)\n        return to.dataloaders(path=path, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/collab.py\nType:      method\n\n\n\n\n\ndls=CollabDataLoaders.from_df(df)\n\nbatch 데이터들의 group\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n    \n  \n  \n    \n      0\n      59\n      14\n      3.986921\n    \n    \n      1\n      23\n      3\n      4.393831\n    \n    \n      2\n      43\n      15\n      1.022492\n    \n    \n      3\n      15\n      16\n      0.857821\n    \n    \n      4\n      81\n      11\n      3.892794\n    \n    \n      5\n      8\n      1\n      4.194341\n    \n    \n      6\n      6\n      18\n      1.124469\n    \n    \n      7\n      41\n      20\n      1.019717\n    \n    \n      8\n      10\n      18\n      0.789071\n    \n    \n      9\n      2\n      4\n      3.822704\n    \n  \n\n\n\n학습 전\n\nX,y= dls.one_batch()\n\n\ntype(X)\n\ntorch.Tensor\n\n\n\ntype(y)\n\ntorch.Tensor\n\n\n\ntype(dls.one_batch())\n\ntuple\n\n\n\nX[0],y[0]\n\n(tensor([74,  5]), tensor([1.0687]))\n\n\n\n99번 user가 13번 아이템을 먹었을때 평점 4.3294\n64번 유저가 15번 아이템을 먹었을때 평점을 4.1146 주었음\n\n\n\n2단계\n\ncollab_learner??\n\n\nSignature:\ncollab_learner(\n    dls,\n    n_factors=50,\n    use_nn=False,\n    emb_szs=None,\n    layers=None,\n    config=None,\n    y_range=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7f6f5cbebca0>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7f6f7682d0d0>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n@delegates(Learner.__init__)\ndef collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, **kwargs):\n    \"Create a Learner for collaborative filtering on `dls`.\"\n    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n    if loss_func is None: loss_func = MSELossFlat()\n    if config is None: config = tabular_config()\n    if y_range is not None: config['y_range'] = y_range\n    if layers is None: layers = [n_factors]\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n    return Learner(dls, model, loss_func=loss_func, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/collab.py\nType:      function\n\n\n\n\n\nlrnr = collab_learner(dls,y_range=(0,5)) # y_range는 평점의 범위\n\ny는 평점이니까 0~5까지의 범위를 넣어주자\n\n\n3단계\n\nlrnr.fit(30) # 총 30번 정도 해야 적합이 잘된다. \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.319048\n      2.344653\n      00:00\n    \n    \n      1\n      2.308136\n      2.343053\n      00:00\n    \n    \n      2\n      2.294945\n      2.327852\n      00:00\n    \n    \n      3\n      2.277872\n      2.286632\n      00:00\n    \n    \n      4\n      2.242915\n      2.204543\n      00:00\n    \n    \n      5\n      2.190223\n      2.074773\n      00:00\n    \n    \n      6\n      2.110882\n      1.897575\n      00:00\n    \n    \n      7\n      2.002486\n      1.683303\n      00:00\n    \n    \n      8\n      1.865617\n      1.440904\n      00:00\n    \n    \n      9\n      1.705019\n      1.189762\n      00:00\n    \n    \n      10\n      1.528594\n      0.947479\n      00:00\n    \n    \n      11\n      1.343253\n      0.728591\n      00:00\n    \n    \n      12\n      1.158638\n      0.542451\n      00:00\n    \n    \n      13\n      0.982688\n      0.394331\n      00:00\n    \n    \n      14\n      0.821111\n      0.282930\n      00:00\n    \n    \n      15\n      0.678422\n      0.203212\n      00:00\n    \n    \n      16\n      0.556185\n      0.148874\n      00:00\n    \n    \n      17\n      0.453426\n      0.112210\n      00:00\n    \n    \n      18\n      0.368528\n      0.088727\n      00:00\n    \n    \n      19\n      0.299861\n      0.073288\n      00:00\n    \n    \n      20\n      0.244360\n      0.064172\n      00:00\n    \n    \n      21\n      0.200107\n      0.058580\n      00:00\n    \n    \n      22\n      0.164968\n      0.055078\n      00:00\n    \n    \n      23\n      0.137080\n      0.052871\n      00:00\n    \n    \n      24\n      0.115055\n      0.051715\n      00:00\n    \n    \n      25\n      0.097788\n      0.051180\n      00:00\n    \n    \n      26\n      0.084044\n      0.051137\n      00:00\n    \n    \n      27\n      0.073312\n      0.050811\n      00:00\n    \n    \n      28\n      0.064564\n      0.050948\n      00:00\n    \n    \n      29\n      0.057734\n      0.051064\n      00:00\n    \n  \n\n\n\nloss가 2.3에서 0.47으로 떨어지는 모습\n\n\n4단계\n- 이미 있는 데이터를 예측\n- 하나의 배치 전체를 예측\nlrnr.model(X)\n만 넣으면 에러뜬다.\n\n!nvidia-smi\n\nTue Sep 20 23:48:21 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n| 30%   51C    P2   130W / 420W |  12684MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     37424      C   ...onda3/envs/csy/bin/python     3419MiB |\n|    0   N/A  N/A    293359      C   ...onda3/envs/csy/bin/python     4789MiB |\n|    0   N/A  N/A    378609      C   ...onda3/envs/csy/bin/python     2605MiB |\n|    0   N/A  N/A    378623      C   ...onda3/envs/csy/bin/python     1869MiB |\n+-----------------------------------------------------------------------------+\n\n\nGPU 확인 가능\nGPU아님 CPU로 올리자\n\nyhat=lrnr.model(X.to(\"cuda:0\"))\nyhat\n\ntensor([1.0097, 3.9209, 4.0095, 1.0768, 1.0179, 3.9701, 1.0862, 4.0802, 4.0528,\n        1.0718, 3.9230, 0.9994, 0.9662, 0.9122, 0.9745, 4.0083, 0.9989, 4.1045,\n        4.1632, 4.0724, 3.9754, 0.9565, 4.0757, 4.0317, 4.0740, 1.0779, 3.9354,\n        0.9951, 3.9031, 1.0241, 4.0253, 4.0965, 4.0368, 4.0944, 1.0856, 4.1450,\n        4.0549, 4.0072, 0.8689, 4.0659, 3.9192, 3.9501, 4.0449, 0.9437, 1.0582,\n        0.9584, 4.0409, 4.0453, 1.0675, 0.9406, 1.0740, 0.9307, 0.9885, 3.9951,\n        3.9118, 4.1501, 0.8893, 0.8946, 3.9687, 1.0579, 4.1036, 3.9685, 1.0809,\n        1.0768], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\ny.reshape(-1)\n\ntensor([1.0687, 3.8248, 3.8609, 1.0261, 0.8480, 4.4577, 1.1617, 4.3921, 4.2632,\n        0.7725, 4.0136, 1.2705, 0.8435, 1.0225, 0.8010, 4.1617, 1.0468, 4.1678,\n        4.5026, 4.0560, 3.8630, 0.9917, 4.0591, 3.7022, 4.1746, 1.3469, 3.7943,\n        1.0213, 3.7841, 0.8235, 4.0407, 3.9853, 4.1260, 4.1900, 1.0309, 4.1798,\n        3.9636, 3.7450, 0.6707, 4.0318, 4.1648, 4.1057, 3.9359, 0.7864, 1.2067,\n        0.8126, 4.0661, 4.1786, 1.3155, 0.9504, 1.1084, 0.8396, 0.8503, 4.0655,\n        3.8489, 4.0402, 0.7891, 0.9279, 4.1935, 0.9436, 4.4777, 4.0123, 1.0577,\n        0.8246])\n\n\n\nlrnr.model()은 GPU메모리에 존재하고 X는 일반메모리에 존재하므로 X를 GPU메모리로 옮겨주어야 함\nX.to(“cuda:0”)을 통하여 X를 GPU메모리로 옮기는 작업을 수행할 수 있다.\n\n- 하나의 유저가 하나의 아이템을 선택했다고 가정하고 예측 (주어진 자료중에서 예측)\n\nX.shape\n\ntorch.Size([64, 2])\n\n\n\nX[0:1]\n\ntensor([[74,  5]])\n\n\n- 1번 user가 커피2 마셨을때 - 예상: 4점 근처\n\nlrnr.model(X[0:1].to(\"cuda:0\"))\n\ntensor([1.0097], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nlrnr.model(tensor([[1,2]]).to(\"cuda:0\"))\n\ntensor([3.9337], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\n18번 유저가 5번 아이템(커피)를 먹는다면?\n\n\nlrnr.model(X[0:1].to(\"cuda:0\"))\n\ntensor([1.0097], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\n평점은 4.1128정도 될것\n\n- 하나의 유저가 하나의 아이템을 선택했다고 가정하고 예측 (주어지지 않은 자료중에서 예측)\n\nX[0:1]\n\ntensor([[74,  5]])\n\n\n\nXnew = torch.tensor([[1,  2]])\n\n\nlrnr.model(Xnew.to(\"cuda:0\"))\n\ntensor([3.9337], device='cuda:0', grad_fn=<AddBackward0>)"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#텍스트분석-실습",
    "href": "posts/ml/2022-09-14-ml_2w.html#텍스트분석-실습",
    "title": "DNN (2주차)",
    "section": "텍스트분석 실습",
    "text": "텍스트분석 실습\ntimeseries 와 text 순서가 중요! - 가장 잘 응용할 수 있는 게 chatbot챗봇 - 나는 \\(\\to\\) 학교에 \\(\\to\\) 갔다.\ntimeseries는 뒤를 정확히 맞춰야 하지만, text는 그렇지 않..?\n\n1단계\n\ndf = pd.DataFrame({'text':['h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??']*20000})\ndf\n\n\n\n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      1\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      2\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      3\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      4\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      ...\n      ...\n    \n    \n      19995\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19996\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19997\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19998\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19999\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n  \n\n20000 rows × 1 columns\n\n\n\n\nTextDataLoaders.from_df??\n\n\nSignature:\nTextDataLoaders.from_df(\n    df,\n    path='.',\n    valid_pct=0.2,\n    seed=None,\n    text_col=0,\n    label_col=1,\n    label_delim=None,\n    y_block=None,\n    text_vocab=None,\n    is_lm=False,\n    valid_col=None,\n    tok_tfm=None,\n    tok_text_col='text',\n    seq_len=72,\n    backwards=False,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_df(cls, df, path='.', valid_pct=0.2, seed=None, text_col=0, label_col=1, label_delim=None, y_block=None,\n                text_vocab=None, is_lm=False, valid_col=None, tok_tfm=None, tok_text_col=\"text\", seq_len=72, backwards=False, **kwargs):\n        \"Create from `df` in `path` with `valid_pct`\"\n        blocks = [TextBlock.from_df(text_col, text_vocab, is_lm, seq_len, backwards, tok=tok_tfm)]\n        if y_block is None and not is_lm:\n            blocks.append(MultiCategoryBlock if is_listy(label_col) and len(label_col) > 1 else CategoryBlock)\n        if y_block is not None and not is_lm: blocks += (y_block if is_listy(y_block) else [y_block])\n        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n        dblock = DataBlock(blocks=blocks,\n                           get_x=ColReader(tok_text_col),\n                           get_y=None if is_lm else ColReader(label_col, label_delim=label_delim),\n                           splitter=splitter)\n        return cls.from_dblock(dblock, df, path=path, seq_len=seq_len, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/text/data.py\nType:      method\n\n\n\n\nis_lm = False\n다음 자료를 예측하고 싶을때\nis_lm = True\n\nclassification을 수행하고 싶을 때\n생성에 목적\nis_lm: text의 생성에 관심이 있다면 True로 설정할 것\n\n\ndls = TextDataLoaders.from_df(df,text_col='text',is_lm=True) \n\n\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n      h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o .\n    \n    \n      1\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n    \n    \n      2\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n    \n    \n      3\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n    \n    \n      4\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n    \n    \n      5\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n    \n    \n      6\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n    \n    \n      7\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n    \n    \n      8\n      ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n    \n  \n\n\n\n위의 결과에서 xxbos는 하나의 내용이 끝나고 다른 내용이 시작된다는 의미\n\n\n2단계\n\nlanguage_model_learner??\n\n\nSignature:\nlanguage_model_learner(\n    dls,\n    arch,\n    config=None,\n    drop_mult=1.0,\n    backwards=False,\n    pretrained=True,\n    pretrained_fnames=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7fcb79d04940>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n@delegates(Learner.__init__)\ndef language_model_learner(dls, arch, config=None, drop_mult=1., backwards=False, pretrained=True, pretrained_fnames=None, **kwargs):\n    \"Create a `Learner` with a language model from `dls` and `arch`.\"\n    vocab = _get_text_vocab(dls)\n    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n    meta = _model_meta[arch]\n    learn = LMLearner(dls, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n    url = 'url_bwd' if backwards else 'url'\n    if pretrained or pretrained_fnames:\n        if pretrained_fnames is not None:\n            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n        else:\n            if url not in meta:\n                warn(\"There are no pretrained weights for that architecture yet!\")\n                return learn\n            model_path = untar_data(meta[url] , c_key='model')\n            try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n            except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n        learn = learn.load_pretrained(*fnames)\n    return learn\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/text/learner.py\nType:      function\n\n\n\n\n\nlrnr = language_model_learner(dls, AWD_LSTM)\n\n\n\n3단계\n\nlrnr.fit(5)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.926327\n      0.851078\n      00:15\n    \n    \n      1\n      1.030813\n      0.366239\n      00:15\n    \n    \n      2\n      0.636483\n      0.251018\n      00:15\n    \n    \n      3\n      0.478914\n      0.209533\n      00:15\n    \n    \n      4\n      0.431652\n      0.190482\n      00:15\n    \n  \n\n\n\n\n\n4단계\n\nlrnr.predict('h e',n_words=30)\n\n\n\n\n'h e l l o ? h e l l o ! h e l l o ! h e l l o ! h e l l o ? ?'"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#gan-intro",
    "href": "posts/ml/2022-09-14-ml_2w.html#gan-intro",
    "title": "DNN (2주차)",
    "section": "GAN intro",
    "text": "GAN intro\n- 저자: 이안굿펠로우 (이름이 특이함. 좋은친구..) - 천재임 - 지도교수가 요수아 벤지오\n- 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. - https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음 \\(\\to\\) 48978회 인용..)\n- 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..)\n- 무슨내용? 생성모형\n\n생성모형이란? (쉬운 설명)\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n\n\nGAN의 응용분야\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면? - https://www.lgsl.kr/sto/stories/60/ALMA2020070001\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요…..\n\n\n\n생성모형이란? 통계학과 버전의 설명\n\n제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\ny의 평균적인 확률이 나올떄 x로 y 를 예측할 수 있다고 한단\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n분류할 수 았다는게 생성할 수 있다는 건 아니니까\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다.\n\n\nGAN의 원리\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n\n\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n\n\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n\n\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n\n\n위조범: 더 정교하게 만들자..\n\n\n경찰: 더 판별능력을 업그레이드 하자!\n\n\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2w.html#gan-실습",
    "href": "posts/ml/2022-09-14-ml_2w.html#gan-실습",
    "title": "DNN (2주차)",
    "section": "GAN 실습",
    "text": "GAN 실습\n\n1단계\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\nDataBlock??\n\n\nInit signature:\nDataBlock(\n    blocks=None,\n    dl_type=None,\n    getters=None,\n    n_inp=None,\n    item_tfms=None,\n    batch_tfms=None,\n    *,\n    get_items=None,\n    splitter=None,\n    get_y=None,\n    get_x=None,\n)\nSource:        \nclass DataBlock():\n    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n    get_x=get_items=splitter=get_y = None\n    blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n    _methods = 'get_items splitter get_y get_x'.split()\n    _msg = \"If you wanted to compose several transforms in your getter don't forget to wrap them in a `Pipeline`.\"\n    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs):\n        blocks = L(self.blocks if blocks is None else blocks)\n        blocks = L(b() if callable(b) else b for b in blocks)\n        self.type_tfms = blocks.attrgot('type_tfms', L())\n        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n        for b in blocks:\n            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n        if dl_type is not None: self.dl_type = dl_type\n        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n        if self.get_x:\n            if len(L(self.get_x)) != self.n_inp:\n                raise ValueError(f'get_x contains {len(L(self.get_x))} functions, but must contain {self.n_inp} (one for each input)\\n{self._msg}')\n            self.getters[:self.n_inp] = L(self.get_x)\n        if self.get_y:\n            n_targs = len(self.getters) - self.n_inp\n            if len(L(self.get_y)) != n_targs:\n                raise ValueError(f'get_y contains {len(L(self.get_y))} functions, but must contain {n_targs} (one for each target)\\n{self._msg}')\n            self.getters[self.n_inp:] = L(self.get_y)\n        if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n        self.new(item_tfms, batch_tfms)\n    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n        lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n    def new(self, item_tfms=None, batch_tfms=None):\n        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n        return self\n    @classmethod\n    def from_columns(cls, blocks=None, getters=None, get_items=None, **kwargs):\n        if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n        get_items = _zip if get_items is None else compose(get_items, _zip)\n        return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n    def datasets(self, source, verbose=False):\n        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n        splits = (self.splitter or RandomSplitter())(items)\n        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n        dsets = self.datasets(source, verbose=verbose)\n        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n    _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n                 datasets=\"Create a `Datasets` object from `source`\",\n                 dataloaders=\"Create a `DataLoaders` object from `source`\")\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\n\n\nTransformBlock\n\nfastai.data.block.TransformBlock\n\n\n\nImageBlock\n\n<function fastai.vision.data.ImageBlock(cls=<class 'fastai.vision.core.PILImage'>)>\n\n\n\ngenerate_noise\n\n<function fastai.vision.gan.generate_noise(fn, size=100)>\n\n\n\ndblock = DataBlock(blocks=(TransformBlock,ImageBlock),\n          get_x = generate_noise,\n          get_items=get_image_files,\n          item_tfms=Resize(32))\ndls = dblock.dataloaders(path) \n\n\ndls.show_batch()\n\n\n\n\n\n\n2단계\n\nbasic_generator??\n\n\nSignature:\nbasic_generator(\n    out_size,\n    n_channels,\n    in_sz=100,\n    n_features=64,\n    n_extra_layers=0,\n    ks=3,\n    stride=1,\n    padding=None,\n    bias=None,\n    ndim=2,\n    norm_type=<NormType.Batch: 1>,\n    bn_1st=True,\n    act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n    transpose=False,\n    init='auto',\n    xtra=None,\n    bias_std=0.01,\n    dilation: Union[int, Tuple[int, int]] = 1,\n    groups: int = 1,\n    padding_mode: str = 'zeros',\n    device=None,\n    dtype=None,\n)\nSource:   \n@delegates(ConvLayer.__init__)\ndef basic_generator(out_size, n_channels, in_sz=100, n_features=64, n_extra_layers=0, **kwargs):\n    \"A basic generator from `in_sz` to images `n_channels` x `out_size` x `out_size`.\"\n    cur_size, cur_ftrs = 4, n_features//2\n    while cur_size < out_size:  cur_size *= 2; cur_ftrs *= 2\n    layers = [AddChannels(2), ConvLayer(in_sz, cur_ftrs, 4, 1, transpose=True, **kwargs)]\n    cur_size = 4\n    while cur_size < out_size // 2:\n        layers.append(ConvLayer(cur_ftrs, cur_ftrs//2, 4, 2, 1, transpose=True, **kwargs))\n        cur_ftrs //= 2; cur_size *= 2\n    layers += [ConvLayer(cur_ftrs, cur_ftrs, 3, 1, 1, transpose=True, **kwargs) for _ in range(n_extra_layers)]\n    layers += [nn.ConvTranspose2d(cur_ftrs, n_channels, 4, 2, 1, bias=False), nn.Tanh()]\n    return nn.Sequential(*layers)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      function\n\n\n\n\n\nbasic_critic??\n\n\nSignature:\nbasic_critic(\n    in_size,\n    n_channels,\n    n_features=64,\n    n_extra_layers=0,\n    norm_type=<NormType.Batch: 1>,\n    ks=3,\n    stride=1,\n    padding=None,\n    bias=None,\n    ndim=2,\n    bn_1st=True,\n    act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n    transpose=False,\n    init='auto',\n    xtra=None,\n    bias_std=0.01,\n    dilation: Union[int, Tuple[int, int]] = 1,\n    groups: int = 1,\n    padding_mode: str = 'zeros',\n    device=None,\n    dtype=None,\n)\nSource:   \n@delegates(ConvLayer.__init__)\ndef basic_critic(in_size, n_channels, n_features=64, n_extra_layers=0, norm_type=NormType.Batch, **kwargs):\n    \"A basic critic for images `n_channels` x `in_size` x `in_size`.\"\n    layers = [ConvLayer(n_channels, n_features, 4, 2, 1, norm_type=None, **kwargs)]\n    cur_size, cur_ftrs = in_size//2, n_features\n    layers += [ConvLayer(cur_ftrs, cur_ftrs, 3, 1, norm_type=norm_type, **kwargs) for _ in range(n_extra_layers)]\n    while cur_size > 4:\n        layers.append(ConvLayer(cur_ftrs, cur_ftrs*2, 4, 2, 1, norm_type=norm_type, **kwargs))\n        cur_ftrs *= 2 ; cur_size //= 2\n    init = kwargs.get('init', nn.init.kaiming_normal_)\n    layers += [init_default(nn.Conv2d(cur_ftrs, 1, 4, padding=0), init), Flatten()]\n    return nn.Sequential(*layers)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      function\n\n\n\n\n\ncounterfeiter = basic_generator(32,n_channels=3,n_extra_layers=1)\npolice = basic_critic(32,n_channels=3,n_extra_layers=1)\n\n\n32는 사이즈\n채널은 컬러이면 3이지만 이건 흑백이라도 3으로 표현해봄\n\n\nGANLearner.wgan??\n\n\nSignature:\nGANLearner.wgan(\n    dls,\n    generator,\n    critic,\n    switcher=None,\n    clip=0.01,\n    switch_eval=False,\n    gen_first=False,\n    show_img=True,\n    cbs=None,\n    metrics=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7fcb79d04940>,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n    @classmethod\n    def wgan(cls, dls, generator, critic, switcher=None, clip=0.01, switch_eval=False, **kwargs):\n        \"Create a WGAN from `data`, `generator` and `critic`.\"\n        if switcher is None: switcher = FixedGANSwitcher(n_crit=5, n_gen=1)\n        return cls(dls, generator, critic, _tk_mean, _tk_diff, switcher=switcher, clip=clip, switch_eval=switch_eval, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      method\n\n\n\n\n\nlrnr = GANLearner.wgan(dls,counterfeiter,police) \n\n\n\n3단계\n- lrnr.fit(10) 진행\n\nlrnr.fit(10)\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.543594\n      0.420719\n      0.420719\n      -0.754515\n      00:04\n    \n    \n      1\n      -0.578704\n      0.376830\n      0.376830\n      -0.759121\n      00:04\n    \n    \n      2\n      -0.581667\n      0.335058\n      0.335058\n      -0.765852\n      00:04\n    \n    \n      3\n      -0.581913\n      0.350422\n      0.350422\n      -0.766256\n      00:04\n    \n    \n      4\n      -0.576919\n      0.239790\n      0.239790\n      -0.757070\n      00:04\n    \n    \n      5\n      -0.568187\n      0.165856\n      0.165856\n      -0.738596\n      00:04\n    \n    \n      6\n      -0.561763\n      0.312277\n      0.312277\n      -0.738768\n      00:04\n    \n    \n      7\n      -0.545816\n      0.312626\n      0.312626\n      -0.735054\n      00:04\n    \n    \n      8\n      -0.530404\n      0.315626\n      0.315626\n      -0.713279\n      00:04\n    \n    \n      9\n      -0.552664\n      0.292665\n      0.292665\n      -0.719266\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총20회\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.534434\n      0.329432\n      0.329432\n      -0.738360\n      00:04\n    \n    \n      1\n      -0.491044\n      0.241687\n      0.241687\n      -0.282900\n      00:04\n    \n    \n      2\n      -0.430823\n      0.247032\n      0.247032\n      -0.631827\n      00:04\n    \n    \n      3\n      -0.509287\n      0.228638\n      0.228638\n      -0.702186\n      00:04\n    \n    \n      4\n      -0.541639\n      0.306787\n      0.306787\n      -0.737486\n      00:04\n    \n    \n      5\n      -0.490239\n      0.270219\n      0.270219\n      -0.686973\n      00:04\n    \n    \n      6\n      -0.456657\n      0.370165\n      0.370165\n      -0.651278\n      00:04\n    \n    \n      7\n      -0.375928\n      0.254674\n      0.254674\n      -0.463629\n      00:04\n    \n    \n      8\n      -0.505262\n      0.241540\n      0.241540\n      -0.706440\n      00:04\n    \n    \n      9\n      -0.511837\n      0.264010\n      0.264010\n      -0.717528\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총30회\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.389076\n      0.203898\n      0.203898\n      -0.610006\n      00:04\n    \n    \n      1\n      -0.404953\n      0.248211\n      0.248211\n      -0.564691\n      00:04\n    \n    \n      2\n      -0.399689\n      0.157126\n      0.157126\n      -0.475484\n      00:04\n    \n    \n      3\n      -0.412959\n      0.160083\n      0.160083\n      -0.628447\n      00:04\n    \n    \n      4\n      -0.419133\n      0.140253\n      0.140253\n      -0.315640\n      00:04\n    \n    \n      5\n      -0.412665\n      0.360084\n      0.360084\n      -0.504751\n      00:04\n    \n    \n      6\n      -0.419645\n      0.331901\n      0.331901\n      -0.627747\n      00:04\n    \n    \n      7\n      -0.393825\n      0.099620\n      0.099620\n      -0.479805\n      00:04\n    \n    \n      8\n      -0.383802\n      0.332651\n      0.332651\n      -0.485545\n      00:04\n    \n    \n      9\n      -0.329964\n      0.066743\n      0.066743\n      -0.331843\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총 60회\n\nlrnr.fit(30)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.280188\n      0.083489\n      0.083489\n      -0.421842\n      00:04\n    \n    \n      1\n      -0.211743\n      0.066232\n      0.066232\n      -0.485185\n      00:04\n    \n    \n      2\n      -0.548622\n      0.439374\n      0.439374\n      -0.729976\n      00:04\n    \n    \n      3\n      -0.184136\n      0.166024\n      0.166024\n      -0.196536\n      00:04\n    \n    \n      4\n      -0.180048\n      0.283176\n      0.283176\n      -0.343643\n      00:04\n    \n    \n      5\n      -0.082062\n      -0.111767\n      -0.111767\n      -0.232821\n      00:06\n    \n    \n      6\n      -0.134064\n      -0.252754\n      -0.252754\n      -0.056792\n      00:05\n    \n    \n      7\n      -0.024693\n      -0.019944\n      -0.019944\n      -0.060462\n      00:04\n    \n    \n      8\n      -0.067052\n      -0.176633\n      -0.176633\n      -0.087246\n      00:05\n    \n    \n      9\n      -0.051849\n      0.077216\n      0.077216\n      -0.063890\n      00:04\n    \n    \n      10\n      -0.062414\n      0.576616\n      0.576616\n      0.007565\n      00:04\n    \n    \n      11\n      -0.028339\n      -0.177623\n      -0.177623\n      -0.165999\n      00:04\n    \n    \n      12\n      -0.285967\n      0.256777\n      0.256777\n      -0.486597\n      00:04\n    \n    \n      13\n      -0.040034\n      0.200722\n      0.200722\n      -0.002714\n      00:05\n    \n    \n      14\n      -0.082319\n      -0.218245\n      -0.218245\n      -0.076548\n      00:04\n    \n    \n      15\n      -0.126090\n      -0.120631\n      -0.120631\n      -0.311712\n      00:04\n    \n    \n      16\n      -0.120472\n      0.024194\n      0.024194\n      -0.165129\n      00:04\n    \n    \n      17\n      -0.213207\n      0.447029\n      0.447029\n      -0.441171\n      00:04\n    \n    \n      18\n      -0.104892\n      0.076353\n      0.076353\n      -0.305898\n      00:04\n    \n    \n      19\n      -0.077636\n      -0.229590\n      -0.229590\n      -0.206540\n      00:04\n    \n    \n      20\n      -0.037347\n      -0.169197\n      -0.169197\n      -0.070061\n      00:04\n    \n    \n      21\n      -0.063813\n      -0.283316\n      -0.283316\n      0.009801\n      00:04\n    \n    \n      22\n      -0.037806\n      -0.101751\n      -0.101751\n      -0.020896\n      00:04\n    \n    \n      23\n      -0.057209\n      -0.012665\n      -0.012665\n      -0.095574\n      00:04\n    \n    \n      24\n      -0.036946\n      0.090177\n      0.090177\n      -0.049521\n      00:04\n    \n    \n      25\n      -0.050363\n      -0.206716\n      -0.206716\n      -0.035576\n      00:04\n    \n    \n      26\n      -0.047856\n      0.052171\n      0.052171\n      -0.017636\n      00:04\n    \n    \n      27\n      -0.009292\n      -0.027788\n      -0.027788\n      -0.003629\n      00:04\n    \n    \n      28\n      -0.032223\n      -0.223866\n      -0.223866\n      -0.011261\n      00:04\n    \n    \n      29\n      -0.028316\n      -0.006388\n      -0.006388\n      -0.024545\n      00:05\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n4단계 (없음)"
  },
  {
    "objectID": "posts/ml/2022-11-29-13wk-2-final.html",
    "href": "posts/ml/2022-11-29-13wk-2-final.html",
    "title": "Deep Learning final example",
    "section": "",
    "text": "기말고사"
  },
  {
    "objectID": "posts/ml/2022-11-29-13wk-2-final.html#hihello-90점",
    "href": "posts/ml/2022-11-29-13wk-2-final.html#hihello-90점",
    "title": "Deep Learning final example",
    "section": "1. hi?hello!! (90점)",
    "text": "1. hi?hello!! (90점)\n아래와 같은 데이터가 있다고 하자.\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5], txt_y[:5]\n\n(['h', 'i', '?', 'h', 'e'], ['i', '?', 'h', 'e', 'l'])\n\n\ntxt_x와 txt_y를 이용하여 아래와 같은 순서로 다음문자를 예측하고 싶은 신경망을 설계하고 싶다.\nh \\(\\to\\) i \\(\\to\\) ? \\(\\to\\) h \\(\\to\\) e \\(\\to\\) l \\(\\to\\) l \\(\\to\\) o \\(\\to\\) ! \\(\\to\\) ! \\(\\to\\) h \\(\\to\\) i \\(\\to\\) ? \\(\\to\\) h \\(\\to\\) e \\(\\to\\) \\(\\dots\\)\n(1)-(6) 의 풀이에 공통적으로 필요한 과정 정리\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n(1) torch.nn.RNN()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\nrnn = torch.nn.RNN(7,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n_water = torch.zeros(1,8).to(\"cuda:0\")\nfor epoc in range(500):\n    ## 1\n    hidden, hT = rnn(x)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(2) torch.nn.RNNCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(12345)\nrnncell = torch.nn.RNNCell(7,8).to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = [] \n    ht = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht = rnncell(xt,ht) \n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.005, 0.   , 0.982, 0.013, 0.   , 0.   ],\n       [0.   , 0.999, 0.   , 0.   , 0.   , 0.001, 0.   ],\n       [0.   , 0.   , 1.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.001, 0.999, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 1.   , 0.   , 0.   , 0.   , 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(3) torch.nn.Module을 상속받은 클래스를 정의하고 (2)의 결과와 동일한 적합값이 나오는 신경망을 설계한 뒤 학습하라. (초기값을 적절하게 설정할 것)\n\nclass를 이용하지 않으면 점수없음.\ntorch.nn.RNN(), torch.nn.RNNCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.RNN(), torch.nn.RNNCell()을 코드에 포함시키는 것이 가능)\n\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(7,8)\n        self.h2h = torch.nn.Linear(8,8) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,xt,ht):\n        ht = self.tanh(self.i2h(xt)+self.h2h(ht))\n        return ht\n\n\nrnncell = rNNCell().to(\"cuda:0\")\nlinr = torch.nn.Linear(8,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(linr.parameters()),lr=0.1)\n\n\n## 초기화의 설정을 위한 코드\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(7,8).to(\"cuda:0\")\n_linr = torch.nn.Linear(8,7).to(\"cuda:0\")\nrnncell.i2h.weight.data = _rnncell.weight_ih.data \nrnncell.h2h.weight.data = _rnncell.weight_hh.data \nrnncell.h2h.bias.data = _rnncell.bias_hh.data\nrnncell.i2h.bias.data = _rnncell.bias_ih.data\nlinr.weight.data = _linr.weight.data \nlinr.bias.data = _linr.bias.data \n\n\nfor epoc in range(100):\n    ## 1\n    hidden = [] \n    ht = torch.zeros(8).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht = rnncell(xt,ht)\n        # ot = linr(ht) \n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.005, 0.008, 0.972, 0.014, 0.001, 0.   ],\n       [0.   , 0.997, 0.002, 0.   , 0.   , 0.001, 0.   ],\n       [0.   , 0.001, 0.999, 0.   , 0.001, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.999, 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n       [0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.   ],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.001],\n       [0.999, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.998, 0.   , 0.   , 0.   , 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(4) torch.nn.LSTM()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n(풀이)\n\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n_water = torch.zeros(1,4).to(\"cuda:0\")\nfor epoc in range(500):\n    ## 1\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nyhat=soft(output)    \nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(5) torch.nn.LSTMCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1\n    hidden = []\n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden)\n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.014, 0.084, 0.081, 0.822, 0.   , 0.   ],\n       [0.002, 0.91 , 0.   , 0.083, 0.003, 0.   , 0.001],\n       [0.001, 0.   , 0.999, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.005, 0.072, 0.917, 0.004, 0.   ],\n       [0.   , 0.   , 0.004, 0.   , 0.001, 0.995, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.999, 0.001],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999],\n       [0.998, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.99 , 0.   , 0.006, 0.001, 0.   , 0.003, 0.   ],\n       [0.007, 0.   , 0.992, 0.   , 0.   , 0.001, 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);\n\n\n\n\n(6) (5)의 결과와 동일한 적합값을 출력하는 신경망을 직접설계한 뒤 학습시켜라. (초기값을 적절하게 설정할 것)\n\nclass를 이용하지 않아도 무방함.\ntorch.nn.LSTM(), torch.nn.LSTMCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.LSTM(), torch.nn.LSTMCell()을 코드에 포함시키는 것이 가능)\n\n\nclass lSTMCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(7,16)\n        self.h2h = torch.nn.Linear(4,16) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,xt,past):\n        ht,ct = past \n        ifgo = self.i2h(xt) + self.h2h(ht) \n        it = sig(ifgo[0:4])\n        ft = sig(ifgo[4:8])\n        gt = tanh(ifgo[8:12])\n        ot = sig(ifgo[12:16])\n        ct = ft*ct + it*gt\n        ht = ot*self.tanh(ct) \n        return ht,ct\n\n\nlstmcell = lSTMCell().to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\n# 초기값셋팅\ntorch.manual_seed(43052) \n_lstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n_linr = torch.nn.Linear(4,7).to(\"cuda:0\")\nlstmcell.i2h.weight.data = _lstmcell.weight_ih.data \nlstmcell.h2h.weight.data = _lstmcell.weight_hh.data \nlstmcell.i2h.bias.data = _lstmcell.bias_ih.data\nlstmcell.h2h.bias.data = _lstmcell.bias_hh.data\nlinr.weight.data = _linr.weight.data \nlinr.bias.data = _linr.bias.data \n\n\nfor epoc in range(100):\n    ## 1\n    hidden = []     \n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\nyhat[:10].to(\"cpu\").detach().numpy().round(3)\n\narray([[0.   , 0.014, 0.084, 0.081, 0.822, 0.   , 0.   ],\n       [0.002, 0.91 , 0.   , 0.083, 0.003, 0.   , 0.001],\n       [0.001, 0.   , 0.999, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.001, 0.005, 0.072, 0.917, 0.004, 0.   ],\n       [0.   , 0.   , 0.004, 0.   , 0.001, 0.995, 0.   ],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.999, 0.001],\n       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999],\n       [0.998, 0.001, 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.99 , 0.   , 0.006, 0.001, 0.   , 0.003, 0.   ],\n       [0.007, 0.   , 0.992, 0.   , 0.   , 0.001, 0.   ]], dtype=float32)\n\n\n\nplt.matshow(yhat.to(\"cpu\").data[:10],cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(7),labels=['!','?','h','i','e','l','o']);"
  },
  {
    "objectID": "posts/ml/2022-11-29-13wk-2-final.html#다음을-읽고-참-거짓을-판단하여라.-10점",
    "href": "posts/ml/2022-11-29-13wk-2-final.html#다음을-읽고-참-거짓을-판단하여라.-10점",
    "title": "Deep Learning final example",
    "section": "2. 다음을 읽고 참 거짓을 판단하여라. (10점)",
    "text": "2. 다음을 읽고 참 거짓을 판단하여라. (10점)\n(1) RNN은 LSTM에 비하여 장기기억에 유리하다.\n참\n(2) torch.nn.Embedding(num_embeddings=2,embedding_dim=1)와 torch.nn.Linear(in_features=1,out_features=1)의 학습가능한 파라메터수는 같다.\n참\n(3)아래와 같은 네트워크를 고려하자.\nnet = torch.nn.Linear(1,1)\n차원이 (n,1) 인 임의의 텐서에 대하여 net(x)와 net.forword(x)의 출력결과는 같다.\n참\n(4) 아래와 같이 a,b,c,d 가 반복되는 문자열이 반복되는 자료에서 다음문자열을 맞추는 과업을 수행하기 위해서는 반드시 순환신경망의 형태로 설계해야만 한다\na,b,c,d,a,b,c,d,…\n거짓\n(5) RNN 혹은 LSTM 으로 신경망을 설계할 시 손실함수는 항상 torch.nn.CrossEntropyLoss 를 사용해야 한다.\n거짓"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html",
    "href": "posts/ml/2022-12-14-study.html",
    "title": "study",
    "section": "",
    "text": "study"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#임베딩",
    "href": "posts/ml/2022-12-14-study.html#임베딩",
    "title": "study",
    "section": "임베딩",
    "text": "임베딩\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 1, 0]), tensor([1, 0, 1, 0, 1]))\n\n\n\nclass mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.l1 = torch.nn.Embedding(num_embeddings=2,embedding_dim=1)\n        self.a1 = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=2)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        yhat = self.l2(self.a1(self.l1(x)))\n        ## 정의 끝\n        return yhat\n\n\nnet = mynet2()\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    yhat = net(x)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#두-개의-은닉노드-이용",
    "href": "posts/ml/2022-12-14-study.html#두-개의-은닉노드-이용",
    "title": "study",
    "section": "두 개의 은닉노드 이용",
    "text": "두 개의 은닉노드 이용\n\ntxt = list('abcd')*100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n\nl1=torch.nn.Embedding(num_embeddings=4,embedding_dim=2)\n\n\na1 = torch.nn.Tanh()\n\n\nl2 = torch.nn.Linear(in_features=2,out_features=4)\n\n\nnet = torch.nn.Sequential(\n    l1,a1,l2)\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    yhat=net(x)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nnet[:-1]\n\nSequential(\n  (0): Embedding(4, 2)\n  (1): Tanh()\n)\n\n\n결과 시각화 코드 10주"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#순환신경망-class-사용-rnn",
    "href": "posts/ml/2022-12-14-study.html#순환신경망-class-사용-rnn",
    "title": "study",
    "section": "순환신경망 Class 사용 RNN",
    "text": "순환신경망 Class 사용 RNN\n10주\n\ntxt = list('AbAcAd')*100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell()\n\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n\nT = len(x) \nfor epoc in range(100): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n시각화코드는 10주_순환신경망구현1-성공에"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#순환신경망-rnn",
    "href": "posts/ml/2022-12-14-study.html#순환신경망-rnn",
    "title": "study",
    "section": "순환신경망 RNN",
    "text": "순환신경망 RNN\n\ntxt = list('AbAcAd')*100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))).float()\n\n\ntorch.manual_seed(2) #1 \nrnn = torch.nn.RNN(4,3).to(\"cuda:0\") \ncook = torch.nn.Linear(3,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n\n_water = torch.zeros(1,3).to(\"cuda:0\") \nfor epoc in range(500):\n    ## 1\n    hidden,hT = rnn(x.to(\"cuda:0\"),_water) \n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y.to(\"cuda:0\")) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#순환신경망-class-사용-lstm",
    "href": "posts/ml/2022-12-14-study.html#순환신경망-class-사용-lstm",
    "title": "study",
    "section": "순환신경망 Class 사용 LSTM",
    "text": "순환신경망 Class 사용 LSTM\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\nclass lSTMCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(7,16)\n        self.h2h = torch.nn.Linear(4,16) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,xt,past):\n        ht,ct = past \n        ifgo = self.i2h(xt) + self.h2h(ht) \n        it = sig(ifgo[0:4])\n        ft = sig(ifgo[4:8])\n        gt = tanh(ifgo[8:12])\n        ot = sig(ifgo[12:16])\n        ct = ft*ct + it*gt\n        ht = ot*self.tanh(ct) \n        return ht,ct\n\n\nlstmcell = lSTMCell().to(\"cuda:0\")\nlinr = torch.nn.Linear(4,7).to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\n# 초기값셋팅\ntorch.manual_seed(43052) \n_lstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n_linr = torch.nn.Linear(4,7).to(\"cuda:0\")\nlstmcell.i2h.weight.data = _lstmcell.weight_ih.data \nlstmcell.h2h.weight.data = _lstmcell.weight_hh.data \nlstmcell.i2h.bias.data = _lstmcell.bias_ih.data\nlstmcell.h2h.bias.data = _lstmcell.bias_hh.data\nlinr.weight.data = _linr.weight.data \nlinr.bias.data = _linr.bias.data \n\n\nfor epoc in range(10):\n    ## 1\n    hidden = []     \n    ht = torch.zeros(4).to(\"cuda:0\")\n    ct = torch.zeros(4).to(\"cuda:0\")\n    for xt,yt in zip(x,y): \n        ht,ct = lstmcell(xt,(ht,ct))\n        hidden.append(ht) \n    hidden = torch.stack(hidden)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\n\n\nyhat\n\ntensor([[0.0909, 0.0859, 0.0356,  ..., 0.1820, 0.2308, 0.1257],\n        [0.3443, 0.1629, 0.2108,  ..., 0.1008, 0.0506, 0.0774],\n        [0.3998, 0.0325, 0.5100,  ..., 0.0133, 0.0269, 0.0119],\n        ...,\n        [0.0655, 0.0525, 0.0455,  ..., 0.1652, 0.2569, 0.2828],\n        [0.3850, 0.0844, 0.3754,  ..., 0.0464, 0.0423, 0.0478],\n        [0.4012, 0.0217, 0.5328,  ..., 0.0084, 0.0254, 0.0065]],\n       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
  },
  {
    "objectID": "posts/ml/2022-12-14-study.html#순환신경망-lstm",
    "href": "posts/ml/2022-12-14-study.html#순환신경망-lstm",
    "title": "study",
    "section": "순환신경망 LSTM",
    "text": "순환신경망 LSTM\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \ntxt_x = txt[:-1]\ntxt_y = txt[1:] \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\")\nfor epoc in range(50):\n    ## 1 \n    hidden, (hT,cT) =lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nsoft(output)\n\ntensor([[9.9875e-01, 1.5434e-06, 6.5715e-04, 1.8973e-05, 9.5753e-05, 4.7651e-04],\n        [3.0184e-06, 3.7971e-05, 9.8694e-01, 3.6741e-03, 3.6129e-04, 8.9867e-03],\n        [9.9999e-01, 2.9932e-06, 5.9745e-07, 8.3266e-06, 1.0668e-07, 4.8888e-07],\n        ...,\n        [3.9604e-05, 8.6161e-06, 1.5918e-03, 1.1244e-07, 9.9808e-01, 2.7556e-04],\n        [9.9993e-01, 3.3252e-07, 9.5155e-06, 4.8129e-07, 2.7274e-05, 3.2102e-05],\n        [8.0918e-07, 8.0716e-03, 5.9763e-04, 7.7044e-05, 6.8931e-05, 9.9118e-01]],\n       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n\n\n\n조각난 시계열\n12주차\n\ntxt = list('hi!')*3 + list('hi?')*3\ntxt_x = txt[:-1] \ntxt_y = txt[1:] \nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\") \n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f1462eb7510>\n\n\n\n\n\n\ntxt = list('hi!')*3 + list('hi?')*3\ntxt1= txt[:9]\ntxt2= txt[9:]\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(xx) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f1462e93690>"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html",
    "href": "posts/ml/2022-10-19-ml_7w.html",
    "title": "CNN (7주차)",
    "section": "",
    "text": "기계학습 특강 (7주차) 10월19일 [딥러닝의 기초 - 드랍아웃, 이미지자료분석]"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#imports",
    "href": "posts/ml/2022-10-19-ml_7w.html#imports",
    "title": "CNN (7주차)",
    "section": "imports",
    "text": "imports\n\nimport torch\nfrom fastai.vision.all import *\nimport matplotlib.pyplot as plt\n\nimport torchvision\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#깊은신경망-오버피팅",
    "href": "posts/ml/2022-10-19-ml_7w.html#깊은신경망-오버피팅",
    "title": "CNN (7주차)",
    "section": "깊은신경망– 오버피팅",
    "text": "깊은신경망– 오버피팅\n\n데이터\n- model: \\(y_i = (0\\times x_i) + \\epsilon_i\\)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)\n\n\n\n\n\n\n모든 데이터를 사용하여 적합 (512, relu, 1000 epochs)\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n전체데이터를 8:2로 나누어서 8만을 학습\n- 데이터를 8:2로 나눈다\n\nxtr = x[:80]\nytr = y[:80] \nxtest = x[80:] \nytest = y[80:] \n\n\nx.shape, xtr.shape, xtest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\ny.shape, ytr.shape, ytest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\n\n\n\n\n- (xtr,ytr) 만 가지고 net를 학습시킨다.\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    # yhat\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \n#plt.plot(xtr,net(xtr).data,'--k') \n#plt.plot(xtest,net(xtest).data,'--k') \n\n\n\n\n(서연 필기) 오차항이 너무 잘 따라가면 영향을 미칠 수 있다.\n데이터에 비해 노드 수가 많으면 오버피팅의 가능성 - 한 변수로 모든 변수 맞추는 우연을 마주한다면? - 모델에 비해 feature가 너무 클때? - 위를 예로 들면 input은 1이었는데 output은 512렸다\n차원의 저주"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#깊은신경망-드랍아웃",
    "href": "posts/ml/2022-10-19-ml_7w.html#깊은신경망-드랍아웃",
    "title": "CNN (7주차)",
    "section": "깊은신경망– 드랍아웃",
    "text": "깊은신경망– 드랍아웃\n\n오버피팅의 해결\n- 오버피팅의 해결책: 드랍아웃\n동등한 초기값에서 시작한다고 설명 - manual_seed 정해준거\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    #\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n계속 바뀌는 plot\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in training mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in training mode')\n\n\n\n\n\n- 올바른 사용법\n\nnet.training\n\nTrue\n\n\nevaliation method 사용\n\nnet.eval()\nnet.training\n\nFalse\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in evaluation mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in evaluation mode')\n\n\n\n\n\n\n\n드랍아웃 레이어\n\n_x = torch.linspace(0,1,101) \n_x \n\ntensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900, 1.0000])\n\n\n\ndout = torch.nn.Dropout(0.9)\ndout(_x)\n\ntensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 1.3000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 2.9000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.1000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.9000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.1000,\n        0.0000, 0.0000, 0.0000, 0.0000, 7.6000, 7.7000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.9000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000])\n\n\n\n90%의 드랍아웃: 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 0으로 만든다. + 그리고 0이 되지않고 살아남은 값들은 10배 만큼 값이 커진다.\n\n- 드랍아웃레이어 정리 - 구조: 입력 -> 드랍아웃레이어 -> 출력 - 역할: (1) 입력의 일부를 임의로 0으로 만드는 역할 (2) 0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정 - 효과: 오버피팅을 억제하는 효과가 있음 (왜??) - 추측일뿐! - 의미: each iteration (each epoch x) 마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨. - 느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전반적으로 개선됨\n(서연 필기) 지배적인 예측 값들보다 비지배적인 예측값을 건들려고 하면 의미가 없음."
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-data",
    "href": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-data",
    "title": "CNN (7주차)",
    "section": "이미지자료분석– data",
    "text": "이미지자료분석– data\n- download data\n\npath = untar_data(URLs.MNIST)\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-예비학습",
    "href": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-예비학습",
    "title": "CNN (7주차)",
    "section": "이미지자료분석– CNN 예비학습",
    "text": "이미지자료분석– CNN 예비학습\n\n기존의 MLP 모형\n- 교재의 모형\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: Sigmoid\"\n}\n''')\n\n\n\n\n- 왜 28 by 28 이미지를 784개의 벡터로 만든 다음에 모형을 돌려야 하는가?\n- 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어서 이미지자료를 분석하는 느낌\n- observation의 차원은 \\(784\\)가 아니라 \\(1\\times (28\\times 28)\\)이 되어야 맞다.\n\n\n새로운 아키텍처의 제시\n- 예전\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\\(l_1\\): 선형변환, feature를 뻥튀기하는 역할\n\n\\(\\sim\\) 꺾인 선이 많아진다\n\n\\(relu\\): 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화\n\\(l_2\\): 선형변환, 뻥튀기된 feature를 요약 하는 역할 (=데이터를 요약하는 역할)\n\n- 새로운 아키텍처 - \\(conv\\): feature를 뻥튀기하는 역할 (2d ver \\(l_1\\) 느낌) - \\(relu\\): - \\(pooling\\): 데이터를 요약하는 역할\n\n\nCONV 레이어 (선형변환의 2D 버전)\n- 우선 연산하는 방법만 살펴보자.\n(예시1)\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[-0.1733, -0.4235],\n           [ 0.1802,  0.4668]]]]),\n tensor([0.2037]))\n\n\n\n_X = torch.arange(4).reshape(1,1,2,2).float()\n_X\n\ntensor([[[[0., 1.],\n          [2., 3.]]]])\n\n\n\n(-0.1733)*0 + (-0.4235)*1 +\\\n(0.1802)*2 + (0.4668)*3 + 0.2037\n\n1.541\n\n\n\n_conv(_X)\n\ntensor([[[[1.5410]]]], grad_fn=<ThnnConv2DBackward0>)\n\n\n\ntorch.__version__\n\n'1.10.1'\n\n\n(예시2) 잘하면 평균도 계산하겠다?\n\n_conv.weight.data = torch.tensor([[[[1/4, 1/4],[1/4,1/4]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data,_conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]),\n tensor([0.]))\n\n\n\n_conv(_X) , (0+1+2+3)/4\n\n(tensor([[[[1.5000]]]], grad_fn=<ThnnConv2DBackward0>), 1.5)\n\n\n(예시3) 이동평균?\n\n_X = torch.arange(0,25).float().reshape(1,1,5,5) \n_X\n\ntensor([[[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]])\n\n\n\n_conv(_X)\n\ntensor([[[[ 3.,  4.,  5.,  6.],\n          [ 8.,  9., 10., 11.],\n          [13., 14., 15., 16.],\n          [18., 19., 20., 21.]]]], grad_fn=<ThnnConv2DBackward0>)\n\n\n(예시4) window size가 증가한다면? (2d의 이동평균느낌)\n\n_conv = torch.nn.Conv2d(1,1,(3,3)) # 입력1, 출력1, (3,3) window size\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data = torch.tensor([[[[1/9,1/9,1/9],[1/9,1/9,1/9],[1/9,1/9,1/9]]]])\n\n(3,3)이나~ 3이나~\n\n_X,_conv(_X)\n\n(tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n           [ 5.,  6.,  7.,  8.,  9.],\n           [10., 11., 12., 13., 14.],\n           [15., 16., 17., 18., 19.],\n           [20., 21., 22., 23., 24.]]]]),\n tensor([[[[ 6.0000,  7.0000,  8.0000],\n           [11.0000, 12.0000, 13.0000],\n           [16.0000, 17.0000, 18.0000]]]], grad_fn=<ThnnConv2DBackward0>))\n\n\n\n(1+2+3+6+7+8+11+12+13)/9\n\n7.0\n\n\n(예시5) 피처뻥튀기\n\n_X = torch.tensor([1.0,1.0,1.0,1.0]).reshape(1,1,2,2)\n_X\n\ntensor([[[[1., 1.],\n          [1., 1.]]]])\n\n\n\n_conv = torch.nn.Conv2d(1,8,(2,2))\n_conv.weight.data.shape,_conv.bias.data.shape\n\n(torch.Size([8, 1, 2, 2]), torch.Size([8]))\n\n\n\n_conv(_X).shape\n\ntorch.Size([1, 8, 1, 1])\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n\ntorch.sum(_conv.weight.data[0,...])+_conv.bias.data[0],\\\ntorch.sum(_conv.weight.data[1,...])+_conv.bias.data[1]\n\n(tensor(-0.3464), tensor(0.2739))\n\n\n결국 아래를 계산한다는 의미\n\ntorch.sum(_conv.weight.data,axis=(2,3)).reshape(-1)+ _conv.bias.data\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345])\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n(잔소리) axis 사용 익숙하지 않으면 아래 꼭 들으세요..\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6주차)-4월11일.html , numpy공부 4단계: 축\n\n\n\nReLU (2d)\n\n_X = torch.randn(25).reshape(1,5,5)\n_X\n\ntensor([[[ 0.2656,  0.0780,  3.0465,  1.0151, -2.3908],\n         [ 0.4749,  1.6519,  1.5454,  1.0376,  0.9291],\n         [-0.7858,  0.4190,  2.6057, -0.4022,  0.2092],\n         [ 0.9594,  0.6408, -0.0411, -1.0720, -2.0659],\n         [-0.0996,  1.1351,  0.9758,  0.4952, -0.5475]]])\n\n\n\na1=torch.nn.ReLU()\n\n\na1(_X)\n\ntensor([[[0.2656, 0.0780, 3.0465, 1.0151, 0.0000],\n         [0.4749, 1.6519, 1.5454, 1.0376, 0.9291],\n         [0.0000, 0.4190, 2.6057, 0.0000, 0.2092],\n         [0.9594, 0.6408, 0.0000, 0.0000, 0.0000],\n         [0.0000, 1.1351, 0.9758, 0.4952, 0.0000]]])\n\n\n\n\nMaxpooling 레이어\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n\n\n_X = torch.arange(16).float().reshape(1,4,4) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]),\n tensor([[[ 5.,  7.],\n          [13., 15.]]]))\n\n\n가장 중요한 특징만 남게 될 것이다.\n\n_X = torch.arange(25).float().reshape(1,5,5) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.,  8.],\n          [16., 18.]]]))\n\n\n버려지는 데이터\n\n_X = torch.arange(36).float().reshape(1,6,6) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10., 11.],\n          [12., 13., 14., 15., 16., 17.],\n          [18., 19., 20., 21., 22., 23.],\n          [24., 25., 26., 27., 28., 29.],\n          [30., 31., 32., 33., 34., 35.]]]),\n tensor([[[ 7.,  9., 11.],\n          [19., 21., 23.],\n          [31., 33., 35.]]]))"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-구현-cpu",
    "href": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-구현-cpu",
    "title": "CNN (7주차)",
    "section": "이미지자료분석– CNN 구현 (CPU)",
    "text": "이미지자료분석– CNN 구현 (CPU)\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\n(1) Conv2d\n\nc1 = torch.nn.Conv2d(1,16,(5,5))\nprint(X.shape)\nprint(c1(X).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(2) ReLU\n\na1 = torch.nn.ReLU()\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(3) MaxPool2D\n\nm1 =  torch.nn.MaxPool2d((2,2)) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\n\n\n\n\n(4) 적당히 마무리하고 시그모이드 태우자\n- 펼치자.\n(방법1)\n\nm1(a1(c1(X))).reshape(-1,2304).shape\n\ntorch.Size([12665, 2304])\n\n\n\n16*12*12 \n\n2304\n\n\n(방법2)\n\nflttn = torch.nn.Flatten()\n\n\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\n\n\n- 2304 \\(\\to\\) 1 로 차원축소하는 선형레이어를 설계\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\n\n\n- 시그모이드\n\na2 = torch.nn.Sigmoid()\n\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\nprint(a1(l1(flttn(m1(a1(c1(X)))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\ntorch.Size([12665, 1])\n\n\n- 네트워크 설계\n\nnet = torch.nn.Sequential(\n    c1, # 2d: 컨볼루션(선형변환), 피처 뻥튀기 \n    a1, # 2d: 렐루(비선형변환)\n    m1, # 2d: 맥스풀링: 데이터요약\n    flttn, # 2d->1d \n    l1, # 1d: 선형변환\n    a2 # 1d: 시그모이드(비선형변환) \n)\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nt1= time.time()\nfor epoc in range(100): \n    ## 1\n    yhat = net(X) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4\n    optimizr.step()\n    optimizr.zero_grad()\nt2= time.time()\nt2-t1\n\n39.31594634056091\n\n\n\nplt.plot(y)\nplt.plot(net(X).data,'.')\nplt.title('Traning Set',size=15)\n\nText(0.5, 1.0, 'Traning Set')\n\n\n\n\n\n\nplt.plot(yy)\nplt.plot(net(XX).data,'.')\nplt.title('Test Set',size=15)\n\nText(0.5, 1.0, 'Test Set')"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-구현-gpu",
    "href": "posts/ml/2022-10-19-ml_7w.html#이미지자료분석-cnn-구현-gpu",
    "title": "CNN (7주차)",
    "section": "이미지자료분석– CNN 구현 (GPU)",
    "text": "이미지자료분석– CNN 구현 (GPU)\n\n1. dls\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\nlen(X)/10\n\n1266.5\n\n\n\nlen(XX)\n\n2115\n\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\n\n2. lrnr 생성: 아키텍처, 손실함수, 옵티마이저\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn)\n\n\n\n3. 학습\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.901239\n      0.605223\n      00:00\n    \n    \n      1\n      0.660227\n      0.370985\n      00:00\n    \n    \n      2\n      0.507106\n      0.213785\n      00:00\n    \n    \n      3\n      0.393017\n      0.113283\n      00:00\n    \n    \n      4\n      0.304846\n      0.065374\n      00:00\n    \n    \n      5\n      0.238648\n      0.042887\n      00:00\n    \n    \n      6\n      0.189261\n      0.031143\n      00:00\n    \n    \n      7\n      0.152003\n      0.024236\n      00:00\n    \n    \n      8\n      0.123435\n      0.019730\n      00:00\n    \n    \n      9\n      0.101176\n      0.016531\n      00:00\n    \n  \n\n\n\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\n\n4. 예측 및 시각화\n\nnet.to(\"cpu\") \n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n- 결과를 시각화하면 아래와 같다.\n\nplt.plot(net(X).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(XX).data,'.')\nplt.title(\"Test Set\",size=15)\n\nText(0.5, 1.0, 'Test Set')\n\n\n\n\n\n1/10만 사용했는데 잘 training된 것 같다\n- 빠르고 적합결과도 좋음\n\n\nLrnr 오브젝트\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nnet\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nid(lrnr.model), id(net)\n\n(140021490006720, 140021490006720)\n\n\n\nlrnr.model(X)\n\ntensor([[4.5555e-05],\n        [1.2910e-03],\n        [6.6828e-04],\n        ...,\n        [9.8670e-01],\n        [9.8576e-01],\n        [9.9344e-01]], grad_fn=<SigmoidBackward0>)\n\n\n\nnet(X)\n\ntensor([[4.5555e-05],\n        [1.2910e-03],\n        [6.6828e-04],\n        ...,\n        [9.8670e-01],\n        [9.8576e-01],\n        [9.9344e-01]], grad_fn=<SigmoidBackward0>)\n\n\n같은 결과\n20221026 수업\n\n\nBCEWithLogitsLoss\n- BCEWithLogitsLoss = Sigmoid + BCELoss - 왜 써요? 수치적으로 더 안정\ntorch.nn.BCEWithLogitsLoss - This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n- 사용방법\n\ndls 만들기\n\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\ntorch.utils.data.TensorDataset?\n\n\nInit signature: torch.utils.data.TensorDataset(*args, **kwds)\nDocstring:     \nDataset wrapping tensors.\nEach sample will be retrieved by indexing tensors along the first dimension.\nArgs:\n    *tensors (Tensor): tensors that have the same size of the first dimension.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/utils/data/dataset.py\nType:           type\nSubclasses:     \n\n\n\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\nlrnr생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    #torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCEWithLogitsLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.956781\n      0.642780\n      00:00\n    \n    \n      1\n      0.709626\n      0.419758\n      00:00\n    \n    \n      2\n      0.554641\n      0.248010\n      00:00\n    \n    \n      3\n      0.431661\n      0.118707\n      00:00\n    \n    \n      4\n      0.331514\n      0.059536\n      00:00\n    \n    \n      5\n      0.256312\n      0.035956\n      00:00\n    \n    \n      6\n      0.200917\n      0.025288\n      00:00\n    \n    \n      7\n      0.159611\n      0.019510\n      00:00\n    \n    \n      8\n      0.128254\n      0.015889\n      00:00\n    \n    \n      9\n      0.104057\n      0.013373\n      00:00\n    \n  \n\n\n\n\n예측 및 시각화\n\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n)\n\n\n시각화 위해서 cpu로 옮겨주기\n\nnet(X)\n\ntensor([[-9.4061],\n        [-6.7910],\n        [-7.9819],\n        ...,\n        [ 4.3685],\n        [ 4.4061],\n        [ 5.4793]], grad_fn=<AddmmBackward0>)\n\n\nsigmoid 취하기 전이지 우리는 bcewithlogiticsLoss 썼잖아, 그래서 0~1사이 아님\n\na2(torch.tensor(0))\n\ntensor(0.5000)\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).data,',',color=\"C1\")\nax[1].plot(y)\nax[1].plot(a2(net(X)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(a2(net(XX)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_1.html",
    "href": "posts/ml/2022-10-26-ml_8w_1.html",
    "title": "CNN (8주차) 1",
    "section": "",
    "text": "기계학습 특강 (8주차) 10월26일–(1) [이미지자료분석 - CNN 다중클래스 분류, fastai metric 사용]"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_1.html#imports",
    "href": "posts/ml/2022-10-26-ml_8w_1.html#imports",
    "title": "CNN (8주차) 1",
    "section": "imports",
    "text": "imports\n\nimport torch \nimport torchvision\nimport numpy as np\nfrom fastai.vision.all import * \n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');\n\n\n#hide\ngraphviz.set_jupyter_format('png')\n\n'svg'"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_1.html#cnn-다중클래스-분류",
    "href": "posts/ml/2022-10-26-ml_8w_1.html#cnn-다중클래스-분류",
    "title": "CNN (8주차) 1",
    "section": "CNN 다중클래스 분류",
    "text": "CNN 다중클래스 분류\n\n결론 (그냥 외우세요)\n- 2개의 class를 구분하는 문제가 아니라 \\(k\\)개의 class를 구분해야 한다면?\n일반적인 개념\n\n손실함수: BCE loss \\(\\to\\) Cross Entropy loss\n마지막층의 선형변환: torch.nn.Linear(?,1) \\(\\to\\) torch.nn.Linear(?,k)\n마지막층의 활성화: sig \\(\\to\\) softmax\n\n파이토치 한정 - y의형태: (n,) vector + int형 // (n,k) one-hot encoded vector + float형 - 손실함수: torch.nn.BCEWithLogitsLoss, \\(\\to\\) torch.nn.CrossEntropyLoss - 마지막층의 선형변환: torch.nn.Linear(?,1) \\(\\to\\) torch.nn.Linear(?,k) - 마지막층의 활성화: None \\(\\to\\) None (손실함수에 이미 마지막층의 활성화가 포함)\n\n\n실습: 3개의 클래스를 구분\n\npath = untar_data(URLs.MNIST)\n\ntraining set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n다중일때 int가 아닌float으로서 y를 정의해준 모습\ntest set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\ndls\n\n\nlen(X)\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr\n\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\n\nnet1(X).shape\n\ntorch.Size([18623, 2304])\n\n\n\nnet = torch.nn.Sequential(\n    net1,\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\nloss_fn = torch.nn.CrossEntropyLoss() \n\n\nlrnr = Learner(dls,net,loss_fn) \n\nadam기본인 learner\n\n학습\n\n지금은 epoch당 11번 도는 설정, 18623/1862 = 11.xx\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.532752\n      1.059955\n      00:00\n    \n    \n      1\n      1.190896\n      0.830852\n      00:00\n    \n    \n      2\n      1.008513\n      0.646931\n      00:00\n    \n    \n      3\n      0.865353\n      0.427843\n      00:00\n    \n    \n      4\n      0.728408\n      0.264087\n      00:00\n    \n    \n      5\n      0.602026\n      0.179980\n      00:00\n    \n    \n      6\n      0.497519\n      0.137681\n      00:00\n    \n    \n      7\n      0.415113\n      0.112264\n      00:00\n    \n    \n      8\n      0.349265\n      0.096033\n      00:00\n    \n    \n      9\n      0.296159\n      0.084770\n      00:00\n    \n  \n\n\n\n\n예측\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n  (1): Linear(in_features=2304, out_features=3, bias=True)\n)\n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      2.838031\n      -14.031689\n      -1.230620\n      0\n    \n    \n      1\n      -0.732540\n      -6.829875\n      -0.657546\n      0\n    \n    \n      2\n      2.525343\n      -7.813309\n      -2.658828\n      0\n    \n    \n      3\n      1.173236\n      -5.229916\n      -2.532024\n      0\n    \n    \n      4\n      0.102843\n      -3.444337\n      -1.044323\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3142\n      -2.697058\n      -3.533814\n      -0.154926\n      2\n    \n    \n      3143\n      -5.334007\n      -6.445426\n      2.196163\n      2\n    \n    \n      3144\n      -3.041989\n      -5.655945\n      1.335649\n      2\n    \n    \n      3145\n      -4.720510\n      -5.899189\n      1.208340\n      2\n    \n    \n      3146\n      -2.413806\n      -3.101650\n      0.852677\n      2\n    \n  \n\n3147 rows × 4 columns\n\n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      2.838031\n      -14.031689\n      -1.230620\n      0\n    \n    \n      1\n      -0.732540\n      -6.829875\n      -0.657546\n      0\n    \n    \n      2\n      2.525343\n      -7.813309\n      -2.658828\n      0\n    \n    \n      3\n      1.173236\n      -5.229916\n      -2.532024\n      0\n    \n    \n      4\n      0.102843\n      -3.444337\n      -1.044323\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      975\n      1.330218\n      -6.934738\n      -0.893682\n      0\n    \n    \n      976\n      3.073657\n      -11.082842\n      -3.012246\n      0\n    \n    \n      977\n      3.607128\n      -7.156256\n      -5.264734\n      0\n    \n    \n      978\n      1.993969\n      -7.487792\n      -2.306112\n      0\n    \n    \n      979\n      1.534865\n      -7.852367\n      -1.404178\n      0\n    \n  \n\n980 rows × 4 columns\n\n\n\n\n대체적으로 첫번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==1')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      980\n      -4.239265\n      2.068619\n      -1.274470\n      1\n    \n    \n      981\n      -4.559580\n      2.755761\n      -1.822832\n      1\n    \n    \n      982\n      -4.617976\n      1.838857\n      -0.515022\n      1\n    \n    \n      983\n      -4.119075\n      2.247138\n      -0.991911\n      1\n    \n    \n      984\n      -3.344346\n      1.100410\n      -1.496944\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2110\n      -4.141958\n      2.405002\n      -1.260467\n      1\n    \n    \n      2111\n      -4.405143\n      2.479209\n      -1.356262\n      1\n    \n    \n      2112\n      -3.695343\n      1.773260\n      -1.218412\n      1\n    \n    \n      2113\n      -3.986775\n      2.423826\n      -1.349702\n      1\n    \n    \n      2114\n      -4.925949\n      2.532830\n      -1.160674\n      1\n    \n  \n\n1135 rows × 4 columns\n\n\n\n\n대체적으로 두번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==2')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      2115\n      -4.723238\n      -3.105680\n      1.052694\n      2\n    \n    \n      2116\n      -2.576618\n      -7.337523\n      2.118495\n      2\n    \n    \n      2117\n      -3.796456\n      -6.393374\n      2.169248\n      2\n    \n    \n      2118\n      -3.276625\n      -2.622900\n      0.176427\n      2\n    \n    \n      2119\n      -4.627345\n      -5.335648\n      1.157538\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3142\n      -2.697058\n      -3.533814\n      -0.154926\n      2\n    \n    \n      3143\n      -5.334007\n      -6.445426\n      2.196163\n      2\n    \n    \n      3144\n      -3.041989\n      -5.655945\n      1.335649\n      2\n    \n    \n      3145\n      -4.720510\n      -5.899189\n      1.208340\n      2\n    \n    \n      3146\n      -2.413806\n      -3.101650\n      0.852677\n      2\n    \n  \n\n1032 rows × 4 columns\n\n\n\n\n대체적으로 세번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n- 예측하는방법? - 칼럼0의 숫자가 크다 -> y=0일 확률이 큼 - 칼럼1의 숫자가 크다 -> y=1일 확률이 큼 - 칼럼2의 숫자가 크다 -> y=2일 확률이 큼\n\n\n공부: Softmax\n- 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음.\n- torch.nn.Softmax() 손계산\n(예시1) – 잘못계산\n\ntorch.nn.Softmax?\n\n\nInit signature: torch.nn.Softmax(dim: Union[int, NoneType] = None) -> None\nDocstring:     \nApplies the Softmax function to an n-dimensional input Tensor\nrescaling them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\nSoftmax is defined as:\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\nWhen the input Tensor is a sparse tensor then the unspecifed\nvalues are treated as ``-inf``.\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\nArgs:\n    dim (int): A dimension along which Softmax will be computed (so every slice\n        along dim will sum to 1).\n.. note::\n    This module doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use `LogSoftmax` instead (it's faster and has better numerical properties).\nExamples::\n    >>> m = nn.Softmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/nn/modules/activation.py\nType:           type\nSubclasses:     \n\n\n\n\n\nsftmax = torch.nn.Softmax(dim=0) # columns\n\n\n_netout = torch.tensor([[-2.0,-2.0,0.0],\n                        [3.14,3.14,3.14],\n                        [0.0,0.0,2.0],\n                        [2.0,2.0,4.0],\n                        [0.0,0.0,0.0]])\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout) \n\ntensor([[0.0041, 0.0041, 0.0115],\n        [0.7081, 0.7081, 0.2653],\n        [0.0306, 0.0306, 0.0848],\n        [0.2265, 0.2265, 0.6269],\n        [0.0306, 0.0306, 0.0115]])\n\n\n(예시2) – 이게 맞게 계산되는 것임\n\nsftmax = torch.nn.Softmax(dim=1) # rows\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout)\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시3) – 차원을 명시안하면 맞게 계산해주고 경고 줌\n\nsftmax = torch.nn.Softmax()\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout)\n\n/tmp/ipykernel_2380807/3715462293.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  sftmax(_netout)\n\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시4) – 진짜 손계산\n\n_netout \n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\ntorch.exp(_netout)\n\ntensor([[ 0.1353,  0.1353,  1.0000],\n        [23.1039, 23.1039, 23.1039],\n        [ 1.0000,  1.0000,  7.3891],\n        [ 7.3891,  7.3891, 54.5981],\n        [ 1.0000,  1.0000,  1.0000]])\n\n\n\n0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs\n\n(0.10648512513773022, 0.10648512513773022, 0.7870297497245397)\n\n\n\nnp.exp(_netout[1])/np.exp(_netout[1]).sum() # 두번째 obs \n\ntensor([0.3333, 0.3333, 0.3333])\n\n\n\nnp.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(),1,_netout)\n\narray([[0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334],\n       [0.10650699, 0.10650699, 0.78698605],\n       [0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334]], dtype=float32)\n\n\n위에서 1은 축방향을 의미\n\n\n공부: CrossEntropyLoss\n\n# torch.nn.CrossEntropyLoss() 손계산: one-hot version\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y_onehot = torch.tensor([[0,0,1],\n                          [0,1,0],\n                          [0,0,1],\n                          [0,0,1],\n                          [1,0,0]])*1.0\n_y_onehot\n\ntensor([[0., 0., 1.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 1.],\n        [1., 0., 0.]])\n\n\n위에서 꼭 1.0 곱해줌으로써 int가 아닌 float으로 만들어주기\n\nsftmax = torch.nn.Softmax(dim=1) \nsftmax(_netout), _y_onehot\n\n(tensor([[0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333],\n         [0.1065, 0.1065, 0.7870],\n         [0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333]]),\n tensor([[0., 0., 1.],\n         [0., 1., 0.],\n         [0., 0., 1.],\n         [0., 0., 1.],\n         [1., 0., 0.]]))\n\n\n- 계산결과\n\nloss_fn(_netout,_y_onehot)\n\ntensor(0.5832)\n\n\n\n- torch.sum(torch.log(sftmax(_netout)) * _y_onehot)/5 \n\ntensor(0.5832)\n\n\n- 계산하는 방법도 중요한데 torch.nn.CrossEntropyLoss() 에는 softmax 활성화함수가 이미 포함되어 있다는 것을 확인하는 것이 더 중요함.\n- 따라서 torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다.\n\n\n# torch.nn.CrossEntropyLoss() 손계산: lenght \\(n\\) vertor version\n\n_netout \n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y = torch.tensor([2,1,2,2,0])\n\n원핫인코딩 안하면 int로 만든 다음에 넣기, float은 또 계산되지 않음!\n\nloss_fn(_netout,_y)\n\ntensor(0.5832)\n\n\n\n\n\n실습: \\(k=2\\)로 두면 이진분류도 가능\n- download data\n\npath = untar_data(URLs.MNIST) \n\ntraining\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\ny_onehot = torch.nn.functional.one_hot(y).float()\n#y_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],y))).float()\n\nfloat만들어주기 원핫인코딩이기\ntest\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\nyy_onehot = torch.nn.functional.one_hot(yy).float()\n#yy_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],yy))).float()\n\n\ndls\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot) \nds2 = torch.utils.data.TensorDataset(XX,yy_onehot) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n\n학습\n\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.233556\n      0.787265\n      00:00\n    \n    \n      1\n      0.829398\n      0.433228\n      00:00\n    \n    \n      2\n      0.650216\n      0.319202\n      00:00\n    \n    \n      3\n      0.540207\n      0.183107\n      00:00\n    \n    \n      4\n      0.444210\n      0.113277\n      00:00\n    \n    \n      5\n      0.365939\n      0.074700\n      00:00\n    \n    \n      6\n      0.303410\n      0.049914\n      00:00\n    \n    \n      7\n      0.253710\n      0.035714\n      00:00\n    \n    \n      8\n      0.214157\n      0.027470\n      00:00\n    \n    \n      9\n      0.182333\n      0.022121\n      00:00\n    \n  \n\n\n\n\n예측 및 시각화\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=2, bias=True)\n)\n\n\n\nsftmax = torch.nn.Softmax(dim=1) \nsig = torch.nn.Sigmoid()\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).diff(axis=1).data,',',color=\"C1\") # u2-u1\nax[1].plot(y)\nax[1].plot(sftmax(net(X))[:,1].data,',')\n#ax[1].plot(sig(net(X).diff(axis=1)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).diff(axis=1).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(sftmax(net(XX))[:,1].data,',')\n#ax[1].plot(sig(net(XX).diff(axis=1)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')\n\n\n\n\n\n- note: softmax(u1,u2)=[sig(u1-u2), sig(u2-u1)]=[1-sig(u2-u1),sig(u2-u1)]\n\\(\\frac{1}{e^{u_1}+e^{u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+e^{u_2-u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+1} \\to sig(u_2-u_1)\\)\n\n\n공부: 이진분류에서 소프트맥스 vs 시그모이드\n- 이진분류문제 = “y=0 or y=1” 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제\n- softmax, sigmoid - softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. - sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다.\n- 그런데 “실패확률=1-성공확률” 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까)\n- 아래는 사실상 같은 모형이다.\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y1\"\n    \"node2\" -> \"y1\"\n    \"...\" -> \"y1\"\n    \"node2304\" -> \"y1\"\n    \n    \"node1\" -> \"y2\"\n    \"node2\" -> \"y2\"\n    \"...\" -> \"y2\"\n    \"node2304\" -> \"y2\"    \n    label = \"Layer: Softmax\"\n}\n''')\n\n\n\n\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node2304\" -> \"y\"\n    label = \"Layer: Sigmoid\"\n}\n''')\n\n\n\n\n- 둘은 사실상 같은 효과를 주는 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. \\(\\to\\) sigmoid를 사용하는 모형이 비용은 싸고(학습할 파라메터가 적음) 효과는 동일하다는 말 \\(\\to\\) 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. - softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음\n\n\n소프트맥스 vs 시그모이드 정리\n- 결론 1. 소프트맥스는 시그모이드의 확장이다. 2. 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다.\n- 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (흑백이미지를 칼라잉크로 출력하는 느낌)\n참고\n\n\n\n\\(y\\)\n분포가정\n마지막층의 활성화함수\n손실함수\n\n\n\n\n3.45, 4.43, … (연속형)\n정규분포\nNone (or Identity)\nMSE\n\n\n0 or 1\n이항분포 with \\(n=1\\) (=베르누이)\nSigmoid\nBCE\n\n\n[0,0,1], [0,1,0], [1,0,0]\n다항분포 with \\(n=1\\)\nSoftmax\nCross Entropy"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_8w_1.html#fastai-metric-사용",
    "href": "posts/ml/2022-10-26-ml_8w_1.html#fastai-metric-사용",
    "title": "CNN (8주차) 1",
    "section": "fastai metric 사용",
    "text": "fastai metric 사용\n\n데이터준비\n- download data\n\npath = untar_data(URLs.MNIST)\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))\n\n\n\n\n사용자정의 메트릭이용\n\ndls 만들기\n\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \n\n\ndef acc(yhat,y) : \n    return ((yhat>0.5)==y).float().mean()\n\n\ndef err(yhat,y):\n    return 1-((yhat>0.5)==y).float().mean()\n\n\nlrnr = Learner(dls,net,loss_fn,metrics=[acc,err])\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      acc\n      err\n      time\n    \n  \n  \n    \n      0\n      1.012566\n      0.676096\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.738655\n      0.477148\n      0.994799\n      0.005201\n      00:00\n    \n    \n      2\n      0.603908\n      0.335415\n      0.985816\n      0.014184\n      00:00\n    \n    \n      3\n      0.497049\n      0.183633\n      0.995745\n      0.004255\n      00:00\n    \n    \n      4\n      0.394664\n      0.097668\n      0.995745\n      0.004255\n      00:00\n    \n    \n      5\n      0.309929\n      0.056333\n      0.995745\n      0.004255\n      00:00\n    \n    \n      6\n      0.244836\n      0.037147\n      0.995745\n      0.004255\n      00:00\n    \n    \n      7\n      0.195441\n      0.027278\n      0.995745\n      0.004255\n      00:00\n    \n    \n      8\n      0.157570\n      0.021531\n      0.995745\n      0.004255\n      00:00\n    \n    \n      9\n      0.128163\n      0.017795\n      0.997163\n      0.002837\n      00:00\n    \n  \n\n\n\n\n예측\n\n\n생략\n\n\n\nfastai지원 메트릭이용– 잘못된사용\n\ndls 만들기\n\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\naccuracy??\n\n\nSignature: accuracy(inp, targ, axis=-1)\nSource:   \ndef accuracy(inp, targ, axis=-1):\n    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n    pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n    return (pred == targ).float().mean()\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\n\nerror_rate??\n\n\nSignature: error_rate(inp, targ, axis=-1)\nSource:   \ndef error_rate(inp, targ, axis=-1):\n    \"1 - `accuracy`\"\n    return 1 - accuracy(inp, targ, axis=axis)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.971997\n      0.616424\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.671642\n      0.380434\n      0.463357\n      0.536643\n      00:00\n    \n    \n      2\n      0.525948\n      0.232161\n      0.463357\n      0.536643\n      00:00\n    \n    \n      3\n      0.414203\n      0.123899\n      0.463357\n      0.536643\n      00:00\n    \n    \n      4\n      0.322394\n      0.071857\n      0.463357\n      0.536643\n      00:00\n    \n    \n      5\n      0.252299\n      0.045784\n      0.463357\n      0.536643\n      00:00\n    \n    \n      6\n      0.199783\n      0.032276\n      0.463357\n      0.536643\n      00:00\n    \n    \n      7\n      0.160118\n      0.024500\n      0.463357\n      0.536643\n      00:00\n    \n    \n      8\n      0.129659\n      0.019576\n      0.463357\n      0.536643\n      00:00\n    \n    \n      9\n      0.105914\n      0.016207\n      0.463357\n      0.536643\n      00:00\n    \n  \n\n\n\n\n이상하다..?\n\n\n예측\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nplt.plot(yy)\nplt.plot(lrnr.model(XX).data,'.')\n\n\n\n\n\n맞추는건 잘 맞추는데?\n\n\n\nfastai지원 메트릭이용– 올바른 사용(1)\n- 가정 - X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. - y의 형태는 (n,) 벡터이다. 즉 \\(n\\times 1\\) 이 아니라 그냥 길이가 \\(n\\)인 벡터로 가정한다. - y의 각 원소는 0,1,2,3,… 와 같이 카테고리를 의미하는 숫자이어야 하며 이 숫자는 int형으로 저장되어야 한다. - loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\ndls 만들기\n\n지원하는 함수로 바꿔주기\n\ny.to(torch.int64).reshape(-1),yy.to(torch.int64).reshape(-1)\n\n(tensor([0, 0, 0,  ..., 1, 1, 1]), tensor([0, 0, 0,  ..., 1, 1, 1]))\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.038122\n      0.539247\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.621439\n      0.261176\n      0.977778\n      0.022222\n      00:00\n    \n    \n      2\n      0.451623\n      0.118811\n      0.989125\n      0.010875\n      00:00\n    \n    \n      3\n      0.333172\n      0.059299\n      0.995272\n      0.004728\n      00:00\n    \n    \n      4\n      0.250918\n      0.037678\n      0.996217\n      0.003783\n      00:00\n    \n    \n      5\n      0.193416\n      0.026810\n      0.996217\n      0.003783\n      00:00\n    \n    \n      6\n      0.152078\n      0.020631\n      0.996217\n      0.003783\n      00:00\n    \n    \n      7\n      0.121511\n      0.016605\n      0.996690\n      0.003310\n      00:00\n    \n    \n      8\n      0.098301\n      0.013718\n      0.997636\n      0.002364\n      00:00\n    \n    \n      9\n      0.080287\n      0.011546\n      0.998109\n      0.001891\n      00:00\n    \n  \n\n\n\n\n\nfastai지원 메트릭이용– 올바른 사용(2)\n- 가정 - X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. - y의 형태는 (n,클래스의수)로 가정한다. 즉 y가 one_hot 인코딩된 형태로 가정한다. - y의 각 원소는 0 혹은 1이다. - loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\ndls 만들기\n\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n# y_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\n# yy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\naccuracy_multi\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      1.038750\n      0.569555\n      0.463357\n      00:00\n    \n    \n      1\n      0.640057\n      0.285553\n      0.977778\n      00:00\n    \n    \n      2\n      0.469265\n      0.137582\n      0.987943\n      00:00\n    \n    \n      3\n      0.348698\n      0.064898\n      0.995035\n      00:00\n    \n    \n      4\n      0.262547\n      0.038338\n      0.996217\n      00:00\n    \n    \n      5\n      0.201805\n      0.025988\n      0.996690\n      00:00\n    \n    \n      6\n      0.158089\n      0.019443\n      0.996927\n      00:00\n    \n    \n      7\n      0.125811\n      0.015470\n      0.997163\n      00:00\n    \n    \n      8\n      0.101381\n      0.012772\n      0.998109\n      00:00\n    \n    \n      9\n      0.082515\n      0.010802\n      0.998582\n      00:00"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html",
    "href": "posts/ml/2022-10-05-ml-HW.html",
    "title": "Homework",
    "section": "",
    "text": "기계학습 특강 (6주차) 10월5일 Homework"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html#imports",
    "href": "posts/ml/2022-10-05-ml-HW.html#imports",
    "title": "Homework",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport torchvision\nfrom fastai.data.all import *\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html#숙제-해설-및-풀이는-여기참고",
    "href": "posts/ml/2022-10-05-ml-HW.html#숙제-해설-및-풀이는-여기참고",
    "title": "Homework",
    "section": "숙제 (해설 및 풀이는 여기참고)",
    "text": "숙제 (해설 및 풀이는 여기참고)\n\n숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라\n\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,64)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,64)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n위에서 \\(a_1\\)은 relu를, \\(a_2\\)는 sigmoid를 의미한다.\n\n“y=0”은 숫자0을 의미하도록 하고 “y=1”은 숫자1을 의미하도록 설정하라.\n\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\n\n\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\n\n\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\n\n\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28).float()\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라.\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\nyhat.data\n\ntensor([[nan],\n        [nan],\n        [nan],\n        ...,\n        [nan],\n        [nan],\n        [nan]])\n\n\n학습이 잘 되지 않았다.\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!)\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nX = X/255\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr=torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래와 같은 수식을 이용하여 accuracy를 계산하라.\n\n\\(\\text{accuracy}=\\frac{1}{n}\\sum_{i=1}^n I(\\tilde{y}_i=y_i)\\) - \\(\\tilde{y}_i = \\begin{cases}  1 & \\hat{y}_i > 0.5 \\\\  0 & \\hat{y}_i \\leq 0.5 \\end{cases}\\) - \\(I(\\tilde{y}_i=y_i) = \\begin{cases} 1 & \\tilde{y}_i=y_i \\\\ 0 & \\tilde{y}_i \\neq y_i \\end{cases}\\)\n단, \\(n\\)은 0과 1을 의미하는 이미지의 수\n\nytilde = (yhat > 0.5) * 1\n\n\nytilde\n\ntensor([[0],\n        [0],\n        [0],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\n(ytilde == y) * 1\n\ntensor([[1],\n        [1],\n        [1],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\ntorch.sum((ytilde == y) * 1)\n\ntensor(12661)\n\n\n\ntorch.sum((ytilde == y) * 1)/len(y)\n\ntensor(0.9997)\n\n\n\nprint(\"accuraccy: \",torch.sum((ytilde == y) * 1)/len(y))\n\naccuraccy:  tensor(0.9997)"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-midterm.html",
    "href": "posts/ml/2022-11-02-ml-midterm.html",
    "title": "Midterm",
    "section": "",
    "text": "중간고사 대체과제"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-midterm.html#크롤링을-통한-이미지-분석-및-cam",
    "href": "posts/ml/2022-11-02-ml-midterm.html#크롤링을-통한-이미지-분석-및-cam",
    "title": "Midterm",
    "section": "1. 크롤링을 통한 이미지 분석 및 CAM",
    "text": "1. 크롤링을 통한 이미지 분석 및 CAM\n(1) 두 가지 키워드로 크롤링을 수행하여 이미지자료를 모아라. (키워드는 각자 마음에 드는 것으로 설정할 것)\n힌트1: hynn, iu 라는 키워드로 크롤링하여 이미지자료를 모으는 코드\n\n#\n# 크롤링에 필요한 준비작업들\n#!pip install -Uqq duckduckgo_search\nfrom duckduckgo_search import ddg_images\nfrom fastdownload import download_url\nfrom fastcore.all import *\ndef search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n# \n# 폴더만드는코드 -- 사실 손으로 만들어도 무방함.. \n!mkdir images\n!mkdir images/train\n!mkdir images/test \n!mkdir images/train/iu\n!mkdir images/train/hynn\n!mkdir images/test/iu\n!mkdir images/test/hynn\ndownload_images(dest='./images/train/iu',urls=search_images('iu',max_images=200)) # iu 라는 키워드로 200개 이미지 크롤링 -> ./images/train/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/hynn',urls=search_images('hynn',max_images=200)) # hynn 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/hynn 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/iu',urls=search_images('iu kpop',max_images=200))  # iu kpop 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/hynn',urls=search_images('hynn kpop',max_images=200)) # hynn kpop 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/hynn 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/iu',urls=search_images('iu photo',max_images=50)) # iu photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/hynn',urls=search_images('hynn photo',max_images=50)) # hynn photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/hynn 에 저장 \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n힌트2: 불량이미지 삭제\nbad_images = verify_images(get_image_files('./images'))\nbad_images\n\n불량이미지 목록\n\nbad_images.map(Path.unlink)\n\n불량이미지는 dls를 불러올때 방해되므로 제거\n\n\n!mkdir images/train/bezos\n!mkdir images/train/musk\n!mkdir images/test/bezos\n!mkdir images/test/musk\n\n\ndownload_images(dest='./images/train/bezos',urls=search_images('jeff bezos',max_images=200))\ntime.sleep(10)\ndownload_images(dest='./images/train/musk',urls=search_images('elon musk',max_images=200)) \ntime.sleep(10)\ndownload_images(dest='./images/train/bezos',urls=search_images('jeff bezos rich',max_images=200))  \ntime.sleep(10) \ndownload_images(dest='./images/test/musk',urls=search_images('elon musk rich',max_images=200)) \ntime.sleep(10)\n\ndownload_images(dest='./images/test/bezos',urls=search_images('jeff bezos photo',max_images=50)) # iu photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/musk',urls=search_images('elon usk photo',max_images=50)) # hynn photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/hynn 에 저장 \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n\n\nbad_images = verify_images(get_image_files('./images'))\nbad_images\n\n(#25) [Path('images/train/bezos/1d39746b-ad4d-49ac-ac25-0f569a94e3f2.jpg'),Path('images/train/bezos/ee365bea-bb15-4b79-9ebc-350d5a823ca8.JPG'),Path('images/train/bezos/289cbafa-0fbc-4be9-a39f-68429c876095.jpg'),Path('images/train/bezos/21495a5a-9602-42f5-8f7b-4978856bd1e8.jpg'),Path('images/train/bezos/3bd6c738-5361-4f95-9b3b-643904f5a135.jpg'),Path('images/train/bezos/2ca0722d-cb88-4f18-bc5f-c410bd16243f.jpg'),Path('images/train/bezos/e9924f91-aac6-4a11-a235-dacb7f80d576.jpg'),Path('images/train/bezos/fcc25b56-8807-4e52-bc37-6e1718994d8a.jpg'),Path('images/train/bezos/07e554ae-1261-4c81-b908-7c8d94e94702.jpg'),Path('images/train/bezos/ea6592bc-eff0-4db6-b93c-3b6e85897877.jpg')...]\n\n\n\nbad_images.map(Path.unlink)\n\n(#25) [None,None,None,None,None,None,None,None,None,None...]\n\n\n(2) ImageDataLoaders.from_folder 를 이용하여 dls를 만들어라.\n힌트1: dls를 만드는 코드\ndls = ImageDataLoaders.from_folder(path = './images', train='train',valid='test',item_tfms=Resize(512),bs=8) \ndls.show_batch()\n\ndls = ImageDataLoaders.from_folder(path = './images', train='train',valid='test',item_tfms=Resize(512),bs=8) \n\n\ndls.show_batch()\n\n\n\n\n(3) resnet34를 이용하여 학습하라.\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.623527\n      0.999150\n      0.695341\n      00:07\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.350595\n      1.298542\n      0.695341\n      00:09\n    \n    \n      1\n      0.228978\n      1.561831\n      0.759857\n      00:08\n    \n    \n      2\n      0.185354\n      1.211755\n      0.806452\n      00:08\n    \n    \n      3\n      0.210802\n      1.586954\n      0.731183\n      00:08\n    \n    \n      4\n      0.210048\n      0.952422\n      0.820789\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n(4) CAM (class activation mapping)을 이용하여 (3)의 모형의 판단근거를 시각화하라.\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nprint(net1(_X).shape)\nprint(net2[0](net1(_X)).shape)\nprint(net2[1](net2[0](net1(_X))).shape)\nprint(net2[2](net2[1](net2[0](net1(_X)))).shape)\n\ntorch.Size([8, 512, 16, 16])\ntorch.Size([8, 1024, 1, 1])\ntorch.Size([8, 1024])\ntorch.Size([8, 1024])\n\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -> (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -> (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -> (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy) \n\n\nlrnr2.loss_func, lrnr.loss_func\n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.473621\n      7.780233\n      0.831541\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.358073\n      1.187476\n      0.767025\n      00:08\n    \n    \n      1\n      0.378113\n      3.513460\n      0.236559\n      00:09\n    \n    \n      2\n      0.410308\n      0.585618\n      0.713262\n      00:08\n    \n    \n      3\n      0.339331\n      0.728930\n      0.713262\n      00:08\n    \n    \n      4\n      0.250342\n      0.674079\n      0.745520\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\npath = './images'\n\n\nfig, ax = plt.subplots(5,5) \nk=200\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_bezos = why[0,0,:,:] \n        why_musk = why[0,1,:,:] \n        bezosprob, muskprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if bezosprob>muskprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_bezos.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"bezos(%2f)\" % bezosprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_musk.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"musk(%2f)\" % muskprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-midterm.html#overparameterized-model",
    "href": "posts/ml/2022-11-02-ml-midterm.html#overparameterized-model",
    "title": "Midterm",
    "section": "2. Overparameterized Model",
    "text": "2. Overparameterized Model\n(풀이 있음)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n(1) 아래의 모형을 가정하고 \\(\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(x) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[6.2839]]), tensor([3.1322]))\n\n\n(2) 아래의 모형을 가정하고 \\(\\beta_0\\)를 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_0 + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nW0hat = torch.tensor([0.0], requires_grad=True)\n\n\nW0hat\n\ntensor([0.], requires_grad=True)\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,(0*x+W0hat).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = 0 * x + W0hat\n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    W0hat.data = W0hat.data - 0.1*W0hat.grad\n    W0hat.grad = None\n\n\nplt.plot(x,y,'.')\nplt.plot(x,(0*x+W0hat).data,'--')\n\n\n\n\n\nW0hat\n\ntensor([3.1188], requires_grad=True)\n\n\n(3) 아래의 모형을 가정하고 \\(\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nnet = torch.nn.Linear(1,1,bias = False)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nfor epoc in range(100):\n    yhat = net(x) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet.weight.data\n\ntensor([[6.2637]])\n\n\n(4) 아래의 모형을 가정하고 \\(\\alpha_0,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\\(\\hat{\\alpha}_0+\\hat{\\beta}_0\\)은 얼마인가? 이 값과 문제 (1)에서 추정된 \\(\\hat{\\beta_0}\\)의 값과 비교하여 보라.\n\n_1= torch.ones([1000,1])\nX = torch.concat([_1,x],axis=1)\n\n\nnet = torch.nn.Linear(in_features=2,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(X) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[1.5093, 6.2918]]), tensor([1.6958]))\n\n\n\n1.7377 + 1.4358\n\n3.1734999999999998\n\n\n\n6.2363\n\n6.2363\n\n\n(5) 아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라. – 이거 제가 힌트를 잘못줬어요.. 문제가 좀 어렵게나왔네요 ㅠㅠ\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\\(\\hat{\\alpha}_0+\\hat{\\beta}_0\\), \\(\\hat{\\alpha}_1 + \\hat{\\beta}_1\\)의 값은 각각 얼마인가? 이 값들을 (1) 에서 추정된 \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) 값들과 비교하라.\n\n_1= torch.ones([1000,1])\nX = torch.concat([_1,x,x],axis=1)\n\n\nnet = torch.nn.Linear(in_features=3,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(X) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[1.2913, 3.4037, 2.8956]]), tensor([1.9138]))\n\n\n\n1.9599 + 1.2138\n\n3.1737\n\n\n\n3.2835 + 2.9585\n\n6.242\n\n\n(6) 다음은 위의 모형에 대하여 학생들이 discussion한 결과이다. 올바르게 해석한 학생을 모두 골라라.\n민정: \\((x_i,y_i)\\)의 산점도는 직선모양이고 직선의 절펴과 기울기 모두 유의미해 보이므로 \\(y_i = \\beta_0 + \\beta_1 x_i\\) 꼴을 적합하는게 좋겠다.\n슬기: 나도 그렇게 생각해. 그래서 (2)-(3)과 같이 기울기를 제외하고 적합하거나 절편을 제외하고 적합하면 underfitting의 상황에 빠질 수 있어.\n성재: (2)의 경우 사실상 \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i\\)를 추정하는 것과 같아지게 되지.\n세민: (4)의 경우 \\({\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_n \\end{bmatrix}\\) 와 같이 설정하고 네트워크를 아래와 같이 설정할 경우 얻어지는 모형이야.\nnet = torch.nn.Linear(in_features=2,out_features=1,bias=True)\n구환: 모델 (4)-(5)는 표현력은 (1)과 동일하지만 추정할 파라메터는 (1)보다 많으므로 효율적인 모델이라고 볼 수 없어.\nanswer : 민정, 슬기, 세민, 구환\n이 문제의 경우 풀이를 여기에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-midterm.html#차원축소기법과-표현학습",
    "href": "posts/ml/2022-11-02-ml-midterm.html#차원축소기법과-표현학습",
    "title": "Midterm",
    "section": "3. 차원축소기법과 표현학습",
    "text": "3. 차원축소기법과 표현학습\n다음은 아이리스데이터를 불러오는 코드이다. (아이리스 데이터에 대한 자세한 설명은 생략한다. 잘 모르는 학생은 구글검색을 해볼 것)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/STML2022/master/_notebooks/iris.csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      Sepal Length\n      Sepal Width\n      Petal Length\n      Petal Width\n      Species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\n\nX = torch.tensor(df.drop(columns=['Species']).to_numpy(), dtype=torch.float32)\n\n(1) 아래를 만족하도록 적당한 아키텍처, 손실함수를 설계하라. (손실함수는 MSE를 이용)\n\n\\(\\underset{(150,4)}{\\bf X} \\overset{l_1}{\\to} \\underset{(150,2)}{\\bf Z} \\overset{l_2}{\\to} \\underset{(150,4)}{\\bf \\hat X}\\)\n\\({\\bf \\hat X} \\approx {\\bf X}\\)\n\n차원축소\n\nX[:5]\n\ntensor([[5.1000, 3.5000, 1.4000, 0.2000],\n        [4.9000, 3.0000, 1.4000, 0.2000],\n        [4.7000, 3.2000, 1.3000, 0.2000],\n        [4.6000, 3.1000, 1.5000, 0.2000],\n        [5.0000, 3.6000, 1.4000, 0.2000]])\n\n\n\nX.shape\n\ntorch.Size([150, 4])\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(4,2,bias=False),\n    torch.nn.Linear(2,4,bias = False)\n)\n\n\nZ = net[0](X)\n\n\nZ.type()\n\n'torch.FloatTensor'\n\n\n\nZ[:5]\n\ntensor([[-0.8808, -1.5322],\n        [-0.8771, -1.5868],\n        [-0.8091, -1.4178],\n        [-0.8198, -1.4608],\n        [-0.8528, -1.4621]], grad_fn=<SliceBackward0>)\n\n\n\nXhat = net[1](net[0](X))\n\n\nXhat.type()\n\n'torch.FloatTensor'\n\n\n\nXhat[:5]\n\ntensor([[-0.6900, -1.1176, -0.2331, -0.6170],\n        [-0.6969, -1.1495, -0.2264, -0.6290],\n        [-0.6355, -1.0328, -0.2131, -0.5692],\n        [-0.6478, -1.0610, -0.2137, -0.5826],\n        [-0.6646, -1.0692, -0.2276, -0.5922]], grad_fn=<SliceBackward0>)\n\n\n(2) 아래코드를 이용하여 \\({\\bf X}\\), \\({\\bf Z}\\), \\({\\bf \\hat{X}}\\)를 시각화 하라.\n(시각화예시)\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z,Xhat],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // before learning',size=25);\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // before learning',size=25);\n\n\n\n\n(3) 네트워크를 학습시키고 \\({\\bf X}, {\\bf Z}, {\\bf \\hat{X}}\\)를 시각화하라.\n(시각화예시)\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z,Xhat],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // after learning',size=25);\n\noptimizr= torch.optim.Adam(net.parameters())\n\n\nloss_fn= torch.nn.MSELoss()\n\n\nfor epoc in range(2000): \n    ## 1 \n    Z = net[0](X) \n    Xhat = net[1](Z) \n    ## 2 \n    loss=loss_fn(Xhat,X) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // after learning',size=25);\n\n\n\n\n(4) (3)의 결과로 학습된 \\(Z\\)를 입력벡터로 하고 \\(Z \\to y=\\text{Species}\\) 로 향하는 적당한 네트워크를 설계한 뒤 학습하라.\nx->y가는 mapping 안 찾아도 - z->y 가는 mapping 적절히 잘 찾으면 - x->y 적용가능한 linear function 찾기 가능\n\nZ.shape\n\ntorch.Size([150, 2])\n\n\n\nnet = torch.nn.Linear(2,4)\n\n\nloss_fn= torch.nn.CrossEntropyLoss()\n\n\noptimizr= torch.optim.Adam(net.parameters())\n\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \n\n\nmapping = {'setosa':0,'versicolor':1,'virginica':2}\n\n\ny_base = list(df['Species'])\n\n\ny = torch.tensor(f(y_base,mapping))\n\n\ny.unique()\n\ntensor([0, 1, 2])\n\n\n\nfor epoc in range(2000): \n    ## 1 \n    yhat = net(Z.data)\n    ## 2 \n    loss=loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nnet.weight\n\nParameter containing:\ntensor([[ 0.1512,  1.0270],\n        [ 0.1297, -1.1137],\n        [ 0.6069, -1.1514],\n        [-0.5099, -0.4560]], requires_grad=True)\n\n\n(5) (1)~(4)의 결과를 토의한 내용이다. 적절하게 토의한 사람을 모두 고르라.\n규빈: \\({\\bf Z}\\)는 \\({\\bf X}\\)보다 적은 feature를 가지고 있다. 또한 적절한 선형변환을 하면 \\({\\bf X}\\)와 비슷한 \\({\\bf \\hat X}\\)을 만들 수 있으므로 \\({\\bf X}\\)의 정보량 대부분 유지한채로 효과적으로 차원을 줄인 방법이라 볼 수 있다.\n민정: 즉 \\({\\bf X}\\)에서 \\({\\bf y}\\)로 가는 맵핑을 학습하는 과업은 \\({\\bf Z}\\)에서 \\({\\bf y}\\)로 가는 맵핑을 학습하는 과업은 거의 동등하다고 볼 수 있다.\n성재: \\({\\bf Z}\\)의 차원을 (n,4)로 설정한다면 이론상 \\({\\bf X}\\)와 동일한 \\({\\bf \\hat X}\\)을 만들어 낼 수 있다.\n슬기: \\({\\bf Z}\\)의 차원이 (n,2)일지라도 경우에 따라서 \\({\\bf X}\\)와 동일한 \\({\\bf \\hat X}\\)을 만들어 낼 수 있다.\nanswer : 규빈, 민정, 성재, 슬기"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_9w.html",
    "href": "posts/ml/2022-11-02-ml_9w.html",
    "title": "RNN (9주차)",
    "section": "",
    "text": "기계학습 특강 (9주차) 11월02일 [순환신경망– ab예제, embedding layer]"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_9w.html#import",
    "href": "posts/ml/2022-11-02-ml_9w.html#import",
    "title": "RNN (9주차)",
    "section": "import",
    "text": "import\n\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_9w.html#define-some-funtions",
    "href": "posts/ml/2022-11-02-ml_9w.html#define-some-funtions",
    "title": "RNN (9주차)",
    "section": "Define some funtions",
    "text": "Define some funtions\n- 활성화함수들\n\nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\nplt.title(\"tanh(x)\", size=15)\n\nText(0.5, 1.0, 'tanh(x)')\n\n\n\n\n\nhyperblic tangent(https://en.wikipedia.org/wiki/Hyperbolic_functions) - sigmoid(범위가0 ~ 1)와 차이점(범위가 -1 ~ 1)\n- 문자열 -> 숫자로 바꾸는 함수\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \n\n(사용예시1)\n\ntxt = ['a','b','a']\nmapping = {'a':33,'b':-22}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [33, -22, 33]\n\n\n(사용예시2)\n\ntxt = ['a','b','a']\nmapping = {'a':[1,0],'b':[0,1]}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [[1, 0], [0, 1], [1, 0]]"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_9w.html#exam1-ab",
    "href": "posts/ml/2022-11-02-ml_9w.html#exam1-ab",
    "title": "RNN (9주차)",
    "section": "Exam1: ab",
    "text": "Exam1: ab\n\ndata\n\ntxt = list('ab')*100\ntxt[:10]\n\n['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])\n\n\n\n\n선형모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터정리\n\nx = torch.tensor(f(txt_x,{'a':0,'b':1})).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,{'a':0,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습 및 결과 시각화\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n\n잘 학습이 안되었다.\n\n- 학습이 잘 안된 이유\n\npd.DataFrame({'x':x[:5].reshape(-1),'y':y[:5].reshape(-1)})\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      1.0\n    \n    \n      1\n      1.0\n      0.0\n    \n    \n      2\n      0.0\n      1.0\n    \n    \n      3\n      1.0\n      0.0\n    \n    \n      4\n      0.0\n      1.0\n    \n  \n\n\n\n\n현재 \\(\\hat{y}_i = \\hat{w}x_i\\) 꼴의 아키텍처이고 \\(y_i \\approx \\hat{w}x_i\\) 가 되는 적당한 \\(\\hat{w}\\)를 찾아야 하는 상황 - \\((x_i,y_i)=(0,1)\\) 이면 어떠한 \\(\\hat{w}\\)를 선택해도 \\(y_i \\approx \\hat{w}x_i\\)를 만드는 것이 불가능\n- \\((x_i,y_i)=(1,0)\\) 이면 \\(\\hat{w}=0\\)일 경우 \\(y_i \\approx \\hat{w}x_i\\)로 만드는 것이 가능\n상황을 종합해보니 \\(\\hat{w}=0\\)으로 학습되는 것이 그나마 최선\n0에 무엇을 곱하든 0이 되어서 학습이 안 돼\n\n\n(풀이2) 1개의 파라메터 – 성공, but 확장성이 없는 풀이\n- 0이라는 값이 문제가 되므로 인코딩방식의 변경\n\nx = torch.tensor(f(txt_x,{'a':-1,'b':1})).float().reshape(-1,1) \ny = torch.tensor(f(txt_y,{'a':-1,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[-1.],\n         [ 1.],\n         [-1.],\n         [ 1.],\n         [-1.]]),\n tensor([[ 1.],\n         [-1.],\n         [ 1.],\n         [-1.],\n         [ 1.]]))\n\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(2000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과는 성공\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n\n딱봐도 클래스가 3개일 경우 확장이 어려워 보인다.\n\n원핫인코딩해줘야 좋은데 그러면 마지막 무조건 softmax 그러면 loss는 BCELoss\n\n\n\n로지스틱 모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터를 다시 a=0, b=1로 정리\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 결과해석: 예상되었던 실패임 - 아키텍처는 \\(\\hat{y}_i = \\text{sig}(\\hat{w}x_i)\\) 꼴이다. - \\((x_i,y_i)=(0,1)\\) 이라면 어떠한 \\(\\hat{w}\\)을 선택해도 \\(\\hat{w}x_i=0\\) 이다. 이경우 \\(\\hat{y}_i = \\text{sig}(0) = 0.5\\) 가 된다. - \\((x_i,y_i)=(1,0)\\) 이라면 \\(\\hat{w}=-5\\)와 같은 값으로 선택하면 \\(\\text{sig}(-5) \\approx 0 = y_i\\) 와 같이 만들 수 있다. - 상황을 종합하면 net의 weight는 \\(\\text{sig}(\\hat{w}x_i) \\approx 0\\) 이 되도록 적당한 음수로 학습되는 것이 최선임을 알 수 있다.\n\nnet.weight # 적당한 음수값으로 학습되어있음을 확인\n\nParameter containing:\ntensor([[-2.8288]], requires_grad=True)\n\n\n\n\n(풀이2) 2개의 파라메터 + 좋은 초기값 – 성공\n- 동일하게 a=0, b=1로 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 네트워크에서 bias를 넣기로 결정함\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- net의 초기값을 설정 (이것은 좋은 초기값임)\n\nnet.weight.data = torch.tensor([[-5.00]])\nnet.bias.data = torch.tensor([+2.500])\n\n\nnet(x)[:10]\n\ntensor([[ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\nbias 쓰게 되면서 한쪽을 뭉개주는 효과?\n- 학습후결과\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n\n(풀이3) 2개의 파라메터 + 나쁜초기값 – 성공\n- a=0, b=1\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 이전과 동일하게 바이어스가 포함된 네트워크 설정\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 초기값설정 (이 초기값은 나쁜 초기값임)\n\nnet.weight.data = torch.tensor([[+5.00]])\nnet.bias.data = torch.tensor([-2.500])\n\n\nnet(x)[:10]\n\ntensor([[-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전상태: 반대모양으로 되어있다.\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n결국 수렴하긴 할듯\n\n\n\n(풀이4) 3개의 파라메터를 쓴다면?\n- a=0, b=1로 코딩\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 3개의 파라메터를 사용하기 위해서 아래와 같은 구조를 생각하자.\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n위와 같은 네트워크를 설정하면 3개의 파라메터를 사용할 수 있다. 적절한 ACTIVATION_FUNCTION을 골라야 하는데 실험적으로 tanh가 적절하다고 알려져있다. (\\(\\to\\) 그래서 우리도 실험적으로 이해해보자)\n\n(예비학습1) net(x)와 사실 net.forwardx(x)는 같다.\n\nnet(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n\nnet.forward(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n그래서 net.forward를 재정의하면 net(x)의 기능을 재정의 할 수 있다.\n\nnet.forward = lambda x: 1 \n\n\n“lambda x: 1” 은 입력이 x 출력이 1인 함수를 의미 (즉 입력값에 상관없이 항상 1을 출력하는 함수)\n“net.forward = lambda x:1” 이라고 새롭게 선언하였므로 앞으론 net.forward(x), net(x) 도 입력값에 상관없이 항상 1을 출력하게 될 것임\n\n\nnet(x)\n\n1\n\n\n(예비학습2) torch.nn.Module을 상속받아서 네트워크를 만들면 (= “class XXX(torch.nn.Module):” 와 같은 방식으로 클래스를 선언하면) 약속된 아키텍처를 가진 네트워크를 찍어내는 함수를 만들 수 있다.\n(예시1)\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet1()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시2)\n\nclass Mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.ReLU()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet2()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.RuLU(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시3)\n\nclass Mynet3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet3()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n클래스에 대한 이해가 부족한 학생을 위한 암기방법\nstep1: 아래와 코드를 복사하여 틀을 만든다. (이건 무조건 고정임, XXXX 자리는 원하는 이름을 넣는다)\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        \n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\n\nnet(x)에 사용하는 x임, yhat은 net.forward(x) 함수의 리턴값임\n사실, x/yhat은 다른 변수로 써도 무방하나 (예를들면 input/output 이라든지) 설명의 편의상 x와 yhat을 고정한다.\n\nstep2: def __init__(self):에 사용할 레이어를 정의하고 이름을 붙인다. 이름은 항상 self.xxx 와 같은 식으로 정의한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\nstep3: def forward:에 “x –> yhat” 으로 가는 과정을 묘사한 코드를 작성하고 yhat을 리턴하도록 한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        u = self.xxx1(x) \n        v = self.xxx2(u)\n        yhat = self.xxx3(v) \n        ## 정의 끝\n        return yhat\n예비학습 끝\n\n- 우리가 하려고 했던 것: 아래의 아키텍처에서\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\nACTIVATION의 자리에 tanh가 왜 적절한지 직관을 얻어보자.\n- 실험결과1(Sig): Sigmoid activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet1()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_1(x):=Sigmoid(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n큰 폭 -> 학습 속도가 빠르다\n- 실험결과2(ReLU): RuLU activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet2()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=ReLU(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 실험결과3(Tanh): Tanh activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet3()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=Tanh(x)$\",size=20)        \nfig.tight_layout()\n\n\n\n\n- 실험해석 - sig: 주황색선의 변동폭이 작음 + 항상 0.5근처로 머무는 적합값이 존재 - relu: 주황색선의 변동폭이 큼 + 항상 0.5근처로 머무는 적합값이 존재 - tanh: 주황색선의 변동폭이 큼 + 0.5근처로 머무는 적합값이 존재X\n- 실험해보니까 tanh가 우수한것 같다. \\(\\to\\) 앞으로는 tanh를 쓰자.\n\\(x \\to wx \\to \\tanh \\to wx \\to sig \\to y\\) - x가 양이면 wx 양수 이런 식으로 y로 가게끔 설정하면 설명의 여지가 존재(?)\n(서연 필기)sigmoid하면 0에 머무르는 값 존재해서 0.5에 머무르는 경향, 조금 사용하면 학습 능력이 떨어지기도\n\n\n\n소프트맥스로 확장\n\n(풀이1) 로지스틱모형에서 3개의 파라메터 버전을 그대로 확장\n\nmapping = {'a':[1,0],'b':[0,1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2)\nx[:5],y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]]),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]]))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nsoft(net(x))[:2]\n\ntensor([[0.0048, 0.9952],\n        [0.9953, 0.0047]], grad_fn=<SliceBackward0>)\n\n\n\ny[:5][:,1]\n\ntensor([1., 0., 1., 0., 1.])\n\n\n\nplt.plot(y[:5][:,1],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\nb,a,b,a,,,…\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5])\nax[1].imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7fe521441490>\n\n\n\n\n\n비슷하게 나왔다, 학습이 잘 되었다(중간 대체과제 참고)"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_9w.html#embedding-layer",
    "href": "posts/ml/2022-11-02-ml_9w.html#embedding-layer",
    "title": "RNN (9주차)",
    "section": "Embedding Layer",
    "text": "Embedding Layer\n\nmotive\n- 결국 최종적으로는 아래와 같은 맵핑방식이 확장성이 있어보인다.\n\nmapping = {'a':[1,0,0],'b':[0,1,0],'c':[0,0,1]} # 원핫인코딩 방식 \n\n- 그런데 매번 \\(X\\)를 원핫인코딩하고 Linear 변환하는것이 번거로운데 이를 한번에 구현하는 함수가 있으면 좋겠다. \\(\\to\\) torch.nn.Embedding Layer가 그 역할을 한다.\nx dimension은 3(원핫인코딩)\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping))\ny = torch.tensor(f(list('bca')*100,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1)\n\n\nebdd(x)[:5]\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], grad_fn=<SliceBackward0>)\n\n\n- 그런데 사실 언뜻보면 아래의 linr 함수와 역할의 차이가 없어보인다.\n\ntorch.manual_seed(43052)\nlinr = torch.nn.Linear(in_features=1,out_features=1)\n\n\nlinr(x.float().reshape(-1,1))[:5]\n\ntensor([[-0.8470],\n        [-1.1937],\n        [-1.5404],\n        [-0.8470],\n        [-1.1937]], grad_fn=<SliceBackward0>)\n\n\n- 차이점: 파라메터수에 차이가 있다.\n파라메터 적게 쓰는게 비용측면에서 좋으니까\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843]], requires_grad=True)\n\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n결국 ebdd는 아래의 구조에 해당하는 파라메터들이고\n\n$=\n\\[\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nnet(x)=\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\\\ -0.8178 \\\\ -0.7052  \\end{bmatrix}\\]\n$\n\nlinr는 아래의 구조에 해당하는 파라메터이다.\n\n\\(\\text{x[:5]}= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\quad net(x)= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\times (-0.3467) + (-0.8470)=\\begin{bmatrix} -0.8470 \\\\ -1.1937 \\\\ -1.5404 \\\\ -0.8470 \\\\ -1.1937 \\end{bmatrix}\\)\n\n\n\n연습 (ab문제 소프트맥스로 확장한 것 다시 풀이)\n- 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 1, 0]), tensor([1, 0, 1, 0, 1]))\n\n\n- torch.nn.Embedding 을 넣은 네트워크\nnum_embedding이 2인 이유 a,b만 있어서\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\n\nsoft(net(x[:5]))\n\ntensor([[0.0040, 0.9960],\n        [0.9960, 0.0040],\n        [0.0040, 0.9960],\n        [0.9960, 0.0040],\n        [0.0040, 0.9960]], grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7f1d7067ac50>"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html",
    "href": "posts/ml/2022-09-29-ml_4w.html",
    "title": "DNN (4주차)",
    "section": "",
    "text": "기계학습 특강 (4주차) 9월28일 [회귀분석(2)–step1~4, step1의 다른표현, step4의 다른표현, 로지스틱 intro]"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#imports",
    "href": "posts/ml/2022-09-29-ml_4w.html#imports",
    "title": "DNN (4주차)",
    "section": "imports",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#numpy-torch-선택학습",
    "href": "posts/ml/2022-09-29-ml_4w.html#numpy-torch-선택학습",
    "title": "DNN (4주차)",
    "section": "numpy, torch (선택학습)",
    "text": "numpy, torch (선택학습)\n\nnumpy, torch는 엄청 비슷해요\n- torch.tensor() = np.array() 처럼 생각해도 무방\n\nnp.array([1,2,3]), torch.tensor([1,2,3])\n\n(array([1, 2, 3]), tensor([1, 2, 3]))\n\n\n- 소수점의 정밀도에서 차이가 있음 (torch가 좀 더 쪼잔함)\n\nnp.array([3.123456789])\n\narray([3.12345679])\n\n\n\ntorch.tensor([3.123456789])\n\ntensor([3.1235])\n\n\n(서연필기)tensor는 gpu에 저장하기 때문에 메모리 아끼기 위해 정밀도가 낮은 경향이 있다.\n- 기본적인 numpy 문법은 np 대신에 torch를 써도 무방 // 완전 같지는 않음\n\nnp.arange(10), torch.arange(10)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\n\nnp.linspace(0,1,10), torch.linspace(0,1,10)\n\n(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n         1.0000]))\n\n\n\nnp.random.randn(10)\n\narray([-0.90388568,  0.51779102,  0.73699131, -0.88030899,  1.71668715,\n       -0.70735651, -0.29752154,  1.10432159,  0.23592126,  0.91669421])\n\n\n\ntorch.randn(10)\n\ntensor([ 0.6896,  1.8534, -0.3807,  1.3676,  0.0515,  0.4350,  0.6051, -1.5075,\n         0.1474,  0.3098])\n\n\n\n\nlength \\(n\\) vector, \\(n \\times 1\\) col-vector, \\(1 \\times n\\) row-vector\n브로드캐스팅 길이가 3인 벡터와 1인벡터를 더하면 오류 뜨지 않고 더해줌\n- 길이가 3인 벡터 선언방법\n\na = torch.tensor([1,2,3])\na.shape\n\ntorch.Size([3])\n\n\n- 3x1 col-vec 선언방법\n(방법1)\n\na = torch.tensor([[1],[2],[3]])\na.shape\n\ntorch.Size([3, 1])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(3,1)\na.shape\n\ntorch.Size([3, 1])\n\n\n- 1x3 row-vec 선언방법\n(방법1)\n\na = torch.tensor([[1,2,3]])\na.shape\n\ntorch.Size([1, 3])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(1,3)\na.shape\n\ntorch.Size([1, 3])\n\n\n- 3x1 col-vec 선언방법, 1x3 row-vec 선언방법에서 [[1],[2],[3]] 혹은 [[1,2,3]] 와 같은 표현이 이해안되면 아래링크로 가셔서\nhttps://guebin.github.io/STBDA2022/2022/03/14/(2주차)-3월14일.html\n첫번째 동영상 12:15 - 22:45 에 해당하는 분량을 학습하시길 바랍니다.\n\n\ntorch의 dtype\n- 기본적으로 torch는 소수점으로 저장되면 dtype=torch.float32 가 된다. (이걸로 맞추는게 편리함)\n\ntsr = torch.tensor([1.23,2.34])\ntsr\n\ntensor([1.2300, 2.3400])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n- 정수로 선언하더라도 dtype를 torch.float32로 바꾸는게 유리함\n(안 좋은 선언예시)\n\ntsr = torch.tensor([1,2])\ntsr \n\ntensor([1, 2])\n\n\n\ntsr.dtype\n\ntorch.int64\n\n\n(좋은 선언예시1)\n\ntsr = torch.tensor([1,2],dtype=torch.float32)\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(좋은 선언예시2)\n\ntsr = torch.tensor([1,2.0])\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(사실 int로 선언해도 나중에 float으로 바꾸면 큰 문제없음)\n\ntsr = torch.tensor([1,2]).float()\ntsr\n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n- 왜 정수만으로 torch.tensor를 만들때에도 torch.float32로 바꾸는게 유리할까? \\(\\to\\) torch.tensor끼리의 연산에서 문제가 될 수 있음\n별 문제 없을수도 있지만\n\ntorch.tensor([1,2])-torch.tensor([1.0,2.0]) \n\ntensor([0., 0.])\n\n\n아래와 같이 에러가 날수도 있다\n(에러1)\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]]) \n\nRuntimeError: expected scalar type Float but found Long\n\n\n(에러2)\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]])\n\nRuntimeError: expected scalar type Long but found Float\n\n\n(해결1) 둘다 정수로 통일\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1],[2]])\n\ntensor([[1],\n        [2]])\n\n\n(해결2) 둘다 소수로 통일 <– 더 좋은 방법임\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1.0],[2.0]])\n\ntensor([[1.],\n        [2.]])\n\n\n\n\nshape of vector\n- 행렬곱셈에 대한 shape 조심\n\nA = torch.tensor([[2.00,0.00],[0.00,3.00]]) \nb1 = torch.tensor([[-1.0,-5.0]])\nb2 = torch.tensor([[-1.0],[-5.0]])\nb3 = torch.tensor([-1.0,-5.0])\n\n\nA.shape,b1.shape,b2.shape,b3.shape\n\n(torch.Size([2, 2]), torch.Size([1, 2]), torch.Size([2, 1]), torch.Size([2]))\n\n\n- A@b1: 계산불가, b1@A: 계산가능\n\nA@b1\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2)\n\n\n\nb1@A\n\ntensor([[ -2., -15.]])\n\n\n- A@b2: 계산가능, b2@A: 계산불가\n\nA@b2\n\ntensor([[ -2.],\n        [-15.]])\n\n\n\nb2@A\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2)\n\n\n- A@b3: 계산가능, b3@A: 계산가능\n\n(A@b3).shape ## b3를 마치 col-vec 처럼 해석\n\ntorch.Size([2])\n\n\n\n(b3@A).shape ## b3를 마지 row-vec 처럼 해석\n\ntorch.Size([2])\n\n\n\n뒤에 놓으면 b3를 컬럼벡터로 인식\n앞에 놓으면 b3를 로우벡터로 인식\n\n- 브로드캐스팅\n\na = torch.tensor([1,2,3])\na - 1\n\ntensor([0, 1, 2])\n\n\n\nb = torch.tensor([[1],[2],[3]])\nb - 1\n\ntensor([[0],\n        [1],\n        [2]])\n\n\n계산이 되지 않아야 맞지 않나\n\na - b # a를 row-vec 로 해석\n\ntensor([[ 0,  1,  2],\n        [-1,  0,  1],\n        [-2, -1,  0]])\n\n\n잘못 계싼할 수 있으니 dimension 명시해주자"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#review-step14",
    "href": "posts/ml/2022-09-29-ml_4w.html#review-step14",
    "title": "DNN (4주차)",
    "section": "Review: step1~4",
    "text": "Review: step1~4\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \ndf\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -2.482113\n      -8.542024\n    \n    \n      1\n      -2.362146\n      -6.576713\n    \n    \n      2\n      -1.997295\n      -5.949576\n    \n    \n      3\n      -1.623936\n      -4.479364\n    \n    \n      4\n      -1.479192\n      -4.251570\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      2.244400\n      10.325987\n    \n    \n      96\n      2.393501\n      12.266493\n    \n    \n      97\n      2.605604\n      13.098280\n    \n    \n      98\n      2.605658\n      12.546793\n    \n    \n      99\n      2.663240\n      13.834002\n    \n  \n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df.x, df.y,'o')\n\n\n\n\n\ntorch.tensor(df.x)\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632], dtype=torch.float64)\n\n\n(서연필기)\nfloat64 숫자 정밀 저장\nfloat32이면 dtype=torch.float64)꼬리표가 붙지 않음\n_trt = torch.tensor(df.x).float()\n_trt = torch.tensor(df.x,dtype=float30)\n같은 역할, 메모리 적게 쓰기 위해 타입 바꿔주자\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)\n컬럼형식으로 받아주기 위해 변경\n\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)\ny= torch.tensor(df.y,dtype=torch.float32).reshape(100,1)\n_1= torch.ones([100,1])\nX = torch.concat([_1,x],axis=1)\n\ntorch.ones([100,1])\ntorch.tensor([[1]*100,x]).T\n같은 셋\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\nrequires_grad=True \nreshape 미분 가능 옵션 주기 전에 shape 정해주자\n\nplt.plot(x,y,'o')\n#plt.plot(x,-5+10*x,'--')\nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nver1: loss = sum of squares error\n\nalpha = 1/1000\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.sum((y-yhat)**2)\n    # step3: 미분 \n    loss.backward()\n    # step4: update \n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nnote: 왜 What = What - alpha*What.grad 는 안되는지?\n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n\nWhat.data\n\ntensor([[2.4290],\n        [4.0144]])\n\n\nWhat과 What.data는 달라요, requires_grad=True 미분 가능 꼬리표가 붙지 않기 때문!\n\n\nver2: loss = mean squared error = MSE\n\nalpha = 1/10\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.mean((y-yhat)**2)\n    # step3: 미분 \n    loss.backward()\n    # step4: update \n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n(서연필기)mean 정의 - 데이터를 더 효율적으로 학습 가능, 데이터 수만큼 안 해도 돼, 계산 덜 해도 돼"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#step1의-다른버전-net-설계만",
    "href": "posts/ml/2022-09-29-ml_4w.html#step1의-다른버전-net-설계만",
    "title": "DNN (4주차)",
    "section": "step1의 다른버전 – net 설계만",
    "text": "step1의 다른버전 – net 설계만\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n\ntorch.nn.Linear?\n\n\nInit signature:\ntorch.nn.Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n) -> None\nDocstring:     \nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\nArgs:\n    in_features: size of each input sample\n    out_features: size of each output sample\n    bias: If set to ``False``, the layer will not learn an additive bias.\n        Default: ``True``\nShape:\n    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n    - Output: :math:`(*, H_{out})` where all but the last dimension\n      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\nAttributes:\n    weight: the learnable weights of the module of shape\n        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n        :math:`k = \\frac{1}{\\text{in\\_features}}`\n    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n            If :attr:`bias` is ``True``, the values are initialized from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\nExamples::\n    >>> m = nn.Linear(20, 30)\n    >>> input = torch.randn(128, 20)\n    >>> output = m(input)\n    >>> print(output.size())\n    torch.Size([128, 30])\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/nn/modules/linear.py\nType:           type\nSubclasses:     NonDynamicallyQuantizableLinear, LazyLinear, Linear, Linear\n\n\n\n\ninput 잡는 법 - x의 컬럼 부분을 input이라고 생각하자\n\nx.shape\n\ntorch.Size([100, 1])\n\n\noutput 잡는 법 - y의 컬럼 부분을 output이라고 생각하자\n\ny.shape\n\ntorch.Size([100, 1])\n\n\n\n_net =  torch.nn.Linear(in_features=1, out_features=1, bias=True) \n\n\n_net(x).shape\n\ntorch.Size([100, 1])\n\n\n\n_net.bias # w0\n\nParameter containing:\ntensor([-0.1281], requires_grad=True)\n\n\n\n_net.weight # w1\n\nParameter containing:\ntensor([[0.1433]], requires_grad=True)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1, out_features=1, bias=True) \n\n\nnet.bias, net.weight\n\n(Parameter containing:\n tensor([-0.8470], requires_grad=True),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\nw0hat = -0.847\nw1hat = -0.3467\nplt.plot(x,w0hat+w1hat*x,'--')\n\n\n\n\n출력결과 같음을 확인\n- net에서 \\(\\hat{w}_0, \\hat{w}_1\\) 의 값은?\n\nnet.weight # w1 \n\nParameter containing:\ntensor([[-0.3467]], requires_grad=True)\n\n\n\nnet.bias # w0 \n\nParameter containing:\ntensor([-0.8470], requires_grad=True)\n\n\n\n_yhat = -0.8470 + -0.3467*x \n\n\nplt.plot(x,y,'o')\nplt.plot(x, _yhat,'--')\nplt.plot(x,net(x).data,'-.')\n\n\n\n\n- 수식표현: \\(\\hat{y}_i = \\hat{w}_0 + \\hat{w}_1 x_i = \\hat{b} + \\hat{w}x_i = -0.8470 + -0.3467 x_i\\) for all \\(i=1,2,\\dots,100\\).\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 입력이 x가 아닌 X를 넣고 싶다면? (보통 잘 안하긴 해요, 왜? bias=False로 주는게 귀찮거든요) - X는 바이어스가 고려된 상황\n\nnet(X) ## 그대로 쓰면 당연히 에러\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (100x2 and 1x1)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2, out_features=1, bias=False) \n\n\nnet(X).shape\n\ntorch.Size([100, 1])\n\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n위에 \\(w_0,w_1\\) 순\n\nnet.bias\n\nbias 없음을 확인\n\nplt.plot(x,y,'o') \nplt.plot(x,net(X).data, '--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]]), '-.')\n\n\n\n\n- 수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix}\\)\n\n\n잘못된사용1\n\n_x = x.reshape(-1)\n\n\n_x\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1,out_features=1) \n\n\nnet(_x)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x100 and 1x1)\n\n\nnet(_x.reshape(100,1))\n과 같이 정의\n\n\n잘못된사용2\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2,out_features=1) # bias=False를 깜빡..\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.2549], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]])+0.2549,'-.')\n\n\n\n\n\n수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} + \\hat{b}= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix} + 0.2549\\)"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#step1의-다른버전-끝까지",
    "href": "posts/ml/2022-09-29-ml_4w.html#step1의-다른버전-끝까지",
    "title": "DNN (4주차)",
    "section": "step1의 다른버전 – 끝까지",
    "text": "step1의 다른버전 – 끝까지\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1,bias=True)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\nnet.weight,net.bias\n\n(Parameter containing:\n tensor([[10.]], requires_grad=True),\n Parameter containing:\n tensor([-5.], requires_grad=True))\n\n\n- step1\n\nyhat = net(x) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\nplt.plot(x,-5+10*x,'--')\n\n\n\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad,net.weight.grad\n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n- step4\n(업데이트전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n(업데이트)\n\nnet.bias.data = net.bias.data - 0.1*net.bias.grad \nnet.weight.data = net.weight.data - 0.1*net.weight.grad \n\n\nnet.bias.grad = None \nnet.weight.grad = None \n\n(업데이트후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-3.6577], requires_grad=True),\n Parameter containing:\n tensor([[8.8111]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n- 반복\n\nnet = torch.nn.Linear(1,1)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\n\n\nfor epoc in range(30):\n    yhat = net(x) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.bias.data = net.bias.data - 0.1*net.bias.grad\n    net.weight.grad = None\n    net.bias.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 준비\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n- step1\n\nyhat = net(X)\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n- step4\n(업데이트전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n(업데이트)\n\nnet.weight.data = net.weight.data - 0.1*net.weight.grad\n\n\nnet.weight.grad = None\n\n(업데이트후)\n\nnet.weight\n\nParameter containing:\ntensor([[-3.6577,  8.8111]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n- 반복\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = net(X) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.weight.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#step4의-다른버전-옵티마이저",
    "href": "posts/ml/2022-09-29-ml_4w.html#step4의-다른버전-옵티마이저",
    "title": "DNN (4주차)",
    "section": "step4의 다른버전: 옵티마이저!",
    "text": "step4의 다른버전: 옵티마이저!\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]]) \nnet.bias.data = torch.tensor([[-5.0]]) \n\n\ntorch.optim.SGD?\n\n\nInit signature:\ntorch.optim.SGD(\n    params,\n    lr=<required parameter>,\n    momentum=0,\n    dampening=0,\n    weight_decay=0,\n    nesterov=False,\n)\nDocstring:     \nImplements stochastic gradient descent (optionally with momentum).\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\\:nesterov\\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: nesterov                                                \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t-1} + \\mu \\textbf{b}_t                             \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                    \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float): learning rate\n    momentum (float, optional): momentum factor (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    dampening (float, optional): dampening for momentum (default: 0)\n    nesterov (bool, optional): enables Nesterov momentum (default: False)\nExample:\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> optimizer.zero_grad()\n    >>> loss_fn(model(input), target).backward()\n    >>> optimizer.step()\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et. al. and implementations in some other frameworks.\n    Considering the specific case of Momentum, the update can be written as\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n    This is in contrast to Sutskever et. al. and\n    other frameworks which employ an update of the form\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n    The Nesterov version is analogously modified.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/optim/sgd.py\nType:           type\nSubclasses:     \n\n\n\n\nStocastic Gradiant Decscent\n\nnet.parameters()\n\n<generator object Module.parameters at 0x7f5f0d522740>\n\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n- step1~3\n\nyhat = net(x)     \n\n\nloss = torch.mean((y-yhat)**2) \n\n\nloss.backward() \n\n- step4\n(update 전)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 전\n\n(tensor([[10.]]), tensor([[-5.]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 청소전 \n\n(tensor([[11.8893]]), tensor([[-13.4225]]))\n\n\n(update)\n\noptimizr.step() \noptimizr.zero_grad() \n\n(update 후)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 되었음 \n\n(tensor([[8.8111]]), tensor([[-3.6577]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 0으로 초기화하였음 \n\n(tensor([[0.]]), tensor([[0.]]))\n\n\n- 반복\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(30): \n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2) \n    loss.backward() \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 바로 반복하겠습니다..\n\nnet = torch.nn.Linear(2,1,bias=False) \nnet.weight.data = torch.tensor([[-5.0, 10.0]])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(30): \n    yhat = net(X)\n    loss = torch.mean((y-yhat)**2) \n    loss.backward() \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#appendix-net.parameters의-의미-선택학습",
    "href": "posts/ml/2022-09-29-ml_4w.html#appendix-net.parameters의-의미-선택학습",
    "title": "DNN (4주차)",
    "section": "Appendix: net.parameters()의 의미? (선택학습)",
    "text": "Appendix: net.parameters()의 의미? (선택학습)\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters)) & {'__iter__'}\n\nset()\n\n\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 무슨의미?\n\n_generator = net.parameters()\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n\n_generator.__next__()\n\nStopIteration: \n\n\n- 이건 이런느낌인데?\n\n_generator2 = iter([net.weight,net.bias])\n\n\n_generator2\n\n<list_iterator at 0x7f5f0d2cdeb0>\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nStopIteration: \n\n\n- 즉 아래는 같은코드이다.\n### 코드1\n_generator = net.parameters() \ntorch.optim.SGD(_generator,lr=1/10) \n### 코드2\n_generator = iter([net.weight,net.bias])\ntorch.optim.SGD(_generator,lr=1/10) \n### 코드3 (이렇게 써도 코드2가 실행된다고 이해할 수 있음)\n_iterator = [net.weight,net.bias]\ntorch.optim.SGD(_iterator,lr=1/10) \n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트(iterable object)로 만드는 함수라 이해할 수 있다.\n- 응용예제1\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step();optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n- 응용예제2\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10)\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = b+ w*x \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step(); optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#logistic-regression",
    "href": "posts/ml/2022-09-29-ml_4w.html#logistic-regression",
    "title": "DNN (4주차)",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 증가함.\n- (X,y)는 어떤모양?\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n_df \n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -6\n      0\n    \n    \n      1\n      -5\n      0\n    \n    \n      2\n      -4\n      0\n    \n    \n      3\n      -3\n      0\n    \n    \n      4\n      -2\n      0\n    \n    \n      5\n      -1\n      0\n    \n    \n      6\n      0\n      1\n    \n    \n      7\n      1\n      0\n    \n    \n      8\n      2\n      1\n    \n    \n      9\n      3\n      1\n    \n    \n      10\n      4\n      1\n    \n    \n      11\n      5\n      1\n    \n    \n      12\n      6\n      1\n    \n  \n\n\n\n\n\nplt.plot(_df.x,_df.y,'o')\n\n\n\n\n- (예비학습) 시그모이드라는 함수가 있음\n\nxx = torch.linspace(-6,6,100)\ndef f(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))\nplt.plot(xx,f(2.5*xx-1.2)) # 영향을 크게 받을 때 + 운적인 요소 영향 받을 때(절편) -> 모델링하는 과정\n\n\n\n\n베르누이 특정 확률로 0 또는 1 뽑기\n\n\nmodel\n- \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n\\(y_i \\sim Ber(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\hat{pi}_\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n\\(y_i=1\\) \\(\\hat{y_i} = 1\\)loss가 0 근처 \\(\\hat{y_i} = 0\\) loss가- 무한대\n\\(y_i = 0\\) \\(\\hat{y_i} = 0\\)loss가 0근처 \\(\\hat{y_i} = 1\\) loss가 1\n\n\ntoy example\n- 예제시작\n\ntorch.bernoulli?\n\n\nDocstring:\nbernoulli(input, *, generator=None, out=None) -> Tensor\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\nThe :attr:`input` tensor should be a tensor containing probabilities\nto be used for drawing the binary random number.\nHence, all values in :attr:`input` have to be in the range:\n:math:`0 \\leq \\text{input}_i \\leq 1`.\nThe :math:`\\text{i}^{th}` element of the output tensor will draw a\nvalue :math:`1` according to the :math:`\\text{i}^{th}` probability value given\nin :attr:`input`.\n.. math::\n    \\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})\nThe returned :attr:`out` tensor only has values 0 or 1 and is of the same\nshape as :attr:`input`.\n:attr:`out` can have integral ``dtype``, but :attr:`input` must have floating\npoint ``dtype``.\nArgs:\n    input (Tensor): the input tensor of probability values for the Bernoulli distribution\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\nExample::\n    >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n    >>> a\n    tensor([[ 0.1737,  0.0950,  0.3609],\n            [ 0.7148,  0.0289,  0.2676],\n            [ 0.9456,  0.8937,  0.7202]])\n    >>> torch.bernoulli(a)\n    tensor([[ 1.,  0.,  0.],\n            [ 0.,  0.,  0.],\n            [ 1.,  1.,  1.]])\n    >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n    >>> torch.bernoulli(a)\n    tensor([[ 1.,  1.,  1.],\n            [ 1.,  1.,  1.],\n            [ 1.,  1.,  1.]])\n    >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n    >>> torch.bernoulli(a)\n    tensor([[ 0.,  0.,  0.],\n            [ 0.,  0.,  0.],\n            [ 0.,  0.,  0.]])\nType:      builtin_function_or_method\n\n\n\n\n\ntorch.bernoulli(torch.tensor([0.5]*100)) # 0.5의 확률ㄹ 0 또는 1 뽑아\n\ntensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n        1., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n\n\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함\ny = torch.bernoulli(v) \n\n\nv\n\ntensor([[0.0025],\n        [0.0025],\n        [0.0025],\n        ...,\n        [0.9818],\n        [0.9819],\n        [0.9820]])\n\n\n\nu\n\ntensor([[-6.0000],\n        [-5.9950],\n        [-5.9900],\n        ...,\n        [ 3.9900],\n        [ 3.9950],\n        [ 4.0000]])\n\n\n\n#plt.scatter(x,y,alpha=0.05)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\n\n\n\n\n\n우리의 목적: \\(x\\)가 들어가면 빨간선 \\(\\hat{y}\\)의 값을 만들어주는 mapping을 학습해보자.\n\n\nw0hat = 10\nw1hat = 3\nyhat = f(w0hat + w1hat*x)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat,'--r')\n\n\n\n\n\nl1 = torch.nn.Linear(1,1)\n\n\nl1.bias.data = torch.tensor([-1.0])\nl1.weight.data = torch.tensor([[1.0]])\n\n\na1 = torch.nn.Sigmoid()\n\n\nw0hat = -1\nw1hat = 3\nyhat = a1(w0hat + w1hat*x)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat,'--r')\n\n\n\n\n\nfor epoc in range(6000):\n    ## step1 \n    yhat = a1(l1(x))\n    ## step2 \n    loss = torch.mean((y-yhat)**2) ## loss 를 원래 이렇게 하는건 아니에요.. \n    ## step3 \n    loss.backward()\n    ## step4 \n    l1.bias.data = l1.bias.data - 0.1 * l1.bias.grad \n    l1.weight.data = l1.weight.data - 0.1 * l1.weight.grad \n    l1.bias.grad = None \n    l1.weight.grad = None \n\n\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,a1(l1(x)).data,'--r')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4w.html#숙제",
    "href": "posts/ml/2022-09-29-ml_4w.html#숙제",
    "title": "DNN (4주차)",
    "section": "숙제",
    "text": "숙제"
  },
  {
    "objectID": "posts/ml/2022-09-07-ml_1w.html",
    "href": "posts/ml/2022-09-07-ml_1w.html",
    "title": "DNN (1주차)",
    "section": "",
    "text": "기계학습 특강 (1주차) 9월7일 [pytorch]\n\n우리의 1차 목표: 이미지 -> 개/고양이 판단하는 모형을 채용하고, 그 모형에 데이터를 넣어서 학습하고, 그 모형의 결과를 판단하고 싶다. (즉 클래시파이어를 만든다는 소리)\n\n\n우리의 2차 목표: 그 모형에 “새로운” 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리)\n\n\nimport\n\nfrom fastai.vision.all import *\n\n\n#!nvidia-smi\n\n\nURLs.PETS\n\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images')\n\n\n\nPILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\n_lst = ['/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg','/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg']\n_lst\n\n['/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg',\n '/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg']\n\n\n\n_lst[0]\n\n'/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg'\n\n\n\nPILImage.create(_lst[1])\n\n\n\n\n\nfilenames = get_image_files(path)\nfilenames\n\n(#7390) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nprint(filenames[0])\nPILImage.create(filenames[0])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg\n\n\n\n\n\n\nprint(filenames[1])\nPILImage.create(filenames[1])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg\n\n\n\n\n\n\nprint(filenames[2])\nPILImage.create(filenames[2])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg\n\n\n\n\n\n\nprint(filenames[3])\nPILImage.create(filenames[3])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg\n\n\n\n\n\n\nprint(filenames[4])\nPILImage.create(filenames[4])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg\n\n\n\n\n\n\nprint(filenames[5])\nPILImage.create(filenames[5])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg\n\n\n\n\n\n\nprint(filenames[6])\nPILImage.create(filenames[6])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg\n\n\n\n\n\n\nprint(filenames[7])\nPILImage.create(filenames[7])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg\n\n\n\n\n\n\nprint(filenames[8])\nPILImage.create(filenames[8])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg\n\n\n\n\n\n\nprint(filenames[9])\nPILImage.create(filenames[9])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg\n\n\n\n\n\n\nprint(filenames[20])\nPILImage.create(filenames[20])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Maine_Coon_266.jpg\n\n\n\n\n\nvector로 되어 있는 tensor\n\n\n'A'.isupper()\n\nTrue\n\n\n\n\ndef f(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\nf('dddd')\n\n'dog'\n\n\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nImageDataLoaders.from_name_func??\n\n\nSignature:\nImageDataLoaders.from_name_func(\n    path,\n    fnames,\n    label_func,\n    valid_pct=0.2,\n    seed=None,\n    item_tfms=None,\n    batch_tfms=None,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    def from_name_func(cls, path, fnames, label_func, **kwargs):\n        \"Create from the name attrs of `fnames` in `path`s with `label_func`\"\n        if sys.platform == 'win32' and isinstance(label_func, types.LambdaType) and label_func.__name__ == '<lambda>':\n            # https://medium.com/@jwnx/multiprocessing-serialization-in-python-with-pickle-9844f6fa1812\n            raise ValueError(\"label_func couldn't be lambda function on Windows\")\n        f = using_attr(label_func, 'name')\n        return cls.from_path_func(path, fnames, f, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/data.py\nType:      method\n\n\n\n\ndls는 object - 동사 - 명사(method)\nsize가 다르기 때문에 dls 적용이 되지 않아 resize로 조정을 해주었다.\n\npath\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images')\n\n\n\ndls = ImageDataLoaders.from_name_func(path,filenames,f,item_tfms=Resize(224))\n#dls\n\n\ndls.show_batch(max_n=16)\n\n\n\n\n\n\n학습\n\nobject\n\nnoun\n\n\ndata\n채용할 모델의 이름\n평가기준 metric\n\n\nverb\n\n\n학습\n판단\n\n\nysj = cnn_learner(dls,resnet34,metrics=error_rate)\n\n\nysj.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.138703\n      0.014957\n      0.004060\n      00:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.061359\n      0.010080\n      0.002706\n      00:11\n    \n  \n\n\n\n\n\n\n기존 데이터를 잘 맞추는지 확인\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nysj.predict(PILImage.create(filenames[0]))\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.5260e-07]))\n\n\n\nysj.predict(filenames[0])\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.5260e-07]))\n\n\n\nfilenames[1]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg')\n\n\n\nysj.predict(filenames[1])\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.0373e-04, 9.9980e-01]))\n\n\n\nysj.show_results()\n\n\n\n\n\n\n\n\n\n오답분석\n\nchecker = Interpretation.from_learner(ysj)\n\n\n\n\n\nchecker.plot_top_losses(k=16)\n\n\n\n\n\n\n좋은 모델인가?\n\nPILImage.create('2022-01-13-cat.jpg')\n\n\n\n\n\nysj.predict(PILImage.create('2022-01-13-cat.jpg'))\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.8330e-16]))\n\n\n\nPILImage.create(requests.get('https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg').content)\n\n\n\n\n\nimg=PILImage.create(requests.get('https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.1535e-06, 1.0000e+00]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-cat1.png?raw=true').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9982e-01, 1.8307e-04]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-cat2.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.0889e-07]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani01.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([9.5189e-06, 9.9999e-01]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani02.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.0720e-05, 9.9998e-01]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani03.jpg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([0.0513, 0.9487]))\n\n\n\n\n\nhomework\n\n임의의 사진으로 잘 맞추는지 확인\n\n\nPILImage.create('2022-09-07-dogs.jpeg')\n\n\n\n\n\nysj.predict(PILImage.create('2022-09-07-dogs.jpeg'))\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.7947e-04, 9.9972e-01]))\n\n\n\nimg2=PILImage.create('2022-09-07-dogs.jpeg')\nysj.predict(img2)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.7947e-04, 9.9972e-01]))\n\n\n\nPILImage.create(requests.get('https://media.npr.org/assets/img/2021/08/11/gettyimages-1279899488_wide-f3860ceb0ef19643c335cb34df3fa1de166e2761-s900-c85.webp').content)\n\n\n\n\n\nimg=PILImage.create(requests.get('https://media.npr.org/assets/img/2021/08/11/gettyimages-1279899488_wide-f3860ceb0ef19643c335cb34df3fa1de166e2761-s900-c85.webp').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.5169e-10]))"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html",
    "href": "posts/ml/2022-10-05-ml-5w.html",
    "title": "DNN (5주차)",
    "section": "",
    "text": "기계학습 특강 (5주차) 10월5일 [딥러닝의 기초 - 로지스틱(2), 깊은신경망(1)]"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#imports",
    "href": "posts/ml/2022-10-05-ml-5w.html#imports",
    "title": "DNN (5주차)",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\n시각화를 위한 준비함수들\n준비1 loss_fn을 plot하는 함수\n\ndef plot_loss(loss_fn,ax=None):\n    if ax==None:\n        fig = plt.figure()\n        ax=fig.add_subplot(1,1,1,projection='3d')\n        ax.elev=15;ax.azim=75\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) \n    ax.scatter(-1,5,l(-1,5),s=200,marker='*') # 실제로 -1,5에서 최소값을 가지는건 아님.. \n\n\n\\(y_i \\sim Ber(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\) 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음.\n\n준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수\n\ndef learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    what_history = [] \n\n    for epoc in range(1000): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        optimizr.zero_grad() \n\n        ## record \n        if epoc % 20 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            what_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    return yhat_history, loss_history, what_history\n\n\n20에폭마다 yhat, loss, what을 기록\n\n준비3: 애니메이션을 만들어주는 함수\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\ndef show_lrpr2(net,loss_fn,optimizr,suptitle=''):\n    yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7,2.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ax1.set_xticks([]);ax1.set_yticks([])\n    ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([])\n    ax2.elev = 15; ax2.azim = 75\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,v,'--')\n    ax1.scatter(x,y,alpha=0.05)\n    line, = ax1.plot(x,yhat_history[0],'--') \n    plot_loss(loss_fn,ax2)\n    fig.suptitle(suptitle)\n    fig.tight_layout()\n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\n준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#logistic-intro-review-alpha",
    "href": "posts/ml/2022-10-05-ml-5w.html#logistic-intro-review-alpha",
    "title": "DNN (5주차)",
    "section": "Logistic intro (review + \\(\\alpha\\))",
    "text": "Logistic intro (review + \\(\\alpha\\))\n- 모델: \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n\\(y_i \\sim Ber(\\pi_i),\\quad\\) where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n- toy example\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함\ny = torch.bernoulli(v) \n\n\nnote: \\((w_0,w_1)\\)의 true는 \\((-1,5)\\)이다. -> \\((\\hat{w}_0, \\hat{w}_1)\\)을 적당히 \\((-1,5)\\)근처로 추정하면 된다는 의미\n\n\nplt.scatter(x,y,alpha=0.05)\nplt.plot(x,v,'--r')\n\n\n\n\n- step1: yhat을 만들기\n(방법1)\n\nx.shape\n\ntorch.Size([2000, 1])\n\n\n뒤의 1이 input feature로서 입력\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)\na1 = torch.nn.Sigmoid() \nyhat = a1(l1(x))\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n\nl1.weight,l1.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n(방법2)\n\\(x \\overset{l1}{\\to} u \\overset{a1}{\\to} v = \\hat{y}\\)\n\\(x \\overset{net}{\\to} \\hat{y}\\)\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)\na1 = torch.nn.Sigmoid() \nnet = torch.nn.Sequential(l1,a1) \nyhat = net(x)\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n(방법3)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n중간과정보기 힘들다.\n\n\nlen(net)\n\n2\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[0](x)\n\ntensor([[-0.5003],\n        [-0.5007],\n        [-0.5010],\n        ...,\n        [-1.1930],\n        [-1.1934],\n        [-1.1937]], grad_fn=<AddmmBackward0>)\n\n\n\nnet[1]\n\nSigmoid()\n\n\n\nnet[1](net[0](x))\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n\nl1, a1 = net\n\n\nl1\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\na1\n\nSigmoid()\n\n\n\nid(l1),id(net[0])\n\n(140123913611200, 140123913611200)\n\n\n\n- step2: loss (일단 MSE로..)\n(방법1)\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(0.2823, grad_fn=<MeanBackward0>)\n\n\n(방법2)\n\nloss_fn = torch.nn.MSELoss()\nloss = loss_fn(yhat,y) # yhat을 먼저쓰자!\nloss\n\ntensor(0.2823, grad_fn=<MseLossBackward0>)\n\n\n- step3~4는 동일\n- 반복 (준비+for문)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#로지스틱bceloss",
    "href": "posts/ml/2022-10-05-ml-5w.html#로지스틱bceloss",
    "title": "DNN (5주차)",
    "section": "로지스틱–BCEloss",
    "text": "로지스틱–BCEloss\n- BCEloss로 바꾸어서 적합하여 보자.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)  #. -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) \n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 왜 잘맞지? -> “linear -> sigmoid” 와 같은 net에 BCEloss를 이용하면 손실함수의 모양이 convex 하기 때문에 - “linear -> sigmoid” 로 \\(\\hat{y}\\)을 구하고 BCEloss로 loss를 계산하면 그 모영아 convex하므로\nBCSloss에는 local error에 빠지지 않아, loss는 있아.\n\nplot_loss 함수소개 = 이 예제에 한정하여 \\(\\hat{w}_0,\\hat{w}_1,loss(\\hat{w}_0,\\hat{w}_1)\\)를 각각 \\(x,y,z\\) 축에 그려줍니다.\n\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n시각화1: MSE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\n\n\nl1.bias.data,l1.weight.data\n\n(tensor([0.6245]), tensor([[-0.2593]]))\n\n\n\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nl1.bias.data,l1.weight.data\n\n(tensor([-3.]), tensor([[-1.]]))\n\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, bad_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#로지스틱adam-국민옵티마이저",
    "href": "posts/ml/2022-10-05-ml-5w.html#로지스틱adam-국민옵티마이저",
    "title": "DNN (5주차)",
    "section": "로지스틱–Adam (국민옵티마이저)",
    "text": "로지스틱–Adam (국민옵티마이저)\n\n시각화1: MSE, 좋은초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05)  ## <-- 여기를 수정!\n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값 –> 이걸 아담으로! (혼자해봐요..)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값 –> 이걸 아담으로! (혼자해봐요..)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, bad_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(참고) Adam이 우수한 이유? SGD보다 두 가지 측면에서 개선이 있었음. 1. 그런게 있음.. 2. 가속도의 개념을 적용!!"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#깊은신경망로지스틱-회귀의-한계",
    "href": "posts/ml/2022-10-05-ml-5w.html#깊은신경망로지스틱-회귀의-한계",
    "title": "DNN (5주차)",
    "section": "깊은신경망–로지스틱 회귀의 한계",
    "text": "깊은신경망–로지스틱 회귀의 한계\n\n신문기사 (데이터의 모티브)\n- 스펙이 높아도 취업이 안된다고 합니다..\n중소·지방 기업 “뽑아봤자 그만두니까”\n중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다.\n중소 IT업체 관계자는 “기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일”이라며 “명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다”고 말했다.\n\n\n가짜데이터\n- 위의 기사를 모티브로 한 데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n\n로지스틱 회귀로 적합\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n#yhat=net(x)\n\n\nloss_fn = torch.nn.BCELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nloss = loss_fn(net(x),y) \n# loss = loss_fn(yhat,y) \n# loss = -torch.mean((y)*torch.log(yhat)+(1-y)*torch.log(1-yhat))\nloss\n\ntensor(0.6403, grad_fn=<BinaryCrossEntropyBackward0>)\n\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 이건 epoc=6억번으로 설정해도 못 맞출 것 같다 (증가하다가 감소하는 underlying을 설계하는 것이 불가능) \\(\\to\\) 모형의 표현력이 너무 낮다.\n\n\n해결책\n- sigmoid 넣기 전의 상태가 꺽인 그래프 이어야 한다.\n\nsig = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0');ax[0,1].plot(sig(u1),'--o',color='C0')\nax[1,0].plot(u2,'--o',color='C1');ax[1,1].plot(sig(u2),'--o',color='C1')\nax[2,0].plot(u3,'--o',color='C2');ax[2,1].plot(sig(u3),'--o',color='C2')\nax[3,0].plot(u4,'--o',color='C3');ax[3,1].plot(sig(u4),'--o',color='C3')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#깊은신경망dnn을-이용한-해결",
    "href": "posts/ml/2022-10-05-ml-5w.html#깊은신경망dnn을-이용한-해결",
    "title": "DNN (5주차)",
    "section": "깊은신경망–DNN을 이용한 해결",
    "text": "깊은신경망–DNN을 이용한 해결\n- 목표: 아래와 같은 벡터 \\({\\boldsymbol u}\\)를 만들어보자.\n\\({\\boldsymbol u} = [u_1,u_2,\\dots,u_{2000}], \\quad u_i = \\begin{cases} 9x_i +4.5& x_i <0 \\\\ -4.5x_i + 4.5& x_i >0 \\end{cases}\\)\n\n꺽인 그래프를 만드는 방법1\n\nu = [9*xi+4.5 if xi <0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()]\nplt.plot(u,'--')\n\n\n\n\n\n\n꺽인 그래프를 만드는 방법2\n- 전략: 선형변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환\n(예비학습) ReLU 함수란?\n\\(ReLU(x) = \\max(0,x)\\)\n\nrelu=torch.nn.ReLU()\nplt.plot(x,'--r')\nplt.plot(relu(x),'--b')\n\n\n\n\n\n빨간색: x, 파란색: relu(x)\n\n예비학습끝\n우리 전략 다시 확인: 선형변환1 -> 렐루 -> 선형변환2\n(선형변환1)\n\nplt.plot(x);plt.plot(-x)\n\n\n\n\n(렐루)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.5)\nplt.plot(relu(x),'--',color='C0');plt.plot(relu(-x),'--',color='C1')\n\n\n\n\n(선형변환2)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.2)\nplt.plot(relu(x),'--',color='C0',alpha=0.2);plt.plot(relu(-x),'--',color='C1',alpha=0.2)\nplt.plot(-4.5*relu(x)-9.0*relu(-x)+4.5,'--',color='C2') \n\n\n\n\n이제 초록색선에 sig를 취하기만 하면?\n\nplt.plot(sig(-4.5*relu(x)-9.0*relu(-x)+4.5),'--',color='C2')\n\n\n\n\n정리하면!\n\nfig = plt.figure(figsize=(8, 4))\nspec = fig.add_gridspec(4, 4)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title('x'); ax1.plot(x,'--',color='C0')\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title('-x'); ax2.plot(-x,'--',color='C1')\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title('relu(x)'); ax3.plot(relu(x),'--',color='C0')\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title('relu(-x)'); ax4.plot(relu(-x),'--',color='C1')\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title('u'); ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')\nax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title('yhat'); ax6.plot(sig(-4.5*relu(x)-9*relu(-x)+4.5),'--',color='C2')\nfig.tight_layout()\n\n\n\n\n\n이런느낌으로 \\(\\hat{\\boldsymbol y}\\)을 만들면 된다.\n\n\n\ntorch.nn.Linear()를 이용한 꺽인 그래프 구현\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) \na1 = torch.nn.ReLU()\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) \na2 = torch.nn.Sigmoid() \n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2) \n\n\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[-0.3467],\n         [-0.8470]], requires_grad=True),\n Parameter containing:\n tensor([0.3604, 0.9336], requires_grad=True),\n Parameter containing:\n tensor([[ 0.2880, -0.6282]], requires_grad=True),\n Parameter containing:\n tensor([0.2304], requires_grad=True))\n\n\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\nl2.weight.data = torch.tensor([[ -4.5, -9.0]])\nl2.bias.data= torch.tensor([4.5])\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[ 1.],\n         [-1.]], requires_grad=True),\n Parameter containing:\n tensor([0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[-4.5000, -9.0000]], requires_grad=True),\n Parameter containing:\n tensor([4.5000], requires_grad=True))\n\n\n\nplt.plot(l1(x).data)\n\n\n\n\n\nplt.plot(a1(l1(x)).data)\n\n\n\n\n\nplt.plot(l2(a1(l1(x))).data,color='C2')\n\n\n\n\n\nplt.plot(a2(l2(a1(l1(x)))).data,color='C2')\n#plt.plot(net(x).data,color='C2')\n\n\n\n\n- 수식표현\n\n\\({\\bf X}=\\begin{bmatrix} x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}\\)\n\\(l_1({\\bf X})={\\bf X}{\\bf W}^{(1)}\\overset{bc}{+} {\\boldsymbol b}^{(1)}=\\begin{bmatrix} x_1 & -x_1 \\\\ x_2 & -x_2 \\\\ \\dots & \\dots \\\\ x_n & -x_n\\end{bmatrix}\\)\n\n\\({\\bf W}^{(1)}=\\begin{bmatrix} 1 & -1 \\end{bmatrix}\\)\n\\({\\boldsymbol b}^{(1)}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\)\n\n\\((a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big)=\\begin{bmatrix} \\text{relu}(x_1) & \\text{relu}(-x_1) \\\\ \\text{relu}(x_2) & \\text{relu}(-x_2) \\\\ \\dots & \\dots \\\\ \\text{relu}(x_n) & \\text{relu}(-x_n)\\end{bmatrix}\\)\n\\((l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\\\ =\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\\({\\bf W}^{(2)}=\\begin{bmatrix} -4.5 \\\\ -9 \\end{bmatrix}\\)\n\\(b^{(2)}=4.5\\)\n\n\\(net({\\bf X})=(a_2 \\circ l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{sig}\\Big(\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\Big)\\\\=\\begin{bmatrix} \\text{sig}\\Big(-4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5\\Big) \\\\ \\text{sig}\\Big(-4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\Big)\\\\ \\dots \\\\ \\text{sig}\\Big(-4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\Big)\\end{bmatrix}\\)\n\n- 차원만 따지자\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nStep1 ~ Step4\n- 준비\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2), #u1=l1(x), x:(n,1) --> u1:(n,2) \n    torch.nn.ReLU(), # v1=a1(u1), u1:(n,2) --> v1:(n,2) \n    torch.nn.Linear(in_features=2,out_features=1), # u2=l2(v1), v1:(n,2) --> u2:(n,1) \n    torch.nn.Sigmoid() # v2=a2(u2), u2:(n,1) --> v2:(n,1) \n) \n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters()) # lr은 디폴트값으로..\n\n- 반복\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\nplt.title(\"before\")\n\nText(0.5, 1.0, 'before')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 3000 epochs\")\n\nText(0.5, 1.0, 'after 3000 epochs')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 6000 epochs\")\n\nText(0.5, 1.0, 'after 6000 epochs')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-5w.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "href": "posts/ml/2022-10-05-ml-5w.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "title": "DNN (5주차)",
    "section": "깊은신경망–DNN으로 해결가능한 다양한 예제",
    "text": "깊은신경망–DNN으로 해결가능한 다양한 예제\n\n예제1\n- 언뜻 생각하면 방금 배운 기술은 sig를 취하기 전이 꺽은선인 형태만 가능할 듯 하다. \\(\\to\\) 그래서 이 역시 표현력이 부족할 듯 하다. \\(\\to\\) 그런데 생각보다 표현력이 풍부한 편이다. 즉 생각보다 쓸 만하다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv')\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n이거 시그모이드 취하기 직전은 step이 포함된 듯 \\(\\to\\) 그래서 꺽은선으로는 표현할 수 없는 구조임 \\(\\to\\) 그런데 사실 대충은 표현가능\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=16), # x:(n,1) --> u1:(n,16) \n    torch.nn.ReLU(), # u1:(n,16) --> v1:(n,16)\n    torch.nn.Linear(in_features=16,out_features=1), # v1:(n,16) --> u2:(n,1) \n    torch.nn.Sigmoid() # u2:(n,1) --> v2:(n,1) \n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,16)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,16)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(20000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()    \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제2\n- 사실 꺽은선의 조합으로 꽤 많은걸 표현할 수 있거든요? \\(\\to\\) 심지어 곡선도 대충 맞게 적합된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv')\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\nx=torch.tensor(df.x).float().reshape(-1,1)\ny=torch.tensor(df.y).float().reshape(-1,1)\n\n(풀이1)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(20000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n(풀이2) – 풀이1보다 좀 더 잘맞음. 잘 맞는 이유? 좋은초기값 (=운)\n\ntorch.manual_seed(5)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\n풀이1에서 에폭을 많이 반복하면 풀이2의 적합선이 나올까? –> 안나옴!! (local min에 빠졌다)\n\n\n\n예제3\n\nimport seaborn as sns\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)})\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n\nx1 = torch.tensor(df.x1).float().reshape(-1,1) \nx2 = torch.tensor(df.x2).float().reshape(-1,1) \nX = torch.concat([x1,x2],axis=1) \ny = torch.tensor(df.y).float().reshape(-1,1) \n\n\nX.shape\n\ntorch.Size([2000, 2])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,2)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(3000):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ndf2 = df.assign(yhat=yhat.reshape(-1).detach().tolist())\ndf2\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n      yhat\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n      0.342171\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n      0.599576\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n      0.106441\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n      0.916042\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n      0.910025\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n      0.253134\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n      0.480342\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n      0.397069\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n      0.268198\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n      0.102882\n    \n  \n\n2000 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow')\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 결과시각화\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)},ax=ax[0])\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow',ax=ax[1])\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 교훈: underlying이 엄청 이상해보여도 생각보다 잘 맞춤"
  },
  {
    "objectID": "posts/ap/index.html",
    "href": "posts/ap/index.html",
    "title": "Advanced Probability Theory",
    "section": "",
    "text": "Advanced Probability Theory"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html",
    "title": "5wk: 측도론 (1)_2",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xOLs7lnyb8ZjM3KB-N2u7I"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#수학과의-기호",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#수학과의-기호",
    "title": "5wk: 측도론 (1)_2",
    "section": "수학과의 기호",
    "text": "수학과의 기호\n- 아래는 기호는 몇 가지 영어단어의 축약형이다.\n\nfor all: \\(\\forall\\)\nexists: \\(\\exists\\)\nsuch that, satisfying: \\({\\sf s.t.}\\), \\({\\sf st}\\)\nif-then, implies, therefore: \\(\\Rightarrow\\)\nif and only if: \\(\\Leftrightarrow\\)\nbecause: \\(\\because\\)\ntherefore: \\(\\therefore\\)\nquod erat: \\(\\square\\), \\(\\blacksquare\\)\n\n- 예시1: 모든 실수 \\(x\\)에 대하여, \\(x^2\\)은 양수이다.\n언어\n\nfor any \\(x\\) in \\(\\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor arbitrary \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor any choice of \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor all \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nif \\(x \\in \\mathbb{R}\\), then \\(x^2 \\geq 0\\).\n\n기호\n\n\\(\\forall x \\in \\mathbb{R}\\): \\(x^2\\geq 0\\).\n\\(\\forall x \\in \\mathbb{R}\\), \\(x^2\\geq 0\\).\n\\(x^2 \\geq 0\\), for all \\(x \\in \\mathbb{R}\\).\n\\(x^2 \\geq 0\\), \\(\\forall x \\in \\mathbb{R}\\).\n\\(x \\in \\mathbb{R} \\Rightarrow x^2 \\geq 0\\).\n\n\n거의 쓰는 사람 마음임, 그런데 뉘앙스가 조금씩 다름.\n\n- 예시2: \\(\\Omega\\)의 임의의 부분집합 \\(A\\),\\(B\\)에 대하여, \\(A=B\\) 일 필요충분조건은 \\(A\\subset B\\) 이고 \\(B \\subset A\\) 이어야 한다.\n언어\n\nfor all \\(A,B \\subset \\Omega\\), \\(A=B\\) if and only if (1) \\(A \\subset B\\) and (2) \\(B \\subset A\\).\n\n기호\n\n\\(A = B \\Leftrightarrow A \\subset B \\text{ and } B \\subset A, \\forall A,B \\in \\Omega\\).\n\\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big), \\forall A,B \\in \\Omega\\).\n\\(\\forall A,B \\subset \\Omega\\): \\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big)\\)\n\n\n의미가 때로는 모호할때가 있지만 눈치껏 알아먹어야 한다.\n\n- 예시3: 임의의 양수 \\(\\epsilon>0\\)에 대하여 \\(|x| \\leq \\epsilon\\)이라면 \\(x=0\\)일 수 밖에 없다.\n언어\n\nIf \\(|x|< \\epsilon\\) for all \\(\\epsilon>0\\), then \\(x=0\\).\nIf \\(|x|< \\epsilon\\), \\(\\forall \\epsilon>0\\), then \\(x=0\\).\nFor all \\(\\epsilon>0\\), \\(|x|< \\epsilon\\) implies \\(x=0\\). – 틀린표현\n\n기호\n\n\\(|x| < \\epsilon,~ \\forall \\epsilon>0 \\Rightarrow x=0\\)\n\\(\\forall \\epsilon>0: |x| < \\epsilon \\Rightarrow x=0\\) – 애매하다?\n\\(\\big(\\forall \\epsilon>0:|x| < \\epsilon\\big) \\Rightarrow x=0\\)\n\\(\\big(\\forall \\epsilon>0\\big)\\big(|x| < \\epsilon \\Rightarrow x=0\\big)\\) – 틀린표현"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#기타-약어-및-상투적인-표현",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#기타-약어-및-상투적인-표현",
    "title": "5wk: 측도론 (1)_2",
    "section": "기타 약어 및 상투적인 표현",
    "text": "기타 약어 및 상투적인 표현\n- 약어\n\n\\({\\sf WLOG}\\): Without Loss Of Generality\n\\({\\sf WTS}\\): What/Want To Show\n\\({\\sf iff}\\): if and only if\n\\({\\sf Q.E.D.}\\): 증명완료 (쓰지마..)\n\\({\\sf LHS}\\): Left Hand Side\n\\({\\sf RHS}\\): Right Hand Side\n\n- 상투적인 표현\n\nIt suffices to show that, It is sufficient to show that"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#before",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#before",
    "title": "5wk: 측도론 (1)_2",
    "section": "Before",
    "text": "Before\n- 아래의 기호를 약속\n\n전체집합: \\(\\Omega\\)\n관심있는 집합의 모임: \\({\\cal A} \\subset 2^{\\Omega}\\)\n\n- \\(\\Omega \\neq \\emptyset\\), \\({\\cal A} \\neq \\emptyset\\) 를 가정.\n- 약속: 집합 \\({\\cal A} \\subset 2^{\\Omega}\\)에 대하여 아래와 같은 용어를 약속하자.\n\n\\(\\cap\\)-closed (closed under intersection) or a \\(\\pi\\)-system: \\(\\forall A,B \\in {\\cal A}:~ A \\cap B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cap\\)-closed (closed under countable interserction): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cap_{i=1}^{\\infty} A_i \\in {\\cal A}\\)\n\\(\\cup\\)-closed (closed under unions): \\(\\forall A,B \\in {\\cal A}:~ A\\cup B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cup\\)-closed (closed under countable unois): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cup_{i=1}^{\\infty}A_i \\in {\\cal A}\\)\n＼-closed (closed under differences): \\(\\forall A,B \\in {\\cal A}:~ A-B \\in {\\cal A}\\)\n\\(^c\\)-closed (closed under complements): \\(\\forall A \\in {\\cal A}:~ A^c \\in {\\cal A}\\)\n\n- 우리만의 약속:\n\n앞으로 서로소인 집합들에 대한 합집합은 기호로 \\(\\uplus\\)라고 표현하겠다.\n따라서 앞으로 \\(B_1 \\uplus B_2\\)의 의미는 (1) \\(B_1 \\cup B_2\\) (2) \\(B_1 \\cap B_2 = \\emptyset\\) 을 의미한다고 정의하겠다. (꼭 서로소임을 명시하지 않아도)\n\\(\\sigma\\)-\\(\\uplus\\)-closed 의 의미는 \\(\\uplus_{i=1}^{\\infty}B_i \\in {\\cal A}, \\forall \\{B_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:\\) 의 의미이다.\n\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\) 가 여집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)가 교집합1에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 합집합2에 닫혀있음.\n\\({\\cal A}\\)가 가산교집합3에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 가산합집합4에 닫혀있음.\n\n(증명) 생략\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\)가 차집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)가 가산합집합에 닫혀있다. \\(\\Rightarrow\\) \\({\\cal A}\\)가 가산교집합에 닫혀있다.\n\\(\\forall \\{A_i\\} \\subset {\\cal A},~ \\exists \\{B_i\\} \\subset {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\uplus_{i=1}^{\\infty} B_i\\).5\n\n(증명)\n\nNote: \\(A\\cap B = A-(A-B)\\).\nNote: \\(\\cap_{i=1}^{\\infty}A_i = \\cap_{i=2}^{n}(A_1\\cap A_i)= \\cap_{i=2}^{n}(A_1 - (A_1-A_i))=A_1 - \\cup_{i=2}^{n}(A_1-A_i)\\).\nNote: \\(\\cup_{i=1}^{\\infty}A_i = A_1 \\uplus(A_2-A_1) \\uplus \\big((A_3-A_1) - A_2 \\big) \\uplus \\big(\\big((A_4-A_1)-A_2\\big)-A_3\\big)\\uplus \\cdots\\)\n\n\n차집합에 닫혀있다는 것은 매우 좋은 성질임."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#시그마필드-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#시그마필드-starstarstar",
    "title": "5wk: 측도론 (1)_2",
    "section": "시그마필드 (\\(\\star\\star\\star\\))",
    "text": "시그마필드 (\\(\\star\\star\\star\\))\n- 정의: 시그마필드 (\\(\\sigma\\)-field, \\(\\sigma\\)-algebra)\n집합 \\({\\cal F} \\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal F}\\)를 \\(\\Omega\\)에 대한 시그마필드라고 부른다.\n\n\\(\\Omega \\in {\\cal F}\\).\n\\({\\cal F}\\)는 여집합에 닫혀있다.\n\\({\\cal F}\\)는 가산합집합에 닫혀있다.\n\n- 시그마필드의 정의에서 1을 생략하기도 한다. 이럴 경우는 특별히 \\({\\cal F}\\neq\\emptyset\\)임을 강조한다. 1을 생략할 수 있는 논리는 아래와 같다.\n\n\\({\\cal F}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal F}\\).\n2번 원리에 의하여 \\(A^c \\in {\\cal F}\\).\n시그마필드는 합집합에 닫혀있으므로 \\(A\\cup A^c \\in {\\cal F}\\)."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#알지브라-필드-star",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#알지브라-필드-star",
    "title": "5wk: 측도론 (1)_2",
    "section": "알지브라, 필드 (\\(\\star\\))",
    "text": "알지브라, 필드 (\\(\\star\\))\n- 정의1: 알지브라, 필드 (algebra, field)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 알지브라 역시 1의 조건을 생략하기도 한다.\n- 전체집합을 포함 \\(\\Rightarrow\\) (차집합에 닫혀있음 \\(\\Rightarrow\\) 여집합에 닫혀있음) \\(\\Rightarrow\\) 따라서 대수는 여집합에 닫혀있다.\n- 차집합에 닫혀있음 \\(\\Rightarrow\\) 교집합에 닫혀있게 된다.\n\n혹은 (여집합에 닫혀있음 & 합집합에 닫혀있음) \\(\\Rightarrow\\) 교집합에 닫혀있음.\n\n- 정의2: 알지브라의 또 다른 정의\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\n- 여집합에 닫혀있음 \\(\\Rightarrow\\) (합집합에 닫혀있음 \\(\\Leftrightarrow\\) 교집합에 닫혀있음) \\(\\Rightarrow\\) 2번 조건을 합집합으로 바꿔도 무방\n- 정의3: 알지브라의 또 또 다른 정의 (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 알지브라의 예시\n\n\\(\\Omega = \\{H,T\\}\\), \\({\\cal A} = 2^\\Omega\\) 일때, \\({\\cal A}\\)는 알지브라이다. (\\(|\\Omega| <\\infty\\) 이라면 “시그마필드 = 알지브라(필드)” 이다.)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#링",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#링",
    "title": "5wk: 측도론 (1)_2",
    "section": "링",
    "text": "링\n- 정의: 링 (ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다. (이럴경우 특별히 \\({\\cal A}\\neq \\emptyset\\) 임을 강조한다.)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 원소 \\(A\\)는 가져야 한다.\n\n조건2에 의하여 \\(A-A\\) 역시 \\({\\cal A}\\)의 원소이다.\n\n- 링은 차집합에 닫혀있음 \\(\\Rightarrow\\) 링은 교집합에도 닫혀있음 \\(\\Rightarrow\\) 링은 교집합과 합집합 모두에 닫혀 있다.\n- 링과 알지브라의 차이는 전체집합이 포함되느냐 마느냐임 \\(\\Rightarrow\\) 그런데 이 차이로 인해 알지브라는 여집합에 닫혀있지만 링은 여집합에 닫혀있지 않게 된다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#시그마링",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#시그마링",
    "title": "5wk: 측도론 (1)_2",
    "section": "시그마링",
    "text": "시그마링\n- 정의: 시그마링 (\\(\\sigma\\)-ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 가산합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#세미알지브라-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#세미알지브라-starstarstar",
    "title": "5wk: 측도론 (1)_2",
    "section": "세미알지브라 (\\(\\star\\star\\star\\))",
    "text": "세미알지브라 (\\(\\star\\star\\star\\))\n- 정의1: 세미알지브라 (semi-algebra) // ref : 위키북스\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A,B \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A-B = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 차집합에 반쯤 닫혀있다고 표현한다. 즉 차집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n\\(A \\in {\\cal A}\\)이면 조건3에 의하여 \\(\\emptyset\\)6을 \\({\\cal A}\\)의 원소들의 countable union으로 만들 수 있어야 한다. 이 조건을 만족하기 위해서는 \\(\\emptyset \\in {\\cal A}\\)이어야만 한다.\n\n- 정의2: 세미알지브라의 또 다른 정의 // ref: 세미링의 위키에서 언급, Durret의 정의.\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A^c = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 여집합에 반쯤 닫혀있다고 표현한다. 즉 여집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 이 정의에서도 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n3에 의하여 \\(A^c=\\uplus_{i=1}^{n}B_i\\)를 만족하는 \\(B_1,\\dots, B_n\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n2에 의하여 \\(A \\cap B_1=\\emptyset\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n\n- Note: 정의2의 3번조건은 정의1의 3번조건보다 강한 조건이다. (정의2의 조건3 \\(\\Rightarrow\\) 정의1의 조건3)\n\n증명은 세미링/위키 에서 스스로 확인\n\n- 교재의 정의: 정의2에서 \\(\\Omega \\in {\\cal A}\\)이 생략되어 있음.\n\n왜 생략할 수 있는지 모르겠음. (교재가 틀렸을 수도 있음)\n\n- 세미알지브라의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미알지브라이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\}, \\Omega \\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\}, \\Omega \\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c\\},\\{d\\}, \\Omega \\}\\)\n예시4: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\Omega \\}\\)\n예시5: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\{a,b\\},\\{b,c\\},\\Omega \\}\\)\n\n\n세미알지브라는 전체집합이 몇개의 파티션으로 쪼개져서 원소로 들어가는 느낌이 있음.\n\n- 세미알지브라의 예시\\((\\star)\\): 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n- 세미알지브라가 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라가 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n- 교재의 언급 (p3)\n\n\n\n그림1: 교재에서의 세미알지브라 설명"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#세미링-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#세미링-starstarstar",
    "title": "5wk: 측도론 (1)_2",
    "section": "세미링 \\((\\star\\star\\star)\\)",
    "text": "세미링 \\((\\star\\star\\star)\\)\n- 정의: 세미링\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 차집합에 반쯤 닫혀있다.\n\n- 세미링에서도 공집합포함 조건을 생략할 수 있다.\n- 세미링의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미링이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\}\\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\}\\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset,\\{a,b,c\\},\\{b,c,d\\}, \\{a\\},\\{b,c\\},\\{d\\}\\}\\)\n\n\n전체집합이 포함될 필요가 없는 세미알지브라 느낌임.\n\n- 세미링의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미링이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n\n- 세미링이 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미링이 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\cup \\{\\emptyset\\}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#파이시스템-starstar",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#파이시스템-starstar",
    "title": "5wk: 측도론 (1)_2",
    "section": "파이시스템 (\\(\\star\\star\\))",
    "text": "파이시스템 (\\(\\star\\star\\))\n- 정의: \\(\\pi\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 파이스시템 이라고 부른다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\n- 파이시스템임을 강조하기 위해서 \\({\\cal A}\\) 대신에 \\({\\cal P}\\) 라고 교재에서 표현하기도 한다.\n- 파이시스템의 예시: 아래는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 파이시스템이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\)\n예시3: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\)\n예시4: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#람다시스템-starstar",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#람다시스템-starstar",
    "title": "5wk: 측도론 (1)_2",
    "section": "람다시스템 (\\(\\star\\star\\))",
    "text": "람다시스템 (\\(\\star\\star\\))\n- 정의1: \\(\\lambda\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall B_1,B_2,\\dots \\in {\\cal A}\\) such that \\(B_1,B_2\\dots\\) are disjoint: \\[\\uplus_{i=1}^{\\infty} B_i \\in {\\cal A}\\]\n\n\n람다시스템은 1. 전체집합이 포함되고 2. 두 집합이 포함관계에 있는 경우 차집합에 닫혀있으며 3. 서로소인 가산합집합에 닫혀있다.\n\n- 람다시스템은 여집합에 닫혀있다. 그리고 람다시스템은 공집합을 포함한다.\n- 람다시스템의 느낌: 3주차 시그마필의 motivation에서 소개한 거의 모든 예제는 사실 람다시스템이다.\n\n람다시스템의 원칙1,2,3은 사실 확률의 공리와 깊게 관련되어있음.\n내 생각: 딘킨은 확률의 공리에 착안해서 람다시스템을 만들지 않았을까?\n\n- 아래는 모두 람다시스템의 예시이다.\n\n\\(\\Omega=\\{H,T\\}\\), \\({\\cal L}=\\{\\emptyset, \\{H\\},\\{T\\},\\Omega\\}\\) – 3주차 예제1\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=2^\\Omega\\) – 3주차 예제4\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\}\\) – 3주차 예제5\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1,2,3\\},\\{3,4,5\\},\\Omega\\}\\) – 3주차 예제6\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\Omega\\}\\) – 3주차 예제8\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\{3,4\\}, \\{1,2\\},\\Omega\\}\\) – 3주차 예제9,10\n\\(\\Omega=(0,2\\pi]\\), \\({\\cal L}=\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\{x\\}: x\\in \\mathbb{Q} \\cap \\Omega \\}\\) – 3주차 예제11\n\\(\\Omega=\\{1,2,3,4\\}\\), \\({\\cal L}=\\{\\emptyset, \\{1,2\\}, \\{1,3\\}, \\{1,4\\}, \\{2,3\\}, \\{2,4\\}, \\{3,4\\}, \\Omega\\}\\) – 3주차 예제12에서 교집합 안넣은 버전\n\n- 정의2: \\(\\lambda\\)-system (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(A_1 \\subset A_2 \\subset \\dots\\): \\[\\cup_{i=1}^{\\infty} A_i \\in {\\cal A}\\]\n\n- Note: 정의1의 3번조건과 정의2의 3번조건은 서로 동치관계이다.\n- 교재에서의 파이시스템, 람다시스템 설명\n\n\n\n그림2: 교재에서의 파이시스템과 람다시스템\n\n\n\n위의 정의에서 기호 \\(A_n \\uparrow A\\)의 의미는 “\\(A_1 \\subset A_2 \\subset \\dots\\) and \\(\\cup_{i}^{\\infty}A_i=A\\)”를 뜻하는 축약표현이다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk_2.html#정리",
    "href": "posts/ap/2023-04-05-ap-5wk_2.html#정리",
    "title": "5wk: 측도론 (1)_2",
    "section": "정리",
    "text": "정리\n- 정리표 (hw): 물음표를 채워라\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i \\to \\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nsemi-ring\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nsemi-algebra\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nring\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nalgebra\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-ring\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\\(\\lambda\\)-system\n\\(X\\)\n\\(O\\)\n\\(\\Delta\\)’\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n- 다이어그램 (포함관계)\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n RING  \n\ncluster_1\n\n ALGEBRA  \n\ncluster_2\n\n LAMBDA   \n\nσ－ring\n\n σ－ring   \n\nring\n\n ring   \n\nσ－ring->ring\n\n    \n\nsemiring\n\n semiring   \n\nring->semiring\n\n    \n\nπ－system\n\n π－system   \n\nsemiring->π－system\n\n    \n\nσ－algebra\n\n σ－algebra   \n\nσ－algebra->σ－ring\n\n    \n\nalgebra\n\n algebra   \n\nσ－algebra->algebra\n\n    \n\nλ－system\n\n λ－system   \n\nσ－algebra->λ－system\n\n    \n\nalgebra->ring\n\n    \n\nsemialgebra\n\n semialgebra   \n\nalgebra->semialgebra\n\n    \n\nsemialgebra->semiring\n\n   \n\n\n\n\n\n- 다이어그램 (이해용) – 그림은 더럽지만..\n\n\n\n\n\n\n\nG\n\n \n\ncluster_1\n\n ALGEBRA  \n\ncluster_2\n\n LAMBDA  \n\ncluster_0\n\n RING   \n\nsemiring\n\n semiring   \n\nring\n\n ring   \n\nsemiring->ring\n\n  ∪－stable   \n\nsemialgebra\n\n semialgebra   \n\nsemiring->semialgebra\n\n  Ω－contained   \n\nσ－ring\n\n σ－ring   \n\nring->σ－ring\n\n  σ－∪－stable   \n\nalgebra\n\n algebra   \n\nring->algebra\n\n  Ω－contained   \n\nσ－algebra\n\n σ－algebra   \n\nσ－ring->σ－algebra\n\n  Ω－contained   \n\nsemialgebra->algebra\n\n  ∪－stable   \n\nalgebra->σ－algebra\n\n  σ－∪－stable   \n\nλ－system\n\n λ－system   \n\nλ－system->σ－algebra\n\n  ∩－stable   \n\nπ－system\n\n π－system   \n\nπ－system->semiring\n\n  ＼－semistable"
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html",
    "href": "posts/ap/2023-05-09-ap-10wk.html",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-ymuuOEv4Zru7SF5duhH7dC"
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution",
    "href": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "분포 (distribution)",
    "text": "분포 (distribution)\n- (예제1) – 동전예제\n동전을 던지는 예제로 만들어지는 아래와 같은 확률공간 \\((\\Omega,{\\cal F},P)\\) 를 생각하자.\n\n\\(\\Omega = \\{H,T\\}\\)\n\\({\\cal F} = 2^\\Omega\\)\n\\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\)1"
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-1",
    "href": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-1",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "분포 (distribution)",
    "text": "분포 (distribution)\n확률변수 \\(X:\\Omega \\to \\mathbb{R}\\)를 아래와 같이 정의하자.\n\n\\(X(H)=1\\)\n\\(X(T)=0\\)\n\n이제 \\(B \\in {\\cal R}\\) 에 대하여 아래와 같은 표현들을 고려하자.\n\n\\(P(X \\in B)\\) // 고등학교 부터 쓰던 그 표현\n\\(P(\\{\\omega: X(\\omega) \\in B\\})\\) // 이번에 배운 표현, 표현1의 정확한 버전\n\\(P(X^{-1}(B))\\) // 표현2의 다른 버전, inverse image의 느낌이 확 살아 있음\n\\((P \\circ X^{-1})(B)\\) // 생각해보니까 이것도 가능함. \\(P\\), \\(X\\) 모두 함수였잖아?\n\n표현4를 좀 더 살펴보자. 기호를 간단하게 하기위해서 \\(\\mu_X:=P\\circ X^{-1}\\)로 정의하자.\n\n\\(P(\\emptyset) = 0 \\Leftrightarrow \\mu_X(\\emptyset) = 0\\)\n\\(P(\\{H\\}) = \\frac{1}{2} \\Leftrightarrow \\mu_X(\\{0\\}) = \\frac{1}{2}\\)\n\\(P(\\{T\\}) = \\frac{1}{2} \\Leftrightarrow \\mu_X(\\{1\\}) = \\frac{1}{2}\\)\n\\(P(\\{H,T\\}) = 1 \\Leftrightarrow \\mu_X(\\{0,1\\}) = 1\\)\n\n- (예제2) – 주머니 예제\n주머니에 하얀공과 빨간공이 하나씩 있다고 하자. 주머니에 손을 넣어 이중 하나의 공을 뽑는 시행을 한다고 하자. 이러한 상황으로 만들어지는 아래와 같은 확률공간 \\((\\Omega,{\\cal F},P)\\) 를 생각하자.\n\n\\(\\Omega = \\{R,W\\}\\)\n\\({\\cal F} = 2^\\Omega\\)\n\\(P(\\{R\\})=P(\\{W\\})=\\frac{1}{2}\\)\n\n확률변수 \\(X:\\Omega \\to \\mathbb{R}\\)를 아래와 같이 정의하자.\n\n\\(X(R)=1\\)\n\\(X(W)=0\\)\n\n이제 \\(B \\in {\\cal R}\\) 에 대하여 아래와 같은 표현들을 고려하자.\n\n\\(P(X \\in B)\\)\n\\((P \\circ X^{-1})(B)=\\mu_X(B)\\)\n\n두 표현을 비교하여 살펴보자.\n\n\\(P(\\emptyset) = 0 \\Leftrightarrow \\mu_X(\\emptyset) = 0\\)\n\\(P(\\{R\\}) = \\frac{1}{2} \\Leftrightarrow \\mu_X(\\{0\\}) = \\frac{1}{2}\\)\n\\(P(\\{W\\}) = \\frac{1}{2} \\Leftrightarrow \\mu_X(\\{1\\}) = \\frac{1}{2}\\)\n\\(P(\\{R,W\\}) = 1 \\Leftrightarrow \\mu_X(\\{0,1\\}) = 1\\)"
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-2",
    "href": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-2",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "분포 (distribution)",
    "text": "분포 (distribution)\n- 생각의 시간1: 예제1,2를 관찰하며 생각\n\n예제1,2의 공통속성: 제1과 예제2는 어떠한 공통점이 있다. 비록 outcome, event, \\(\\sigma\\)-field, \\(P\\), \\(X\\) 가 모두 다르지만 사실 어떻게 보면 기호의 차이만 있을 뿐 “확률과 관련된 시행이 어떠한 결과로 나타나는지”에 관련한 본질적인 면에서 같다고 볼 수 있다.2\n\\(\\mu_X\\)가 \\(P\\)보다 예제1,2의 공통속성3을 나타내기에 유리한 것 같은데?"
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-3",
    "href": "posts/ap/2023-05-09-ap-10wk.html#분포-distribution-3",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "분포 (distribution)",
    "text": "분포 (distribution)\n- 생각의 시간2: \\(\\mu_X\\)는 언제나 잘 정의되는가?\n\\((\\Omega,{\\cal F}, P)\\)가 확률공간이고 \\(X:\\Omega \\to \\mathbb{R}\\)이 확률변수라면, \\(\\mu_X:=P\\circ X^{-1}\\)는 언제나 잘 정의된다.\n\n시그마필드: 모든 \\(B \\in {\\cal R}\\)에 대하여 \\(X^{-1}(B)\\)가 시그마필드의 원소가 아닐 수 없다. (만약 그렇다면 \\(X\\)는 확률변수가 아닌걸?)\n메져: 모든 \\(B \\in {\\cal R}\\)에 대하여 \\(P(X^{-1}(B))\\)의 값을 모순되게 정의할 수 없다. (만약 그렇다면 \\((\\Omega, {\\cal F}, P)\\)는 확률공간이 아닌걸?)\n\n결론: \\(\\mu_X\\)는 안전해!\n- 생각의 시간3: \\(\\mu_X\\)도 확률측도의 조건을 만족한다. 구체적으로는 \\((\\mathbb{R}, {\\cal R})\\)에서의 확률측도가 된다. 아래를 체크하자.\n\n정의역: \\(\\mu_X\\)는 시그마필드를 정의역으로 가진다.\n함수값: \\(\\mu_X(\\emptyset)=0\\), \\(\\mu_X(\\mathbb{R})=1\\) 이며 \\(\\mu_X(\\cdot)\\)은 항상 양의값을 가진다.\n\\(\\sigma\\)-add: \\(\\mu_X\\)는 \\({\\cal R}\\)의 모든 서로소인 집합에 대하여 \\(\\sigma\\)-additivity 가 성립한다.\n\n따라서 \\(P\\)가 \\((\\Omega,{\\cal F})\\)에서의 확률측도이듯이 \\(\\mu_X\\)는 \\((\\mathbb{R}, {\\cal R})\\)에서의 확률측도이다.\n- (정의): \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, P)\\)에서 정의된 확률변수라고 하자. 이때 \\(\\mu_X:=P \\circ X^{-1}\\)로 정의가능한 함수 \\(\\mu_X: {\\cal R} \\to [0,1]\\) 를 \\(X\\)의 distribution 라고 부른다.\n\n여기에서 “\\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, P)\\)에서의 확률변수”라는 말이 얼마나 많은 구질구질한 선언을 대신 하는지 생각해보라. 제대로 쓰려면 아마 “\\(\\Omega\\)를 어떠한 실험에 의하여 발생한 outcome들의 집합이라고 하자. 그리고 \\({\\cal F}\\)를 \\(\\Omega\\)에 대한 시그마필드라고 하자. 즉 \\({\\cal F}\\)는 … 을 만족하는 집합이다. \\((\\Omega, {\\cal F})\\)을 묶어서 가측공간이라고 하자. \\(P\\)는 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)에 대한 확률측도라고 하자. 즉 \\(P\\)는 … 를 만족하는 함수이다. 그리고 \\(X\\)는 \\((\\Omega,{\\cal F}) \\to (\\mathbb{R}, {\\cal R})\\)인 확률변수라고 하자. 즉 \\(X\\)는 임의의 \\(B \\in {\\cal R}\\)에 대하여 … 를 만족하는 함수이다. 여기에서 \\({\\cal R}\\)은 Borel sets이다. 즉 \\({\\cal R}\\)은 … 를 만족하는 집합이다.” 와 같은 방식으로 써야할 것이다.\n\n- \\(\\mu_X\\)는 \\((\\mathbb{R}, {\\cal R})\\)에서의 확률측도이므로 \\((\\mathbb{R},{\\cal R},\\mu_X)\\)는 확률공간이 된다. 그런데 \\(\\mu_X\\)가 \\(X\\)에 의하여 정의되므로, 확률공간 \\((\\mathbb{R},{\\cal R},\\mu_X)\\) 역시 \\(X\\)에 의하여 정의되는데 이러한 이유로 확률공간 \\((\\mathbb{R}, {\\cal R}, \\mu_X)\\)를 \\(X\\)에 의하여 유도된 확률공간이라고 표현하기도 한다.\n- \\((\\mathbb{R}, {\\cal R}, \\mu_X)\\)가 \\(X\\)에 의하여 유도된 확률공간이라는 선언의 숨은 의미4: 함수 \\(X\\)가 잘 정의된다면 (\\(X\\)가 확률변수라면!) 공간 \\((\\Omega, {\\cal F}, P)\\)와 공간 \\((\\mathbb{R}, {\\cal R}, \\mu_X)\\)는 대등한 역할을 한다. 즉 \\(\\Omega\\)의 임의의 원소는 \\(\\mathbb{R}\\)의 임의의 원소로 바꾸어 생각할 수 있고, \\({\\cal F}\\)의 임의의 원소는 \\({\\cal R}\\)의 임의의 원소로 대치할 수 있으며, \\({\\cal F}\\)의 임의의 원소(event)를 측도 \\(P\\)로 재는 일은 \\({\\cal R}\\)의 임의의 원소를 측도 \\(\\mu_X\\)로 재는 일과 동치로 해석할 수 있다."
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#분포함수-distribution-function",
    "href": "posts/ap/2023-05-09-ap-10wk.html#분포함수-distribution-function",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "분포함수 (distribution function)",
    "text": "분포함수 (distribution function)\n- 모티브: \\(\\mu_X:{\\cal R} \\to [0,1]\\) 는 정의역이 집합이라서 아쉬움. (솔직히 우리한테 친숙한 형태는 아님) 만약에\n\n집합 \\(\\to\\) 숫자\n\n와 같은 방식으로 랜덤성을 정의하지 않고\n\n숫자 \\(\\to\\) 숫자\n\n와 같은 방식으로 랜덤성을 정의할 수 있다면 어떨까?\n- 결국 랜덤성을 기술하려면 \\(P\\)를 기술해야한다. 그런데 \\(P\\)를 기술하기가 좀 까다로울 경우가 많은데 그것을 단순화 하기 위한 노력의 시작은 카라테오도리의 확장정리였다.5 그리고 이 노력의 마지막은 이제 소개하는 분포함수이다.\n- (예제1) – 동전예제 다시\n동전을 던지는 예제로 만들어지는 아래와 같은 확률공간 \\((\\Omega,{\\cal F},P)\\) 를 생각하자.\n\n\\(\\Omega = \\{H,T\\}\\)\n\\({\\cal F} = 2^\\Omega\\)\n\\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\)\n\n확률변수 \\(X:\\Omega \\to \\mathbb{R}\\)를 아래와 같이 정의하자.\n\n\\(X(H)=1\\)\n\\(X(T)=0\\)\n\n이제 아래와 같은 함수를 정의하자.\n\\[F_X(x)=\\begin{cases} 0 & x<0 \\\\ \\frac{1}{2} & 0\\leq x < 1 \\\\ 1 & 1 \\leq x \\end{cases}\\]\n이 함수는 동전예제가 가지는 랜덤성을 완전히 설명한다. 즉 \\(F_X:\\mathbb{R} \\to [0,1]\\)를 정의하는 일은 \\(P:{\\cal F} \\to [0,1]\\)를 정의하는 일과 동치이다. 왜 그런지 논의하라.\n(해설)\n복습 – 강의노트 06주차 파이시스템에서의 확장이론\n\nThm: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n체크\n\n\\(\\mu_X\\)는 \\((\\mathbb{R}, {\\cal R})\\)에서의 확률측도이다. 따라서 \\((\\mathbb{R}, {\\cal R}, \\mu_X)\\)는 확률공간이다.\n\n진짜해설\n\\(F_X(x)=\\mu_X((-\\infty,x])\\)로 쓸 수 있다. 따라서 모든 실수 \\(x\\in \\mathbb{R}\\)에 대하여 \\(F_X(x)\\)의 값을 정의하는 일은 모든 \\({\\cal A}=\\{(-\\infty,x]: x\\in \\mathbb{R}\\}\\) 에서 \\(\\mu_X:{\\cal A} \\to [0,1]\\) 을 정의하는 일과 동치이다. 그런데 \\({\\cal A}\\)는 파이시스템이므로 \\({\\cal A}\\)에서의 \\(\\mu_X\\)값만 결정해도 \\({\\cal R}\\)의 모든 집합에서의 \\(\\mu_X\\)값이 올바르게 결정된다. 그런데 공간 \\((\\mathbb{R}, {\\cal R}, \\mu_X)\\)는 \\(X\\)에 의하여 유도된 공간이므로 \\((\\mathbb{R}, {\\cal R})\\)에서 \\(\\mu_X\\)를 정의하는 일은 \\(P\\)를 정의하는 일과 같다.\n\n\\(\\mathbb{R}\\)에서 \\(F_X(x)\\)를 정의 \\(\\Leftrightarrow\\) \\({\\cal A}\\)에서 \\(\\mu_X\\)를 정의 \\(\\Leftrightarrow\\) \\({\\cal R}\\)에서 \\(\\mu_X\\)를 정의 \\(\\Leftrightarrow\\) \\({\\cal F}\\)에서 \\(P\\)를 정의\n\n- (정의): \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, P)\\)에서 정의된 확률변수라고 하자. \\(F_X: \\mathbb{R} \\to [0,1]\\) 인 함수를 아래와 같이 정의하자.\n\\[F_X(x) = \\mu_X((-\\infty, x])\\]\n함수 \\(F\\)를 확률변수 \\(X\\)의 distribution function 이라고 한다.\n- 위의 정의에서 함수 \\(F_X(x)\\)를 \\(F_X(x) = P(X\\leq x)\\)로 표현할 수도 있다.6\n- (예제3) – 주사위를 던지는 예제\n분포함수의 위력을 살펴보기 위하여 주사위를 던지는 예제로 만들어지는 아래와 같은 확률공간 \\((\\Omega,{\\cal F},P)\\) 를 생각하자.\n\n\\(\\Omega = \\{1,2,3,4,5,6\\}\\)\n\\({\\cal F} = 2^\\Omega\\)\n\\(P(\\{1\\})=\\dots=P(\\{6\\})=\\frac{1}{6}\\)\n\n확률변수를 \\(X: \\Omega \\to \\mathbb{R}\\)을 \\(X(\\omega)=\\omega\\)와 같이 정의하자. \\(X\\)의 distribution fucntion 을 구하라.\n(풀이)\n생략\n- 약속: \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, P)\\)에서 정의된 확률변수라고 하자. 아래와 같은 표현을 약속하자.\n\n\\(X \\sim \\mu_X\\) \\(\\Leftrightarrow\\) \\(X\\)의 distribution 이 \\(\\mu_X\\)이다.\n\\(X \\sim F_X\\) \\(\\Leftrightarrow\\) \\(X\\)의 distribution function이 \\(F_X\\)이다.\n\n- 약속2: \\(X\\)를 확률공간 \\((\\Omega_X, {\\cal F}_X, P_X)\\)에서 정의된 확률변수라고 하고, \\(Y\\)를 확률공간 \\((\\Omega_Y, {\\cal F}_Y, P_Y)\\)에서 정의된 확률변수라고 하자.\n\n\\(X \\overset{d}{=} Y\\) \\(\\Leftrightarrow\\) \\(\\forall B \\in {\\cal R}: \\mu_X(B) = \\mu_Y(B)\\)\n\\(X \\overset{d}{=} Y\\) \\(\\Leftrightarrow\\) \\(\\forall k \\in {\\mathbb R}: F_X(k) = F_Y(k)\\)\n\\(X \\overset{d}{=} Y\\) \\(\\Leftrightarrow\\) \\(\\forall k \\in {\\mathbb R}: P_X(X\\leq k) = P_Y(Y\\leq k)\\)\n\n\n만약에 랜덤성을 기술하는 언어가 \\(P\\)하나 뿐이었다면 “같은 분포를 가진다”와 같은 개념을 수식화 하기 불리하다.\n\n- Thm: 임의의 분포함수 \\(F:\\mathbb{R} \\to [0,1]\\)는 (1) 비감소 (2) \\(\\lim_{x \\to -\\infty}F(x)=0\\) and \\(\\lim_{x \\to \\infty}F(x)=1\\) (3) 오른쪽연속의 성질을 가진다.\n- Thm: 임의의 함수 \\(F:\\mathbb{R} \\to \\mathbb{R}\\)가 (1) 비감소 (2) \\(\\lim_{x \\to -\\infty}F(x)=0\\) and \\(\\lim_{x \\to \\infty}F(x)=1\\) (3) 오른쪽연속의 성질을 가진다면, \\(F\\)는 어떠한 확률변수 \\(X\\)의 분포함수이다."
  },
  {
    "objectID": "posts/ap/2023-05-09-ap-10wk.html#밀도함수-density-function",
    "href": "posts/ap/2023-05-09-ap-10wk.html#밀도함수-density-function",
    "title": "10wk: 확률변수, 분포 (2)",
    "section": "밀도함수 (density function)",
    "text": "밀도함수 (density function)\n- (정의) \\(X\\)를 확률공간 \\((\\Omega, {\\cal F}, P)\\)에서 정의된 확률변수라고 하고 \\(F_X\\)를 \\(X\\)의 분포함수 라고 하자. 만약에 \\(F_X\\)가 아래와 같은 방식으로 표현된다면 \\(f_X\\)를 \\(X\\)를 밀도함수 (density function) 이라고 한다.\n\\[F_X(x)=\\int_{-\\infty}^xf_X(y)dy\\]\n- 저런 표현이 존재하지 않는다면 어쩌지?"
  },
  {
    "objectID": "posts/ap/2023-03-14-ap-2wk.html",
    "href": "posts/ap/2023-03-14-ap-2wk.html",
    "title": "2주차: 측도론",
    "section": "",
    "text": "강의영상\n\nhttps://youtube.com/playlist?list=PLQqh36zP38-zlQzcT1FJ8lBWRGkqBIsEu\n\n\n\n강의노트 다운로드\n\n\n예비개념1: 귀류법\n- 귀류법: 니 논리 대로면… <- 인터넷 댓글에 많음..\n님 논리대로면..\n- XXX가 문제 없으면 서울 전체가 문제가 없고 (애초에 서울은 문제도 아니라는데 왜 이소리는 하고 계신지 모르겠지만)\n- 수도권 모 대학이 문제가 없으면 전체가 문제가 없겠네요?\n- 지방도 1개 대학이 문제가 없으니 전체가 문제 없겠네요?\n와우! 모든 문제가 해결되었습니다! 출산율 감소로 인한 한국대학의 위기가 해결되었.. 아니 애초에 위기가 없었군요!.\n어휴.. ㅠㅠ\nref: 하이브레인넷\n\n\n예비개념2: 일반화\n- 연필의 정의: 필기도구의 하나. 흑연과 점토의 혼합물을 구워 만든 가느다란 심을 속에 넣고, 겉은 나무로 둘러싸서 만든다. 1565년에 영국에서 처음으로 만들었다.\n- 질문: 아래는 연필인가?\n\n\n\ncardinality\n\nref: https://en.wikipedia.org/wiki/Cardinality\n\n- \\(A=\\{2,4,6\\}\\) \\(\\Rightarrow\\) \\(|A|=3\\), \\(A\\) has a cardinality of 3.\n- \\(A=\\{1,2,3,4,\\dots\\}=\\mathbb{N}\\) \\(\\Rightarrow\\) \\(|A|=?\\)\n\nCardinal number: 유한집합에서의 “갯수”라는 개념을 좀 더 일반화 하여 무한집합으로 적용하고 싶다.\n유한집합: 우리가 친숙한 size 와 그 뜻이 같음\n무한집합: 무한집합의 경우는 그 동작원리가 조금 더 복잡함\n\n- 질문: \\(|\\mathbb{Q}| < |\\mathbb{Q}^c|\\) ??\nBijection, injection and surjection (예비학습)\n\nref: https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection\n\n\n- 용어 정리\n\nsurjective = onto = 전사 = 위로의 함수\ninjective = one-to-one = 단사 = 일대일 함수\nbijective = one-to-one and onto, one-to-one correspondence = 전단사 = 일대일 대응\n\n- 따지는 방법:\n\n단사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 단사함수이다. \\(\\Leftrightarrow\\) \\(\\forall x_1,x_2 \\in X\\): \\(x_1\\neq x_2 \\Rightarrow f(x_1)\\neq f(x_2)\\)\n전사: 함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전사함수이다. \\(\\Leftrightarrow\\) \\(\\forall y \\in Y ~\\exists x \\in X\\) such that \\(f(x)=y\\).\n\n- 성질1: 어떤함수가 전사함수 & 단사함수 \\(\\Rightarrow\\) 전단사함수\n- 성질2:\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 단사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\leq |Y|\\)\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전사함수 \\(f\\)가 존재한다. \\(\\Rightarrow\\) \\(|X| \\geq |Y|\\)\n\n(예비학습 끝)\n- 성질1~2로 유추하면 아래와 같은 사실을 주장 할 수 있지 않을까?\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 향하는 전단사함수가 존재한다 \\(\\Rightarrow\\) \\(|X|=|Y|\\)\n\n- 그렇다면 우리가 주장하고 싶은 것은 아래와 같이 된다.\n\n유리수집합의 무리수집합의 cardinality는 다르다.\n유리수집합과 무리수집합사이의 전단사함수는 존재할 수 없다.\n\n\n\n유리수집합의 카디널리티\n- 우리가 궁극적으로 궁금한 것\n\n유리수집합과 무리수집합의 카디널리티는 다를까?\n\n- 그냥 궁금한 것\n\n자연수의 집합, 비음인 정수의 집합, 음의 정수의 집합, 정수의 집합, 짝수의 집합, 홀수의 집합의 카디널리티는 어떠할까?\n\n- (예제1)\n집합 \\(X=\\{1,2,3\\}\\), \\(Y=\\{2,4,6\\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\n아래의 질문에 대답해보자.\n\n(단사) 함수 \\(f\\)는 정의역의 모든 값에 대해 함수값이 모두 다른가? // \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) 함수 \\(f\\)는 공역=치역인가? // \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\).\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사 함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다.\n- (예제2)\n집합 \\(X=\\{1,2,3,\\dots \\}\\), \\(Y=\\{2,4,6,\\dots \\}\\)을 생각하자. 적당한 함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=2\\)\n\\(f(2)=4\\)\n\\(f(3)=6\\)\n\\(\\dots\\)\n\n아래의 질문에 대답해보자.\n\n(단사) 함수 \\(f\\)는 정의역의 모든 값에 대해 함수값이 모두 다른가? // \\(\\forall x_1,x_2 \\in X\\), \\(x_1\\neq x_2\\) \\(\\Rightarrow\\) \\(f(x_1)\\neq f(x_2)\\)?\n(전사) 함수 \\(f\\)는 공역=치역인가? // \\(\\forall y \\in Y~ \\exists x \\in X\\) such that \\(f(x)=y\\).\n\n1의 질문과 2의 질문이 모두 맞으므로 함수 \\(f\\)는 전단사함수이다. 집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사 함수가 존재하므로 집합 \\(X\\)와 집합 \\(Y\\)의 카디널리티는 동일하다.\n- \\(\\aleph_0\\) (알레프 널, 혹은 알레프 제로라고 읽음)\n\n자연수집합 \\(\\mathbb{N}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{N}|=\\aleph_0\\).\n짝수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이고, 홀수인 자연수 집합의 카디널리티는 \\(\\aleph_0\\)이다.\n정수집합 \\(\\mathbb{Z}\\)의 카디널리티는 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Z}|=\\aleph_0\\).\n\n- 느낌: \\(\\aleph_0\\)를 2배,3배,4배 하여도 \\(\\aleph_0\\)이다.\n\n즉 무한집합의 경우, 본인과 카디널넘버가 같은 진 부분집합이 존재할 수 있다. (유한집합에서는 불가능하겠지)\n무한집합의 정의: 집합 \\(A\\)가 무한집합이다. \\(\\Leftrightarrow\\) \\(A\\)와 동일한 카디널리티를 가지는 \\(A\\)의 진 부분집합이 존재한다.\n\n- (예제3)\n원소의 수가 \\(n\\)인 임의의 유한집합 \\(A\\)에 대하여 \\(|A|=n\\) 이다.\n- (예제4)\n유리수집합의 카디널리티는 얼마인가? (https://en.wikipedia.org/wiki/Rational_number)\n집합 \\(X\\)를 자연수의 집합이라고 하자. 집합 \\(Y\\)를 아래그림에 있는 숫자들의 집합이라고 하자.1\n\n예를들어 집합 \\(X\\)와 집합 \\(Y\\)를 앞의 몇개만 써보면\n\n\\(X=\\{1,2,3,4,5,6,\\dots\\}\\)\n\\(Y=\\{1,\\frac{2}{1},\\frac{1}{2},\\frac{3}{1},\\frac{2}{2},\\frac{1}{3},\\dots \\}\\)\n\n함수 \\(f\\)를 아래와 같이 정의하자.\n\n\\(f(1)=1\\)\n\\(f(2)=2/1\\)\n\\(f(3)=1/2\\)\n\\(f(4)=3/1\\)\n\\(f(5)=2/2\\)\n\\(f(6)=1/3\\)\n\\(\\dots\\)\n\n함수 \\(f\\)는 \\(X\\)에서 \\(Y\\)로 향하는 전단사함수이다. \\(\\Rightarrow\\) \\(|X|=\\aleph_0=|Y|\\)\n(관찰) 임의의 양의 유리수의 집합 \\(\\mathbb{Q}^+\\)는 모두 \\(Y\\)에 포함되어 있다. \\(\\Rightarrow\\) \\(X \\subset \\mathbb{Q}^+ \\subset Y\\) \\(\\Rightarrow\\) \\(|\\mathbb{Q}^+|=\\aleph_0\\)\n(생각) 그럼 음의 유리수의 집합 \\(\\mathbb{Q}^-\\)의 카디널넘버 역시 \\(\\aleph_0\\)이다. 즉 \\(|\\mathbb{Q}^-|=\\aleph_0\\).\n(결론) 그럼 유리수의 카디널넘버는 \\(\\aleph_0\\)이다.2 좀 더 자극적으로 말하면 “자연수의 갯수와 유리수의 갯수는 같다” 라고 말할 수 있다.\n- 조금 무식하게 쓰면 아래와 같이 쓸 수 있다.\n\n\\(\\aleph_0 + 1 = \\aleph_0\\)\n\\(\\aleph_0 \\times 2 = \\aleph_0\\)\n\\(\\aleph_0 \\times \\aleph_0 = \\aleph_0^2 = \\aleph_0\\)\n\n\n\n실수집합의 카디널리티\n- 아래의 관계가 성립했다.\n\n\\(|\\mathbb{N}| = \\aleph_0\\)\n\\(|\\mathbb{N}\\cup \\{0\\}| = \\aleph_0\\)\n\\(|\\mathbb{Z}| = \\aleph_0\\)\n\\(|\\mathbb{Q}| = \\aleph_0\\)\n\n- 그렇다면 아래는 어떠할까?\n\\[|\\mathbb{R}|=??\\]\n(주장) 실수에 포함된 카디널넘버는 유리수의 카디널넘버 보다 크다.\n\n\\(\\mathbb{Q}\\)에서 \\(\\mathbb{R}\\)로 가는 단사함수는 존재하지만 전사함수는 존재할 수 없음을 보이면 된다.\n\\(\\mathbb{N}\\)에서 \\(\\mathbb{R}\\)로 가는 단사함수는 존재하지만 전사함수는 존재할 수 없음을 보여도 상관없다.3\n\n(단사)\n자연수에서 실수로 가는 단사함수는 존재한다. (자연수는 실수의 부분집합이니까)\n(전사)\n소망: \\(\\mathbb{N}\\)에서 \\(\\mathbb{R}\\)로 향하는 전사는 존재할 수 없음을 보이고 싶음.\n소망2: 그런데 \\(\\mathbb{N}\\)에서 \\([0,1]\\)로 향하는 전사가 존재할 수 없음을 보여도 충분함.\n전략: \\(\\mathbb{N}\\)에서 \\([0,1]\\)로 가는 전사가 존재한다고 가정하고 모순을 이끌어 내자.\n1. 아래와 같은 주장을 하는 가상의 인물을 세움:\n\n\\(\\mathbb{N}\\)에서 \\([0,1]\\)로 향하는 전사함수가 존재한다.\n\n2. 그 가상의 인물이 하는 주장을 잘 생각해보면 아래와 같음\n\\(f\\)는 정의역이 자연수이고 공역이 실수인 함수이므로 아래와 같은 형태일 것임.\n\n\\(f(1)=0.2344253456\\cdots\\)\n\\(f(2)=0.3459837981\\cdots\\)\n\\(f(3)=0.5452349871\\cdots\\)\n\\(\\dots\\)\n\n그 가상의 인물의 주장대로라면\n\\[[0,1]=\\{f(1),f(2),f(3),\\dots\\}\\]\n이라는 의미임.4\n3. 전사함수의 정의에 의하여 아래가 성립해야 함\n\n\\(\\forall y\\in [0,1] ~\\exists x \\in \\mathbb{N}\\) such that \\(f(x)=y\\)\n\n아래의 원리에 따라서 \\(y=0.x_1x_2x_3\\cdots\\)를 뽑는다면?\n\n\\(y\\)의 첫번째 소수점의 값 \\(x_1\\)은 \\(f(1)\\)의 첫번째 소수점과 다르게 한다. \\(\\Rightarrow\\) \\(y\\neq f(1)\\) \\(\\Rightarrow\\) \\(y \\notin \\{f(1)\\}\\)\n\\(y\\)의 두번째 소수점의 값 \\(x_2\\)은 \\(f(2)\\)의 두번째 소수점과 다르게 한다. \\(\\Rightarrow\\) \\(y\\neq f(1)\\) and \\(y\\neq f(2)\\) \\(\\Rightarrow\\) \\(y \\notin \\{f(1), f(2)\\}\\)\n\n이러한 \\(y\\)는 분명히 실수이지만 \\(y \\notin \\{f(1),f(2),f(3),\\dots,\\}\\) 이다.5\n\n\n무리수집합의 카디널리티\n(주장) 무리수집합의 카디널리티는 \\(\\aleph_0\\)가 아니다.\n(쉐도복싱) 무리수집합의 카디널리티가 \\(\\aleph_0\\) 이라고 하자.\n\n\\(\\mathbb{R} = \\mathbb{Q} \\cup \\mathbb{Q}^c\\)\n\\(|\\mathbb{Q}|=\\aleph_0\\) 이므로 \\(\\mathbb{Q}\\)와 \\(\\mathbb{N}\\)사이에는 전단사함수가 존재함.\n\\(|\\mathbb{Q}^c|=\\aleph_0\\) 이므로 \\(\\mathbb{Q}^c\\)와 \\(\\mathbb{N}^{-}=\\{-1,-2,\\dots\\}\\)사이에는 전단사함수가 존재함.\n따라서 \\(\\mathbb{Q} \\cup \\mathbb{Q}^c\\) 와 \\(\\mathbb{N} \\cup \\mathbb{N}^-\\) 사이에는 전단사함수가 존재함. (모순)\n\n\n\n\n\n\nFootnotes\n\n\n그래서 일단 집합 \\(Y\\)는 양의 유리수의 집합을 포함한다↩︎\n\\(\\mathbb{Q} = \\mathbb{Q}^+ \\cup \\{0\\} \\cup \\mathbb{Q}^-\\)↩︎\n\\(|\\mathbb{Q}|=|\\mathbb{N}|=\\aleph_0\\)↩︎\n다시 말하면 \\([0,1]\\) 사이의 모든 실수는 “셀수있다”라는 의미임↩︎\n모순이네?↩︎"
  },
  {
    "objectID": "posts/ap/2023-03-07-ap_1wk.html",
    "href": "posts/ap/2023-03-07-ap_1wk.html",
    "title": "1주차: 측도론",
    "section": "",
    "text": "측도론\n\n교재\n\n1,2,3 장 진도나갈 예정!\n\n교수님 lecture note\n\n확률의 공리, 확률이라면 지켜져야 할, 위배되서는 안 될,\n확률의 공리 3개\n\n\\(P(\\Omega) = 1\\)\n\n\n어떤 실험의 결과는 표본공간 \\(\\Omega\\)에서 항상 일어난다.\n\n\n사건 \\(A \\in \\Omega\\)에 대해, \\(0 \\le P(A) \\le 1\\)\n\n\n어느 사건도 확률이 음수가 될 수 없고 1보다 클 수도 없다.\n\n\n서로 배반인 사건 \\(A\\)와 \\(B\\)에 대해, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n서로 배반인 두 사건 \\(A\\)와 \\(B\\)에 대해 합사건의 확률은 각각의 확률의 합과 같다.\n\n\n확률로서 가능한 정의\n\n경우의 수\n면적/길이의 개념\n\n\n\n예제1: 동전\n- \\(\\Omega =\\{H,T\\}\\): sample space\n- \\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\): prob\n- 질문: \\(\\Omega\\)의 임의의(=모든) 부분 집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이 정의할 수 있을까?\n\n당연한거 아냐?\n이게 왜 안돼?\n\n- 질문에 대한 대답\n\n\\(\\Omega\\)의 부분집합: \\(\\emptyset, \\Omega, \\{H\\},\\{T\\}\\)\n\\(P(\\{H\\})=\\frac{1}{2}\\), \\(P(\\{T\\})=\\frac{1}{2}\\), \\(P(\\Omega)=P(\\{H,T\\})=1\\), \\(P(\\emptyset)=0\\)\n\n- 모순없이의 의미?\n\n우리가 상식적으로 확률에 적용가능한 어떠한 연산들이 있음. (확률의 공리 + 기본성질) // 네이버검색\n이러한 연산을 적용해도 상식적인 수준에서 납득이 가야함\n\n(상식적인 연산 적용 예시1)\n\\(\\{H\\} \\subset \\Omega \\Rightarrow P(\\{H\\})<P(\\Omega)\\)\n\n집합 \\(\\{H\\}\\)은 집합 \\(\\Omega\\)보다 작은 집합임\n상식적으로 작은집합이 일어날 확률이 큰 집합이 일어날 확률보다 클 수 없음\n동전 예제의 경우 모든 \\(A,B \\subset \\Omega\\) 에 대하여, \\(A\\subset B\\) 이라면 \\(P(A) < P(B)\\) 가 성립함\n\n(상식적인 연산 적용 예시2)\n\\(\\{H\\} \\cap \\{T\\} = \\emptyset \\Rightarrow P(\\{H\\} \\cup \\{T\\})=P(\\{H\\}) + P(\\{T\\}) =1\\)\n\n우리의 상식에 따르면 \\(A,B\\)가 서로소인 사건이라면 \\(P(A)+P(B)\\)이어야 함.1\n이 예제는 실제로 그러함.\n사실 이 예제의 경우 \\(P(\\{H\\} \\cup \\{T\\})=P(\\Omega)=1\\) 와 같이 계산할 수도 있음.\n하지만 어떠한 방식으로 계산해도 모순이 없음.\n\n\n\n예제2: 바늘이 하나만 있는 시계\n- \\(\\Omega = [0,2\\pi)\\)\n\n시계바늘을 돌려서 나오는 각도를 재는일 \\(\\Leftrightarrow\\) \\([0,2\\pi)\\)사이의 숫자중에 하나를 뽑는 일\n\n- 질문: 바늘을 랜덤으로 돌렸을때 12시-6시 사이에 바늘이 있을 확률? \\(\\frac{1}{2}\\)\n\n\\(\\Omega^* = [0,\\pi)\\)\n\\(P(\\Omega^*)= \\frac{1}{2}\\)\n\n- 계산하는 방법? 아래와 같이 계산하면 가능!!\n\\[\\forall \\Omega^* \\subset \\Omega, \\quad P(\\Omega^*)=\\frac{m(\\Omega^*)}{m(\\Omega)}\\]\n단 여기에서 \\(m\\)은 구간의 길이를 재는 함수라고 하자.\n연습: \\(m\\)의 사용\n\n\\(m(\\Omega)=m\\big([0,2\\pi)\\big)=2\\pi\\)\n\\(m(\\Omega^*) = m\\big([0,\\pi)\\big)= \\pi\\)\n\n- 위와 같은 방식으로 확률을 정의하면 잘 정의될까? 이게 쉽지 않음. 왜냐하면 확률을 잘 정의하기 위해서는\n\n\\(\\Omega\\)의 모든 부분집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 모순없이\n\n정의할 수 있어야 하는데, 이게 쉬운일이 아님.\n(질문0) 그냥 몸풀기 용 질문\n\n\\(\\Omega^*=\\emptyset\\) 일 확률이 얼마인가?\n\n(답변)\n\n0 이야2\n\n(질문1) 첫번째 도전적인 질문\n\n\\(\\Omega^* =\\{0\\}\\)일 확률이 얼마인가?\n\n(답변)\n\n즉 바늘침이 정확하게 12시를 가르킬 확률이 얼마냐는 것\n한 점으로 이루어진 집합 \\(\\{0\\}\\)은 분명히 \\(\\Omega=[0,2\\pi)\\)의 부분집합 이므로 앞서 논의한대로라면 이러한 집합에 대한 확률을 명확하게, 모순없이 정의할 수 있어야 함\n많은 사람들이 이 질문에 대한 답은 \\(0\\) 이라고 알고 있고 그 이유를 “점의 길이는 0 이니까” 라고 이해하고 있음.3\n\n답변이 사실 좀 찝찝해. 바늘침이 정확하게 12시를 가르키는 것은 우리가 분명 하루에 한번씩은 경험하는 사건임. 그런데 그 사건이 일어날 확률은 0이다?4\n(참견질문) 생각해보니까 이런게 있었잖아?\n\\[A \\subset B \\Rightarrow P(A)<P(B)\\]\n그런데 \\(\\emptyset \\subset \\{0\\}\\) 인데 \\(P(\\emptyset)=P(\\{0\\})\\) 이다..?\n(답변)\n\n원래식 \\(A \\subset B \\Rightarrow P(A)\\leq P(B)\\) 이 성립함\n즉 \\(A\\)가 \\(B\\)의 진 부분집합이더라도 \\(P(A)=P(B)\\)인 경우가 존재함.\n\n(질문2) 두번째 질문은 아래와 같다.\n\n그렇다면 사건 \\(\\{0,\\pi\\}\\)가 일어날 확률은 얼마인가?\n\n(답변)\n\n질문을 다시 풀어쓰면 바늘침이 정확하게 12시를 가르키거나 혹은 정확하게 6시를 가르킬 확률이 얼마냐는 것\n따라서 이 질문에 대한 대답은 \\(0+0=0\\) 이므로 \\(0\\)이라고 주장할 수 있음.\n\n(질문3) 세번째 질문은 아래와 같다.\n\n구간 \\([0,2\\pi)\\)는 무수히 많은 점들이 모여서 만들어지는 집합이다. 그런데 점 하나의 길이는 0이다. 0을 무수히 더해도 0이다. 그러므로 구간 \\([0,2\\pi)\\)의 길이도 0이 되어야 한다. 이것은 모순아닌가?\n\n(답변)\n\n까다롭다.\n\\(m([0,2\\pi))=0\\) 임을 인정하면 전체확률은 1이어야 한다는 기본상식5에 어긋나 모순이 생김.\n질문의 논리는 타당해보임. 이 논리의 약점은 딱히 없어보임. 굳이 약점이 있다면 “무한”이라는 개념?\n어쩔수없이 직관에 근거한 약간의 약속을 또 다시 해야할 것 같음. 예를들면 “점들을 유한번 합치면 그냥 많은 점들이지만 무한히 합치면 이것은 선분이 된다. 따라서 길이가 생긴다.” 와 같이.\n우리는 이 약속을 “무한번의 기적”이라고 칭하자.\n\n(질문4) 그렇다면 아래의 질문은 어떻게 대답할 수 있을까?\n\n\\([0,\\pi)\\) 에서 유리수만 뽑아낸 집합이 있다고 생각하자. 편의상 이 집합을 \\(\\mathbb{Q}\\) 라고 하자. 이 집합은 분명히 무한개의 점을 포함하고 있다. 그렇다면 이 집합도 길이가 있는가? 있다면 얼마인가?\n\n(답변)\n\n이미 점들의 길이를 무한번 더하면 길이가 생긴다고 주장한 상태이므로 (무한번의 기적) 길이가 0이라고 주장할 수 없다. 따라서 길이가 있다고 주장해야 한다.\n\\(\\pi\\)말고 딱히 떠오르는 수가 없는데 단순히 길이가 \\(\\pi\\)라고 주장한다면 바로 모순에 빠짐을 알 수 있다.6\n길이는 일단 0보다 커야하고 \\(\\pi\\)보다 작아야함은 자명하므로 그 사이에 있는 어떤 값이 길이라고 주장하자.7\n따라서 (질문4)에 대한 답은 ‘’구체적으로 얼마인지는 모르겠지만 길이가 분명 존재하고 그 길이는 0 보다 크고 \\(\\pi\\) 보다는 작은 어떠한 값 \\(a\\)이다.’’ 정도로 정리할 수 있다.\n즉 \\(m(\\mathbb{Q})=a\\).\n\n\\([0,\\pi] = [0,\\pi) \\cup \\{ \\pi \\}\\)\n둘이 서로소니까\n(질문5) – 외통수\n질문4로부터 만들어지는 논리는 빌드업1-3으로 이어지는 콤보질문을 적절하게 대답하지 못한다. (질문이 좀 길어서 나누어서 설명합니다)\n(빌드업1) – 평행이동은 길이를 변화시키지 않아, 그렇지?\n\n\\(\\mathbb{Q}\\)의 모든점에 \\(\\sqrt{2}\\)를 더한다. 이 점들로 집합을 만들어 \\(\\mathbb{Q}_{\\sqrt{2}}\\)를 만든다.\n여기에서 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}}\\)의 길이는 사실 쉽게 \\(a\\)라고 정의할 수 있음8. 즉, \\(m(\\mathbb{Q}_{\\sqrt{2}})=a\\).\n\n(빌드업2) – 겹치지 않게 평행이동 시킨다음에 길이를 더한다면?\n이제 \\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)를 생각하자. 아래의 성질을 관찰할 수 있다.\n\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 \\(\\Omega\\)의 부분집합 \\(\\Rightarrow\\) 따라서 길이를 명확하고 모순없이 정의할 수 있어야 함\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)의 길이는 각각 \\(a\\)로 정의할 수 있다.9\n\\(P(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3})=P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)10\n\n굳이 \\(P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})\\)를 계산하면 아래와 같이 계산할 수 있겠다.\n\\[P(\\mathbb{Q}_{\\sqrt{2}}) + P(\\mathbb{Q}_{\\sqrt{2}/2})+ P(\\mathbb{Q}_{\\sqrt{2}/3})=\\frac{a}{2\\pi}+\\frac{a}{2\\pi}+\\frac{a}{2\\pi}=3 \\times \\frac{a}{2\\pi}\\]\n(빌드업3) – 그런데 난 겹치지않게 평행이동시킬 방법을 무한대로 알고 있는데?\n눈 여겨볼 점은 아래 식이 성립해야 한다는 것이다. (\\(\\because\\) 확률의 공리)11\n\\[P\\big(\\mathbb{Q}_{\\sqrt{2}} \\cup \\mathbb{Q}_{\\sqrt{2}/2} \\cup \\mathbb{Q}_{\\sqrt{2}/3}\\big) = 3 \\times \\frac{a}{2\\pi} \\leq 1 \\quad \\cdots (\\star)\\]\n\n그런데 \\((\\star)\\)에서 좌변의 값은 편의에 따라서 값을 임의로 키울 수 있다.\n이렇게 임의로 키워진 좌변의 값이라도 항상 그 값은 1보다 작아야 하는데 (확률의 공리), 이게 가능하려면 \\(\\alpha=0\\)인 경우 말고 없다.\n그런데 \\(\\alpha=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n\n그런데 임의로 좌변의 값을 키워도 항상 그 값은 1보다 작아야 하는데 이러한 \\(\\alpha\\)는 0이외에 불가능하다.\n그런데 \\(\\alpha=0\\) 이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n모순을 막기 위한 부등호 \\(<\\) \\(\\le\\)\n\n\n르벡메져\n르벡메져\n- 예제2에서의 마지막 질문은 지금까지 제시한 논리로 방어가 불가능. 이처럼 논리적인 모순없는 체계를 만드는 것은 매우 어려운 일임.\n- 결론적으로 말하면 길이를 재는 함수 \\(m\\)을 아래와 가정하면 위의 모든 질문에 대한 대답을 논리적 모순없이 설계할 수 있다.\n\n한 점에 대한 길이는 \\(0\\) 이다.\n\\([0,2\\pi)\\) 사이의 모든 유리수를 더한 집합은 그 길이가 \\(0\\)이다.\n\\([0,2\\pi)\\) 사이의 모든 무리수를 더한 집합은 그 길이가 \\(2\\pi\\)이다.\n\n참고로 르벡측도(Lebesgue measure)를 사용하면 위의 성질을 만족한다.12 따라서 르벡측도를 활용하여 확률을 정의하는 것이 모순을 최대한 피할 수 있다.\n\n\n\n\n\nFootnotes\n\n\n확률의 공리↩︎\n이걸 좀 더 엄밀하게 따질수도 있는데 일단 직관적으로 0이라 생각하고 넘어가자↩︎\n이해 안되면 약속이라고 생각하자.↩︎\n자연어에서는 “확률=0” 와 “불가능” 은 동일하지만 여기서는 아니다.↩︎\n심지어 이건 확률의 공리↩︎\n왜 모순에 빠지냐면 \\([0,\\pi)\\)에서 무리수만 뽑아낸 집합의 길이가 뭐냐고 물을경우 0이라고 말해야함↩︎\n구체적으로 어떤값인지는 모른다고 하자.↩︎\n평행이동은 길이를 변화시킬 수 없으니까↩︎\n평행이동은 길이를 변화시키지 않으니까↩︎\n\\(\\mathbb{Q}_{\\sqrt{2}},\\mathbb{Q}_{\\sqrt{2}/2},\\mathbb{Q}_{\\sqrt{2}/3}\\)는 모두 서로소 임을 이용↩︎\n첫 등호는 서로소인 사건에 대한 공리, 그다음 부등호는 확률의 총합은 1보다 같거나 작다라는 공리↩︎\n물론 르벡측도의 정의가 위와 같지는 않다↩︎"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html",
    "href": "posts/ap/2023-04-05-ap-5wk.html",
    "title": "5wk: 측도론 (1)",
    "section": "",
    "text": "- 이론: \\(|x|<\\epsilon, ~\\forall \\epsilon>0 ~\\Leftrightarrow x=0\\)\n(증명)\n“\\(\\Leftarrow\\)” 자명함.\n“\\(\\Rightarrow\\)”\n\\({\\sf Suppose}: |x| > 0\\) – 귀류법\n\\({\\sf Choose}:~ \\epsilon=\\frac{1}{2}|x|\\)\n\\(\\Rightarrow\\) \\(0<\\frac{1}{2}|x|<|x|\\)\n\\(\\Rightarrow\\) 모순\n- 이론: \\(|x| < \\frac{1}{n},~ \\forall n \\in \\mathbb{N} ~ \\Leftrightarrow ~x=0\\)\n(증명)\n“\\(\\Leftarrow\\)” 자명함.\n“\\(\\Rightarrow\\)”\n\\({\\sf Suppose}: |x| > 0\\) – 귀류법\n\\({\\sf Choose}:~ \\epsilon=\\frac{1}{2}|x|\\)\n\\(\\Rightarrow\\) \\(0<\\epsilon<|x|\\)\n\\(\\Rightarrow\\) \\(\\exists n: ~0<\\frac{1}{n}<\\epsilon<|x|\\) (\\(\\because\\) 아르키메데스의 성질)\n\\(\\Rightarrow\\) 모순"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#수학과의-기호",
    "href": "posts/ap/2023-04-05-ap-5wk.html#수학과의-기호",
    "title": "5wk: 측도론 (1)",
    "section": "수학과의 기호",
    "text": "수학과의 기호\n- 아래는 기호는 몇 가지 영어단어의 축약형이다.\n\nfor all: \\(\\forall\\)\nexists: \\(\\exists\\)\nsuch that, satisfying: \\({\\sf s.t.}\\), \\({\\sf st}\\)\nif-then, implies, therefore: \\(\\Rightarrow\\)\nif and only if: \\(\\Leftrightarrow\\)\nbecause: \\(\\because\\)\ntherefore: \\(\\therefore\\)\nquod erat: \\(\\square\\), \\(\\blacksquare\\)\n\n- 예시1: 모든 실수 \\(x\\)에 대하여, \\(x^2\\)은 양수이다.\n언어\n\nfor any \\(x\\) in \\(\\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor arbitrary \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor any choice of \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nfor all \\(x \\in \\mathbb{R}\\), \\(x^2 \\geq 0\\).\nif \\(x \\in \\mathbb{R}\\), then \\(x^2 \\geq 0\\).\n\n기호\n\n\\(\\forall x \\in \\mathbb{R}\\): \\(x^2\\geq 0\\).\n\\(\\forall x \\in \\mathbb{R}\\), \\(x^2\\geq 0\\).\n\\(x^2 \\geq 0\\), for all \\(x \\in \\mathbb{R}\\).\n\\(x^2 \\geq 0\\), \\(\\forall x \\in \\mathbb{R}\\).\n\\(x \\in \\mathbb{R} \\Rightarrow x^2 \\geq 0\\).\n\n\n거의 쓰는 사람 마음임, 그런데 뉘앙스가 조금씩 다름.\n\n- 예시2: \\(\\Omega\\)의 임의의 부분집합 \\(A\\),\\(B\\)에 대하여, \\(A=B\\) 일 필요충분조건은 \\(A\\subset B\\) 이고 \\(B \\subset A\\) 이어야 한다.\n언어\n\nfor all \\(A,B \\subset \\Omega\\), \\(A=B\\) if and only if (1) \\(A \\subset B\\) and (2) \\(B \\subset A\\).\n\n기호\n\n\\(A = B \\Leftrightarrow A \\subset B \\text{ and } B \\subset A, \\forall A,B \\subset \\Omega\\).\n\\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big), \\forall A,B \\subset \\Omega\\).\n\\(\\forall A,B \\subset \\Omega\\): \\(A = B \\Leftrightarrow \\big(A \\subset B \\text{ and } B \\subset A\\big)\\)\n\n\n의미가 때로는 모호할때가 있지만 눈치껏 알아먹어야 한다.\n\n- 예시3: 임의의 양수 \\(\\epsilon>0\\)에 대하여 \\(|x| \\leq \\epsilon\\)이라면 \\(x=0\\)일 수 밖에 없다.\n언어\n\nIf \\(|x|< \\epsilon\\) for all \\(\\epsilon>0\\), then \\(x=0\\).\nIf \\(|x|< \\epsilon\\), \\(\\forall \\epsilon>0\\), then \\(x=0\\).\nFor all \\(\\epsilon>0\\), \\(|x|< \\epsilon\\) implies \\(x=0\\). – 틀린표현\n\n기호\n\n\\(|x| < \\epsilon,~ \\forall \\epsilon>0 \\Rightarrow x=0\\)\n\\(\\forall \\epsilon>0: |x| < \\epsilon \\Rightarrow x=0\\) – 애매하다?\n\\(\\big(\\forall \\epsilon>0:|x| < \\epsilon\\big) \\Rightarrow x=0\\)\n\\(\\big(\\forall \\epsilon>0\\big)\\big(|x| < \\epsilon \\Rightarrow x=0\\big)\\) – 틀린표현"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#기타-약어-및-상투적인-표현",
    "href": "posts/ap/2023-04-05-ap-5wk.html#기타-약어-및-상투적인-표현",
    "title": "5wk: 측도론 (1)",
    "section": "기타 약어 및 상투적인 표현",
    "text": "기타 약어 및 상투적인 표현\n- 약어\n\n\\({\\sf WLOG}\\): Without Loss Of Generality\n\\({\\sf WTS}\\): What/Want To Show\n\\({\\sf iff}\\): if and only if\n\\({\\sf Q.E.D.}\\): 증명완료 (쓰지마..)\n\\({\\sf LHS}\\): Left Hand Side\n\\({\\sf RHS}\\): Right Hand Side\n\n- 상투적인 표현\n\nIt suffices to show that, It is sufficient to show that"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#before",
    "href": "posts/ap/2023-04-05-ap-5wk.html#before",
    "title": "5wk: 측도론 (1)",
    "section": "Before",
    "text": "Before\n- 아래의 기호를 약속\n\n전체집합: \\(\\Omega\\)\n관심있는 집합의 모임: \\({\\cal A} \\subset 2^{\\Omega}\\)\n\n- \\(\\Omega \\neq \\emptyset\\), \\({\\cal A} \\neq \\emptyset\\) 를 가정.\n- 약속: 집합 \\({\\cal A} \\subset 2^{\\Omega}\\)에 대하여 아래와 같은 용어를 약속하자.\n\n\\(\\cap\\)-closed (closed under intersection) or a \\(\\pi\\)-system: \\(\\forall A,B \\in {\\cal A}:~ A \\cap B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cap\\)-closed (closed under countable interserction): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cap_{i=1}^{\\infty} A_i \\in {\\cal A}\\)\n\\(\\cup\\)-closed (closed under unions): \\(\\forall A,B \\in {\\cal A}:~ A\\cup B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cup\\)-closed (closed under countable unois): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cup_{i=1}^{\\infty}A_i \\in {\\cal A}\\)\n＼-closed (closed under differences): \\(\\forall A,B \\in {\\cal A}:~ A-B \\in {\\cal A}\\)\n\\(^c\\)-closed (closed under complements): \\(\\forall A \\in {\\cal A}:~ A^c \\in {\\cal A}\\)\n\n- 우리만의 약속:\n\n앞으로 서로소인 집합들에 대한 합집합은 기호로 \\(\\uplus\\)라고 표현하겠다.\n따라서 앞으로 \\(B_1 \\uplus B_2\\)의 의미는 (1) \\(B_1 \\cup B_2\\) (2) \\(B_1 \\cap B_2 = \\emptyset\\) 을 의미한다고 정의하겠다. (꼭 서로소임을 명시하지 않아도)\n\\(\\sigma\\)-\\(\\uplus\\)-closed 의 의미는 \\(\\uplus_{i=1}^{\\infty}B_i \\in {\\cal A}, \\forall \\{B_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:\\) 의 의미이다.\n\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\) 가 여집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)가 교집합1에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 합집합2에 닫혀있음.\n\\({\\cal A}\\)가 가산교집합3에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 가산합집합4에 닫혀있음.\n\n(증명) 생략\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\)가 차집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)가 가산합집합에 닫혀있다. \\(\\Rightarrow\\) \\({\\cal A}\\)가 가산교집합에 닫혀있다.\n\\(\\forall \\{A_i\\} \\subset {\\cal A},~ \\exists \\{B_i\\} \\subset {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\uplus_{i=1}^{\\infty} B_i\\).5\n\n(증명)\n\nNote: \\(A\\cap B = A-(A-B)\\).\nNote: \\(\\cap_{i=1}^{\\infty}A_i = \\cap_{i=2}^{n}(A_1\\cap A_i)= \\cap_{i=2}^{n}(A_1 - (A_1-A_i))=A_1 - \\cup_{i=2}^{n}(A_1-A_i)\\).\nNote: \\(\\cup_{i=1}^{\\infty}A_i = A_1 \\uplus(A_2-A_1) \\uplus \\big((A_3-A_1) - A_2 \\big) \\uplus \\big(\\big((A_4-A_1)-A_2\\big)-A_3\\big)\\uplus \\cdots\\)\n\n\n차집합에 닫혀있다는 것은 매우 좋은 성질임."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#시그마필드-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk.html#시그마필드-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "시그마필드 (\\(\\star\\star\\star\\))",
    "text": "시그마필드 (\\(\\star\\star\\star\\))\n- 정의: 시그마필드 (\\(\\sigma\\)-field, \\(\\sigma\\)-algebra)\n집합 \\({\\cal F} \\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal F}\\)를 \\(\\Omega\\)에 대한 시그마필드라고 부른다.\n\n\\(\\Omega \\in {\\cal F}\\).\n\\({\\cal F}\\)는 여집합에 닫혀있다.\n\\({\\cal F}\\)는 가산합집합에 닫혀있다.\n\n- 시그마필드의 정의에서 1을 생략하기도 한다. 이럴 경우는 특별히 \\({\\cal F}\\neq\\emptyset\\)임을 강조한다. 1을 생략할 수 있는 논리는 아래와 같다.\n\n\\({\\cal F}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal F}\\).\n2번 원리에 의하여 \\(A^c \\in {\\cal F}\\).\n시그마필드는 합집합에 닫혀있으므로 \\(A\\cup A^c \\in {\\cal F}\\)."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#알지브라-필드-star",
    "href": "posts/ap/2023-04-05-ap-5wk.html#알지브라-필드-star",
    "title": "5wk: 측도론 (1)",
    "section": "알지브라, 필드 (\\(\\star\\))",
    "text": "알지브라, 필드 (\\(\\star\\))\n- 정의1: 알지브라, 필드 (algebra, field)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 알지브라 역시 1의 조건을 생략하기도 한다.\n- 전체집합을 포함 \\(\\Rightarrow\\) (차집합에 닫혀있음 \\(\\Rightarrow\\) 여집합에 닫혀있음) \\(\\Rightarrow\\) 따라서 대수는 여집합에 닫혀있다.\n- 차집합에 닫혀있음 \\(\\Rightarrow\\) 교집합에 닫혀있게 된다.\n\n혹은 (여집합에 닫혀있음 & 합집합에 닫혀있음) \\(\\Rightarrow\\) 교집합에 닫혀있음.\n\n- 정의2: 알지브라의 또 다른 정의\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\n- 여집합에 닫혀있음 \\(\\Rightarrow\\) (합집합에 닫혀있음 \\(\\Leftrightarrow\\) 교집합에 닫혀있음) \\(\\Rightarrow\\) 2번 조건을 합집합으로 바꿔도 무방\n- 정의3: 알지브라의 또 또 다른 정의 (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 알지브라의 예시\n\n\\(\\Omega = \\{H,T\\}\\), \\({\\cal A} = 2^\\Omega\\) 일때, \\({\\cal A}\\)는 알지브라이다. (\\(|\\Omega| <\\infty\\) 이라면 “시그마필드 = 알지브라(필드)” 이다.)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#링",
    "href": "posts/ap/2023-04-05-ap-5wk.html#링",
    "title": "5wk: 측도론 (1)",
    "section": "링",
    "text": "링\n- 정의: 링 (ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다. (이럴경우 특별히 \\({\\cal A}\\neq \\emptyset\\) 임을 강조한다.)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 원소 \\(A\\)는 가져야 한다.\n\n조건2에 의하여 \\(A-A\\) 역시 \\({\\cal A}\\)의 원소이다.\n\n- 링은 차집합에 닫혀있음 \\(\\Rightarrow\\) 링은 교집합에도 닫혀있음 \\(\\Rightarrow\\) 링은 교집합과 합집합 모두에 닫혀 있다.\n- 링과 알지브라의 차이는 전체집합이 포함되느냐 마느냐임 \\(\\Rightarrow\\) 그런데 이 차이로 인해 알지브라는 여집합에 닫혀있지만 링은 여집합에 닫혀있지 않게 된다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#시그마링",
    "href": "posts/ap/2023-04-05-ap-5wk.html#시그마링",
    "title": "5wk: 측도론 (1)",
    "section": "시그마링",
    "text": "시그마링\n- 정의: 시그마링 (\\(\\sigma\\)-ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 가산합집합에 닫혀있다.\n\n- 여기에서 1의 조건을 생략할 수 있다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#세미알지브라-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk.html#세미알지브라-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "세미알지브라 (\\(\\star\\star\\star\\))",
    "text": "세미알지브라 (\\(\\star\\star\\star\\))\n- 정의1: 세미알지브라 (semi-algebra) // ref : 위키북스\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A,B \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A-B = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 차집합에 반쯤 닫혀있다고 표현한다. 즉 차집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n\\(A \\in {\\cal A}\\)이면 조건3에 의하여 \\(\\emptyset\\)6을 \\({\\cal A}\\)의 원소들의 countable union으로 만들 수 있어야 한다. 이 조건을 만족하기 위해서는 \\(\\emptyset \\in {\\cal A}\\)이어야만 한다.\n\n- 정의2: 세미알지브라의 또 다른 정의 // ref: 세미링의 위키에서 언급, Durret의 정의.\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A^c = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 여집합에 반쯤 닫혀있다고 표현한다. 즉 여집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 이 정의에서도 세미알지브라는 공집합을 포함한다. (이때 \\({\\cal A}\\neq \\emptyset\\)임을 강조함)\n\n\\({\\cal A}\\)는 공집합이 아니므로 최소한 하나의 집합 \\(A\\)는 포함해야 한다. 즉 \\(A \\in {\\cal A}\\).\n3에 의하여 \\(A^c=\\uplus_{i=1}^{n}B_i\\)를 만족하는 \\(B_1,\\dots, B_n\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n2에 의하여 \\(A \\cap B_1=\\emptyset\\) 역시 \\({\\cal A}\\)에 포함되어야 한다.\n\n- Note: 정의2의 3번조건은 정의1의 3번조건보다 강한 조건이다. (정의2의 조건3 \\(\\Rightarrow\\) 정의1의 조건3)\n\n증명은 세미링/위키 에서 스스로 확인\n\n- 교재의 정의: 정의2에서 \\(\\Omega \\in {\\cal A}\\)이 생략되어 있음.\n\n왜 생략할 수 있는지 모르겠음. (교재가 틀렸을 수도 있음)\n\n- 세미알지브라의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미알지브라이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\}, \\Omega \\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\}, \\Omega \\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c\\},\\{d\\}, \\Omega \\}\\)\n예시4: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\Omega \\}\\)\n예시5: \\(\\Omega=\\{a,b,c,d\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c\\},\\{d\\}, \\{a,b\\},\\{b,c\\},\\Omega \\}\\)\n\n\n세미알지브라는 전체집합이 몇개의 파티션으로 쪼개져서 원소로 들어가는 느낌이 있음.\n\n- 세미알지브라의 예시\\((\\star)\\): 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty \\leq a < b \\leq \\infty \\}\\)\n\n- 세미알지브라가 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라가 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty \\leq a < b \\leq \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty \\leq a < b \\leq \\infty \\}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#세미링-starstarstar",
    "href": "posts/ap/2023-04-05-ap-5wk.html#세미링-starstarstar",
    "title": "5wk: 측도론 (1)",
    "section": "세미링 \\((\\star\\star\\star)\\)",
    "text": "세미링 \\((\\star\\star\\star)\\)\n- 정의: 세미링\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 차집합에 반쯤 닫혀있다.\n\n- 세미알지브라의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega\\)에 대한 세미알지브라이다.\n\n예시1: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b,c,d\\} \\}\\)\n예시2: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset, \\{a\\},\\{b\\},\\{c,d\\} \\}\\)\n예시3: \\(\\Omega=\\{a,b,c,d,e,f\\}\\), \\({\\cal A} = \\{\\emptyset,\\{a,b,c\\},\\{b,c,d\\}, \\{a\\},\\{b,c\\},\\{d\\},\\}\\)\n\n\n전체집합이 포함될 필요가 없는 세미알지브라 느낌임.\n\n- 세미알지브라의 예시: 아래의 \\({\\cal A}\\)는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\)\n\n- 세미알지브라가 아닌 예시: 아래의 \\({\\cal A}\\)는 \\(\\Omega=\\mathbb{R}\\)에 대한 세미알지브라가 아니다.\n\n예시1: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#파이시스템-starstar",
    "href": "posts/ap/2023-04-05-ap-5wk.html#파이시스템-starstar",
    "title": "5wk: 측도론 (1)",
    "section": "파이시스템 (\\(\\star\\star\\))",
    "text": "파이시스템 (\\(\\star\\star\\))\n- 정의: \\(\\pi\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 파이스시템 이라고 부른다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\n- 파이시스템임을 강조하기 위해서 \\({\\cal A}\\) 대신에 \\({\\cal P}\\) 라고 교재에서 표현하기도 한다.\n- 파이시스템의 예시: 아래는 모두 \\(\\Omega=\\mathbb{R}\\)에 대한 파이시스템이다.\n\n예시1: \\({\\cal A} = \\{(a,b]: -\\infty < a < b < \\infty \\}\\)\n예시2: \\({\\cal A} = \\{[a,b): -\\infty < a < b < \\infty \\}\\)\n예시3: \\({\\cal A} = \\{(a,b): -\\infty < a < b < \\infty \\}\\)\n예시4: \\({\\cal A} = \\{[a,b]: -\\infty < a < b < \\infty \\}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#람다시스템-starstar",
    "href": "posts/ap/2023-04-05-ap-5wk.html#람다시스템-starstar",
    "title": "5wk: 측도론 (1)",
    "section": "람다시스템 (\\(\\star\\star\\))",
    "text": "람다시스템 (\\(\\star\\star\\))\n- 정의1: \\(\\lambda\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall B_1,B_2,\\dots \\in {\\cal A}\\) such that \\(B_1,B_2\\dots\\) are disjoint: \\[\\uplus_{i=1}^{\\infty} B_i \\in {\\cal A}\\]\n\n\n람다시스템은 1. 전체집합이 포함되고 2. 두 집합이 포함관계에 있는 경우 차집합에 닫혀있으며 3. 서로소인 가산합집합에 닫혀있다.\n\n- 람다시스템임을 강조하기위해서 \\({\\cal A}\\) 대신에 \\({\\cal L}\\) 이라고 교재에서 표현하기도 한다.\n- 람다시스템의 느낌: 3주차 시그마필드의 motivation에서 소개한 거의 모든 예제는 사실 람다시스템이다.\n\n람다시스템의 원칙1,2,3은 사실 확률의 공리와 깊게 관련되어있음.\n내 생각: 딘킨은 확률의 공리에 착안해서 람다시스템을 만들지 않았을까?\n\n- 아래는 모두 람다시스템의 예시이다.\n\n\\(\\Omega=\\{H,T\\}\\), \\({\\cal L}=\\{\\emptyset, \\{H\\},\\{T\\},\\Omega\\}\\) – 3주차 예제1\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=2^\\Omega\\) – 3주차 예제4\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\}\\) – 3주차 예제5\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1,2,3\\},\\{3,4,5\\},\\Omega\\}\\) – 3주차 예제6\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\Omega\\}\\) – 3주차 예제8\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\), \\({\\cal L}=\\{\\emptyset,\\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\{3,4\\}, \\{1,2\\},\\Omega\\}\\) – 3주차 예제9,10\n\\(\\Omega=(0,2\\pi]\\), \\({\\cal L}=\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\{x\\}: x\\in \\mathbb{Q} \\cap \\Omega \\}\\) – 3주차 예제11\n\\(\\Omega=\\{1,2,3,4\\}\\), \\({\\cal L}=\\{\\emptyset, \\{1,2\\}, \\{1,3\\}, \\{1,4\\}, \\{2,3\\}, \\{2,4\\}, \\{3,4\\}, \\Omega\\}\\) – 3주차 예제12에서 교집합 안넣은 버전\n\n- 정의2: \\(\\lambda\\)-system (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(A_1 \\subset A_2 \\subset \\dots\\): \\[\\cup_{i=1}^{\\infty} A_i = A \\in {\\cal A}\\]\n\n- Note: 정의1의 3번조건과 정의2의 3번조건은 서로 동치관계이다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#정리",
    "href": "posts/ap/2023-04-05-ap-5wk.html#정리",
    "title": "5wk: 측도론 (1)",
    "section": "정리",
    "text": "정리\n- 정리표 (hw): 물음표를 채워라\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i=\\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nsemi-ring\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nsemi-algebra\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(O\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nring\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nalgebra\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\sigma\\)-ring\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\lambda\\)-system\n\\(?\\)\n\\(?\\)\n\\(\\Delta'\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n- 다이어그램 (포함관계)\n\n\n\n\n\n\n\nG\n\n \n\ncluster_0\n\n RING  \n\ncluster_1\n\n ALGEBRA  \n\ncluster_2\n\n LAMBDA   \n\nσ－ring\n\n σ－ring   \n\nring\n\n ring   \n\nσ－ring->ring\n\n    \n\nsemiring\n\n semiring   \n\nring->semiring\n\n    \n\nπ－system\n\n π－system   \n\nsemiring->π－system\n\n    \n\nσ－algebra\n\n σ－algebra   \n\nσ－algebra->σ－ring\n\n    \n\nalgebra\n\n algebra   \n\nσ－algebra->algebra\n\n    \n\nλ－system\n\n λ－system   \n\nσ－algebra->λ－system\n\n    \n\nalgebra->ring\n\n    \n\nsemialgebra\n\n semialgebra   \n\nalgebra->semialgebra\n\n    \n\nsemialgebra->semiring\n\n   \n\n\n\n\n\n- 다이어그램 (이해용) – 그림은 더럽지만..\n\n\n\n\n\n\n\nG\n\n \n\ncluster_2\n\n LAMBDA  \n\ncluster_1\n\n ALGEBRA  \n\ncluster_0\n\n RING   \n\nsemiring\n\n semiring   \n\nring\n\n ring   \n\nsemiring->ring\n\n  ∪－stable   \n\nsemialgebra\n\n semialgebra   \n\nsemiring->semialgebra\n\n  Ω－contained   \n\nσ－ring\n\n σ－ring   \n\nring->σ－ring\n\n  σ－∪－stable   \n\nalgebra\n\n algebra   \n\nring->algebra\n\n  Ω－contained   \n\nσ－algebra\n\n σ－algebra   \n\nσ－ring->σ－algebra\n\n  Ω－contained   \n\nsemialgebra->algebra\n\n  ∪－stable   \n\nalgebra->σ－algebra\n\n  σ－∪－stable   \n\nλ－system\n\n λ－system   \n\nλ－system->σ－algebra\n\n  ∩－stable   \n\nπ－system\n\n π－system   \n\nπ－system->semiring\n\n  ＼－semistable"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#준비",
    "href": "posts/ap/2023-04-05-ap-5wk.html#준비",
    "title": "5wk: 측도론 (1)",
    "section": "준비",
    "text": "준비\n- 예제1: (\\(\\star\\)) 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실에 대하여 참 거짓을 판단하라.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\n(증명)\n편의상 \\({\\cal F}= \\cap_{i \\in I} {\\cal F}_i\\) 라고 하자. \\({\\cal F}\\)가 시그마필드임을 보이기 위해서는\n\n\\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(A_1,A_2 \\dots \\in {\\cal F} \\Rightarrow \\cup_{i}A_i \\in {\\cal F}\\)\n\n만 보이면 된다. (이럴때는 전체집합 조건하나를 빼는게 유리하다)\n1번체크\n\\(A \\in {\\cal F} \\Rightarrow \\forall i: A \\in {\\cal F}_i \\Rightarrow \\forall i: A^c \\in {\\cal F}_i \\Rightarrow A^c \\in {\\cal F}\\)\n2번체크\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\forall i: A_1,A_2,\\dots \\in {\\cal F}_i \\Rightarrow \\forall i: \\cup_jA_j \\in {\\cal F}_i \\Rightarrow \\cup_jA_j \\in {\\cal F}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#증명",
    "href": "posts/ap/2023-04-05-ap-5wk.html#증명",
    "title": "5wk: 측도론 (1)",
    "section": "증명",
    "text": "증명\n- 귀찮아서 만든 이론1: 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다. (그리고 당연히 smallest 조건에 의에서 유일성이 보장됨)\n(엄밀하지 않은 증명)\n존재성\n존재야 하겠지..\n유일성\n\\({\\sf LET}\\): \\({\\cal A}\\)를 포함하는 시그마필드를 구하고 \\({\\cal F}_1\\)이라고 하자. 또 \\({\\cal A}\\)를 포함하는 시그마필드를 구하고 \\({\\cal F}_2\\), \\({\\cal F}_3\\) 등으로 놓는다.\n\\({\\sf LET}\\): 이제 \\({\\cal F} = {\\cal F}_1 \\cap {\\cal F}_2 \\cap \\dots = \\bigcap_{i=1}^{\\infty} {\\cal F}_i\\)를 정의하자.\n\\({\\sf WTS}\\):\n\n\\({\\cal F}\\)는 \\({\\cal A}\\)를 포함한다. 즉 \\({\\cal A} \\subset {\\cal F}\\) 이다.\n\\({\\cal F}\\)는 시그마필드이다.\n\n2는 이미 공부한 내용임. 1은 \\(A \\in {\\cal A} \\Rightarrow A \\in {\\cal F}_i, \\forall i \\Rightarrow A \\in {\\cal F}\\) 이므로 클리어\n(엄밀하지 않은 이유)\n위의 증명은 마치 \\({\\cal A}\\)를 포함하는 시그마필드가 countable 인 것 같은 착각을 준다. 하지만 uncoutable도 가능하다.\n- 아래는 교재의 언급 (p3)\n\n\n\n그림1: Durret교재에서 언급된 “귀찮아서 만든 이론1”"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#준비과정",
    "href": "posts/ap/2023-04-05-ap-5wk.html#준비과정",
    "title": "5wk: 측도론 (1)",
    "section": "준비과정",
    "text": "준비과정\n- 이론: 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실이 성립한다.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\\({\\cal A}_i\\)가 모두 시그마링, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 시그마링이다.\n\\({\\cal A}_i\\)가 모두 알지브라라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 알지브라이다.\n\\({\\cal A}_i\\)가 모두 링이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 링이다.\n\\({\\cal A}_i\\)가 모두 람다시스템이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 람다시스템이다.\n\n\n세미알지브라, 세미링은 성립안함.\n\n\n파이시스템은 애매함. \\(\\cap_{i \\in I}{\\cal A}_i\\)가 공집합이 아니면 성립함.\n\n- 예제1: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A}_1 = \\{\\emptyset, \\{1\\}, \\{2,3\\}, \\{4\\}, \\Omega\\}\\)\n\\({\\cal A}_2 = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{3,4\\}, \\Omega\\}\\)\n\n\\({\\cal A}_1, {\\cal A}_2\\)는 모두 세미알지브라이다. 하지만 \\({\\cal A}_1 \\cap {\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)은 세미알지브라가 아니다.\n\n이 예제에서 세미알지브라를 세미링으로 바꾸고 읽어도 성립함.\n\n- 이론: 임의의 \\({\\cal A}\\)에 대하여 아래는 존재한다.\n\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마필드, \\(\\sigma({\\cal A})\\)\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마링\n\\({\\cal A}\\)를 포함하는 가장 작은 알지브라\n\\({\\cal A}\\)를 포함하는 가장 작은 링\n\\({\\cal A}\\)를 포함하는 가장 작은 람다시스템, \\(l({\\cal A})\\)\n\n- 참고: “\\({\\cal A}\\)를 포함하는 가장 작은 세미링”, 혹은 “\\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라”와 같은 것은 존재하지 않음.\n- 예제2: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A} = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)\n\n이때 \\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라가\n\\[{\\cal A}_1 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2,3,4\\}\\}\\]\n라고 주장할 수는 없음. 왜냐하면\n\\[{\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2\\},\\{3\\},\\{4\\}\\}\\]\n역시 \\({\\cal A}\\)를 포함하는 세미알지브라이지만 \\({\\cal A}_1 \\not \\subset {\\cal A}_2\\)이므로.\n- 이론: \\({\\cal P}\\)가 파이시스템이라고 하자. 아래가 성립한다.\n\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마필드는 그 자체로 파이시스템이다. (즉 \\(\\sigma({\\cal P})\\)는 파이시스템이다)\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 알지브라는 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 람다시스템은 그 자체로 파이시스템이다?? (즉 \\(l({\\cal P})\\)는 파이시스템이다?)\n\n- 1-4는 자명한데, 5는 자명하지 않다. 하지만 성립한다. (5의 증명은 복잡함. 그냥 암기하자.)\n- 이론: \\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n(증명) 아래의 표를 살펴보면 간단하게 증명가능하다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i=\\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(\\lambda\\)-system\n\\(X\\)\n\\(O\\)\n\\(\\Delta'\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#딘킨의-pi-lambda-정리",
    "href": "posts/ap/2023-04-05-ap-5wk.html#딘킨의-pi-lambda-정리",
    "title": "5wk: 측도론 (1)",
    "section": "딘킨의 \\(\\pi-\\lambda\\) 정리",
    "text": "딘킨의 \\(\\pi-\\lambda\\) 정리\n- 이론: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver1. (\\(\\star\\))\n\\({\\cal P}\\)가 파이시스템이면 \\(l({\\cal P})=\\sigma({\\cal P})\\)이다.\n(증명)\n\\(l(\\cal P) \\subset \\sigma({\\cal P})\\) 임을 보이고, \\(l(\\cal P) \\supset \\sigma({\\cal P})\\) 임을 보이면된다.\n“\\(\\subset\\)”: 당연하다.7\n“\\(\\supset\\)”: \\(l({\\cal P})\\)가 시그마필드임을 보이면 자동으로 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)임이 보여진다.\n\\(l({\\cal P})\\)이 시그마필드임은 아래를 조합하면 간단히 증명된다.\n\n파이시스템 \\({\\cal P}\\)를 포함하는 가장 작은 람다시스템 \\(l({\\cal P})\\)은 그 자체로 파이시스템이다.\n\\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n\n- 생각의 시간\n\n시그마필드(=잴 수 있는 집합의 모임)을 만들기 위해서는, 그 모임(=collection)이 파이시스템이면서 동시에 람다시스템임을 보이면 된다.\n딘킨의 정리는 적당한 파이시스템을 만들고 그것을 통하여 잴 수 있는 집합의 모임을 확률의 공리에 맞게만 설정한다면, 그것이 시그마필드가 된다는 것을 보이는 것이다.\n\n- 제 생각\n\n메져가 “선분의 길이”를 일반화 하는 개념이라 생각한다면 파이시스템에서 시작하여 시그마필드로 확장하는 것이 자연스럽다.\n메져가 “확률”을 일반화하는 개념이라 생각한다면 람다시스템에서 시작하는게 자연스럽다.8\n딘킨의 \\(\\pi-\\lambda\\) 정리는 두 흐름을 합치는 정리이다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#알-필요-없는-거-책에-있어서-한다",
    "href": "posts/ap/2023-04-05-ap-5wk.html#알-필요-없는-거-책에-있어서-한다",
    "title": "5wk: 측도론 (1)",
    "section": "알 필요 없는 거 (책에 있어서 한다)",
    "text": "알 필요 없는 거 (책에 있어서 한다)\n- 이론: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.\n\\({\\cal P}\\)가 파이시스템이고 \\({\\cal L}\\)이 \\({\\cal P}\\)를 포함하는 람다시스템이라면 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)이다.\n(설명)\nDurret에 나온 딘킨의 \\(\\pi-\\lambda\\) thm 이다. 굉장히 불친절한 편인데, ver2가 증명되면 ver1은 자명하게9 임플라이 되므로 ver2를 대신 state한 것이다.\n\nver2가 ver1를 임플라이 하는 이유: ver1의 \\(l({\\cal P}) \\subset \\sigma({\\cal P})\\)은 당연하고 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)만 보이면 되는데, 이미 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)임을 보였으므로 \\(l({\\cal P})\\)의 정의에 의하여 \\({\\cal L} \\supset l({\\cal P}) \\supset \\sigma({\\cal P})\\)이 성립한다.\n\n- 교재의 언급 (p 456)\n\n\n\n그림2: 교재에 언급된 딘킨의 정리, 부록에 있음"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#확률공간",
    "href": "posts/ap/2023-04-05-ap-5wk.html#확률공간",
    "title": "5wk: 측도론 (1)",
    "section": "확률공간",
    "text": "확률공간\n- \\(P:{\\cal F} \\to [0,1]\\) 가 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\) 에서의 확률측도라면, \\((\\Omega, {\\cal F}, P)\\) 를 확률공간이라 선언할 수 있다.\n- \\((\\Omega, {\\cal F})\\)가 잴수 있는 공간이라는 선언은 \\({\\cal F}\\)가 \\(\\Omega\\)에 대한 시그마필드라는 것이 내포되어 있다.\n- \\((\\Omega, {\\cal F}, P)\\)가 확률공간이라는 선언에는\n\n\\({\\cal F}\\)는 \\(\\Omega\\)에 대한 시그마필드이며,\n\\(P\\)는 \\((\\Omega, {\\cal F})\\)에서의 확률측도임이 내포되어 있다.\n\n- 교재의 언급 (p1) – 초록색부분\n\n\n\n그림3: 교재에 언급된 확률공간, 잴 수 있는 공간의 정의"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#귀찮아서-만든-이론2의-유일성",
    "href": "posts/ap/2023-04-05-ap-5wk.html#귀찮아서-만든-이론2의-유일성",
    "title": "5wk: 측도론 (1)",
    "section": "귀찮아서 만든 이론2의 “유일성”",
    "text": "귀찮아서 만든 이론2의 “유일성”\n- 귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n- 귀찮아서 만든 이론2는 (1) 업그레이드가 가능하냐 (2) 그 업그레이드가 유일하냐 를 따져야하는데 이중 유일성만을 따져보자.\n- 이론: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n\\({\\cal A}\\)가 파이시스템이라면, \\({\\cal A}\\)에서는 agree하지만 \\(\\sigma({\\cal A})\\)에서는 agree하지 않는 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)는 존재할 수 없다는 의미이다."
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#활용예제-star",
    "href": "posts/ap/2023-04-05-ap-5wk.html#활용예제-star",
    "title": "5wk: 측도론 (1)",
    "section": "활용예제 (\\(\\star\\))",
    "text": "활용예제 (\\(\\star\\))\n(예제1) – 4주차에서 했던 예제에요\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\) 라고 하자.\n- \\({\\cal A}\\)는 파이시스템이다.\n- 아래표의 왼쪽의 \\(P\\)와 같은 확률 측도를 고려하자.\n\n\n\n\n\\(P\\)\n\\(P'\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어?\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\) 이 아닐 수 있어?\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어?\n\n\n\n\\({\\cal A}\\)에서는 \\(P\\)와 그 값이 같지만 \\(\\sigma({\\cal A})-{\\cal A}\\)에서는 다른값을 가질 수도 있는 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P'\\)는 존재하지 않는다.\n즉 \\({\\cal A}\\)가 파이시스템이라면, \\((\\Omega,\\sigma({\\cal A}))\\)에의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값만 define하면 나머지 \\(\\sigma({\\cal A})-{\\cal A}\\)에서의 값은 유니크하게 결정된다.\n- 이 이론에 대한 짧은 생각\n\n생각1: 일단 \\((\\Omega,\\sigma({\\cal A})\\)에서의 확률측도 \\(P\\)의 존재성은 가정하고 들어간다. 즉 “존재한다면 유일하다”는 의미이지, “유일하게 존재한다”의 의미는 아니다.\n생각2: 따라서 이 정리는 “\\({\\cal A}\\)가 파이시스템일 경우, 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)가 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 확률측도 \\(P\\)로 업그레이드가 가능하다면 그 결과는 유일하다” 정도로 해석할 수 있다.\n\n(예제2) – 이것도 4주차에서 했던 예제입니다.\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)는 파이시스템이 아니다. 따라서 \\({\\cal A}\\)에서의 값은 agree하지만 \\((\\Omega, \\sigma({\\cal A}))\\)에서 agree하지 않는 서로 다른 확률측도가 존재할 수 있다.\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\n- 만약에 이 예제에서 \\({\\cal A}\\)를 아래와 같이 수정한다면\n\\[{\\cal A}=\\{\\emptyset, \\{1,2\\}, \\{2,3\\}, \\{2\\}\\}\\]\n이번에는 \\({\\cal A}\\)는 파이시스템이 된다. 따라서 이 경우 \\((\\Omega, \\sigma({\\cal A}))\\)에서의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)의 값에 의하여 유일하게 결정된다.\n(예제3)\n- \\(\\Omega=\\{H,T\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{H\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이므로 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로만 정의해도 무방하다.10\n(예제4) – 통계학과라서 행복해\n- \\(\\Omega=\\{a,b\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이다.\n- 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로 유일하게 결정된다.\n- 그렇지만 가측공간 \\((\\Omega,\\sigma({\\cal A})\\)에서 정의가능한 “측도” \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정되지 않는다.11\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-05-ap-5wk.html#증명-1",
    "href": "posts/ap/2023-04-05-ap-5wk.html#증명-1",
    "title": "5wk: 측도론 (1)",
    "section": "증명",
    "text": "증명\n증명을 하려는 이유\n- 안하려다가 합니다. 왜냐하면 이 증명이 “딘킨의 \\(\\pi-\\lambda\\) 정리”를 활용하는 아주 유명한 증명이라서요.\n- 교재의 언급 (p 456)\n\n\n\n그림4: 교재에서는 카라테오토리 정리의 유일성을 보이기 위해 딘킨의 \\(\\pi-\\lambda\\)정리를 소개함\n\n\n\n교재의 카라테오토리 확장정리는 귀찮아서 만든 이론2의 엄밀한 state이다.\n\n증명 (확률측도라는 가정을 추가하여 교재의 버전을 살짝 쉽게 만듬)"
  },
  {
    "objectID": "posts/ap/2023-03-21-ap-3wk.html",
    "href": "posts/ap/2023-03-21-ap-3wk.html",
    "title": "3주차: 측도론",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yJdGySrpN1x55Vs6SRntEX\n\n\n\n셀 수 있는\n- 셀 수 있는 집합과 셀 수 없는 집합.\n\ncountable: finite, countable many\nuncountable: uncountable many\n\n- 예시1: countable set, uncountable set\n\n\\(\\{1,2,3,4,5\\}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{N}\\)은 셀 수 있는 집합이다.\n\\(\\mathbb{Z}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{Q}\\)는 셀 수 있는 집합이다.\n\\(\\mathbb{R}\\)은 셀 수 없는 집합이다.\n\n- 예시2: countable sum: 아래는 모두 countable sum을 의미한다.\n\n\\(\\sum_{i=1}^{n}a_i\\).\n\\(\\sum_{i \\in I} a_i\\), where \\(I=\\{1,2,3,\\dots,10\\}\\).\n\\(\\sum_{i=1}^{\\infty} a_i\\), \\(\\sum_{i=0}^{\\infty} a_i\\).\n\\(\\sum_{i \\in \\mathbb{N}}a_i\\).\n\\(\\sum_{x \\in \\mathbb{Q}}m(\\{x\\})\\), where \\(m\\) is Lebesgue measure\n\n- 예시3: countable union: 아래는 countalbe union을 의미한다.\n\n\\(\\cup_{i=1}^n A_i\\)\n\\(\\cup_{i=1}^{\\infty} A_i\\)\n\\(\\cup_{x \\in \\mathbb{Q}} \\{x\\}\\)\n\n- 예시4: 아래는 uncountable sum을 의미한다.\n\n\\(\\sum_{x \\in [0,1]}m(\\{x\\})\\), where \\(m\\) is Lebesgue measure\n\n- 예시5: 아래는 uncountable union을 의미한다.\n\n\\(\\cup_{x \\in [0,1]} \\{x\\}\\)\n\n\n\n보충학습: 집합정리\n\n\n\n집합\n카디널리티\n분류\n르벡메져\n\n\n\n\n\\(\\{1,2,3\\}\\)\n3\n가산집합\n0\n\n\n\\(\\mathbb{N}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\(\\mathbb{Z}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\(\\mathbb{Q}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\([0,1]\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,1]\\cap \\mathbb{Q}\\)\n\\(\\aleph_0\\)\n가산집합\n0\n\n\n\\([0,1]\\cup \\mathbb{Q}\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,1]\\cap \\mathbb{Q}^c\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n1\n\n\n\\([0,\\infty)\\)\n\\(2^{\\aleph_0}\\)\n비가산집합\n\\(\\infty\\)\n\n\n비탈리집합\n\\(2^{\\aleph_0}\\)\n비가산집합\nNA\n\n\n칸토어집합\n\\(2^{\\aleph_0}\\)\n비가산집합\n0\n\n\n\n\n\n지금까지의 스토리\n- 지금까지의 이야기.\n\n\\(\\Omega\\)의 모든 부분집합에 대해서 확률을 “무모순”으로 정의하는게 엄청 쉬운일 인줄 알았는데,1\n사실은 그렇지가 않았다.2 확률을 정의하는건 매우 까다로운 일이었다.\n이러한 까다로움을 해결하기 위해서 “르벡메져”라는 새로운 도구를 사용했다. 이 도구는 몇 가지 까다로운 집합에 대하여 확률을 무모순으로 정의할 수 있었다.\n르벡메져는 구간 \\([0,2\\pi)\\)의 모든 유리수 집합의 길이와 구간 \\([0,2\\pi)\\)의 모든 무리수 집합의 길이를 다르게 정의하는 신기한 방식을 사용하는데, 이러한 방식을 납득하기 위한 최소한의 노력으로 “셀 수 있는 무한”과 “셀 수 없는 무한”의 개념을 공부했다.\n하지만 르벡메져를 통해서도 \\(\\Omega\\)의 모든 부분집합에 대하여 길이를 잴 수 없는 집합3이 존재함이 밝혀졌다.\n따라서 \\(\\Omega\\)의 모든 부분집합에 대해서 확률을 “무모순”으로 정의하는 일은 포기하였다.\n대신에 \\(\\Omega\\)의 부분집합 중, 잴 수 있는 집합들에 대해서만 확률을 “무모순”으로 정의하는 일을 시도하고자 한다.\n\n- 앞으로 \\(\\Omega\\)의 부분집합 중, 잴 수 있는 집합들의 모임을 “\\(\\Omega\\)에 대한 시그마필드” 라고 하고 기호로는 \\({\\cal F}\\)로 정의한다.\n- 그런데 “잴 수 있는 집합”이 뭐지????\n\n\n시그마필드 motivation\n(예제1) – 잴 수 있는 집합의 모임\n\\(\\Omega=\\{H,T\\}\\)라고 하자. 아래집합들은 모두 확률을 정의할 수 있는 집합들이다.\n\\[\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\]\n따라서 \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)가 합리적일 것이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n\n이때 \\({\\cal F}\\)는 집합들의 집합인데, 이러한 집합을 collection 이라고 한다.\n\n(예제2) – 집합 \\(A\\)를 잴 수 있다면, 집합 \\(A^c\\)도 잴 수 있어~\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{H\\}, \\Omega\\big\\}\\]\n(해설1)\n이러한 묶음이 의미하는건 “앞면이 나올 확률은 모순없이 정의할 수 있지만, 뒷면이 나오는 확률은 모순없이 정의하는게 불가능해~” 라는 뜻이다. 그런데 뒷면이 나올 확률은 “1-앞면이 나올 확률” 로 모순없이 정의할 수 있으므로 “앞면이 나올 확률이 모순없이 정의되면서” 동시에 “뒷면이 나올 확률이 모순없이 정의되지 않는” 상황은 없다.\n(해설2)\n\\(\\Omega\\)의 어떠한 부분집합 \\(A\\)에 확률이 모순없이 정의된다면 그 집합의 여집합인 \\(A^c\\)에 대하여서도 확률이 모순없이 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A \\subset {\\Omega}: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n(예제3) – 전체집합이 잴 수 있는 집합이니까 공집합도 잴 수 있는 집합이야\n\\(\\Omega=\\{H,T\\}\\)라고 하자. \\({\\cal F}\\)를 아래와 같이 정의한다면 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{ \\{H\\}, \\{T\\}, \\Omega\\big\\}\\]\n(해설)\n전체집합의 확률은 \\(P(\\Omega)=1\\)로 정의할 수 있다. 그런데 전체집합의 여집합인 공집합의 확률을 정의할 수 없는건 말이 안되므로 공집합은 \\(\\cal F\\)에 포함되어야 한다.\n(예제4) – 원소의 수가 유한한 경우 \\({\\cal F}=2^\\Omega\\)은 잴 수 있는 집합의 모임이야.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음은 \\({\\cal F}\\)은 합리적이다.\n\\[{\\cal F}=\\text{all subset of $\\Omega$}= 2^\\Omega = \\big\\{ \\emptyset, \\{1\\}, \\{2\\}, \\dots, \\{6\\}, \\dots, \\{1,2,3,4,5\\} \\dots \\Omega\\big\\}\\]\n(해설)\n\\(\\Omega\\)의 모든 부분집합에 대하여 확률을 모순없이 정의할 수 있다. 예를들면\n\n\\(P(\\Omega)=1\\), \\(P(\\emptyset)=0\\)\n\\(P(\\{1\\})=\\frac{1}{6}\\)\n\\(P(\\{1,2,4\\})=\\frac{3}{6}\\)\n\\(P(\\{2,3,4,5,6\\})=\\frac{5}{6}\\)\n\\(\\dots\\)\n\n이런식으로 정의할 수 있다.\n(예제5) – 동일한 \\(\\Omega\\)에 대하여 잴 수 있는 집합의 모임 \\({\\cal F}\\)는 유니크하지 않음.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{6\\}, \\{1,2,3,4,5\\},\\Omega \\big\\}\\]\n(해설)\n어떠한 특수한 상황을 가정하자. 주사위를 던져야하는데 6이 나오면 살수 있고 6이 나오지 않으면 죽는다고 하자. 따라서 던지는 사람 입장에서는 주사위를 던져서 6이 나오는지 안나오는지만 관심있을 것이다. 이 사람의 머리속에서 순간적으로 떠오르는 확률들은 아래와 같다.4\n\n살수있다 => 1/6\n죽는다 => 5/6\n살거나 죽는다 => 1\n살지도 죽지도 않는다 => 0\n\n이러한 확률은 합리적이다. 즉 아래의 집합들만 확률을 정의한다고 해도, 확률을 잘 정의할 수 있을 것 같다.\n\\[\\emptyset, \\{6\\}, \\{1,2,3,4,5\\}, \\Omega\\]\n(예제6) – \\(\\Omega\\)를 어떠한 사건의 집합으로 보느냐에 따라서 \\({\\cal F}\\)를 달리 구성할 수 있다.\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\{2,4,6\\},\\Omega \\big\\}\\]\n(해설)\n전체사건을 “주사위를 던져서 짝이 나오는 사건”, “주사위를 던져서 홀이 나오는 사건” 정도만 구분하겠다는 의미\n(예제7) – \\(A\\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\big\\{\\emptyset, \\{1,3,5\\}, \\Omega \\big\\}\\]\n(해설)\n“주사위를 던져서 홀수가 나올 사건”에 대한 확률을 정의할 수 있는데, 짝수가 나올 사건에 대한 확률을 정의할 수 없다는건 말이 안되는 소리임.\n(예제8) – trivial \\(\\sigma\\)-field\n\\(\\Omega=\\{1,2,3,4,5,6\\}\\)이라고 하자. \\({\\cal F}\\)을 아래와 같이 정의한다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이다.\n\\[{\\cal F}=\\{\\emptyset, \\Omega \\}\\]\n(해설)\n아예 이렇게 잡으면 모순이 일어나진 않음. (쓸모가 없겠지)\n(예제9) – 서로소인 두 집합의 합, 포함관계에 있는 집합의 차\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 어떠한 필요에 따라서 1이 나올 확률과 2가 나올 확률에만 관심이 있고 나머지는 별로 관심이 없다고 하자. 그래서 \\({\\cal F}\\)을 아래와 같이 정의했다고 하자. 이러한 묶음 \\({\\cal F}\\)는 합리적이지 않다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\]\n(해설1)\n\\({\\cal F}\\)은 전체집합과 공집합을 포함하고 여집합에 닫혀있으므로 언뜻 생각해보면 합리적인듯 보이지만 그렇지 않다. 왜냐하면 \\(\\{1,2\\}\\)이 빠졌기 때문이다. 1이 나올 확률 \\(P(\\{1\\})\\)와 2가 나올 확률 \\(P(\\{2\\})\\)를 각각 정의할 수 있는데, 1 또는 2가 나올 확률 \\(P(\\{1,2\\})\\)을 정의할 때 모순이 발생한다는 것은 합리적이지 못하다. 왜냐하면 \\(\\{1\\} \\cap \\{2\\} = \\emptyset\\) 이므로\n\\[P(\\{1\\} \\cup \\{2\\})=P(\\{1\\}) + P(\\{2\\})\\]\n와 같이 정의가능하기 때문이다. 따라서 집합이 아래와 같이 수정되어야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\}\\]\n(해설2)\n생각해보니까 \\(\\{2\\}\\)는 \\(\\{2,3,4\\}\\)의 부분집합이다. 그런데 \\(P(\\{2\\})\\)와 \\(P(\\{2,3,4\\})\\)를 각각 정의할 수 있는데\n\\[P(\\{2,3,4\\} - \\{2\\}) = P(\\{3,4\\})\\]\n를 정의할 수 없는건 말이 안된다. 따라서 \\({\\cal F}\\)를 아래와 같이 수정해야 한다.\n\\[{\\cal F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{3,4\\}, \\{1,2\\} \\}\\]\n(해설3)\n\\(\\Omega\\)의 어떠한 두 부분집합 \\(A\\), \\(B\\)가 서로소라고 상상하자. 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(A\\cup B\\)에 대한 확률도 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n또한 \\(\\Omega\\)의 임의의 두 부분집합이 \\(A \\subset B\\)와 같은 포함관계가 성립할때, 집합 \\(A\\), \\(B\\)에 대한 확률이 각각 무모순으로 정의된다면, 집합 \\(B-A\\)에 대한 확률로 무모순으로 정의되어야 한다.\n\\(\\Leftrightarrow\\) \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n(예제10) – \\({\\cal A}=\\{\\{1\\},\\{2\\}\\}\\) 일때, \\(\\sigma({\\cal A})\\) 를 구하는 문제\n\\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심이 있는 확률은 \\(P(\\{1\\})\\), \\(P(\\{2\\})\\) 밖에 없다고 하자. 이러한 확률들이 무모순으로 정의되기 위한 최소한의 \\({\\cal F}\\)를 정의하라.\n(해설) – 좀 귀찮네..?\n0차수정: \\({\\cal A} = \\big\\{\\{1\\}, \\{2\\}\\big\\}\\)\n1차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\Omega \\big\\}\\)\n2차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\big\\}\\)\n3차수정: \\(\\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega, \\{1,2\\}, \\{3,4\\} \\big\\}\\)\n\n사실 우리가 관심 있는건 \\({\\cal A} = \\{ \\{1\\}, \\{2\\} \\}\\) 뿐 이었음. 그런데 뭔가 \\(P(\\{1\\})\\)와 \\(P(\\{2\\})\\)를 합리적으로 정의하기 위해서 필연적으로 발생하는 어떠한 집합들을 모두 생각하는건 매우 피곤하고 귀찮은 일임. 그래서 “아 모르겠고, \\(\\{1\\}\\) 와 \\(\\{2\\}\\)를 포함하고 확률의 뜻에 모순되지 않게 만드는 최소한의 \\({\\cal F}\\)가 있을텐데, 거기서만 확률을 정의할래!” 라고 쉽게 생각하고 싶은 사람들이 생김. 그러한 공간을 \\(\\sigma({\\cal A})\\)라는 기호로 약속하고 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\) 라는 용어로 부름.\n\n\n생각의 시간1\n우리가 잴 수 있는 집합의 모임들 \\({\\cal F}\\)라는 것은 답을 구체적으로 쓸 수는 없으나 현재까지 파악한 직관에 한정하여 아래와 같은 조건5들을 만족하는 collection이라고 “일단은” 생각할 수 있다.\n\n\\(\\Omega, \\emptyset \\in {\\cal F}\\)\n\\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n\n이것은 우리가 “확률”이라는 개념을 올바르게 정의하기 위해서 필요한 최소한의 합의6이다.\n여기에서 우리가 따져볼 것은 (1) 시그마필드의 조건으로 1~4이면 충분한지 (더 많은 조건들이 필요한건 아닌지) 그리고 (2) 우리가 있었으면 하는 조건들이 꼭 필요한 조건은 맞는지 (예를들면 한두개의 조건이 다른조건을 암시하는건 아닌지) 이다.\n(충분할까?) 조건 1,2,3,4 정도를 만족하는 집합으로 시그마필드를 정의해도 충분할까? 좀 더 많은 조건들이 필요한건 아닐까? 예를들면 아래와 같은 조건들이 필요한건 아닌가?\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n여기에서 잠시 5,6,7,8의 의미를 살펴보자.\n\n3의 확장버전이라고 볼 수 있다. 3은 “각 집합을 잴 수 있다면 서로소인 집합을 유한번 더한 집합도 잴 수 있어야 한다” 라는 의미가 된다. 7은 “각 집합을 잴 수 있다면 서로소인 집합을 셀 수 있는 무한번 더한 집합도 잴 수 있어야 한다” 라는 의미가 된다.\n\n\n(예제11) – 람다시스템\n\\(\\Omega=(0,2\\pi]\\) 라고 하자. \\({\\cal A} = \\{\\{x\\}: x\\in \\mathbb{Q} \\cap \\Omega \\}\\) 이라고 할 때 아래가 성립할까?\n\\[\\mathbb{Q} \\cap \\Omega \\in \\sigma({\\cal A})\\]\n즉 각각의 유리수 한점씩을 잴 수 있을 때7 유리수 전체의 집합 역시 잴 수 있을까?\n(해설1)\n유리수는 셀 수 있는 무한이므로 집합 \\(\\mathbb{Q} \\cap \\Omega\\)의 길이나 확률 따위는 잴 수 있다.\n(해설2)\n확률의 공리중 3을 살펴보면 이미 서로소인 집합의 countable union은 잴 수 있는 대상이라고 생각하고 있다. 이건 마치 “확률은 양수”이어야 한다든가, “전체확률은 1이어야” 한다는 사실처럼 당연한 사실이다.8\n\n\n\n그림1: 위키에서 캡쳐했어요~ 3번째 공리를 살펴보세요\n\n\n\n사실 납득이 되는건 아님. 그렇지만 일단은 “수학자들이 합의해서 이런건 잴 수 있다고 했어. 그러니까 잴 수 있어” 라고 이해하고 넘어가자.\n\n\n생각의 시간2\n이제 5,6의 성질을 살펴보자.\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\n6의 경우는 \\(A\\)와 \\(B\\)가 서로소가 아니더라고 \\(A \\cup B\\)를 잴 수 있느냐? 라는 것이다. (결국 이는 교집합을 잴 수 있느냐? 라는 물음과 같아서 5와 6은 같은 질문이다.)\n\n(예제12) – 교집합을 넣을까 말까\n\\(\\Omega=\\{1,2,3,4\\}\\)라고 하자. 아래와 같은 \\({\\cal F}\\)는 합리적일까?\n\\[{\\cal F}= \\big\\{ \\emptyset, \\{1,2\\}, \\{1,3\\}, \\{1,4\\},\\{2,3\\},\\{2,4\\},\\{3,4\\}, \\Omega\\big\\}\\]\n(해설1) – 틀린해설\n이러한 집합은 원칙 1-4,7 에 위배되지 않는다.\n1. \\(\\Omega, \\emptyset \\in {\\cal F}\\)\n2. \\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n3. \\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n4. \\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n7. \\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n그런데 이 집합은\n\\[\\{1,2\\} \\cap \\{1,3\\} = \\{1\\}\\]\n와 같은 집합이라든가,\n\\[\\{1,2\\} \\cup \\{1,3\\} = \\{1,2,3\\}\\]\n와 같은 집합의 길이를 잴 수 없다. 따라서 아래와 같이 우리가 고등학교때 부터 써왔던 공식을 쓸 수 없다. (ref, Further consequences)\n\\[P(A\\cup B) = P(A) + P(B) - P(A\\cap B)\\]\n이것은 불편하니까 \\(A,B\\)가 잴 수 있다면, \\(A,B\\)의 교집합이나 합집합따위도 잴 수 있다고 정하자.\n(해설1의 반론)\n약속하지 않으면 “불편”하니까 약속하자라는 논리는 말이 되지 않음. 그 논리대로라면 \\(\\Omega\\)의 모든 부분 집합에 대하여 확률을 정의할 수 없다고 하면 “불편”하니까 약속하자라는 논리가 됨. 잴 수 있는 집합의 합집합이나 교집합을 잴 수 있다라는 근거는 없음.\n(해설1의 반론의 반론) – 참고용으로만..\n사실 근거가 있긴함. 즉 \\(A\\)와 \\(B\\)를 각각 잴 수 있다면 \\(A\\), \\(B\\)의 교집합도 잴 수 있음. (그렇다면 자동으로 합집합도 잴 수 있게 됨.) 이것을 지금 수준에서 엄밀하게 따지기 위해서는 “잴 수 있는 집합”의 정의를 해야하는데 지금 수준에서는 까다로움.\n(해설2) – 엄밀한 해설 X\n잴 수 있는 집합을 우리는 지금 까지 당연하게\n\n확률을 잴 수 있는 집합들\n\n로 생각했음, 그런데 원래 잴 수 있는 집합이라는 개념은 “선분의 길이” 따위를 모순없이 정의할 수 있는가? 즉 수직선 \\(\\mathbb{R}\\)의 모든 부분집합의 길이라는 개념을 정의할 수 있는가? 에서 출발하였음. 즉 원래 잴 수 있는 집합이라는 의미는\n\n수직선에서 길이를 잴 수 있는 집합들\n\n이라고 생각해야함. 그렇다면 “길이”라는 개념을 다시 추상화 해야하는데 “길이”라는 개념은 아래의 원칙에 위배되면 안될 것 같음.\n\n\n\n그림2: 위키에서 긁어온 그림. 길이는 1-4의 성질이 있어야 할 것으로 판단됨\n\n\n교집합을 잴 수 없다는 논리라면, 구간 \\([a_1,b_1]\\)의 길이는 잴 수 있고 구간 \\([a_2,b_2]\\)의 길이는 잴 수 있지만 구간 \\([a_1,b_1] \\cap [a_2,b_2]\\)의 길이는 잴 수 없다는 말인데 이는 말이되지 않음.\n\n결론 (엄밀한 해설은 아님): “잴 수 있다” 라는 개념은 확률, 길이에 모두 적용할 수 있어야 한다. 잴 수 있는 대상을 확률로 상상하면 \\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\) 인것이 당연하듯이 잴 수 있는 대상을 길이로 상상하면 \\(A,B \\in {\\cal F} \\Rightarrow A \\cap B \\in {\\cal F}\\) 임은 당연하다.\n\n\n생각의 시간3\n따라서 아래의 성질들은 모두 시그마필드가 가져아할 규칙들로 인정할 수 있다.\n\n\\(\\Omega, \\emptyset \\in {\\cal F}\\)\n\\(\\forall A \\subset \\Omega: ~ A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A\\cap B =\\emptyset\\): \\(A,B \\in {\\cal F} \\Rightarrow A \\cup B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega\\) such that \\(A \\subset B\\): \\(A,B \\in {\\cal F} \\Rightarrow B-A \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cap B \\in {\\cal F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} ~ \\Rightarrow A\\cup B \\in {\\cal F}\\)\n\\(\\forall B_1,B_2,\\dots \\subset \\Omega\\) such that \\(B_1, B_2,\\dots\\) are disjoint: \\(B_1,B_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in {\\cal F}\\)\n\n남은건 8번의 규칙이다.\n\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cup_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n이 89번 규칙은 사실 510, 711번 잘 조합하면 자동으로 이끌어진다. 즉 \\((5), (7) \\Rightarrow (8)\\). 그 외에도 “있었으면 싶은” 규칙은 모두 1-7중 적당한 것을 섞으면 만들 수 있다. 예를들어 아래와 같은 규칙을 고려하자.\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\)\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\)\n\\(\\forall A_1,A_2,\\dots \\subset \\Omega\\): \\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n규칙9는 규칙212와 513로 임플라이 할 수 있고, 규칙10은 규칙614의 확장으로 임플라이 할 수 있고, 규칙11은 규칙 215와 716로 임플라이 할 수 있다.\n\n결론: 규칙 1-8으로 시그마필드를 표현하기에 충분하다.\n\n생각의 시간4\n규칙 1-8중 필요없는 규칙을 제거하자.\n1. 규칙217가 있다면, 규칙1에서 공집합은 빼도 될 것 같다.\n2. 규칙818이 있다면, 규칙319, 규칙620, 규칙721은 필요 없다. 즉 규칙8은 규칙3,6,7의 효과를 모두 가진다.\n3. 규칙222와 규칙623이 있다면, 규칙524는 필요없다. 따라서 규칙225와 규칙826이 있어도 규칙5는 필요없다.\n4. 규칙227와 규칙528가 있다면 규칙429는 필요없다. 그런데 규칙5는 규칙2와 규칙830이 임플라이 하므로 결국 규칙2와 규칙8이 있다면 규칙4가 필요없다.\n5. 결론: 규칙1에서 공집합을 제외한 버전, 그리고 규칙2, 규칙8만 있으면 된다.\n\n\n시그마필드의 정의\n- 시그마필드, 즉 \\(\\Omega\\)의 부분집합 중 “잴 수 있는 집합의 모임”은 Durret 교재에 의하여 아래와 같이 정의된다.\n\n\n\n그림3: Durret교재에서 긁어온 시그마필드의 정의, 드래그한 부분이 정의임\n\n\n- 교재에는 \\(\\Omega \\in {\\cal F}\\)이라는 조건이 빠져있는데, \\(\\Omega \\in {\\cal F}\\)이라는 조건을 포함하여 기억하는 것이 편리하다. (위키등에서 일반적으로 정의할때는 \\(\\Omega \\in {\\cal F}\\) 조건을 포함한다) 즉 위키와 Durret을 적당히 혼합하여 아래와 같이 정의하고 기억하는게 좋다.\n(Def) Let \\(\\Omega\\) be some set, and let \\(2^{\\Omega}\\) represent its power set. Then a subset \\({\\cal F} \\subset 2^\\Omega\\) is called a \\(\\sigma\\)-field if it satisfies the following three properties:\n\n\\(\\Omega \\in {\\cal F}\\)\n\\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(A_1,A_2,A_3\\dots \\in {\\cal F}\\) \\(\\Rightarrow\\) \\(\\cup_{i=1}^{\\infty}A_i \\in {\\cal F}\\)\n\n- 좀 더 편리하게 아래와 같이 기억하면 좋다.\n\n시그마필드는 잴 수 있는 집합의 모임인데 아래와 같은 규칙을 만족해야 한다. (1) 전체집합을 포함한다. (2) 여집합에 닫혀있다. (3) 가산합집합에 닫혀있다.\n\n- 참고1: 시그마필드라는 것은 유일하게 정의되지 않는다. 즉 동일한 \\(\\Omega\\)에 대하여 정의할 수 있는 잴수있는 집합의 모임 \\({\\cal F}\\)는 유일하지 않다.\n- 참고2: 시그마필드는 \\(\\Omega\\)없이 단독으로 정의되지 않는다. 즉\n\\[{\\cal F}=\\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\\]\n는 단지 그냥 시그마필드라고 주장하기 보다 \\(\\Omega=\\{H,T\\}\\)에 대한 시그마필드라고 해야 정확한 표현이다.\n- 참고3: 참고2에 따라서 \\({\\cal F}\\) 단독으로 표기하는 것 보다 \\(\\Omega\\)를 붙여서 \\((\\Omega,{\\cal F})\\)와 같이 쌍으로 표기하는게 더 합리적이다. 앞으로는 이러한 쌍을 measurable space 라고 부른다.\n\n\n\n\n\nFootnotes\n\n\n동전예제↩︎\n바늘이 하나 있는 시계예제↩︎\n비탈리집합↩︎\n공평한 주사위라고 하자..↩︎\n이 조건들은 수정 및 보완 될 예정임↩︎\n모든 사람들이 인정할 수 밖에 없는 합의↩︎\n\\(P(\\{0\\})\\), \\(P(\\{0.21\\})\\), \\(\\dots\\)를 각각 정의가능할 때↩︎\n사실 일반인에게 당연하지 않을 수도 있지만 최소한 수학자들은 당연하게 생각한다. 그래서 우리도 그냥 당연하게 생각하자.↩︎\ncountable union↩︎\n교집합↩︎\n서로소의 countable union↩︎\n여집합↩︎\n교집합↩︎\n2개 집합의 합집합↩︎\n여집합↩︎\n서로소의 countable union↩︎\n여집합↩︎\ncountable union↩︎\ndisjoint union of two sets↩︎\n2개의 합집합↩︎\ncountable union of disjoint sets↩︎\n여집합↩︎\n합집합↩︎\n교집합↩︎\n여집합↩︎\ncountable union↩︎\n여집합↩︎\n교집합↩︎\n포함관계의 차집합↩︎\ncountable union↩︎"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html",
    "href": "posts/ap/2023-04-11-ap-6wk.html",
    "title": "6wk: 측도론 (2)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-y2r-mEbWKnTAC_8CN5HcGo"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#약한조건-약한정리-강한조건-강한정리",
    "href": "posts/ap/2023-04-11-ap-6wk.html#약한조건-약한정리-강한조건-강한정리",
    "title": "6wk: 측도론 (2)",
    "section": "약한조건, 약한정리, 강한조건, 강한정리",
    "text": "약한조건, 약한정리, 강한조건, 강한정리\n- 정리: 어떠한 조건을 만족하면, 어떠한 결론이 나온다.\n\n결론: 우리가 원하는 것.\n조건: 우리가 원하는 것을 얻기 위한 고난과정.\n\n- 결론이 동일하다면 조건이 약할 수록 유리하다.\n\n정리1: 수업에 온라인으로 참석하거나 오프라인으로 참석한다면 모두 출석으로 인정한다.\n정리2: 수업에 오프라인으로 참석할때만 출석으로 인정한다.\n\n\n정리2의 조건이 만족되면 정리1의 조건은 자동으로 만족된다. 따라서 정리2의 조건이 더 강한 조건이다. 조건이 강할수록 불리하므로 정리2가 더 불리하다.\n\n- 조건이 동일하다면 결론이 강한 쪽이 유리하다.\n\n정리1: 중간고사와 기말고사를 모두 응시한다면, B학점 이상이다.\n정리2: 중간고사와 기말고사를 모두 응시한다면, A학점 이상이다.\n\n\n정리2의 결론이 만족되면 정리1의 결론은 자동으로 만족되므로 정리2의 결론이 더 강하다. 결론은 강할수록 유리하므로 정리2가 더 유리하다."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#쓸모없는-측도",
    "href": "posts/ap/2023-04-11-ap-6wk.html#쓸모없는-측도",
    "title": "6wk: 측도론 (2)",
    "section": "쓸모없는 측도",
    "text": "쓸모없는 측도\n- 세상엔 측도의 정의를 만족하지만 쓸모 없는 측도가 있다.\n\n예시1: \\({\\cal F}\\)의 모든 원소의 메져값은 0이다.\n예시2: \\({\\cal F}\\)의 모든 원소의 메져값은 무한대이다.\n\n- 예시2와 같은 측도를 고려하고 싶지 않음 \\(\\Rightarrow\\) 유한측도, 시그마유한측도의 개발"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#쓸모없는-가측공간",
    "href": "posts/ap/2023-04-11-ap-6wk.html#쓸모없는-가측공간",
    "title": "6wk: 측도론 (2)",
    "section": "쓸모없는 가측공간",
    "text": "쓸모없는 가측공간\n- 세상엔 쓸모없는 잴 수 없는 공간이 있다. (유의미한 측도를 주는게 불가능한 잴 수 있는 공간)\n\n예시1: \\({\\cal F}= \\{\\emptyset, \\Omega\\}\\)\n예시2: \\(\\Omega =\\mathbb{R}\\) 일때 \\({\\cal F}=2^{\\mathbb{R}}\\) (르벡메져로 측정불가능함, 모든 원소의 메져를 0으로 잡으면 무모순으로 길이를 정의할 수는 있겠으나 무슨의미?)\n\n- 예시2와 같은 \\({\\cal F}\\)는 고려하고 싶지 않음 \\(\\Rightarrow\\) \\(\\sigma({\\cal A})\\), 카라테오도리 확장정리의 고안."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#유한측도-시그마유한측도",
    "href": "posts/ap/2023-04-11-ap-6wk.html#유한측도-시그마유한측도",
    "title": "6wk: 측도론 (2)",
    "section": "유한측도, 시그마유한측도",
    "text": "유한측도, 시그마유한측도\n- \\(m\\)이 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)에서의 측도라고 하자.\n\n\\(\\forall A \\in {\\cal F}\\), \\(m(A) < \\infty\\) 이면 \\(m\\)을 유한측도라고 한다.1\n\\(\\exists A_1,A_2,\\dots \\in {\\cal F}\\) such that (1) \\(\\cup_{i=1}^{\\infty}A_i = \\Omega\\) (2) \\(\\forall i \\in \\mathbb{N}:~ m(A_i)<\\infty\\) 이면 \\(m\\)을 시그마유한측도라고 한다.\n\n- NOTE: 모든 확률측도는 유한측도이다. 모든 유한측도는 시그마유한측도이다.2\n\n확률측도라는 것은 매우 강한 조건임\n시그마유한측도라는 것은 확률측도보다 훨씬 약한 조건임\n\n- 직관: 제 생각일 뿐이어요..\n\n세상엔 측도의 정의는 만족하지만 쓸모없는 측도가 있다. (모든 원소를 쟀더니 0이더라, 모든 원소를 쟀더니 무한대더라)\n그래서 모든 원소값에 무한대를 주는 측도는 인정하고 싶은 마음이 별로 없음. (하지만 측도의 정의는 만족)\n그래서 그냥 유한측도만 생각하기로 했는데…\n\n- 유한측도는 아니지만 시그마유한측도의 정의를 만족하는 경우 (엄청 중요해 보이는 예제들이 시그마유한측도잖아?)\n\n르벡메져\n카운팅메져: \\(m\\) is counting msr on \\((\\Omega, {\\cal F})\\) iff \\(m(A) = \\begin{cases} |A| & {\\tt if}~ A~{\\tt is~finite} \\\\ \\infty & {\\tt if}~A~{\\tt is~infinite}\\end{cases}\\)\n\n- 시그마유한측도의 느낌: 전체집합을 카운터블 유니온으로 커버하는 메져유한인 집합열이 1개만 있으면 된다.\n(기억해둘만한 예시)\n\\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\) 를 잴 수 있는 공간이라고 하자. \\(m\\)을 공간 \\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\)에서의 카운팅메져라고 하자.\n집합열1\n\n\\(A_1=\\mathbb{N}\\)\n\\(A_2=\\mathbb{N} \\cup \\{0\\}\\)\n\\(A_3=\\mathbb{N} \\cup \\{-1,0\\}\\)\n\\(\\dots\\)\n\n집합열2\n\n\\(B_1=\\{0\\}\\)\n\\(B_2=\\{0,1\\}\\)\n\\(B_3=\\{-1,0,1\\}\\)\n\\(\\dots\\)\n\n집합열1와 집합열2는\n\n(1) \\(\\cup_{i=1}^{\\infty}A_i=\\mathbb{Z}\\), (2) \\(\\forall i \\in \\mathbb{N}:~ m(A_i)=\\infty\\)\n(1) \\(\\cup_{i=1}^{\\infty}B_i=\\mathbb{Z}\\), (2) \\(\\forall i \\in \\mathbb{N}:~ m(B_i)<\\infty\\)\n\n를 만족한다. 즉 집합열1은 전체집합을 카운터블 유니온으로 커버하지만 메져유한은 아니고, 집합열2는 전제집합을 카운터블 유니온으로 커버하고 메져유한이다. 집합열2의 존재로 인하여 \\(m\\)은 \\((\\mathbb{Z}, 2^{\\mathbb{Z}})\\)에서의 시그마유한측도가 된다.\n전체 집합 \\(\\to\\) \\(\\mathbb{Z}\\)\n카운터블 유니온으로 커버 \\(\\to\\) \\(\\mathbb{Z} = \\sum^\\infty_{i=1} A_i\\)\n메져 유한은 아님 \\(\\to\\) \\(m(A_i) <\\infty\\)"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#확률공간",
    "href": "posts/ap/2023-04-11-ap-6wk.html#확률공간",
    "title": "6wk: 측도론 (2)",
    "section": "확률공간",
    "text": "확률공간\n- \\(P:{\\cal F} \\to [0,1]\\) 가 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\) 에서의 확률측도라면, \\((\\Omega, {\\cal F}, P)\\) 를 확률공간이라 선언할 수 있다.\n- \\((\\Omega, {\\cal F})\\)가 잴수 있는 공간이라는 선언은 \\({\\cal F}\\)가 \\(\\Omega\\)에 대한 시그마필드라는 것이 내포되어 있다.\n- \\((\\Omega, {\\cal F}, P)\\)가 확률공간이라는 선언에는\n\n\\({\\cal F}\\)는 \\(\\Omega\\)에 대한 시그마필드이며,\n\\(P\\)는 \\((\\Omega, {\\cal F})\\)에서의 확률측도임이 내포되어 있다.\n\n- 교재의 언급 (p1) – 초록색부분\n\n\n\n그림1: 교재에 언급된 확률공간, 잴 수 있는 공간의 정의"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#시그마유한측도공간",
    "href": "posts/ap/2023-04-11-ap-6wk.html#시그마유한측도공간",
    "title": "6wk: 측도론 (2)",
    "section": "시그마유한측도공간",
    "text": "시그마유한측도공간\n- \\(m:{\\cal F} \\to [0,\\infty]\\)이 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)에서의 시그마유한측도라면, \\((\\Omega, {\\cal F}, m)\\)을 시그마유한측도공간이라 부른다.\n- \\((\\Omega, {\\cal F}, m)\\)이 시그마유한측도공간이라는 선언에는\n\n\\({\\cal F}\\)는 \\(\\Omega\\)에 대한 시그마필드이며,\n\\(m\\)는 \\((\\Omega, {\\cal F})\\)에서의 시그마유한측도임이 내포되어 있다."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#state",
    "href": "posts/ap/2023-04-11-ap-6wk.html#state",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- Thm (귀찮아서 만든 이론1): 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다.\n\n그리고 당연히 smallest 조건에 의에서 유일성이 보장됨"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#증명을-위한-준비학습",
    "href": "posts/ap/2023-04-11-ap-6wk.html#증명을-위한-준비학습",
    "title": "6wk: 측도론 (2)",
    "section": "증명을 위한 준비학습",
    "text": "증명을 위한 준비학습\n- 이론: (\\(\\star\\)) 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실에 성립한다.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\n\\(\\to {\\cal F}_1\\)이 \\(\\Omega\\)에 대하여 시그마필드이고,\\({\\cal F}_2\\)이 \\(\\Omega\\)에 대하여 시그마필드이라면, \\({\\cal F_1}\\cap_{i \\in I}{\\cal F_2}\\) 역시 시그마필드이다.\n(증명)\n편의상 \\({\\cal F}= \\cap_{i \\in I} {\\cal F}_i\\) 라고 하자. \\({\\cal F}\\)가 시그마필드임을 보이기 위해서는\n\n\\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(A_1,A_2 \\dots \\in {\\cal F} \\Rightarrow \\cup_{i}A_i \\in {\\cal F}\\)\n\n만 보이면 된다. (이럴때는 전체집합 조건하나를 빼는게 유리하다)\n\n\n\n\n\n\nNote\n\n\n\n\n전체집합 조건 -> \\(\\Omega\\) 포함\n\\(A \\in {\\cal F} \\Rightarrow A^c \\in {\\cal F}\\)\n\\(A_1,A_2 \\dots \\in {\\cal F} \\Rightarrow \\cup_{i}A_i \\in {\\cal F}\\)\n\n1번 생략하고 2,3번만 증명한 것\n\n\n1번체크\n\\(A \\in {\\cal F} \\Rightarrow \\forall i: A \\in {\\cal F}_i \\Rightarrow \\forall i: A^c \\in {\\cal F}_i \\Rightarrow A^c \\in {\\cal F}\\)\n2번체크\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\forall i: A_1,A_2,\\dots \\in {\\cal F}_i \\Rightarrow \\forall i: \\cup_jA_j \\in {\\cal F}_i \\Rightarrow \\cup_jA_j \\in {\\cal F}\\)\n\n\n\n\n\n\nNote\n\n\n\n시그마 필드가 countable union에 갇혀 있기 때문에 가능"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#증명",
    "href": "posts/ap/2023-04-11-ap-6wk.html#증명",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n- Thm (귀찮아서 만든 이론1): 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다.\n\n그리고 당연히 smallest 조건에 의해 유일성이 보장됨\n\n(증명)\n\\({\\cal A}\\)를 포함하는 모든 시그마필드를 구하고 그걸 교집합하여 결과를 \\({\\cal F}\\)라고 하자. 아래의 사실은 자명하게 성립한다.\n\n시그마필드의 교집합은 시그마필드이므로 \\({\\cal F}\\)는 시그마필드이다.\n교집합을 하면 할수록 집합은 작아지므로 \\({\\cal F}\\)는 위에서 구한 시그마필드중에서 가장 작다.\n\\({\\cal F}\\)는 \\({\\cal A}\\)를 포함한다.\n\n따라서 \\({\\cal F}\\)는 (\\({\\cal A}\\)를 포함하는 모든 시그마필드를 교집합하여 얻은 집합) \\({\\cal A}\\)를 포함하는 가장 작은 시그마필드가 된다.\n- 아래는 교재의 언급 (p3)\n\n\n\n그림2: Durret교재에서 언급된 “귀찮아서 만든 이론1”"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#state-1",
    "href": "posts/ap/2023-04-11-ap-6wk.html#state-1",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- Thm: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver1. (\\(\\star\\))\n\\({\\cal P}\\)가 파이시스템이면 \\(l({\\cal P})=\\sigma({\\cal P})\\)이다."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#증명을-위한-준비학습-1",
    "href": "posts/ap/2023-04-11-ap-6wk.html#증명을-위한-준비학습-1",
    "title": "6wk: 측도론 (2)",
    "section": "증명을 위한 준비학습",
    "text": "증명을 위한 준비학습\n- 이론: 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실이 성립한다.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\\({\\cal A}_i\\)가 모두 시그마링, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 시그마링이다.\n\\({\\cal A}_i\\)가 모두 알지브라라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 알지브라이다.\n\\({\\cal A}_i\\)가 모두 링이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 링이다.\n\\({\\cal A}_i\\)가 모두 람다시스템이라면, \\(\\cap_{i \\in I}{\\cal A_i}\\) 역시 람다시스템이다.\n\n\n세미알지브라, 세미링, 파이시스템은 성립안함.\n\n- 예제1: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A}_1 = \\{\\emptyset, \\{1\\}, \\{2,3\\}, \\{4\\}, \\Omega\\}\\)\n\\({\\cal A}_2 = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{3,4\\}, \\Omega\\}\\)\n\n\\({\\cal A}_1, {\\cal A}_2\\)는 모두 세미알지브라이다. 하지만 \\({\\cal A}_1 \\cap {\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)은 세미알지브라가 아니다.\n\n이 예제에서 세미알지브라를 세미링으로 바꾸고 읽어도 성립함.\n\n- 예제2: 아래를 고려하자.\n\n\\(\\Omega=\\{H,T\\}\\)\n\\({\\cal A}_1 = \\{\\{H\\}\\}\\)\n\\({\\cal A}_2 = \\{\\{T\\}\\}\\)\n\n\\({\\cal A}_1, {\\cal A}_2\\)는 모두 파이시스템이다. 하지만 \\({\\cal A}_1 \\cap {\\cal A}_2 = \\emptyset\\)은 파이시스템이 아니다.\n- 이론: 임의의 \\({\\cal A}\\)에 대하여 아래는 존재한다.\n\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마필드, \\(\\sigma({\\cal A})\\)\n\\({\\cal A}\\)를 포함하는 가장 작은 시그마링\n\\({\\cal A}\\)를 포함하는 가장 작은 알지브라\n\\({\\cal A}\\)를 포함하는 가장 작은 링\n\\({\\cal A}\\)를 포함하는 가장 작은 람다시스템, \\(l({\\cal A})\\)\n\n- 참고: “\\({\\cal A}\\)를 포함하는 가장 작은 세미링”, 혹은 “\\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라”와 같은 것은 존재하지 않음.\n- 예제3: 아래를 고려하자.\n\n\\(\\Omega = \\{1,2,3,4\\}\\)\n\\({\\cal A} = \\{\\emptyset, \\Omega, \\{1\\}\\}\\)\n\n이때 \\({\\cal A}\\)를 포함하는 가장 작은 세미알지브라가\n\\[{\\cal A}_1 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2,3,4\\}\\}\\]\n라고 주장할 수는 없음. 왜냐하면\n\\[{\\cal A}_2 = \\{\\emptyset, \\Omega, \\{1\\}, \\{2\\},\\{3\\},\\{4\\}\\}\\]\n역시 \\({\\cal A}\\)를 포함하는 세미알지브라이지만 \\({\\cal A}_1 \\not \\subset {\\cal A}_2\\)이므로.\n- 이론: \\({\\cal P}\\)가 파이시스템이라고 하자. 아래가 성립한다.\n\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마필드는 그 자체로 파이시스템이다. (즉 \\(\\sigma({\\cal P})\\)는 파이시스템이다)\n\\({\\cal P}\\)를 포함하는 가장 작은 시그마링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 알지브라는 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 링은 그 자체로 파이시스템이다.\n\\({\\cal P}\\)를 포함하는 가장 작은 람다시스템은 그 자체로 파이시스템이다?? (즉 \\(l({\\cal P})\\)는 파이시스템이다?)\n\n- 1-4는 자명한데, 5는 자명하지 않다. 하지만 성립한다. (5의 증명은 복잡함. 그냥 암기하자.)\n- 이론: \\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n(증명) 아래의 표를 살펴보면 간단하게 증명가능하다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i=\\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\n\\(\\lambda\\)-system\n\\(X\\)\n\\(O\\)\n\\(\\Delta'\\)\n\\(X\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(O\\)\n\\(X\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#증명-1",
    "href": "posts/ap/2023-04-11-ap-6wk.html#증명-1",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n(증명)\n\\(l(\\cal P) \\subset \\sigma({\\cal P})\\) 임을 보이고, \\(l(\\cal P) \\supset \\sigma({\\cal P})\\) 임을 보이면된다.\n“\\(\\subset\\)”: 당연하다.3\n“\\(\\supset\\)”: \\(l({\\cal P})\\)가 시그마필드임을 보이면 자동으로 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)임이 보여진다.\n\\(l({\\cal P})\\)이 시그마필드임은 아래를 조합하면 간단히 증명된다.\n\n파이시스템 \\({\\cal P}\\)를 포함하는 가장 작은 람다시스템 \\(l({\\cal P})\\)은 그 자체로 파이시스템이다.\n\\({\\cal A}\\)가 람다시스템이다. \\(\\Rightarrow\\) (\\({\\cal A}\\)는 시그마필드이다. \\(\\Leftrightarrow\\) \\({\\cal A}\\)는 파이시스템이다.)\n\n- 생각의 시간\n\n시그마필드(=잴 수 있는 집합의 모임)을 만들기 위해서는, 그 모임(=collection)이 파이시스템이면서 동시에 람다시스템임을 보이면 된다.\n딘킨의 정리는 적당한 파이시스템을 만들고 그것을 통하여 잴 수 있는 집합의 모임을 확률의 공리에 맞게만 설정한다면, 그것이 시그마필드가 된다는 것을 보이는 것이다.\n\n- 제 생각\n\n메져가 “선분의 길이”를 일반화 하는 개념이라 생각한다면 파이시스템에서 시작하여 시그마필드로 확장하는 것이 자연스럽다.\n메져가 “확률”을 일반화하는 개념이라 생각한다면 람다시스템에서 시작하는게 자연스럽다.4\n딘킨의 \\(\\pi-\\lambda\\) 정리는 두 흐름을 합치는 정리이다."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#딘킨의-pi-lambda-정리-ver2.",
    "href": "posts/ap/2023-04-11-ap-6wk.html#딘킨의-pi-lambda-정리-ver2.",
    "title": "6wk: 측도론 (2)",
    "section": "딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.",
    "text": "딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.\n- 이론: 딘킨의 \\(\\pi-\\lambda\\) 정리 ver2.\n\\({\\cal P}\\)가 파이시스템이고 \\({\\cal L}\\)이 \\({\\cal P}\\)를 포함하는 람다시스템이라면 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)이다.\n(설명)\nDurret에 나온 딘킨의 \\(\\pi-\\lambda\\) thm 이다. 굉장히 불친절한 편인데, ver2가 증명되면 ver1은 자명하게5 임플라이 되므로 ver2를 대신 state한 것이다.\n\nver2가 ver1를 임플라이 하는 이유: ver1의 \\(l({\\cal P}) \\subset \\sigma({\\cal P})\\)은 당연하고 \\(l({\\cal P}) \\supset \\sigma({\\cal P})\\)만 보이면 되는데, 이미 \\(\\sigma({\\cal P}) \\subset {\\cal L}\\)임을 보였으므로 \\(l({\\cal P})\\)의 정의에 의하여 \\({\\cal L} \\supset l({\\cal P}) \\supset \\sigma({\\cal P})\\)이 성립한다.\n\n- 교재의 언급 (p 456)\n\n\n\n그림2: 교재에 언급된 딘킨의 정리, 부록에 있음"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#state-2",
    "href": "posts/ap/2023-04-11-ap-6wk.html#state-2",
    "title": "6wk: 측도론 (2)",
    "section": "state",
    "text": "state\n- 귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n- 귀찮아서 만든 이론2는 (1) 업그레이드가 가능하냐 (2) 그 업그레이드가 유일하냐 를 따져야하는데 이중 유일성만을 따져보자.\n- Thm: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n\\({\\cal A}\\)가 파이시스템이라면, \\({\\cal A}\\)에서는 agree하지만 \\(\\sigma({\\cal A})\\)에서는 agree하지 않는 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)는 존재할 수 없다는 의미이다."
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#활용예제-star",
    "href": "posts/ap/2023-04-11-ap-6wk.html#활용예제-star",
    "title": "6wk: 측도론 (2)",
    "section": "활용예제 (\\(\\star\\))",
    "text": "활용예제 (\\(\\star\\))\n- 아래의 이론을 이해하기 위한 예제들을 살펴보자.\n\n이론: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n(예제1) – 4주차에서 했던 예제에요\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\) 라고 하자.\n- \\({\\cal A}\\)는 파이시스템이다.\n- 아래표의 왼쪽의 \\(P\\)와 같은 확률 측도를 고려하자.\n\n\n\n\n\\(P\\)\n\\(P'\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어?\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\) 이 아닐 수 있어?\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\n\\(\\frac{3}{4}\\) 이 아닐 수 있어?\n\n\n\n\\({\\cal A}\\)에서는 \\(P\\)와 그 값이 같지만 \\(\\sigma({\\cal A})-{\\cal A}\\)에서는 다른값을 가질 수도 있는 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P'\\)는 존재하지 않는다.\n즉 \\({\\cal A}\\)가 파이시스템이라면, \\((\\Omega,\\sigma({\\cal A}))\\)에의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값만 define하면 나머지 \\(\\sigma({\\cal A})-{\\cal A}\\)에서의 값은 유니크하게 결정된다.\n- 이 이론에 대한 짧은 생각\n\n생각1: 일단 \\((\\Omega,\\sigma({\\cal A})\\)에서의 확률측도 \\(P\\)의 존재성은 가정하고 들어간다. 즉 “존재한다면 유일하다”는 의미이지, “유일하게 존재한다”의 의미는 아니다.\n생각2: 따라서 이 정리는 “\\({\\cal A}\\)가 파이시스템일 경우, 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)가 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 확률측도 \\(P\\)로 업그레이드가 가능하다면 그 결과는 유일하다” 정도로 해석할 수 있다.\n\n(예제2) – 이것도 4주차에서 했던 예제입니다.\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)는 파이시스템이 아니다. 따라서 \\({\\cal A}\\)에서의 값은 agree하지만 \\((\\Omega, \\sigma({\\cal A}))\\)에서 agree하지 않는 서로 다른 확률측도가 존재할 수 있다.\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\n- 만약에 이 예제에서 \\({\\cal A}\\)를 아래와 같이 수정한다면\n\\[{\\cal A}=\\{\\emptyset, \\{1,2\\}, \\{2,3\\}, \\{2\\}\\}\\]\n이번에는 \\({\\cal A}\\)는 파이시스템이 된다. 따라서 이 경우 \\((\\Omega, \\sigma({\\cal A}))\\)에서의 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)의 값에 의하여 유일하게 결정된다.\n(예제3)\n- \\(\\Omega=\\{H,T\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{H\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이므로 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로만 정의해도 무방하다.6\n(예제4) – 통계학과라서 행복해\n- \\(\\Omega=\\{a,b\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\}\\}\\) 라고 하자.\n- 여기에서 \\({\\cal A}\\)은 파이시스템이다.\n- 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로 유일하게 결정된다.\n- 그렇지만 가측공간 \\((\\Omega,\\sigma({\\cal A})\\)에서 정의가능한 “측도” \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정되지 않는다.7\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)"
  },
  {
    "objectID": "posts/ap/2023-04-11-ap-6wk.html#증명-2",
    "href": "posts/ap/2023-04-11-ap-6wk.html#증명-2",
    "title": "6wk: 측도론 (2)",
    "section": "증명",
    "text": "증명\n- 아래의 이론에 대한 증명\n\nThm: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n\n증명 (확률측도라는 가정을 추가하여 교재의 버전을 살짝 쉽게 만듬)\nLET\n\n\\((\\Omega, \\sigma({\\cal A}))\\) 는 잴 수 있는 공간임.\n\\(P_1,P_2\\)는 \\((\\Omega, \\sigma({\\cal A}))\\)에서의 확률측도임.\n\\(P_1,P_2\\)는 “\\(\\forall A \\in {\\cal A}: P_1(A)=P_2(A)\\)”를 만족함.\n\n전략: 잴 수 있는 공간 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 두 측도 \\(P_1\\), \\(P_2\\)가 \\({\\cal A}\\)에서는 일치하지만 \\(\\sigma({\\cal A})-{\\cal A}\\)에서는 일치하지 않는 경우를 찾으려 해보고, 그것이 불가능함을 보이자.\nLET: \\(\\tilde{\\cal D}=\\{B \\in \\sigma({\\cal A}): P_1(B) \\neq P_2(B)\\}\\)\nISTST: \\(\\tilde{\\cal D} =\\emptyset\\)\nISTST: \\({\\cal D} = \\{B \\in \\sigma({\\cal A}): P_1(B) = P_2(B) \\} = \\sigma({\\cal A})\\)\nISTST: (1) \\({\\cal D} \\subset \\sigma({\\cal A})\\) (2) \\({\\cal D} \\supset \\sigma({\\cal A})\\)\nISTST: \\({\\cal D} \\supset \\sigma({\\cal A})\\)\nNOTE: IF (1) \\({\\cal D}\\) is containing \\({\\cal A}\\) (2) \\({\\cal D}\\) is \\(\\lambda\\)-system, THEN we can say \\({\\cal D} \\supset \\sigma({\\cal A})=l({\\cal A})\\)\nISTST: (1) \\({\\cal A} \\subset {\\cal D}\\) (2) \\({\\cal D}\\) is \\(\\lambda\\)-system.\nISTST: 1. \\(\\Omega \\in {\\cal D}\\) 2. \\(A,B \\in {\\cal D}, A\\subset B\\) \\(\\Rightarrow\\) \\(B-A \\in {\\cal D}\\) 3. \\(\\forall B_1,B_2,\\dots, \\in {\\cal D}\\), \\(\\uplus_{i=1}^{\\infty} B_i \\in {\\cal D}\\)\nCHECK 1: \\(P_1(\\Omega) = P_2(\\Omega)\\)\nCHECK 2: \\(P_1(B-A) = P_1(B)-P_1(A) = P_2(B) - P_2(A) = P_2(B-A)\\)\nCHECK 3: \\(P_1(\\uplus_{i=1}^{\\infty} B_i)=P_1(B_1)+P_1(B_2)\\dots = P_2(B_1)+P_2(B_2) +\\dots = P_2(\\uplus_{i=1}^\\infty B_i)\\)\n- 보충노트\n\nsupp_6wk.pdf"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#cap_n1infty-frac1nfrac1n0",
    "href": "posts/ap/2023-04-25-ap-8wk.html#cap_n1infty-frac1nfrac1n0",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "\\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})=\\{0\\}\\)",
    "text": "\\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})=\\{0\\}\\)\n- \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n}) =\\{0\\}\\)을 증명하라.\n(증명)\nstep1: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)을 포함한다.\n\\(\\forall n \\in \\mathbb{N}\\): \\(-\\frac{1}{n} < 0 < \\frac{1}{n}\\)\n\\(\\Leftrightarrow \\forall n \\in \\mathbb{N}\\): \\(0 \\in (-\\frac{1}{n},\\frac{1}{n})\\)\n\\(\\Leftrightarrow\\) \\(0 \\in (-\\frac{1}{1},\\frac{1}{1})\\) and \\(\\{0\\} \\in (-\\frac{1}{2},\\frac{1}{2})\\) \\(\\dots\\)\n\\(\\Leftrightarrow\\) \\(0 \\in (-\\frac{1}{1},\\frac{1}{1}) \\cap (-\\frac{1}{2},\\frac{1}{2}) \\cap \\dots\\)\n\\(\\Leftrightarrow\\) \\(0 \\in \\cap_{n=1}^{\\infty}(-\\frac{1}{1},\\frac{1}{1})\\)\nstep2: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)보다 큰 임의의 양수를 포함하지 않는다.\n포함한다고 가정하자. 즉\n\\(\\exists \\delta >0\\) such that \\(0+\\delta \\in \\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)\nNOTE: From \\(\\delta>0\\), \\(\\exists N \\in \\mathbb{N}\\) such that \\(0<\\frac{1}{N}<\\delta\\)\nTHUS \\(\\delta \\notin (-\\frac{1}{N},\\frac{1}{N})\\) \\(\\Rightarrow\\) CONTRADICTION! (\\(\\because \\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n}) \\subset (-\\frac{1}{N},\\frac{1}{N}))\\)\nstep3: \\(\\cap_{n=1}^{\\infty}(-\\frac{1}{n},\\frac{1}{n})\\)은 원소로 \\(0\\)보다 큰 임의의 음수를 포함하지 않는다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#vacuous-truth",
    "href": "posts/ap/2023-04-25-ap-8wk.html#vacuous-truth",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "Vacuous truth",
    "text": "Vacuous truth\n- \\(P \\Rightarrow Q\\) 에서, \\(P\\)가 틀렸거나 \\(P\\)를 만족하는 집합이 공집합일 경우 \\(P\\Rightarrow Q\\)라는 명제는 항상 참이되고 이러한 참을 배큐어스 트루 라고 말한다.\n- 이해를 돕기 위한 예시\n\n명제1: 최규빈교수보다 나이 많은 학생은 A+를 받지 못했다.\n명제2: 최규빈교수보다 나이 많은 학생은 A+를 받았다.\n\n여기에서 명제1,명제2는 모두 참이어야 한다. 그래야 명제1,명제2의 대우는 모두 참이 되며\n\n대우1: A+를 받은 학생은 최규빈교수보다 나이가 적다.\n대우2: A+를 받지 못한 학생은 최규빈교수보다 나이가 적다.\n\n두 대우의 합성명제인 아래도 참이 된다.\n\nA+을 받거나 받지 못한 학생은 최규빈교수보다 나이가 적다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#정의",
    "href": "posts/ap/2023-04-25-ap-8wk.html#정의",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "정의",
    "text": "정의\n- 정의: \\(\\Omega\\)에 대한 부분집합의 모임 \\({\\cal T}\\)가 아래의 조건을 만족하면 \\({\\cal T}\\)를 \\(\\Omega\\)의 토폴로지라고 부른다.\n\n\\(\\emptyset, \\Omega \\in {\\cal T}\\)\n\\(\\forall A,B \\in {\\cal T}:~ A\\cap B \\in {\\cal T}\\) (finite intersection에 닫혀있음)\n\\(\\forall {\\cal A} \\subset {\\cal T}: ~ (\\cup_{A \\in {\\cal A}}A ) \\in {\\cal T}\\) (uncoutable union, arbitrary union에 닫혀있음)\n\n- \\((\\Omega,{\\cal T})\\)를 위상공간 (topological space) 이라고 부른다. 그리고 \\({\\cal T}\\)의 원소를 \\({\\cal T}\\)-open set이라고 부른다.\n- 모티브: 실수위에서의 열린구간 \\((a,b)\\)의 개념을 추상화하고 싶음. 즉 open interval \\(\\overset{일반화}{\\to}\\) open set 을 하고 싶음. 그리고 이러한 open set 만을 모은 collection \\({\\cal T}\\)라는 기호로 표현하고 싶음.\n\n관찰1: \\((1,3) \\cap (2,4) = (2,3)\\) // 2개의 open-interval을 교집합하니 open-interval이 나옴\n관찰2: \\(\\cap_{n=1}^{\\infty}(1-\\frac{1}{n},3+\\frac{1}{n} =[1,3]\\) // countable many한 open-interval을 교집합하면 closed-interval이 나옴\n관찰3: \\(\\cup_{n=1}^{\\infty}(1+\\frac{1}{n},3-\\frac{1}{n})= (1,3)\\) // countable many한 open-interval을 합집합하면 open-interval이 나옴\n관찰4: \\(\\cup_{\\epsilon>0}^{\\infty}(1+\\epsilon,3-\\epsilon) =(1,3)\\) // uncountalbe many한 open-interval을 합집합해도 open-interval이 나옴\n\n- 왜 open interval을 추상화하고 싶을까?\n\nopen interval은 엄청 특이한 성질이 있음. 구간 \\((a,b)\\)의 모든 점 \\(x\\)는 점 \\(x\\)를 포함하는 (아주 작은) 구간 \\((x-\\epsilon,x+\\epsilon)\\) 이 \\((a,b)\\)사이에 존재함.\n이 성질은 극한의 개념을 정의하기에 매우 유리하다. (따라서 연속, 끊어짐 등을 이해하기에도 좋다)\n\n- \\(\\Omega=\\mathbb{R}\\)일 경우 open-set\n\n\\((1,2)\\)\n\\((1,2)\\cup (5,6)\\)\n\\((a-\\epsilon, a+\\epsilon)\\), where \\(\\epsilon>0\\) and \\(a\\in\\mathbb{R}\\)\n\\(\\dots\\)\n\n- 체크\n\n\\(\\Omega=\\mathbb{R}\\), \\({\\cal T}=\\{\\emptyset, \\Omega\\}\\)라고 하자. \\({\\cal T}\\)는 \\(\\Omega\\)에 대한 토폴로지이며 따라서 \\((\\Omega, {\\cal T})\\)는 위상공간이 된다.\n\\(\\Omega=\\mathbb{R}\\), \\({\\cal T}=2^{\\mathbb{R}}\\)라고 하자. 그렇다면 \\({\\cal T}\\)는 \\(\\Omega\\)에 대한 토폴로지이며 따라서 \\((\\Omega,{\\cal T})\\)는 위상공간이 된다.\n\n그렇지만 우린 이런걸 쓰고 싶은게 아니야 (\\(\\star\\))"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#짧은지식",
    "href": "posts/ap/2023-04-25-ap-8wk.html#짧은지식",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "짧은지식",
    "text": "짧은지식\n- 이론: \\(\\Omega=\\mathbb{R}\\) 일때 \\({\\cal U}=\\{O:O = \\cup_{i=1}^{\\infty}(a_i, b_i),~ a_i\\leq b_i \\in \\mathbb{R}\\}\\)라고 하자. 즉 \\({\\cal U}\\)는 open interval의 countable union으로 표현가능한 집합들의 모임이다. 그렇다면 \\((\\mathbb{R}, {\\cal U})\\)는 위상공간이 된다.\n\n그리고 특별히 이러한 위상 \\({\\cal U}\\)를 \\(\\mathbb{R}\\)에서의 standard topology, Euclidean topology, 혹은 usual topology 라고 부른다. 사실 \\({\\cal U}\\)가 바로 우리가 토폴로지를 정의하는 이유이다 (매우 중요하다는 뜻이에요)\n\n\n\\({\\cal U}\\)의 원소를 원래 엄밀하게는 \\({\\cal U}\\)-open set이라고 불러야 하지만 이 경우는 \\({\\cal U}\\)를 생략하여 open set 이라고 부르기도 한다. 즉 우리가 일반적으로 말하는 “실수 \\(\\mathbb{R}\\)에서의 열린집합, 혹은 그냥 열린집합” 은 \\({\\cal U}\\)-open set을 의미한다.\n\n\n이 이론이 의미하는 바는 (1) 실수에서의 열린구간의 일반화 버전은 열린집합이며 (2) 열린집합은 열린구간의 가산합집합으로 표현가능하다 라는 뜻이다.\n\n- 이론: \\((\\mathbb{R},{\\cal U})\\)를 usual topological space라고 하자. 모든 \\(O \\in {\\cal U}\\) 는 아래를 만족한다.\n\n\\(\\forall o \\in O, \\exists B \\subset O\\) such that \\(o \\in B \\subset O \\quad \\cdots (\\star)\\)\n\n\n참고로 어떠한 집합 \\(O\\)에 대하여 \\((\\star)\\)를 만족하는 원소 \\(o\\)를 interior point of \\(O\\) 라고 부른다. 따라서 어떤 집합의 모든 원소가 그 집합의 interior point라면 그 집합은 openset이라고 해석할 수 있다.\n\n\n저는 나이테정리라고 외웠어요..\n\n- 실수에서의 \\({\\cal U}\\)-openset 을 정의하는 방법\n\n열린구간의 가산합집합\n모든원소가 interior point인 집합\n\n- 위상공간 \\((\\mathbb{R},{\\cal U})\\)를 고려하자. 여기에서 \\({\\cal U}=\\{O:O = \\cup_{i=1}^{\\infty}(a_i, b_i),~ a_i\\leq b_i \\in \\mathbb{R}\\}\\)를 의미한다. 아래의 사실들을 관찰하라.\n\n모든 열린구간은 열린집합이다.\n\\((-\\infty, a)\\)와 \\((a,\\infty)\\)는 모두 열린집합이다.\n한점의 원소 \\(\\{a\\}\\)는 닫힌집합이다. (\\(\\{a\\}\\)의 여집합이 열린집합이므로)\n\\((-\\infty, a]\\)와 \\([a,\\infty)\\)는 모두 닫힌집합이다.\n공집합과 \\(\\mathbb{R}\\)은 열린집합이다.1 따라서 공집합과 \\(\\mathbb{R}\\)은 닫힌집합이다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#시그마필드-vs-토폴로지",
    "href": "posts/ap/2023-04-25-ap-8wk.html#시그마필드-vs-토폴로지",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "시그마필드 vs 토폴로지",
    "text": "시그마필드 vs 토폴로지"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#borel-sigma-field",
    "href": "posts/ap/2023-04-25-ap-8wk.html#borel-sigma-field",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "Borel \\(\\sigma\\)-field",
    "text": "Borel \\(\\sigma\\)-field"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#르벡메져",
    "href": "posts/ap/2023-04-25-ap-8wk.html#르벡메져",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "르벡메져",
    "text": "르벡메져"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#불완전한-정의",
    "href": "posts/ap/2023-04-25-ap-8wk.html#불완전한-정의",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "불완전한 정의",
    "text": "불완전한 정의\n- 확률변수: \\(X:\\Omega \\to \\mathbb{R}\\)인 조금 특별한 성질을 가진 함수\n\n정의역: \\(\\Omega\\)\n치역: \\(\\mathbb{R}\\)\n\n(예제1) 동전예제\n1. outcomes2: \\(H\\),\\(T\\).\n2. sample space: \\(\\Omega = \\{H,T\\}\\)\n3. event3: \\(\\emptyset\\), \\(\\{H\\}\\), \\(\\{T\\}\\), \\(\\{H,T\\}\\).\n4. \\(\\sigma\\)-field: \\({\\cal F}=2^\\Omega\\)\n5. probability measure function: \\(P: {\\cal F} \\to [0,1]\\) such that\n\n\\(P(\\emptyset) = 0\\)\n\\(P(\\{H\\}) = \\frac{1}{2}\\)\n\\(P(\\{T\\}) = \\frac{1}{2}\\)\n\\(P(\\Omega) = 1\\)\n\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that\n\n\\(X(H)=1\\)\n\\(X(T)=0\\)\n\n만약에 편의상 \\(\\Omega=\\{H,T\\}=\\{\\omega_1,\\omega_2\\}\\)와 같이 사용한다면\n\n\\(X(\\omega_1)=1\\)\n\\(X(\\omega_2)=0\\)"
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#헷갈려-1-starstarstar",
    "href": "posts/ap/2023-04-25-ap-8wk.html#헷갈려-1-starstarstar",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "헷갈려 (1) (\\(\\star\\star\\star\\))",
    "text": "헷갈려 (1) (\\(\\star\\star\\star\\))\n- 질문1: 아래의 표현 중 옳은 것은?\n\n\\(X(H)=0\\)4\n\\(P(\\{H\\})=\\frac{1}{2}\\)5\n\\(P(\\{\\omega_1\\})=\\frac{1}{2}\\)6\n\\(P(H)=\\frac{1}{2}\\)7\n\\(P(\\{H,T\\})=1\\)8\n\\(P(\\omega_1)=\\frac{1}{2}\\)9\n\n- 질문2: 질문1의 4번의 표현 많이 본적 있다. 예를들어서 고등학교에서 두 사건의 독립에 대해 배울때 아래와 같은 방식으로 표현했었다. // 출처: 네이버 블로그\n\n두 사건 \\(A\\), \\(B\\)에 대하여 \\(P(B|A) =P(B|A^c) =P(B)\\) 이면 두 사건이 독립이라고 한다~~\n\n그렇다면 이 표현은 틀린걸까?\n(해설)\n여기에서 사건 \\(A\\), \\(B\\)는 event을 의미하며 outcome을 의미하는게 아님. 즉 \\(A\\), \\(B\\)는 집합임.\n암기: 확률은 항상 집합을 입력으로 받아야 함!!\n- 질문3(\\(\\star\\star\\star\\)): 수리통계 시간에서 아래와 같은 표현 본 적 있다.\n\\[P(X=1)=\\frac{1}{2}\\]\n그런데 \\(P\\)의 입력으로는 집합이 들어가야하는데, \\(X=1\\)은 그냥 수식임. 그렇다면 이 표현은 틀린 표현일까??\n(해설)\n사실 \\(P(X=1)\\)의 의미는 아래와 같은 표현의 축약형이다.\n\\[P\\big(\\{\\omega: X(\\omega)=1 \\} \\big)\\]\n\\(\\{\\omega: X(\\omega)=1\\} = \\{\\omega_1\\} = \\{H\\}\\) 를 의미하므로 결국\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n이 된다. 따라서 옳은 표현이다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#확률변수에-대한-통찰",
    "href": "posts/ap/2023-04-25-ap-8wk.html#확률변수에-대한-통찰",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "확률변수에 대한 통찰",
    "text": "확률변수에 대한 통찰\n- 아래와 같은 표현을 다시 관찰하자.\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n통찰1. 확률변수가 “함수”라는 사실을 떠올리고 \\(1\\)이라는 값이 확률변수의 “상(image)” 라는 사실을 떠올리면, \\(\\{\\omega: X(\\omega)=1\\}\\)은 1에 대한 “역상(inverse image)”이라고 해석할 수 있다.10\n통찰2. 확률변수의 상은 \\(\\mathbb{R}\\)에 맺히게 되고, 확률변수의 역상은 \\(\\Omega\\)의 부분집합 중 하나에 맺히게 된다.\n통찰3. 문제는 확률변수의 역상이 잴 수 있는 집합에 맺힌다는 보장이 있냐라는 것이다… 즉 이 예제로 한정하면\n\\[\\{\\omega: X(\\omega)=1\\} \\in {\\cal F}\\]\n임을 보장해야 한다는 것이다.\n통찰4. 당연히 이러한 보장을 할 수는 없어보인다. 따라서 \\(X\\)를 단지 그냥\n\n\\(X: \\mathbb{\\Omega} \\to \\mathbb{R}\\)로 가는 함수\n\n가 아니라\n\n\\(X: \\mathbb{\\Omega} \\to \\mathbb{R}\\)로 가는 함수 & 역상이 항상 잴 수 있는 집합이어야 함.\n\n이라는 조건이 필요하다.\n- 역상이 잴 수 있는 집합인 함수를 간단히 잴 수 있는 함수 (measurable function) 라고 한다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#확률변수의-엄밀한-정의",
    "href": "posts/ap/2023-04-25-ap-8wk.html#확률변수의-엄밀한-정의",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "확률변수의 엄밀한 정의",
    "text": "확률변수의 엄밀한 정의\n- 확률변수 (머리속): \\(X:\\Omega \\to \\mathbb{R}\\) 인 잴 수 있는 함수.\n- 확률변수 (엄밀하게): 두 개의 잴 수 있는 공간 \\((\\Omega,{\\cal F})\\)와 \\((\\mathbb{R}, {\\cal R})\\)이 있다고 하자. 확률변수 \\(X\\)는 아래를 만족하는 함수 \\(X:\\Omega \\to \\mathbb{R}\\) 이다.\n\\[\\forall B \\in {\\cal R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n\nNote1: \\(\\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\) for all \\(B \\in {\\cal R}\\) 이라 쓰기도 함. 쓰는사람 마음~\n\n- 왜 정의가 아래와 같지 않을까?\n\\[\\forall B \\subset \\mathbb{R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n\n위의 질문을 위한 보충학습\n(예제) 바늘이 하나 있는 시계\n1. outcomes: \\(0,1,\\frac{\\pi}{3},\\frac{2\\pi}{5},\\pi\\dots\\)\n2. sample space: \\(\\Omega = (0,2\\pi]\\)\n3. event: \\(\\emptyset\\), \\([0,\\frac{2}{\\pi})\\), \\(\\{2\\pi\\}\\), \\(\\dots\\)\n4. \\(\\sigma\\)-field: \\({\\cal F}\\)\n5. probability measure function: \\(P: \\Omega \\to [0,1]\\) such that\n\n\\(P(\\emptyset) = 0\\)\n\\(P([0,\\frac{2}{\\pi}) = \\frac{1}{4}\\)\n\\(P(\\{2\\pi\\}) = 0\\)\n\\(P(\\Omega) = 1\\)\n\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that \\(X(\\omega)=\\omega\\). // 사실 \\(X: (0,2\\pi] \\to (0,2\\pi]\\)\n위의 예제에서 6을 주목하자. 만약에 비탈리집합 \\(V \\subset \\mathbb{R}\\)에 대한 inverse image는 비탈리집합 그 자체가 된다. 따라서 아래와 같이 된다.\n\\[P(X \\in V)=P\\big(\\{\\omega: X(\\omega) \\in V\\}\\big)=P(V)\\]\n\\(V\\)는 르벡메져로는 잴 수 없으므로 \\(P(V)\\)와 같은 표현을 불가함.\n결론: 확률변수 \\(X\\)를 고려할때 정의역의 치역 양쪽의 measurable space를 고려해야함.\n\n- 교재의 정의1\n\n\n\n그림1: Durret에서 긁어온 확률변수의 정의\n\n\n- 교재의 정의2\n\n\n\n그림2: Durret에서 긁어온 확률변수의 정의2\n\n\n- \\(X\\)가 랜덤변수라는 것을 기호로 간단하게 \\(X \\in {\\cal F}\\) 혹은 \\(X : (\\Omega, {\\cal F}) \\to (\\mathbb{R},{\\cal R})\\)라고 쓴다.\n\n사실 \\(X: (\\Omega,{\\cal F}) \\to (\\mathbb{R}, {\\cal R})\\)은 \\(X\\)가 잴 수 있는 함수 (measurable function, measurable map) 임을 나타내는 기호이다."
  },
  {
    "objectID": "posts/ap/2023-04-25-ap-8wk.html#헷갈려-2-starstarstar",
    "href": "posts/ap/2023-04-25-ap-8wk.html#헷갈려-2-starstarstar",
    "title": "8wk: 확률공간,분포,확률변수",
    "section": "헷갈려 (2) (\\(\\star\\star\\star\\))",
    "text": "헷갈려 (2) (\\(\\star\\star\\star\\))\n- 확률변수에 대한 오해1: 학률변수 = 값이 랜덤으로 바뀌는 변수??\n\n함수: \\(y=f(x)\\), \\(f\\): function, \\(x\\): input \\(y\\): output\n확률변수: \\(x=X(\\omega)\\), \\(X\\): function, \\(\\omega\\): outcome11, \\(x\\): realization\n확률변수는 함수이지만 보통 \\(X(\\omega)\\)와 같이 쓰지 않고 \\(X\\)라고 쓴다. \\(\\Rightarrow\\) 혼란의 이유\n\n- 확률변수에 대한 오해2: 확률변수는 결과가 랜덤으로 변하는 함수??\n\n확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n\n- 확률변수에 대한 오해3: 아니야.. 확률변수는 결과가 랜덤으로 바뀌는 느낌이 맞아. 아래의 예시를 봐!\n\\[X = \\begin{cases} 0 & w.p. \\frac{1}{2} \\\\ 1 & w.p. \\frac{1}{2} \\end{cases}\\]\n\n\\(X\\)는 진짜 변수처럼 보이긴함.\n심지어 변수의 값이 랜덤으로 변하는 것 같음.\n\n(해설)\n정확하게는 아래 표현이 맞다.\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\{H\\} \\\\ 1 & \\omega \\in \\{T\\} \\end{cases} \\quad \\text{where } P(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2}.\\]\n- 확률변수에 대한 오해2에 대한 추가설명\n\n확률변수는 결과가 랜덤으로 변하는 함수가 아님, 확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n단지 입력 outcome이 실험에 따라 랜덤으로 변할 수 있는 것임!!\n\n- 요약해보면,\n\n확률변수는 확률과 관련없다.\n간접적으로는 관련이 있다. \\(\\because\\) \\(X\\)의 역상 = \\(\\Omega\\)의 부분집합 = \\(P\\)의 정의역"
  },
  {
    "objectID": "posts/ap/2023-03-29-ap-4wk_2.html",
    "href": "posts/ap/2023-03-29-ap-4wk_2.html",
    "title": "4wk: 측도론 (1)",
    "section": "",
    "text": "- \\(\\Omega \\neq \\emptyset\\) 이고 \\({\\cal A}\\subset 2^\\Omega\\) 라고 하자.\n- 약속: 집합 \\({\\cal A} \\subset 2^{\\Omega}\\)에 대하여 아래와 같은 용어를 약속하자.\n\n\\(\\cap\\)-closed (closed under intersection) or a \\(\\pi\\)-system: \\(\\forall A,B \\in {\\cal A}:~ A \\cap B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cap\\)-closed (closed under countable interserction): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cap_{i=1}^{\\infty} A_i \\in {\\cal A}\\)\n\\(\\cup\\)-closed (closed under unions): \\(\\forall A,B \\in {\\cal A}:~ A\\cup B \\in {\\cal A}\\)\n\\(\\sigma\\)-\\(\\cup\\)-closed (closed under countable unois): \\(\\forall \\{A_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:~ \\cup_{i=1}^{\\infty}A_i \\in {\\cal A}\\)\n\\(-\\)closed (closed under differences): \\(\\forall A,B \\in {\\cal A}:~ A-B \\in {\\cal A}\\)\n\\(^c\\)-closed (closed under complements): \\(\\forall A \\in {\\cal A}:~ A^c \\in {\\cal A}\\)\n\n- 우리만의 약속:\n\n앞으로 서로소인 집합들에 대한 합집합은 기호로 \\(\\uplus\\)라고 표현하겠다.\n따라서 앞으로 \\(B_1 \\uplus B_2\\)의 의미는 (1) \\(B_1 \\cup B_2\\) (2) \\(B_1 \\cap B_2 = \\emptyset\\) 을 의미한다고 정의하겠다. (꼭 서로소임을 명시하지 않아도)\n\\(\\sigma\\)-\\(\\uplus\\)-closed 의 의미는 \\(\\uplus_{i=1}^{\\infty}B_i \\in {\\cal A}, \\forall \\{B_i\\}_{i=1}^{\\infty} \\subset {\\cal A}:\\) 의 의미이다.\n\n- 정의: 시그마필드 (\\(\\sigma\\)-field, \\(\\sigma\\)-algebra)\n집합 \\({\\cal F} \\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal F}\\)를 \\(\\Omega\\)에 대한 시그마필드라고 부른다.\n\n\\(\\Omega \\in {\\cal F}\\).\n\\({\\cal F}\\)는 여집합에 닫혀있다.\n\\({\\cal F}\\)는 가산합집합에 닫혀있다.\n\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\) 가 여집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)가 교집합1에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 합집합2에 닫혀있음.\n\\({\\cal A}\\)가 가산교집합3에 닫혀있음. \\(\\Leftrightarrow\\) \\({\\cal A}\\)가 가산합집합4에 닫혀있음.\n\n(증명) 생략\n- 이론: \\({\\cal A}\\subset 2^{\\Omega}\\)가 차집합에 닫혀있다면, 아래가 성립한다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)가 가산합집합에 닫혀있다. \\(\\Rightarrow\\) \\({\\cal A}\\)가 가산교집합에 닫혀있다.\n\\(\\forall \\{A_i\\} \\subset {\\cal A},~ \\exists \\{B_i\\} \\subset {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\uplus_{i=1}^{\\infty} B_i\\).\n\n(증명) 생략\n\n차집합에 닫혀있다는게 엄청 좋은 거였음.\n\n- 정의: 알지브라, 필드 (algebra, field)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n\nNote: 전체집합을 포함하면 차집합에 닫혀있다면 여집합에 닫혀있게 된다. 따라서 대수는 여집합에 닫혀있다.\n\n\nNote: 여집합에 닫혀있고 합집합에 닫혀있다면 교집합에 닫혀있게 된다. // 차집합에만 닫혀있어도 교집합에 닫혀있게 된다.\n\n- 정의: 알지브라의 또다른 정의\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\n- 정의: 알지브라의 또 또 다른 정의 (교재의 정의)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 대수라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 여집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n- 정의: 링 (ring)\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 차집합에 닫혀있다.\n\\({\\cal A}\\)는 합집합에 닫혀있다.\n\n\nNote: 차집합에 닫혀있으므로 교집합에도 닫혀있다. 따라서 링은 교집합과 합집합 모두에 닫혀 있다.\n\n\nNote: 링에는 전체집합이 없으므로 여집합의 개념이 없다.\n\n- 정의: 세미알지브라 (semi-algebra) // ref : 위키북스\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 링이라고 부른다.\n\n\\(\\emptyset,\\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A,B \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A-B = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 차집합에 반쯤 닫혀있다고 표현한다. 즉 차집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 정의: 세미알지브라의 또 다른 정의 // ref: 세미링의 위키에서 언급있음\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미알지브라 라고 부른다.\n\n\\(\\emptyset, \\Omega \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\(\\forall A \\in {\\cal A}, \\exists \\{B_i\\}_{i=1}^{n} \\subset {\\cal A}\\) such that \\[A^c = \\uplus_{i=1}^{n} B_i.\\]\n\n\n3번을 \\({\\cal A}\\)가 여집합에 반쯤 닫혀있다고 표현한다. 즉 여집합 자체가 \\({\\cal A}\\)에 들어가는건 아니지만 차집합의 disjoint한 조각들은 모두 \\({\\cal A}\\)에 들어간다.\n\n- 정의: 세미링\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 세미링이라고 부른다.\n\n\\(\\emptyset \\in {\\cal A}\\).\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\\({\\cal A}\\)는 차집합에 반쯤 닫혀있다.\n\n- 정의: \\(\\pi\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 파이스시템 이라고 부른다.\n\n\\({\\cal A}\\)는 교집합에 닫혀있다.\n\n- 정의: \\(\\lambda\\)-system\n집합 \\({\\cal A}\\subset 2^{\\Omega}\\)가 아래의 조건을 만족하면 \\({\\cal A}\\)를 \\(\\Omega\\)에 대한 람다시스템 이라고 부른다.\n\n\\(\\Omega \\in {\\cal A}\\)\n\\(\\forall A,B \\in {\\cal A}:~ A\\subset B \\Rightarrow B-A \\in {\\cal A}\\)\n\\(\\forall B_1,B_2,\\dots \\in {\\cal A}\\) such that \\(B_1,B_2\\dots\\) are disjoint: \\[\\uplus_{i=1}^{\\infty} B_i \\in {\\cal A}\\]\n\n\n람다시스템은 1. 전체집합이 포함되고 2. 두 집합이 포함관계에 있는 경우 차집합에 닫혀있으며 3. 서로소인 가산합집합에 닫혀있다.\n\n- 정리표 (hw): 물음표를 채워라\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A \\cap B\\)\n\\(\\emptyset\\)\n\\(A-B\\)\n\\(\\cup_iA_i=\\uplus_i B_i\\)\n\\(\\Omega\\)\n\\(A^c\\)\n\\(A\\cup B\\)\n\\(\\cup_{i=1}^{\\infty}A_i\\)\n\\(\\uplus_{i=1}^{\\infty}B_i\\)\n\\(\\cap_{i=1}^{\\infty}A_i\\)\n\n\n\n\n\\(\\pi\\)-system\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\n\nsemi-ring\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nsemi-algebra\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nring\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\nalgebra\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\sigma\\)-ring\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\lambda\\)-system\n\\(?\\)\n\\(?\\)\n\\(\\Delta\\)’\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\\(?\\)\n\n\n\\(\\sigma\\)-field\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n\n\n- 예제1: (\\(\\star\\)) 임의의 인덱스 집합 \\(I\\neq\\emptyset\\)를 고려하자. 여기에서 \\(I\\)는 uncountable set일 수도 있다. 아래의 사실에 대하여 참 거짓을 판단하라.\n\n\\({\\cal F}_i\\)가 모두 시그마필드라면, \\(\\cap_{i \\in I}{\\cal F_i}\\) 역시 시그마필드이다.\n\n- 귀찮아서 만든 이론1: 모든 \\({\\cal A} \\subset 2^{\\Omega}\\) 에 대하여 smallest \\(\\sigma\\)-field containing \\({\\cal A}\\), 즉 \\(\\sigma({\\cal A})\\)는 존재한다. (그리고 당연히 smallest 조건에 의에서 유일성이 보장됨)\n(증명) \\({\\cal A}\\)를 포함하는 모든 시그마필드를 교집합하면 \\(\\sigma({\\cal A})\\)가 된다.\n\n\n\n그림1: Durret교재에서 언급된 “귀찮아서 만든 이론1”\n\n\n- 이론: 아래의 사실들이 성립한다.\n\n\\({\\cal A}_1 \\subset {\\cal A}_2 \\Rightarrow \\sigma({\\cal A}_1) \\subset \\sigma({\\cal A}_2)\\).\n\\({\\cal A}\\) is \\(\\sigma\\)-field \\(\\Leftrightarrow\\) \\(\\sigma({\\cal A})= {\\cal A}\\)."
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html",
    "href": "posts/ap/2023-04-18-ap-7wk.html",
    "title": "7wk: 측도론 (3)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-yQ5IXoRW0pW0Gyd8MnRwaW"
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#헷갈리는-표현-infty의-포함",
    "href": "posts/ap/2023-04-18-ap-7wk.html#헷갈리는-표현-infty의-포함",
    "title": "7wk: 측도론 (3)",
    "section": "헷갈리는 표현: \\(\\infty\\)의 포함",
    "text": "헷갈리는 표현: \\(\\infty\\)의 포함\n- 자연수집합 \\(\\mathbb{N}\\)은 \\(\\{\\infty\\}\\)를 포함하지 않는다. 마찬가지로 실수집합 \\(\\mathbb{R}\\) 역시 \\(\\{-\\infty\\}, \\{\\infty\\}\\)를 포함하지 않는다. 만약에 이를 포함하고 싶을 경우는 아래와 같이 표현한다.\n\n\\(\\mathbb{R} \\cup \\{-\\infty\\} \\cup \\{\\infty\\} = \\bar{\\mathbb{R}}\\)\n\\(\\mathbb{N} \\cup \\{-\\infty\\}\\)\n\n여기에서 \\(\\bar{\\mathbb{R}}\\)은 확장된 실수라고 부르는데 교재에따라 사용하기도 하고 사용하지 않기도 한다.\n- 만약에 \\(\\mathbb{N}\\)이 \\(\\{\\infty\\}\\)를 포함한다면\n\n\\(\\forall n \\in \\mathbb{N}:~ 0<\\frac{1}{n} \\leq 1\\)\n\n와 같은 표현은 불가능할 것이다.\n- 구간에 대한 표현들: 구간에 대한 몇가지 표현을 정리하면 아래와 같다.\n\n\\((-\\infty, b] = \\{x: x\\leq b, ~x,b \\in \\mathbb{R}\\}\\)\n\\((-\\infty, b) = \\{x: x < b,~ x,b \\in \\mathbb{R}\\}\\)\n\n- 구긴에 대한 표현 응용: 아래와 같은 표현을 고려하자. (교재의 예제 1.1.8과 비슷한 표현)\n\n\\({\\cal A} = \\{(a,b]: -\\infty \\leq a < b \\leq \\infty\\}\\)\n\n\\({\\cal A}\\)의 원소의 형태는\n\n\\(\\{x: a<x\\leq b,~ a,x,b \\in \\mathbb{R}\\}\\)\n\\(\\{x: a<x,~ a,x \\in \\mathbb{R}\\}\\)\n\\(\\{x: x\\leq b,~ x,b \\in \\mathbb{R}\\}\\)\n\\(\\{x: x \\in \\mathbb{R}\\}\\)\n\n이다.\n\n약간 무식하게 생각하면 \\([-\\infty, b) = (-\\infty,b)\\) 로 해석하면 된다. 즉 \\(\\{-\\infty\\} \\notin [-\\infty,b)\\) 이라는 의미! 보는것 처럼 \\([-\\infty, b)\\)와 같은 표현은 엄청난 혼란을 불러오는 표현이므로 사용을 자제한다."
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#메져의-종류와-성질",
    "href": "posts/ap/2023-04-18-ap-7wk.html#메져의-종류와-성질",
    "title": "7wk: 측도론 (3)",
    "section": "메져의 종류와 성질",
    "text": "메져의 종류와 성질\n- 메져의 종류와 성질 요약\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n분류\n\\(m(\\emptyset)=0\\)\n\\(\\sigma\\)-add\n\\(A_i\\uparrow \\Omega\\), \\(m(A_i)<\\infty\\)\n\\(m(\\Omega)<\\infty\\)\n\\(m(\\Omega)=1\\)\n\\(.\\)\nmonotone\n\\(\\sigma\\)-subadd\nconti-below\nconti-above\n\n\n\n\nmsr\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\n\n\\(\\sigma\\)-finite-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(\\Delta\\)\n\n\nfinite-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(X\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\nprob-msr\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(.\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\\(O\\)\n\n\n\n- 용어들\n\n\\(\\sigma\\)-additive: \\(m(\\uplus_{i=1}^{\\infty} B_i) = \\sum_{i=1}^{\\infty} m(B_i)\\)\nmonotone: \\(A\\subset B \\Rightarrow m(A) \\subset m(B)\\)\n\\(\\sigma\\)-subadditive: \\(m(\\cup_{i=1}^{\\infty} A_i) \\leq \\sum_{i=1}^{\\infty} m(A_i)\\)\ncontinuous from below: \\(A_i \\uparrow A\\) \\(\\Rightarrow\\) \\(m(\\lim_{n\\to\\infty}A_i)=\\lim_{n\\to\\infty}m(A_i)\\)\ncontinuous from above: (1) \\(A_i \\downarrow A\\) and (2) \\(m(A_1)<\\infty\\) \\(\\Rightarrow\\) \\(m(\\lim_{n\\to\\infty}A_i)=\\lim_{n\\to\\infty}m(A_i)\\)\n\n- 교재의 언급 (p2. Thm 1.1.1)\n\n\n\n그림1: 메져의 성질 durret p2\n\n\n- \\(\\sigma\\)-finite msr 에 대한 동치조건: \\(m\\)이 \\((\\Omega, {\\cal F})\\)에서의 msr이라면, 아래는 동치이다. (ref: https://en.wikipedia.org/wiki/%CE%A3-finite_measure)\n언어버전\n\nThe set \\(\\Omega\\) can be covered with at most countably many measurable sets with finite measure.\nThe set \\(\\Omega\\) can be covered with at most countably many measurable disjoint sets with finite measure.\nThe set \\(\\Omega\\) can be covered with monotone sequence of measurable sets with finite measure.\n\n수식버전\n\nThere are sets \\(A_1,A_2,\\dots \\in {\\cal A}\\) with \\(m(A_i)<\\infty\\) such that \\(\\cup_{i=1}^{\\infty}A_i=\\Omega\\)\nThere are sets \\(B_1,B_2,\\dots \\in {\\cal A}\\) with \\(m(B_i)<\\infty\\) and \\(B_1,B_2\\dots\\) are disjoints such that \\(\\uplus_{i=1}^{\\infty}B_i=\\Omega\\)\nThere are sets \\(C_1,C_2,\\dots \\in {\\cal A}\\) with \\(m(C_i)<\\infty\\) and $C_1 C_2 $ such that \\(\\cup_{i=1}^{\\infty}C_i=\\Omega\\)"
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#복습-motivating-ex",
    "href": "posts/ap/2023-04-18-ap-7wk.html#복습-motivating-ex",
    "title": "7wk: 측도론 (3)",
    "section": "복습 & Motivating EX",
    "text": "복습 & Motivating EX\n- 귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n- 이론: \\((\\Omega, \\sigma({\\cal A}), P)\\)를 확률공간이라고 하자. 여기에서 \\({\\cal A}\\)는 파이시스템이라고 가정하자. 그렇다면 확률측도 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(P: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다.\n- 이 이론은 확률측도일 경우만 성립하고 측도일 경우는 실패했었다.\n(예제1) – 통계학과라서 행복했던 예제\n\\(\\Omega=\\{a,b\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\}\\}\\) 라고 하자. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 모든 확률측도 \\(P\\)는 \\({\\cal A}\\)에서의 값으로 유일하게 결정됨을 확인하였다. 하지만 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정되지 않는다.\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(\\frac{3}{2}\\)\n\n\n\n- 직관: 그냥 \\({\\cal A}\\)에 \\(\\Omega\\)가 있었다면 되는거 아닌가? 예를들어 아래와 같이 설정한다면?\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(\\frac{3}{2}\\)\n\\(\\frac{3}{2}\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\\(m_1(\\{b\\})=m_2(\\{b\\})=1\\) 일 수밖에 없지 않을까?\n- 혹시 아래와 같이 이론을 수정하면 되지 않을까?\n\n\\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 메져라고 하자. 만약에 \\({\\cal A}\\)가 “전체집합을 포함하는 파이시스템” 이라면 메져 \\(m:\\sigma({\\cal A}) \\to [0,1]\\)의 값은 \\(m: {\\cal A} \\to [0,1]\\)의 값에 의하여 유일하게 결정된다. (거의 맞는데 한 조건이 빠져서 틀렸음)\n\n(예제2)\n\\(\\Omega=\\{a,b,c\\}\\) 이라고 하고 \\({\\cal A} = \\{\\{a\\},\\Omega\\}\\) 라고 하자. 여기에서 \\({\\cal A}\\)는 “\\(\\Omega\\)가 포함된 파이시스템”이다. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한측도 \\(m\\)은 \\({\\cal A}\\)에서의 값으로 유일하게 결정될까?\n(풀이) 아래의 반례가 존재함.\n\n\n\n\n\\(m_1\\)\n\\(m_2\\)\n\n\n\n\n\\(\\{a\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{b\\}\\)\n\\(1\\)\n\\(\\infty\\)\n\n\n\\(\\{c\\}\\)\n\\(\\infty\\)\n\\(5\\)\n\n\n\\(\\{a,b\\}\\)\n\\(\\frac{3}{2}\\)\n\\(\\infty\\)\n\n\n\\(\\{a,c\\}\\)\n\\(\\infty\\)\n\\(\\frac{11}{2}\\)\n\n\n\\(\\{b,c\\}\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n- 이론: \\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 유한측도라고 하자. 그리고 \\({\\cal A}\\)는 전체집합을 포함하는 파이시스템이라고 하자. 그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,M]\\)의 값은 \\(m: {\\cal A} \\to [0,M]\\)의 값에 의하여 유일하게 결정된다. (단, \\(M=m(\\Omega)<\\infty\\))\n(예제3) – \\({\\cal A}\\)가 \\(\\Omega\\)를 포함하지 않는데, 메져가 유일하게 결정될 것 같은 예제\n\\(\\Omega = \\mathbb{Z}\\) 이라고 하자. \\(\\Omega\\)의 부분집합들로 이루어진 집합열 \\(A_1,A_2,\\dots\\) 를 아래와 같이 정의하자.\n\n\\(A_{1} = [-\\frac{1}{2}, \\frac{2}{2}] \\cap \\mathbb{Z} = \\{0, 1\\}\\)\n\\(A_{2} = [-\\frac{2}{2}, \\frac{3}{2}] \\cap \\mathbb{Z} = \\{-1, 0, 1\\}\\)\n\\(A_{3} = [-\\frac{3}{2}, \\frac{4}{2}] \\cap \\mathbb{Z} = \\{-1, 0, 1, 2\\}\\)\n\\(A_{4} = [-\\frac{4}{2}, \\frac{5}{2}] \\cap \\mathbb{Z} = \\{-2, -1, 0, 1, 2\\}\\)\n\\(A_{5} = [-\\frac{5}{2}, \\frac{6}{2}] \\cap \\mathbb{Z} = \\{-2, -1, 0, 1, 2, 3\\}\\)\n\\(\\dots\\)\n\n관심있는 집합들의 모임은 \\({\\cal A}=\\{A_n:n \\in \\mathbb{N}\\}\\)로 정의하자. 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)의 값으로 유일하게 결정될까?\n(관찰)\n풀이에 앞서서 아래의 사실을 관찰해보자.\n\n\\({\\cal A}\\)는 파이시스템이다.\n집합열 \\(A_n\\)의 극한은 \\(\\Omega\\)이다. 집합열 \\(A_n\\)은 증가하는 수열이므로 이 경우 \\(A_n \\uparrow \\Omega\\)라고 표현할 수 있다.\n모든 \\(A_n\\)이 \\({\\cal A}\\)의 멤버라고 했으나 \\(A_n\\)의 극한 \\(\\Omega\\)가 \\({\\cal A}\\)의 멤버라고 한 적은 없다. 따라서 \\({\\cal A}\\)는 전체집합을 포함하지는 않는 파이시스템이다.\n\n(풀이)\n가측공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서 정의가능한 측도 \\(m\\)은 \\({\\cal A}\\)의 값으로 유일하게 결정하는 것이 가능할 것 같다. (실제로 가능해) 왜냐하면\n\n\\(m(A_1),m(A_2), m(A_3) \\dots\\) 의 값이 결정 \\(\\Rightarrow\\) \\(m(\\{0,1\\})\\), \\(m(\\{-1\\})\\), \\(m(\\{2\\})\\), \\(\\dots\\) 의 값이 결정\n\n이므로, 0과 1을 제외한 \\(\\mathbb{Z}\\)의 모든 원소의 길이가 유일하게 결정되니까.\n생각의 시간\n아래의 이론을 다시 관찰하자.\n\n이론: \\((\\Omega, \\sigma({\\cal A}))\\)을 잴 수 있는 공간이라고 하고, \\(m\\)을 이 공간에서의 유한측도라고 하자. 그리고 \\({\\cal A}\\)는 전체집합을 포함하는 파이시스템이라고 하자. 그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,M]\\)의 값은 \\(m: {\\cal A} \\to [0,M]\\)의 값에 의하여 유일하게 결정된다. (단, \\(M=m(\\Omega)<\\infty\\))\n\n(의문1)\n\\({\\cal A}\\)가 꼭 전체집합을 포함할 필요는 없어보인다. 즉 조건 \\(\\Omega \\in {\\cal A}\\)는 굳이 필요 없어보인다. 이 조건은 더 약한 아래의 조건으로 대치가능하다.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(A_i \\uparrow \\Omega\\)\n\n만약에 \\(\\Omega \\in {\\cal A}\\)인 경우는 \\(A_1=\\Omega\\)로 잡으면 위 조건이 그냥 성립한다. 따라서 위의 조건은 \\(\\Omega \\in {\\cal A}\\) 보다 약한 조건이다. 그리고 심지어 위의 조건은 다시 아래의 더 약한 조건으로 바꿀 수 있다.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\Omega\\)\n\n(의문2)\n심지어 \\(m(\\Omega) = \\infty\\) 이어도 상관없다.1 이 예제에서\n\n\\(m(\\{0,1\\})=2\\)\n\\(m(\\{-1\\})=1\\)\n\\(m(\\{2\\})=1\\)\n\\(\\dots\\)\n\n이라고 하면 \\(m\\)은 잴 수 있는 공간 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 카운팅메져가 되고, 그 \\(m\\)은 \\(A \\in {\\cal A}\\)에서의 값으로 유일하게 결정된다. 문제가 생길만한 것은\n\n\\(m(\\{0,1\\})=2\\)\n\\(m(\\{-1\\})=1\\)\n\\(m(\\{2\\})=\\infty\\) <– 이러면 곤란\n\\(\\dots\\)\n\n와 같은 경우이므로, 이 경우만 제약하면 된다. 즉 \\(m\\)이 시그마유한측도라고 제한하면 될 것 같다."
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#state",
    "href": "posts/ap/2023-04-18-ap-7wk.html#state",
    "title": "7wk: 측도론 (3)",
    "section": "state",
    "text": "state\n- Thm: \\((\\Omega, \\sigma({\\cal A}),m)\\)을 시그마유한측도공간(\\(\\sigma\\)-finite measure space)이라고 하자. \\({\\cal A}\\)은 아래를 만족하는 파이시스템이라고 하자.\n\n\\(\\exists A_1,A_2,\\dots \\in {\\cal A}\\) such that \\(\\cup_{i=1}^{\\infty} A_i = \\Omega\\)\n\\(\\forall i \\in \\mathbb{N}:~ m(A_i) <\\infty\\)\n\n그렇다면 메져 \\(m:\\sigma({\\cal A}) \\to [0,\\infty]\\)의 값은 \\(m: {\\cal A} \\to [0,\\infty]\\)의 값에 의하여 유일하게 결정된다.\n\n조건 1,2는 결국 \\(m\\)을 시그마유한측도로 만들어주는 그 집합열이 \\(\\sigma({\\cal A})-{\\cal A}\\)가 아니라 \\({\\cal A}\\)에 있어야 한다는 의미임."
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#증명",
    "href": "posts/ap/2023-04-18-ap-7wk.html#증명",
    "title": "7wk: 측도론 (3)",
    "section": "증명",
    "text": "증명\n- 노트: supp_7wk.pdf\n- 교재의 증명: 교재의 증명은 좀 더 강한 조건에서 했음. (“\\(A_1,A_2,\\dots, {\\cal A}\\) with \\(m(A_i)<\\infty\\) such that \\(A_i \\uparrow \\Omega\\)” 를 가정함.)\n\n\n\n그림2: 카라데오도리 확장정리의 유일성 part 증명, durret p457-8"
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#state-1",
    "href": "posts/ap/2023-04-18-ap-7wk.html#state-1",
    "title": "7wk: 측도론 (3)",
    "section": "state",
    "text": "state\n- Thm: \\({\\cal A}\\)가 \\(\\Omega\\)에 대한 semiring이라고 하자. 함수 \\(\\tilde{m}: {\\cal A} \\to [0,\\infty]\\)가\n\n\\(\\tilde{m}(\\emptyset)=0\\)\n\\(\\tilde{m}(\\uplus_{i=1}^{n} B_i)=\\sum_{i=1}^{n}\\tilde{m}(B_i)\\)\n\\(\\tilde{m}(\\cup_{i=1}^{\\infty} A_i) \\leq \\sum_{i=1}^{\\infty}\\tilde{m}(A_i)\\)\n\\(\\exists A_1,A_2 \\dots \\in {\\cal A}\\) with \\(m(A_i)<\\infty\\) such that \\(\\cup_{i=1}^{\\infty}A_i = \\Omega\\)\n\n를 만족한다면 \\(\\tilde{m}\\)은 \\((\\Omega,\\sigma({\\cal A})\\)에서의 측도 \\(m\\)으로 업그레이드 가능하며, 이 업그레이드 결과는 유일하다.\n\n이 결과를 ver1로 생각하자.\n\n- 교재의 state (ver2, ver3)\n\n\n\n그림3: 카라데오도리 확장저정리 ver2, durret p456\n\n\n\nver1과의 비교: \\({\\cal A}\\)가 알지브라라는 것은 세미링보다 훨씬 강한 조건이다. 또한 measure on an algebra \\({\\cal A}\\)란 것은 1,2,3 조건을 다 합친것 보다 강한 조건이다. \\(\\sigma\\)-finite이라는 조건은 \\({\\cal A}\\)의 차이를 제외하면 동일하다.\n\n\n\n\n그림4: 카라데오도리 확장정리 ver3, durret p5\n\n\n\nver1과의 비교: \\({\\cal A}\\)가 세미알지브라라는 조건은 세미링보다 강한 조건이다. (i), (ii)의 \\({\\cal A}\\)의 차이만 있을 뿐 거의 동일하다. 4의 조건도 \\({\\cal A}\\)의 차이를 제외하고는 동일하다."
  },
  {
    "objectID": "posts/ap/2023-04-18-ap-7wk.html#예제-3월28일-4wk-예제들",
    "href": "posts/ap/2023-04-18-ap-7wk.html#예제-3월28일-4wk-예제들",
    "title": "7wk: 측도론 (3)",
    "section": "예제: 3월28일 (4wk) 예제들",
    "text": "예제: 3월28일 (4wk) 예제들\n(예제1) – motivating EX\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 집합의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\]\n- 소망: 그래도 그냥 \\({\\cal A}\\)에서만 확률 비슷한 함수 \\(\\tilde{P}\\)를 잘 정의하면 \\((\\Omega,\\sigma({\\cal A}))\\)에서의 확률측도로 업그레이드 가능하고 업그레이드 결과가 유일할까?\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 조건체크\n\n\\({\\cal A}\\)는 세미알지브라(그러므로 세미링)이다.\n\\({\\cal A}\\)는 전체집합을 포함하고 있으며 \\({\\tilde P}(\\Omega)=1\\)이다. \\(\\Rightarrow\\) 조건 (4)가 만족.\n\\({\\tilde P}\\)는 (1) \\(\\tilde{P}(\\emptyset)=0\\) 이고 (2) add 를 만족하며 (3) \\(\\sigma\\)-subadd 를 만족한다.\n\n\n참고: 이 예제의 경우 \\(|\\Omega|<\\infty\\) 이므로 \\(\\sigma\\)-subadd 는 subadd 와 같은 성질이다. 그리고 add 는 subadd를 imply 하므로 사실상 (2) 만 체크하면 끝난다.2\n\n(예제2) – motivating EX (2)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\}, \\{3,4\\}, \\Omega\\}\\) 라고 하자. 그리고 아래와 같은 \\(\\sigma({\\cal A})\\)를 다시 상상하자.\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 위의 시그마필드에서 확률을 예제1과 다른 방식으로 정의할 수 도 있다. 예를들면 아래와 같은 방식으로 정의가능하다.\n\n\n\n\n\\(P_1\\)\n\\(\\tilde{P}_1\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\n또한 아래와 같은 방식도 가능하다.\n\n\n\n\n\\(P_2\\)\n\\(\\tilde{P}_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{2\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\n어떠한 방식으로 정의하든 \\({\\cal A}\\)에서 확률 비슷한 것 \\(\\tilde{P}_1,\\tilde{P}_2\\)를 잘 정의하기만 \\(\\sigma({\\cal A})\\)에서의 확률 \\(P\\)로 적절하게 확장할 수 있다. 심지어 이런 확장은 유일한 듯 하다.\n- 당연함. 예제1과 동일하게 \\(\\tilde{P_1}\\)과 \\(\\tilde{P_2}\\)가 add 성질만 만족한다는 사실을 체크하면 끝난다.\n(예제3) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 0\\)\n\\(\\tilde{P}(\\{2,3\\}) = 0\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 체크: 일단 \\({\\cal A}\\)는 세미링이 아니다. 따라서 확장 불가능. 세미링이 맞다고 하여도 subadd가 성립하지 않는다.\n(예제4) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- 체크: \\(\\tilde{P}\\)는 괜찮게 정의되었다. (1)-(4)가 모두 성립한다. (위의 예제와는 다르게 subadd 역시 성립함!!) 하지만 \\({\\cal A}\\)가 세미링이 아니어서 탈락."
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html",
    "href": "posts/ap/2023-05-02-ap-9wk.html",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-yGgU_c-5m38ONMFujHvYLf"
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#확률공간과-용어들",
    "href": "posts/ap/2023-05-02-ap-9wk.html#확률공간과-용어들",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "확률공간과 용어들",
    "text": "확률공간과 용어들\n- 동전예제에서의 확률공간 \\((\\Omega,{\\cal F},P)\\)를 가정하고 용어를 정리해보자.\n\noutcomes: \\(H\\),\\(T\\)\nset of “outcomes”: \\(\\Omega=\\{H,T\\}\\)\nevent: \\(\\emptyset\\), \\(\\{H\\}\\), \\(\\{T\\}\\), \\(\\{H,T\\}\\)\nset of “events”: \\({\\cal F}\\)\nprobabilites: \\(P:{\\cal F} \\to [0,1]\\)\n\n\n\n\n그림1: 확률을 위한 기본용어"
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-불완전한-정의",
    "href": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-불완전한-정의",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "확률변수의 불완전한 정의",
    "text": "확률변수의 불완전한 정의\n- 확률변수: \\(X:\\Omega \\to \\mathbb{R}\\)인 조금 특별한 성질을 가진 함수\n\n정의역: \\(\\Omega\\)\n공역: \\(\\mathbb{R}\\)\n\n(예제1) 동전예제\n1. outcomes1: \\(H\\),\\(T\\).\n2. sample space: \\(\\Omega = \\{H,T\\}\\)\n3. event2: \\(\\emptyset\\), \\(\\{H\\}\\), \\(\\{T\\}\\), \\(\\{H,T\\}\\).\n4. \\(\\sigma\\)-field: \\({\\cal F}=2^\\Omega\\)\n5. probability measure function: \\(P: {\\cal F} \\to [0,1]\\) such that\n\n\\(P(\\emptyset) = 0\\)\n\\(P(\\{H\\}) = \\frac{1}{2}\\)\n\\(P(\\{T\\}) = \\frac{1}{2}\\)\n\\(P(\\Omega) = 1\\)\n\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that\n\n\\(X(H)=1\\)\n\\(X(T)=0\\)\n\n만약에 편의상 \\(\\Omega=\\{H,T\\}=\\{\\omega_1,\\omega_2\\}\\)와 같이 사용한다면\n\n\\(X(\\omega_1)=1\\)\n\\(X(\\omega_2)=0\\)"
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#헷갈려-1-star",
    "href": "posts/ap/2023-05-02-ap-9wk.html#헷갈려-1-star",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "헷갈려 (1) (\\(\\star\\))",
    "text": "헷갈려 (1) (\\(\\star\\))\n- 질문1: 아래의 표현 중 옳은 것은?3\n\n\\(X(H)=0\\)\n\\(P(\\{H\\})=\\frac{1}{2}\\)\n\\(P(\\{\\omega_1\\})=\\frac{1}{2}\\)^\n\\(P(H)=\\frac{1}{2}\\)\n\\(P(\\{H,T\\})=1\\)\n\\(P(\\omega_1)=\\frac{1}{2}\\)\n\n- 질문2: 질문1의 4번의 표현 많이 본적 있다. 예를들어서 고등학교에서 두 사건의 독립에 대해 배울때 아래와 같은 방식으로 표현했었다. // 출처: 네이버 블로그\n\n두 사건 \\(A\\), \\(B\\)에 대하여 \\(P(B|A) =P(B|A^c) =P(B)\\) 이면 두 사건이 독립이라고 한다~~\n\n그렇다면 이 표현은 틀린걸까?\n(해설)\n여기에서 사건 \\(A\\), \\(B\\)는 event을 의미하며 outcome을 의미하는게 아님. 즉 \\(A\\), \\(B\\)는 집합임.\n암기: 확률은 항상 집합을 입력으로 받아야 함!!\n- 질문3(\\(\\star\\star\\star\\)): 수리통계 시간에서 아래와 같은 표현 본 적 있다.\n\\[P(X=1)=\\frac{1}{2}\\]\n그런데 \\(P\\)의 입력으로는 집합이 들어가야하는데, \\(X=1\\)은 그냥 수식임. 그렇다면 이 표현은 틀린 표현일까??\n(해설)\n사실 \\(P(X=1)\\)의 의미는 아래와 같은 표현의 축약형이다.\n\\[P\\big(\\{\\omega: X(\\omega)=1 \\} \\big)\\]\n\\(\\{\\omega: X(\\omega)=1\\} = \\{\\omega_1\\} = \\{H\\}\\) 를 의미하므로 결국\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n이 된다. 따라서 옳은 표현이다."
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#확률변수에-대한-통찰",
    "href": "posts/ap/2023-05-02-ap-9wk.html#확률변수에-대한-통찰",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "확률변수에 대한 통찰",
    "text": "확률변수에 대한 통찰\n- 아래와 같은 표현을 다시 관찰하자.\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})\\]\n통찰1. 확률변수가 “함수”라는 사실을 떠올리고 \\(1\\)이라는 값이 확률변수의 “상(image)” 라는 사실을 떠올리면, \\(\\{\\omega: X(\\omega)=1\\}\\)은 1에 대한 “역상(inverse image)”이라고 해석할 수 있다.4\n통찰2. 확률변수의 상은 \\(\\mathbb{R}\\)에 맺히게 되고, 확률변수의 역상은 \\(\\Omega\\)의 부분집합 중 하나에 맺히게 된다.\n통찰3. 문제는 확률변수의 역상이 잴 수 있는 집합5에 맺힌다는 보장이 있냐라는 것이다… 즉 이 예제로 한정하면\n\\[\\{\\omega: X(\\omega)=1\\} \\in {\\cal F}\\]\n임을 보장해야 한다는 것이다.\n통찰4. 당연히 이러한 보장을 할 수는 없어보인다. 따라서 \\(X\\)를 단지 그냥\n\n\\(X: \\Omega \\to \\mathbb{R}\\)로 가는 함수\n\n가 아니라\n\n\\(X: \\Omega \\to \\mathbb{R}\\)로 가는 함수 & 역상이 항상 잴 수 있는 집합6이어야 함.\n\n이라는 조건이 필요하다.\n- 역상이 잴 수 있는 집합인 함수를 간단히 잴 수 있는 함수 (measurable function) 라고 한다."
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#헷갈려-2-star-확률변수에-대한-오해",
    "href": "posts/ap/2023-05-02-ap-9wk.html#헷갈려-2-star-확률변수에-대한-오해",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "헷갈려 (2) (\\(\\star\\)) – 확률변수에 대한 오해",
    "text": "헷갈려 (2) (\\(\\star\\)) – 확률변수에 대한 오해\n오해1: 학률변수 = 값이 랜덤으로 바뀌는 변수??\n\n함수: \\(y=f(x)\\), \\(f\\): function, \\(x\\): input \\(y\\): output\n확률변수: \\(x=X(\\omega)\\), \\(X\\): function, \\(\\omega\\): outcome7, \\(x\\): realization\n확률변수는 함수이지만 보통 \\(X(\\omega)\\)와 같이 쓰지 않고 \\(X\\)라고 쓴다. \\(\\Rightarrow\\) 혼란의 이유\n\n오해2: 확률변수는 결과가 랜덤으로 변하는 함수??\n\n확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n\n오해3: 아니야.. 확률변수는 결과가 랜덤으로 바뀌는 느낌이 맞아. 아래의 예시를 봐!\n\\[X = \\begin{cases} 0 & w.p. \\frac{1}{2} \\\\ 1 & w.p. \\frac{1}{2} \\end{cases}\\]\n\n\\(X\\)는 진짜 변수처럼 보이긴함.\n심지어 변수의 값이 랜덤으로 변하는 것 같음.\n\n(해설)\n정확하게는 아래 표현이 맞다.\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\{H\\} \\\\ 1 & \\omega \\in \\{T\\} \\end{cases} \\quad \\text{where } P(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2}.\\]\n- 확률변수에 대한 오해2에 대한 추가설명\n\n확률변수는 결과가 랜덤으로 변하는 함수가 아님, 확률변수는 함수일 뿐임. 입력이 정해지면 출력이 고정임!\n동전예제: 입력이 \\(\\omega=H\\)이면 출력은 \\(X(\\omega)=1\\), 입력이 \\(\\omega=T\\)이면 출력은 \\(X(\\omega)=0\\)으로 고정임!\n단지 입력 outcome이 실험에 따라 랜덤으로 변할 수 있는 것임!!\n\n- 요약해보면,\n\n확률변수는 확률과 관련없다.\n간접적으로는 관련이 있다. \\(\\because\\) \\(X\\)의 역상 = \\(\\Omega\\)의 부분집합 = \\(P\\)의 정의역"
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-엄밀한-정의",
    "href": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-엄밀한-정의",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "확률변수의 엄밀한 정의",
    "text": "확률변수의 엄밀한 정의\n- 확률변수 (머리속): \\(X:\\Omega \\to \\mathbb{R}\\) 인 잴 수 있는 함수.\n- 확률변수 (엄밀하게): 두 개의 잴 수 있는 공간 \\((\\Omega,{\\cal F})\\)와 \\((\\mathbb{R}, {\\cal R})\\)이 있다고 하자. 확률변수 \\(X\\)는 아래를 만족하는 함수 \\(X:\\Omega \\to \\mathbb{R}\\) 이다.\n\\[\\forall B \\in {\\cal R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n\nNote: \\(\\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\) for all \\(B \\in {\\cal R}\\) 이라 쓰기도 함. 쓰는사람 마음~"
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#정의에-대한-비판",
    "href": "posts/ap/2023-05-02-ap-9wk.html#정의에-대한-비판",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "정의에 대한 비판",
    "text": "정의에 대한 비판\n- 왜 정의가 아래와 같지 않을까?\n\\[\\forall B \\subset \\mathbb{R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n위의 질문을 위한 보충학습\n(예제) 바늘이 하나 있는 시계\n1. outcomes: \\(0,\\frac{\\pi}{2},\\pi,\\frac{3\\pi}{2},1,2,\\dots\\)\n2. sample space: \\(\\Omega = [0,2\\pi)\\)\n3. event: \\(\\emptyset\\), \\([0,\\frac{2}{\\pi})\\), \\(\\{0\\}\\), \\(\\dots\\)\n4. \\(\\sigma\\)-field: \\({\\cal F}\\)\n5. probability measure function: \\(P\\) such that\n\\[P([a,b)) = \\frac{b-a}{2\\pi}\\]\nwhere \\(0\\leq a<b<2\\pi\\).8\n6. random variable: \\(X: \\Omega \\to \\mathbb{R}\\) such that \\(X(\\omega)=\\omega\\)9\n\n6을 주목하자. 만약에 비탈리집합 \\(V \\subset [0,1] \\subset [0,2\\pi)\\)에 대한 inverse image는 비탈리집합 그 자체가 된다. 따라서 아래와 같이 된다.\n\n\\[P(X \\in V)=P\\big(\\{\\omega: X(\\omega) \\in V\\}\\big)=P(V)\\]\n\n그런데 집합 \\(V\\)는 르벡메져로는 잴 수 없으므로 \\(P(V)\\)와 같은 표현을 불가함.\n\n- 따라서 아래의 정의에서 \\(\\forall B \\in {\\cal R}\\) 대신에 \\(\\forall B \\subset \\mathbb{R}\\)이라고 쓸 수 없다.\n\\[\\forall B \\in {\\cal R}: X^{-1}(B) = \\{\\omega:X(\\omega)\\in B \\} \\in {\\cal F}\\]\n- 결국확률변수를 정의하기 위해서 2개의 가측공간 \\((\\Omega, {\\cal F})\\), \\((\\mathbb{R}, {\\cal R})\\)이 필요함."
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#잴-수-있는-함수",
    "href": "posts/ap/2023-05-02-ap-9wk.html#잴-수-있는-함수",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "잴 수 있는 함수",
    "text": "잴 수 있는 함수\n- 교재의 정의1\n\n\n\n그림2: Durret에서 긁어온 확률변수의 정의\n\n\n\n“\\(X\\) is \\({\\cal F}\\)-measurable” 이라는 의미는, 모든 \\(B \\in {\\cal R}\\)에 대하여 \\(B\\)의 inverse image가 \\({\\cal F}\\)-measurable 하다는 의미.\n\\(X\\)가 랜덤변수라는 것을 기호로 간단하게 \\(X \\in {\\cal F}\\) 라고 씀.\n두개의 가측공간에 대한 언급은 매우 모호하게 되어있음.\n\n- 교재의 정의2\n\n\n\n그림3: Durret에서 긁어온 확률변수의 정의2\n\n\n\n측도의 개념을 정의하고 그 특수한 케이스로 확률측도를 정의하였듯이, 잴 수 있는 함수(measurable map)라는 개념을 정의하고 그 특수한 케이스로 확률변수(혹은 확률벡터)를 정의한다.\n두개의 가측공간이 명확하게 명시되어 있어서 좀 더 이해하기 쉽다.\n\n- 우리는 좀 더 명확한 의미전달을 위해\n\n\\(X\\)를 \\((\\Omega, {\\cal F})\\to (\\mathbb{R},{\\cal R})\\)인 확률변수라고 하자\n\\(X\\)를 \\((\\Omega, {\\cal F})\\to (S,{\\cal S})\\)인 잴 수 있는 함수 (가측함수)라고 하자\n\n와 같은 문장을 쓰겠다."
  },
  {
    "objectID": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-체크",
    "href": "posts/ap/2023-05-02-ap-9wk.html#확률변수의-체크",
    "title": "9wk: 확률변수, 분포 (1)",
    "section": "확률변수의 체크",
    "text": "확률변수의 체크\n(1) 아래와 같은 measurable space를 고려하자.\n\n\\(\\Omega=\\{a,b,c,d\\}\\)\n\\({\\cal F} =\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\{a\\}\\}\\).\n\n아래와 같은 function \\(X:\\Omega \\to \\mathbb{R}\\), \\(Y:\\Omega \\to \\mathbb{R}\\)을 고려하자.\n\n\\(X(a)=1, X(b)=2, X(c)=3, X(d)=4\\)\n\\(Y(a)=1, Y(b)=2, Y(c)=2, Y(c)=2\\)\n\n아래의 물음에 답하라.\n\n\\(X\\)는 \\((\\Omega,{\\cal F})\\to (\\mathbb{R},{\\cal R})\\)인 확률변수인가?\n\\(Y\\)는 \\((\\Omega,{\\cal F})\\to (\\mathbb{R},{\\cal R})\\)인 확률변수인가?\n\n(풀이)\n\\(X\\)는 확률변수가 아님\n집합 \\(\\{2\\} \\in {\\cal R}\\)에 대하여 \\(\\{\\omega: X(\\omega) \\in \\{2\\}\\}=\\{b\\} \\not \\in \\sigma({\\cal A})\\) 이므로 \\(X\\)는 확률변수가 아님\n\\(Y\\)는 확률변수임\n\\(\\forall B \\in {\\cal R}\\)에 대하여 \\(Y^{-1}(B)\\in {\\cal F}\\)가 성립함.\n\n\\(\\{\\omega: Y(\\omega) \\in \\emptyset\\} = \\emptyset \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: Y(\\omega) \\in \\{1\\}\\} = \\{a\\} \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: Y(\\omega) \\in \\{2\\}\\} = \\{b,c,d\\} \\in \\sigma({\\cal A})\\)\n\\(\\{\\omega: Y(\\omega) \\in \\{1,2\\}\\} = \\{a,b,c,d\\} \\in \\sigma({\\cal A})\\)\n위에서 언급되지 않은 \\(B \\in {\\cal R}\\)에 대해서는 모두 \\(Y^{-1}(B)=\\emptyset \\in \\sigma({\\cal A})\\)가 성립함.\n\n(2) 두개의 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)와 \\((S,{\\cal S})\\)를 고려하자. 단,\n\n\\(\\Omega=\\mathbb{R}\\),\n\\({\\cal F} =\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\mathbb{Q}\\}\\),\n\\(S = \\{0,1\\}\\),\n\\({\\cal S} = 2^{S}\\).\n\n아래와 같은 함수 \\(X:\\Omega \\to S\\)을 고려하라.\n\\[X(\\omega) = \\begin{cases}\n0 & \\omega \\in \\mathbb{Q}\\\\\n1 & \\omega \\in \\mathbb{R} - \\mathbb{Q}\n\\end{cases}\\]\n\\(X\\)는 \\((\\Omega,{\\cal F})\\to(S,{\\cal S})\\)인 가측함수인가?\n(풀이) 잴 수 있는 함수임.\nNote: \\(\\sigma({\\cal A})=\\{\\emptyset, \\mathbb{Q}, \\mathbb{Q}^c, \\mathbb{R} \\}, 2^S = \\{\\emptyset, \\{0\\}, \\{1\\}, \\{0,1\\}\\}\\)\n잴 수 있는 함수임을 체크하기 위해서는 \\(2^S\\)의 모든 원소 \\(B\\)에 대하여 \\(X^{-1}(B):= \\{\\omega : X(\\omega) \\in B\\} \\in {\\cal F}\\) 임을 확인하면 된다.\n\n\\(B=\\emptyset\\) 일 경우: \\(\\{\\omega: X(\\omega) \\in \\emptyset\\}=\\emptyset \\in \\sigma({\\cal A})\\)\n\\(B=\\{0\\}\\) 일 경우: \\(\\{\\omega: X(\\omega) \\in \\{0\\}\\}=\\mathbb{Q} \\in \\sigma({\\cal A})\\)\n\\(B=\\{1\\}\\) 일 경우: \\(\\{\\omega: X(\\omega) \\in \\{1\\}\\}=\\mathbb{Q}^c \\in \\sigma({\\cal A})\\)\n\\(B=\\{0,1\\}\\) 일 경우: \\(\\{\\omega: X(\\omega) \\in \\{0,1\\}\\}=\\mathbb{R} \\in \\sigma({\\cal A})\\)\n\n\n이 문제에서 \\((S,{\\cal S})\\)를 \\((\\mathbb{R}, {\\cal R})\\) 바꾸면 풀이의 약간만 수정하여 \\(X \\in {\\cal F}\\)임을 보일 수 있다.\n\n(3) 두개의 잴 수 있는 공간 \\((\\Omega, {\\cal F})\\)와 \\((S,{\\cal S})\\)를 고려하자. 단,\n\n\\(\\Omega=\\mathbb{R}\\),\n\\({\\cal F} =\\sigma({\\cal A})\\) where \\({\\cal A} = \\{\\mathbb{Q}\\}\\),\n\\(S = \\{0,1\\}\\),\n\\({\\cal S} = 2^S\\).\n\n아래와 같은 함수 \\(X:\\Omega \\to S\\)을 고려하라.\n\\[X(\\omega) = \\begin{cases}\n0 & \\omega =0\\\\\n1 & \\omega \\neq 0\n\\end{cases}\\]\n즉 \\(X\\)는 \\((\\Omega,{\\cal F})\\to(S,{\\cal S})\\)인 가측함수인가?\n(풀이) 잴 수 있는 함수가 아님. \\(B=\\{0\\}\\) 일 경우, \\[\\{\\omega: X(\\omega) \\in B\\}=\\{0\\} \\notin \\sigma({\\cal A})\\] 이므로 잴 수 있는 함수의 정의에 만족하지 않음.\n\n이 문제에서 \\((S,{\\cal S})\\)를 \\((\\mathbb{R}, {\\cal R})\\) 바꾸면 풀이의 약간만 수정하여 \\(X \\notin {\\cal F}\\)임을 보일 수 있다."
  },
  {
    "objectID": "posts/ap/2023-03-28-ap-4wk.html",
    "href": "posts/ap/2023-03-28-ap-4wk.html",
    "title": "4wk: 측도론 intro (4)",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-ysaKaydM8V9RauAwOaUhSN"
  },
  {
    "objectID": "posts/ap/2023-03-28-ap-4wk.html#상황1-시그마필드-구하기-귀찮아",
    "href": "posts/ap/2023-03-28-ap-4wk.html#상황1-시그마필드-구하기-귀찮아",
    "title": "4wk: 측도론 intro (4)",
    "section": "상황1: 시그마필드 구하기 귀찮아",
    "text": "상황1: 시그마필드 구하기 귀찮아\n(예제1)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 event의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\{1\\},\\{2\\}\\}\\]\n- 당연히 이러한 이벤트에 대해서만 적절한 확률을 정의하면 좋겠는데, 이는 불가능 하다. 왜냐하면 \\({\\cal A}\\)는 시그마필드가 아니기 때문이다.\n- 따라서 할 수 없이 아래와 같은 방식으로 시그마필드를 구해야 했다.\n\\[{\\cal F} = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 이러한 \\({\\cal F}\\)를 구하기는 것은 귀찮은 일인데, 이를 편리하게 해결하기 위해서 \\(\\sigma({\\cal A})\\)라는 기호를 도입하고 이를 “\\(\\{1\\}\\), \\(\\{2\\}\\)를 원소로 가지는 최소한의 \\({\\cal F}\\)” 라고 생각 하기로 하였다. 즉 앞으로는\n\\[\\sigma({\\cal A})\\]\n라고만 써도 위에서 명시한 \\({\\cal F}\\)를 의미한다고 알아서 생각하면 된다는 것이다.\n걱정: 문제는 이러한 논리전개가 항상 가능하냐는 것이다.\n\n귀찮아서 만든 이론1: 걱정할 필요 없다. 언제나 \\(\\sigma({\\cal A})\\)라는 표현은 가능하다. 즉 \\(\\Omega\\)의 임의의 부분집합에 대하여 우리가 관심있는 집합만 모은 것을 \\({\\cal A}\\)라고 할때, \\({\\cal A}\\)의 모든 원소를 포함하고 시그마필드의 정의를 만족하는 최소한의 시그마필드 \\(\\sigma({\\cal A})\\)는 항상 존재한다.\n\n(예제2)\n\\(\\Omega = \\mathbb{R}\\) 이라고 하자. 이중에서 우리가 관심있는 집합들은 르벡메져로 길이를 명확하게 잴 수 있는 아래와 같은 형태이다.\n\\[[a,b]\\]\n여기에서 \\(a,b \\in \\mathbb{R}\\), \\(a<b\\) 이라고 하자. 따라서 이 경우 \\({\\cal A}\\)를 아래와 같이 설정할 수 있다.\n\\[{\\cal A} = \\big\\{[a,b]: a,b \\in \\mathbb{R}, a<b \\big\\}\\]\n이제 \\(\\sigma({\\cal A})\\)를 상상하자. 이는 \\(\\Omega=\\mathbb{R}\\)에서 잴 수 있는 집합들의 모임이다. 편의상 \\(\\sigma({\\cal A}):={\\cal R}\\)로 정의하자. 여기에서 \\({\\cal R}\\) 상당히 많은 케이스를 포함하는 집합이다. 예를들면 아래와 같은 집합들은 모두 \\({\\cal R}\\)의 원소이다. (즉 아래의 집합은 \\([a,b]\\)를 잴 수 있다고 할때, 당연히 잴 수 있다고 여겨지는 집합들이다.)\n\n\\([0,2)\\)\n\\(\\{2\\}\\)\n\\((0,2)\\)\n\\([0,\\infty)\\), \\((0,\\infty)\\)\n\\((-\\infty,0)\\), \\((-\\infty,0]\\)\n\\([1,2] \\cup [3,4]\\)\n\\((1,2] \\cup [3,4)\\)\n\\(\\mathbb{N}\\), \\(\\mathbb{Z}\\), \\(\\mathbb{Q}\\)\n\\([0,2] \\cap \\mathbb{Q}\\)\n\n\n사실상 \\({\\cal R}=\\sigma({\\cal A})\\)와 같은 기호가 없다면 \\(\\mathbb{R}\\)에서 잴 수 있는 집합들의 모임은 명시적으로 쓰는 것 자체가 불가능함."
  },
  {
    "objectID": "posts/ap/2023-03-28-ap-4wk.html#상황2-확률-정의하기-귀찮아",
    "href": "posts/ap/2023-03-28-ap-4wk.html#상황2-확률-정의하기-귀찮아",
    "title": "4wk: 측도론 intro (4)",
    "section": "상황2: 확률 정의하기 귀찮아",
    "text": "상황2: 확률 정의하기 귀찮아\n(예제1) – motivating EX\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하자. 내가 관심있는 집합의 모음은 아래와 같다.\n\\[{\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\]\n- 여기에서 \\({\\cal A}\\)는 시그마필드가 아니다. 따라서 \\({\\cal A}\\)에서는 확률을 정의할 수 없다. 확률을 정의하려면 \\(\\sigma({\\cal A})\\)에서 정의해야 한다.\n- 소망: 그래도 그냥 \\({\\cal A}\\)에서만 확률 비슷한걸5 잘 정의하면 안될까?\n- 희망: 이게 될 것 같다. 예를들면 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n이 정도만 정의해보자. \\(\\tilde{P}\\)는 정의역이 시그마필드가 아니라는 점만 제외하면 확률의 공리 1,2,3을 따른다. 이렇게 함수 \\(\\tilde{P}\\)를 정의하게 되면\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n에서의 확률 \\(P:\\sigma({\\cal A}) \\to [0,1]\\)는 확률 비슷한 함수 \\(\\tilde{P}\\)를 “알아서, 잘, 센스있게” 확장하여 정의할 수 있다. 구체적으로는 아래와 같이 된다.\n\n\n\n\n\\(P\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\n(예제2) – motivating EX (2)\n- \\(\\Omega=\\{1,2,3,4\\}\\)이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\}, \\{3,4\\}, \\Omega\\}\\) 라고 하자. 그리고 아래와 같은 \\(\\sigma({\\cal A})\\)를 다시 상상하자.\n\\[\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\]\n- 위의 시그마필드에서 확률을 예제1과 다른 방식으로 정의할 수 도 있다. 예를들면 아래와 같은 방식으로 정의가능하다.\n\n\n\n\n\\(P_1\\)\n\\(\\tilde{P}_1\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{3}\\)\n\\(\\frac{1}{3}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{2}{3}\\)\nNone\n\n\n\n또한 아래와 같은 방식도 가능하다.\n\n\n\n\n\\(P_2\\)\n\\(\\tilde{P}_2\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{2\\}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{3,4\\}\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\nNone\n\n\n\n- 어떠한 방식으로 정의하든 \\({\\cal A}\\)에서 확률 비슷한 것 \\(\\tilde{P}_1,\\tilde{P}_2\\)를 잘 정의하기만 \\(\\sigma({\\cal A})\\)에서의 확률 \\(P\\)로 적절하게 확장할 수 있다. 심지어 이런 확장은 유일한 듯 하다.\n\n귀찮아서 만든 이론2: 운이 좋다면, \\({\\cal A}\\) 에서 확률의 공리를 만족하는 적당한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 \\((\\Omega, \\sigma({\\cal A}))\\) 에서의 확률측도 \\(P\\)로 업그레이드 할 수 있으며 업그레이드 결과는 유일하다.\n\n(예제3) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 0\\)\n\\(\\tilde{P}(\\{2,3\\}) = 0\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- \\(\\tilde{P}\\)는 분명히 \\({\\cal A}\\)에서 확률의 공리1-3을 만족한다.\n- 하지만 \\(\\sigma({\\cal A})\\)로의 확장은 불가능하다.\n(예제4) – 운이 안 좋은 경우\n- \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자.\n- 아래와 같은 확률 비슷한 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n- \\(\\tilde{P}\\)는 분명히 \\({\\cal A}\\)에서 확률의 공리1-3을 만족한다.\n- \\(\\sigma({\\cal A})\\)로의 확장도 가능하다. 하지만 유일한 확장을 보장하지 않는다.\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\nNone\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\n(예제5) – 혹시…\n- \\(\\Omega=\\mathbb{R}\\), \\({\\cal A}=\\big\\{[a,b]: a,b \\in \\mathbb{R}, a<b \\big\\}\\) 라고 하자.\n- \\({\\cal A}\\)에서만 측도비슷한 함수 \\(\\tilde{m}([a,b])=b-a\\)를 잘 정의한다면 그것이 \\(\\sigma({\\cal A})\\)에서의 측도 \\(m\\)으로 업그레이드 가능하며, 그 업그레이드 결과는 유일할까?"
  },
  {
    "objectID": "posts/anything/index.html",
    "href": "posts/anything/index.html",
    "title": "Anything",
    "section": "",
    "text": "Study of any subject"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html",
    "href": "posts/anything/2023-05-25-Survival_R.html",
    "title": "Survival Analysis Tutorial with R",
    "section": "",
    "text": "library(survival)\nlibrary(survminer)\nlibrary(epitools)\n\nlibrary(tidyverse)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#독립변수가-범주형변수일때",
    "href": "posts/anything/2023-05-25-Survival_R.html#독립변수가-범주형변수일때",
    "title": "Survival Analysis Tutorial with R",
    "section": "독립변수가 범주형변수일때",
    "text": "독립변수가 범주형변수일때\nref : (odds ratio](https://en.wikipedia.org/wiki/Odds_ratio)\nA의 성공 확률 = \\(\\frac{P(A)}{1-P(A)}\\)\nB의 성공 확률 = \\(\\frac{P(B)}{1-P(B)}\\)\nOdds Ratio = A의 성공 확률 / B의 성공 확률\n= \\(\\frac{\\frac{P(A)}{1-P(A)}}{\\frac{P(B)}{1-P(B)}}\\)\n해석\n\n1보다 클 경우\n\nA집단의 성공할 확률이 B 집단이 성공할 확률보다 높다.\n\n1보다 작을 경우\n\nA집단의 성공할 확률이 B 집단이 성공할 확률보다 낮다.\n\n1일 경우\n\nA집단의 성공할 확률이 B 집단이 성공할 확률보다 같다.\n\n\n\ntmp <- lung %>% mutate(tmp = ifelse(ph.karno < 70, 1, 0));tmp\n\n\ntmp %>% select(sex, tmp) %>% table()\n\n위의 표에서 tmp 가 0인 것이 성공, 1인 것이 실패로 보고, A집단이 성별 = 1, B집단이 성별 = 2로 본다면 오즈비는 아래와 같이 계산된다.\n\n# A의 성공 확률\n(122/137)/(15/137)\n# = (122/137)/((137-122)/137)\n\n\n# B의 성공 확률\n(80/90)/(10/90)\n# = (80/90)/((80-10)/90)\n\n\n# 오즈비\n((122/137)/(15/137))/((80/90)/(10/90))\n\n\nrst <- oddsratio.wald(tmp %>% select(sex, tmp) %>% table())\nrst\n\n\nrst$data\n\n\nrst$measure # Odds Ratio\n\n1보다 크게 나왔으니까 sex=1인 집단이 성공할 활률이 sex=2인 집단이 성공할 확률보다 높다.\n\nrst$p.value # fisher or chi-square test result\n\np값이 0.05보다 높게 나와 오즈비가 유의하지 않다는 것을 알 수 있다.\n\nrst$correction # correction"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#독립변수가-연속형변수일때",
    "href": "posts/anything/2023-05-25-Survival_R.html#독립변수가-연속형변수일때",
    "title": "Survival Analysis Tutorial with R",
    "section": "독립변수가 연속형변수일때",
    "text": "독립변수가 연속형변수일때\n독립변수를 연속형 변수로, 종속변수를 이산형(0 또는 1)binomial으로 지정한 일반화 선형 모형을 사용할 것이다.\n\n\n\n\n\n\nTip\n\n\n\n물론 아래 방식은 독립변수가 범주형 변수일때도 사용 가능하다.\n하지만 독립변수가 연속형 변수일때 독립변수가 범주형일때 사용한 방법으로는 사용이 불가능하다.\n\n\n\nmodel <- glm(tmp ~ wt.loss, data = tmp, family = binomial)\nsummary(model)\n\n\nodds_ratio <- exp(coef(model))[[2]] # 오즈비\nci_lower <- exp(confint(model))[2,][1] # 95% 신뢰구간 하한\nci_upper <- exp(confint(model))[2,][2] # 95% 신뢰구간 상한\nlist(odds_ratio=odds_ratio,ci_lower=ci_lower,ci_upper=ci_upper)"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#odds-ratio-plot",
    "href": "posts/anything/2023-05-25-Survival_R.html#odds-ratio-plot",
    "title": "Survival Analysis Tutorial with R",
    "section": "Odds Ratio Plot",
    "text": "Odds Ratio Plot\nData Ref: stackoverflow\nand Odds plot code made by me\n\ndf <- data.frame(yAxis = length(boxLabels):1, \n                 boxOdds = log(c(0.9410685, \n                                 0.6121181, 1.1232907, 1.2222137, 0.4712629, 0.9376822, 1.0010816, \n                                 0.7121452)), \n                 boxCILow = c(-0.1789719, -0.8468693,-0.00109809, 0.09021224,\n                              -1.0183040, -0.2014975, -0.1001832,-0.4695449), \n                 boxCIHigh = c(0.05633076, -0.1566818, 0.2326694, 0.3104405, \n                               -0.4999281, 0.07093752, 0.1018351, -0.2113544)\n\n\nggplot(data = df,\n       mapping = aes(y = forcats::fct_rev(f = forcats::fct_inorder(f = boxLabels)))) +\n  geom_point(aes(x = boxOdds),size = 1.5, color = \"black\")+\n  geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow), size = .5, height = .2, color = \"gray50\") +\n  theme_classic()+\n  geom_vline(xintercept = 1) + # 오즈비는 1을 기준으로 보기 때문에 1의 수직선을 그려줘야 한다.\n  theme(panel.grid.minor = element_blank()) +\n  labs(x = \"\",  y = \"\", title = \"Odds ratio plot\") + \n  scale_y_discrete(labels = boxLabels)\n\n# the method of save ggplot file\n# You can allocate path\nggsave(file= paste0(path,\"name.png\", sep=''), width=15, height=8, units = c(\"cm\"))"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#hazard-ratio-plot",
    "href": "posts/anything/2023-05-25-Survival_R.html#hazard-ratio-plot",
    "title": "Survival Analysis Tutorial with R",
    "section": "Hazard Ratio Plot",
    "text": "Hazard Ratio Plot\n\ndf <- data.frame(yAxis = length(cova):1, # 그래프에 그려지는 변수의 순서를 지정해주기 위함\n                 boxhz = tab$HR,\n                 boxCILow = tab$lower,\n                 boxCIHigh = tab$upper)\n\n\nggplot(data = df,\n       mapping = aes(y = forcats::fct_rev(f = forcats::fct_inorder(f = cova)))) +\n  geom_point(aes(x = boxhz), size = 1.5, color = \"black\")+\n  geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow), size = .5, height = .2, color = \"gray50\") +\n  theme_classic()+\n  geom_vline(xintercept = 1) +\n  theme(panel.grid.minor = element_blank()) +\n  labs(x = \"\",  y = \"\", title = \"Hazard ratio plot\")  +\n  scale_y_discrete(labels = rev(c('residual disease present (1=no,2=yes)',\n                                  'treatment group',\n                                  'in years')))"
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#log-rank-test",
    "href": "posts/anything/2023-05-25-Survival_R.html#log-rank-test",
    "title": "Survival Analysis Tutorial with R",
    "section": "Log-Rank Test",
    "text": "Log-Rank Test\n\n카플란-마이어 생존분석에서 나온 생존 함수가 유의하게 다른지 검정하는 방법\n서로 다른 두 집단의 생존률(사건 발생률)을 비교하는 비모수적 가설 검정법\nsurvdiff는 집단 간의 생존함수를 비교하기 위해 사용\np값이 0.05보다 작을 경우 생존 곡선 간에 차이가 있는 것으로 본다.\n\n\nsurvdiff(Surv(time = time, event = status) ~ sex, data=lung) # lung 데이터에서 성별에 따른 생존곡선 차이 비교\n\np 값이 0.0001로 나와 생존 곡선 간에 차이가 있다는 것을 알 수 있다."
  },
  {
    "objectID": "posts/anything/2023-05-25-Survival_R.html#survival-plot",
    "href": "posts/anything/2023-05-25-Survival_R.html#survival-plot",
    "title": "Survival Analysis Tutorial with R",
    "section": "Survival plot",
    "text": "Survival plot\nRef : ggkm\n\nggkm <- function(sfit, returns = FALSE,\nxlabs = \"Time\", ylabs = \"survival probability\",\nystratalabs = NULL, ystrataname = NULL,\ntimeby = 100, main = \"Kaplan-Meier Plot\",\npval = TRUE, ...) {\nrequire(plyr)\nrequire(ggplot2)\nrequire(survival)\nrequire(gridExtra)\nif(is.null(ystratalabs)) {\n   ystratalabs <- as.character(levels(summary(sfit)$strata))\n}\nm <- max(nchar(ystratalabs))\nif(is.null(ystrataname)) ystrataname <- \"Strata\"\ntimes <- seq(0, max(sfit$time), by = timeby)\n.df <- data.frame(time = sfit$time, n.risk = sfit$n.risk,\n    n.event = sfit$n.event, surv = sfit$surv, strata = summary(sfit, censored = T)$strata,\n    upper = sfit$upper, lower = sfit$lower)\nlevels(.df$strata) <- ystratalabs\nzeros <- data.frame(time = 0, surv = 1, strata = factor(ystratalabs, levels=levels(.df$strata)),\n    upper = 1, lower = 1)\n.df <- rbind.fill(zeros, .df)\nd <- length(levels(.df$strata))\np <- ggplot(.df, aes(time, surv, group = strata)) +\n    geom_step(aes(linetype = strata), size = 0.7) +\n    theme_bw() +\n    theme(axis.title.x = element_text(vjust = 0.5)) +\n    scale_x_continuous(xlabs, breaks = times, limits = c(0, max(sfit$time))) +\n    scale_y_continuous(ylabs, limits = c(0, 1)) +\n    theme(panel.grid.minor = element_blank()) +\n    theme(legend.position = c(ifelse(m < 10, .28, .35), ifelse(d < 4, .25, .35))) +\n    theme(legend.key = element_rect(colour = NA)) +\n    labs(linetype = ystrataname) +\n    theme(plot.margin = unit(c(0, 1, .5, ifelse(m < 10, 1.5, 2.5)), \"lines\")) +\n    ggtitle(main)\n \nif(pval) {\n    sdiff <- survdiff(eval(sfit$call$formula), data = eval(sfit$call$data))\n    pval <- pchisq(sdiff$chisq, length(sdiff$n)-1, lower.tail = FALSE)\n    pvaltxt <- ifelse(pval < 0.0001, \"p < 0.0001\", paste(\"p =\", sprintf(\"%.4f\", pval) ))\n    p <- p + annotate(\"text\", x = 0.6 * max(sfit$time), y = 0.1, label = pvaltxt)\n}\n## Plotting the graphs\n    print(p)\n    if(returns) return(p)\n   \n}\n\n사용법\n\nlabs=paste(\"성별\",c(\"Male\",\"Female\"))  # legend 항목 지정 test-ref 순\nstrataname=paste(\"Sex\") # legend 명 지정\n\n\nlung$temp=ifelse(lung$sex==2,0,1) # Ref 지정, Ref=0, Test-Ref 순으로 그래프가 그려짐\n# 여기서는 여성(sex=2)를 ref로 지정해주었다.\n\n\n\n\n\n\n\nNote\n\n\n\n변수에 ref가 지정되지 않으면 결과 해석에 혼동이 있을 수 있으니 ref 지정이 필요\n\n\n\nfit=survfit(Surv(time = time, event = status)~temp ,data=lung)\n\nfit\n\n\nggkm(fit,timeby=500,ystratalabs=labs,ystrataname=strataname, main = 'Survival', ylab='Event')\n\n참고로, 아래와 같이 summary 해주면 단변량의 항목별로 시간별로 볼 수 있다.\n또한, 가장 마지막 값 저장하기 위해 아래 코드 사용 가능\nsink('summary.txt')\nsummary(fit) \nsink()\n\nsummary(fit)"
  },
  {
    "objectID": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html",
    "href": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "",
    "text": "PyTorch Geometric Temporal"
  },
  {
    "objectID": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "href": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Applications",
    "text": "Applications\n\nEpidemiological Forecasting\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [02:40<00:00,  1.24it/s]\n\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 1.0232\n\nMSE: 1.0247\n\n\n\n\nShape Check (1)\n\na = torch.randn(20, 1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\n\n\nDoesn’t it have to ‘y_hat’ be the same shape as snapshot.y?\n\nIf we want to compare the y_hat from the model with the values y, the same shape is appropriate to evaluate.\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [01:27<00:00,  2.30it/s]\n\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 1.0232\n\nMSE: 1.2844\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(20, 1).reshape(-1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])"
  },
  {
    "objectID": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "href": "posts/anything/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Web Traffic Prediction",
    "text": "Web Traffic Prediction\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [31:26<00:00, 37.73s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 0.7760\n\nMSE: 0.7939\n\n\n\n\nShape Check (1)\n\na = torch.randn(1068, 1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\nIf the code changes the shape of y_hat?\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [36:39<00:00, 43.99s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 0.7760\n\nMSE: 0.7807\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(1068, 1).reshape(-1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068]) torch.Size([1068]) torch.Size([1068])\n\n\n\n\n\nReferences\n\n@inproceedings{rozemberczki2021pytorch, author = {Benedek Rozemberczki and Paul Scherer and Yixuan He and George Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and Ferenc Beres and and Guzman Lopez and Nicolas Collignon and Rik Sarkar}, title = {{PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models}}, year = {2021}, booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management}, pages = {4564–4573}, }"
  },
  {
    "objectID": "posts/anything/2023-04-27-Clinical Trial Data and Survival Analysis.html",
    "href": "posts/anything/2023-04-27-Clinical Trial Data and Survival Analysis.html",
    "title": "Clinical Trial Data and Survival Analysis",
    "section": "",
    "text": "Clinical Trial Data and Survival Analysis\n\n\nCox 회귀모형\nCox(1972)는 위험함수에 의한 비례위험모형(proportional hazard model)을 제안하였다. 이는 준모수적(semiparametric)방법으로, 생존시간의 분포에 대한 가정을 하지 않을 뿐만 아니라, 시간에 따라 변하는 공변량(time dependent variable)이 있는 경우에도 적용가능하기 때문에 생존분석에서 자주 사용되고 있다.\np개의 공변량을 가진 cox 회귀모형\n\\[h(t) = h_0(t)exp(\\beta_1 x_1 + \\dots + \\beta_p x_p)\\]\n\\(h_0(t)\\)는 모든 공변량 값들이 0일 때의 baseline haxart function(기저위험함수)\n비례위험모형이란 시간에 대해 두 개체 간 위험률(아래)이 일정, 즉 상수 값 산출\n\\(\\frac{h(t|x_1)}{h(t|x_2)} = \\frac{exp(\\beta' x_1}{exp(\\beta' x_2}\\)\n비례위험 가정 만족 시, 개체 간 생존 시간 분포 곡선은 평행, 모형에서 만족해야 함\n모든 개체에서 기저 위험인 \\(h_0(t)\\)는 공통으로 가지고 있지만 상대위험(relative risk)인 \\(exp(\\beta'x)\\)는 각 개체들의 상대적 위험도를 나타냄\n\\(exp(\\beta'x)\\)를 통해 모수 \\(\\beta_j\\)의 해석이 이뤄지고, 이 \\(\\beta_j\\)는 공변량 \\(x_j\\)의 한 단위 증가에 따른 상대위험 정도를 나타냄\n생존확률을 그림으로 나타내었을때 기저위험baseline hazard 가 다를때, 즉 곡선이 평행라지 않는다면 cox의 회귀분석의 비례위험모형 가정에 위배되어 그룹별로 층화된 모형 고려해야 함.\n\nReferences\n\n@article{배종성2006임상시험자료와, title={임상시험자료와 생존분석}, author={배종성 and 김서영}, journal={Journal of the Korean Data Analysis Society}, volume={8}, number={2}, pages={533–545}, year={2006} }"
  },
  {
    "objectID": "posts/anything/2023-04-17-Survival_Analysis.html",
    "href": "posts/anything/2023-04-17-Survival_Analysis.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Survival Analysis\n\n생존분석(Survival Analysis)은 시간에 따른 사건(event) 발생을 다루는 통계적 분석 방법입니다.\n기본적으로 생존분석은 다음과 같은 개념을 사용합니다.\n\n생존시간(survival time) : 어떤 사건이 발생하기까지 걸리는 시간\n생존함수(survival function) : 어떤 시점까지 생존할 확률을 나타내는 함수\n위험함수(hazard function) : 어떤 시점에서 사건이 발생할 위험성을 나타내는 함수\n\n생존분석에서는 위의 개념을 사용하여 시간에 따라 생존함수와 위험함수를 추정합니다. 이를 통해 어떤 사건이 발생하기까지 걸리는 시간과 그 시간에 따른 생존확률을 예측할 수 있습니다.\n생존분석에는 여러 가지 방법이 있지만, 가장 대표적인 방법은 코크스-매키스(Cox-Meier) 생존분석입니다. 이 방법은 다음과 같은 절차를 따릅니다.\n\n데이터 수집 : 연구 대상이 되는 그룹의 데이터를 수집합니다.\n데이터 전처리 : 수집한 데이터를 정리하고 필요한 변수를 선택합니다.\n생존함수 추정 : 적합한 생존모형을 선택하여 생존함수와 위험함수를 추정합니다.\n생존분석 결과 해석 : 추정된 생존함수와 위험함수를 시각화하고, 예측된 생존시간 등을 분석합니다.\n\n생존분석은 데이터의 특성에 따라 다양한 모델을 사용할 수 있습니다.\n일반적으로는 코크스-매키스 생존분석 외에도 침입(입원)한 경우를 대상으로 하는 Kaplan-Meier 생존분석 등이 사용됩니다.\n이들 생존분석 방법은 시간에 따른 사건 발생률, 생존확률 등을 예측하는 데 유용합니다.\nlibrary(survival) # 생존분석 패키지\nsurvfit 함수는 생존분석에서 Kaplan-Meier 생존곡선을 추정하는 함수입니다.\n이 함수는 생존시간 데이터와 이벤트 발생 여부 데이터를 이용해 생존곡선을 추정하고, 추정된 생존곡선을 시각화할 수 있습니다.\nsurvfit 함수를 사용하기 위해서는 먼저 생존시간과 이벤트 발생 여부 데이터를 Surv 함수를 이용해 생성해야 합니다.\nSurv 함수는 생존분석에서 사용되는 시간과 상태(이벤트 발생 여부) 데이터를 담은 객체를 생성합니다. Surv 함수는 다음과 같은 형태로 사용됩니다.\nSurv(time, event)\n여기서 formula는 Surv 객체를 포함한 모형을 지정하는데 사용되는 공식을 나타내며, data는 데이터 프레임을 나타냅니다.\n예를 들어, lung 데이터셋에서 생존시간과 이벤트 발생 여부 데이터를 추출하고, 이를 Surv 함수에 입력하여 생존곡선을 추정하려면 다음과 같이 작성할 수 있습니다.\n예시 및 사용법\nlibrary(survival) # 생존분석 패키지 불러오기\ndata(lung) # R 내장 데이터셋인 lung 데이터 불러오기\n\nsurv_obj <- Surv(time = lung$time, event = lung$status) # 생존시간과 이벤트 발생 여부 데이터 추출\n\nsurvfit_obj <- survfit(surv_obj ~ 1) # Kaplan-Meier 생존곡선 추정\nsurvfit_obj <- survfit(surv_obj ~ 1) # Kaplan-Meier 생존곡선 추정\n\nsummary(survfit_obj)\n\nCall: 사용한 함수 및 입력 정보를 보여줍니다.\ntime n: 생존시간 데이터의 총 관측 수를 보여줍니다.\nevents n: 이벤트 발생 여부 데이터 중 이벤트가 발생한 수를 보여줍니다.\nmedian: 생존곡선에서의 중위 생존시간을 보여줍니다.\n0.95LCL, 0.95UCL: 생존곡선의 95% 신뢰구간을 보여줍니다.\nCall: 생존곡선 상태표를 출력합니다. 상태표는 생존시간의 범위와 이벤트 발생 여부에 따라 생존곡선의 상태를 표시합니다.\nn events: 이벤트 발생 여부 데이터 중 이벤트가 발생한 수를 보여줍니다.\nlog-rank: 로그 랭크 검정에 대한 결과를 보여줍니다. 로그 랭크 검정은 생존곡선 간 차이의 유의성을 검정하는 검정 방법 중 하나입니다.\n\nlibrary(survminer) # 생존분석 결과 시각화 패키지\nsurvminer 패키지는 생존 분석 결과를 시각화하기 위한 패키지로, 생존 함수, 생존 곡선, 누적 발생율 함수 등을 그래프로 그릴 수 있습니다. 이 패키지는 ggplot2 패키지를 기반으로 하여 만들어졌으며, ggplot2의 그래프 작성 방법과 유사합니다.\nsurvminer 패키지에서 제공하는 함수와 예시를 설명하겠습니다.\n\nggsurvplot 함수 ggsurvplot 함수는 생존 곡선을 그리기 위한 함수입니다. 예를 들어, 다음과 같이 생존 분석 결과를 이용하여 생존 곡선을 그릴 수 있습니다.\n\nlibrary(survival)\nlibrary(survminer)\n\ndata(lung)\nfit <- survfit(Surv(time, status) ~ sex, data = lung)\n\nggsurvplot(fit, data = lung, risk.table = TRUE)\n위 코드에서는 lung 데이터셋에서 time과 status 변수를 이용하여 생존 분석을 수행한 뒤, 성별에 따른 생존 곡선을 그리도록 하였습니다. ggsurvplot 함수의 risk.table 인자를 TRUE로 설정하면, 위 그래프와 함께 위험표(risk table)도 함께 출력됩니다.\n\nggcoxzph 함수 ggcoxzph 함수는 Cox 비례위험 모형 가정을 검정하기 위한 그래프를 그리기 위한 함수입니다. 예를 들어, 다음과 같이 Cox 비례위험 모형 가정을 검정하기 위한 그래프를 그릴 수 있습니다.\n\nlibrary(survival)\nlibrary(survminer)\n\ndata(lung)\nfit <- coxph(Surv(time, status) ~ age + sex, data = lung)\n\nggcoxzph(fit, linear.predictions = TRUE)\n위 코드에서는 lung 데이터셋에서 age와 sex 변수를 이용하여 Cox 비례위험 모형을 적합한 뒤, 모형 가정을 검정하기 위한 그래프를 그리도록 하였습니다. ggcoxzph 함수의 linear.predictions 인자를 TRUE로 설정하면, 선형 예측값과 함께 그래프가 출력됩니다.\n\nggforest 함수 ggforest 함수는 Cox 모형에서 변수의 효과를 비교하기 위한 그래프를 그리기 위한 함수입니다. 예를 들어, 다음과 같이 Cox 모형에서 변수의 효과를 비교하는 그래프를 그릴 수 있습니다.\n\nlibrary(survival)\nlibrary(survminer)\n\ndata(lung)\nfit <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)\n\nggforest(fit, data = lung)\n\nggcoxdiagnostics 함수 ggcoxdiagnostics 함수는 Cox 모형의 가정을 검정하기 위한 그래프를 그리기 위한 함수입니다. 예를 들어, 다음과 같이 Cox 모형의 가정을 검정하는 그래프를 그릴 수 있습니다.\n\nlibrary(survival)\nlibrary(survminer)\n\ndata(lung)\nfit <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)\n\nggcoxdiagnostics(fit, type = \"deviance\")\n위 코드에서는 lung 데이터셋에서 age, sex, ph.ecog 변수를 이용하여 Cox 모형을 적합한 뒤, 모형 가정을 검정하는 그래프를 그리도록 하였습니다. ggcoxdiagnostics 함수의 type 인자를 “deviance”로 설정하면, 잔차 그래프와 로그(-로그) 생존 함수 그래프가 출력됩니다.\n\nggdag 함수 ggdag 함수는 DAG(Directed Acyclic Graph)를 그리기 위한 함수입니다. 예를 들어, 다음과 같이 DAG를 그릴 수 있습니다.\n\nlibrary(survminer)\n\nggdag(igraph::make_empty_graph(3) +\n  igraph::make_edges(c(1,2,2,3))\n)\n위 코드에서는 igraph 패키지를 이용하여 빈 그래프를 만든 뒤, 간선을 추가하여 DAG를 그리도록 하였습니다. ggdag 함수는 igraph 객체를 입력으로 받으며, 그래프를 그릴 때는 ggplot2의 기능을 이용합니다."
  },
  {
    "objectID": "posts/anything/2023-04-20-hazard_ratio,odds_ratio.html",
    "href": "posts/anything/2023-04-20-hazard_ratio,odds_ratio.html",
    "title": "Hazard ratio, Odds ratio",
    "section": "",
    "text": "Hazard ratio, Odds ratio\n\n\n로지스틱 회귀분석\n\\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k\\]\n\n# 예제 데이터 생성\nset.seed(123)\nn <- 200\nexam1 <- round(rnorm(n, mean = 70, sd = 10))\nexam2 <- round(rnorm(n, mean = 75, sd = 8))\nexam3 <- round(rnorm(n, mean = 80, sd = 6))\npass <- rbinom(n, 1, plogis(0.3 + 0.1 * exam1 + 0.2 * exam2 + 0.3 * exam3))\n\ndata <- data.frame(exam1, exam2, exam3, pass)\n\n# 로지스틱 회귀분석 모델 학습\nmodel <- glm(pass ~ exam1 + exam2 + exam3, data = data, family = binomial)\n\nWarning message:\n“glm.fit: algorithm did not converge”\n\n\nfamily = binomial에서 이항분포의 개념\n\n# 모델 요약 정보 출력\nsummary(model)\n\n\nCall:\nglm(formula = pass ~ exam1 + exam2 + exam3, family = binomial, \n    data = data)\n\nDeviance Residuals: \n      Min         1Q     Median         3Q        Max  \n2.409e-06  2.409e-06  2.409e-06  2.409e-06  2.409e-06  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)\n(Intercept)  2.657e+01  4.810e+05       0        1\nexam1       -3.384e-11  2.676e+03       0        1\nexam2        2.952e-10  3.178e+03       0        1\nexam3        2.546e-10  4.359e+03       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 0.0000e+00  on 199  degrees of freedom\nResidual deviance: 1.1603e-09  on 196  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n\n\n\n# 각 독립변수의 회귀계수와 p-value 출력\ncoef(summary(model))\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    EstimateStd. Errorz valuePr(>|z|)\n\n\n    (Intercept) 2.656607e+01480958.636 5.523566e-050.9999559\n    exam1-3.384147e-11  2676.045-1.264607e-141.0000000\n    exam2 2.951540e-10  3178.418 9.286192e-141.0000000\n    exam3 2.546355e-10  4359.366 5.841114e-141.0000000\n\n\n\n\n\n# 새로운 데이터에 대한 예측값 계산\nnew_data <- data.frame(exam1 = c(65, 80), exam2 = c(70, 85), exam3 = c(75, 90))\npred <- predict(model, newdata = new_data, type = \"response\")\n\n\n# 예측 결과 출력\nprint(pred)\n\n1 2 \n1 1 \n\n\n\n\n로지스틱 함수와 오즈비\n\\[P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\]\n\\(P(Y=1|X) = P\\)\n라고 할때,\n\\(ln(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 X\\)\nodds = \\(\\frac{p}{1-p}\\)\n\n# exam1 변수의 계수 출력\ncoef(model)[\"exam1\"]\n\nexam1: -3.38414654242419e-11\n\n\n\n# 오즈비 계산\nodds_ratio <- exp(coef(model)[\"exam1\"])\nodds_ratio\n\nexam1: 0.999999999966159\n\n\n\n\n위험비\n\\[HR = \\frac{\\lambda_1(t)}{\\lambda_0(t)} = \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k)\\]\n\\(\\lambda_1(t)\\): 치료군의 위험도\n\\(\\lambda_0(t)\\)는 대조군의 위험도\n위험비는 치료군 대 대조군의 위험도 비율\n1보다 크면 치료군의 위험이 높겠고,\n1보다 작으면 대조군의 위험이 높겠고.\n\n# 데이터 로딩\ndata(lung)\n\n# Cox 모형 적합\nlibrary(survival)\nfit <- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno + meal.cal + wt.loss, data = lung)\n\nWarning message in data(lung):\n“data set ‘lung’ not found”\n\n\n\n# Hazard Ratio 계산\nexp(coef(fit))\n\nage1.0107060959455sex0.576458374699102ph.ecog2.08376570435143ph.karno1.02270907640951pat.karno0.987660215984953meal.cal1.00003329080755wt.loss0.985771582300398"
  },
  {
    "objectID": "posts/ct/2023-01-30-Coding_Test_Q3.html",
    "href": "posts/ct/2023-01-30-Coding_Test_Q3.html",
    "title": "코딩 테스트 공부(Done)",
    "section": "",
    "text": "2022:KAKAO TECH INTERNSHIP\n\n문제 설명 [본 문제는 정확성과 효율성 테스트 각각 점수가 있는 문제입니다.]\n당신은 코딩 테스트를 준비하기 위해 공부하려고 합니다. 코딩 테스트 문제를 풀기 위해서는 알고리즘에 대한 지식과 코드를 구현하는 능력이 필요합니다.\n알고리즘에 대한 지식은 알고력, 코드를 구현하는 능력은 코딩력이라고 표현합니다. 알고력과 코딩력은 0 이상의 정수로 표현됩니다.\n문제를 풀기 위해서는 문제가 요구하는 일정 이상의 알고력과 코딩력이 필요합니다.\n예를 들어, 당신의 현재 알고력이 15, 코딩력이 10이라고 가정해보겠습니다.\nA라는 문제가 알고력 10, 코딩력 10을 요구한다면 A 문제를 풀 수 있습니다.\nB라는 문제가 알고력 10, 코딩력 20을 요구한다면 코딩력이 부족하기 때문에 B 문제를 풀 수 없습니다.\n풀 수 없는 문제를 해결하기 위해서는 알고력과 코딩력을 높여야 합니다. 알고력과 코딩력을 높이기 위한 다음과 같은 방법들이 있습니다.\n알고력을 높이기 위해 알고리즘 공부를 합니다. 알고력 1을 높이기 위해서 1의 시간이 필요합니다.\n코딩력을 높이기 위해 코딩 공부를 합니다. 코딩력 1을 높이기 위해서 1의 시간이 필요합니다.\n현재 풀 수 있는 문제 중 하나를 풀어 알고력과 코딩력을 높입니다. 각 문제마다 문제를 풀면 올라가는 알고력과 코딩력이 정해져 있습니다.\n문제를 하나 푸는 데는 문제가 요구하는 시간이 필요하며 같은 문제를 여러 번 푸는 것이 가능합니다.\n당신은 주어진 모든 문제들을 풀 수 있는 알고력과 코딩력을 얻는 최단시간을 구하려 합니다.\n초기의 알고력과 코딩력을 담은 정수 alp와 cop, 문제의 정보를 담은 2차원 정수 배열 problems가 매개변수로 주어졌을 때, 모든 문제들을 풀 수 있는 알고력과 코딩력을 얻는 최단시간을 return 하도록 solution 함수를 작성해주세요.\n모든 문제들을 1번 이상씩 풀 필요는 없습니다. 입출력 예 설명을 참고해주세요.\n제한사항\n초기의 알고력을 나타내는 alp와 초기의 코딩력을 나타내는 cop가 입력으로 주어집니다.\n\\(0 ≤ alp,cop ≤ 150\\), \\(1 ≤ problems의 길이 ≤ 100\\)\nproblems의 원소는 [alp_req, cop_req, alp_rwd, cop_rwd, cost]의 형태로 이루어져 있습니다.\nalp_req는 문제를 푸는데 필요한 알고력입니다.\n\\(0 ≤ alp_req ≤ 150\\)\ncop_req는 문제를 푸는데 필요한 코딩력입니다.\n\\(0 ≤ cop_req ≤ 150\\)\nalp_rwd는 문제를 풀었을 때 증가하는 알고력입니다.\n\\(0 ≤ alp_rwd ≤ 30\\)\ncop_rwd는 문제를 풀었을 때 증가하는 코딩력입니다.\n\\(0 ≤ cop_rwd ≤ 30\\)\ncost는 문제를 푸는데 드는 시간입니다.\n\\(1 ≤ cost ≤ 100\\)\n정확성 테스트 케이스 제한사항\n\\(0 ≤ alp,cop ≤ 20\\)\n\\(1 ≤ problems의 길이 ≤ 6\\)\n\\(0 ≤ alp_req,cop_req ≤ 20\\)\n\\(0 ≤ alp_rwd,cop_rwd ≤ 5\\)\n\\(1 ≤ cost ≤ 10\\)\n효율성 테스트 케이스 제한사항\n주어진 조건 외 추가 제한사항 없습니다.\n입출력 예\n\n\n\nalp\ncop\nproblems\nresult\n\n\n\n\n10\n10\n[[10,15,2,1,2],[20,20,3,3,4]]\n15\n\n\n0\n0\n[[0,0,2,1,2],[4,5,3,1,2],[4,11,4,0,2],[10,4,0,4,2]]\n13\n\n\n\n입출력 예 설명\n입출력 예 #1\n코딩력 5를 늘립니다. 알고력 10, 코딩력 15가 되며 시간이 5만큼 소요됩니다.\n1번 문제를 5번 풉니다. 알고력 20, 코딩력 20이 되며 시간이 10만큼 소요됩니다. 15의 시간을 소요하여 모든 문제를 풀 수 있는 알고력과 코딩력을 가질 수 있습니다.\n입출력 예 #2\n1번 문제를 2번 풉니다. 알고력 4, 코딩력 2가 되며 시간이 4만큼 소요됩니다.\n코딩력 3을 늘립니다. 알고력 4, 코딩력 5가 되며 시간이 3만큼 소요됩니다.\n2번 문제를 2번 풉니다. 알고력 10, 코딩력 7이 되며 시간이 4만큼 소요됩니다.\n4번 문제를 1번 풉니다. 알고력 10, 코딩력 11이 되며 시간이 2만큼 소요됩니다. 13의 시간을 소요하여 모든 문제를 풀 수 있는 알고력과 코딩력을 가질 수 있습니다.\n제한시간 안내\n정확성 테스트 : 10초\n효율성 테스트 : 언어별로 작성된 정답 코드의 실행 시간의 적정 배수\n\n동적 계획법(Dynamic Programming)\n이미 계산을 했지만, 그 출력 결과를 바탕으로 다시 계산이 됨.(ex. 피보나치 수열)\n\ntop-down\ndown-top\n\n두 가지 방식이 존재한다.\n\ndef solution(alp, cop, problems):\n    max_alp = 0\n    max_cop = 0\n    for alp_req, cop_req, alp_rwd, cop_rwd, cost in problems:\n        max_alp = max(max_alp,alp_req)\n        max_cop = max(max_cop,cop_req)\n    \n    dp = [[float('inf')] * (max_cop + 1) for _ in range(max_alp +1)]\n    \n    alp = min(alp,max_alp)\n    cop = min(cop,max_cop)\n    \n    dp[alp][cop] = 0\n    \n    for i in range(alp, max_alp + 1):\n        for j in range(cop,max_cop +1):\n            if i < max_alp:\n                dp[i+1][j] = min(dp[i+1][j],dp[i][j]+1)\n            if j < max_cop:\n                dp[i][j+1] = min(dp[i][j+1],dp[i][j]+1)\n            for alp_req, cop_req, alp_rwd, cop_rwd, cost in problems:\n                if i >= alp_req and j >= cop_req:\n                    alp_n = min(i + alp_rwd, max_alp)\n                    cop_n = min(j + cop_rwd, max_cop)\n                    dp[alp_n][cop_n] = min(dp[alp_n][cop_n],dp[i][j] + cost)\n                    \n    return dp[max_alp][max_cop]\n\n\nmax_alp = 0\nmax_cop = 0\n\n\nproblems = [[0,0,2,1,2],[4,5,3,1,2],[4,11,4,0,2],[10,4,0,4,2]]\n\n\nalp = 0\ncop = 0\n\n\nfor alp_req, cop_req, alp_rwd, cop_rwd, cost in problems:\n    max_alp = max(max_alp,alp_req)\n    max_cop = max(max_cop,cop_req)\n\n\nmax_alp,max_cop\n\n(10, 11)\n\n\n\ndp = [[float('inf')] * (max_cop + 1) for _ in range(max_alp +1)]\ndp\n\n[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]\n\n\n\nalp = min(alp,max_alp)\ncop = min(cop,max_cop)\n\n\nalp,cop\n\n(0, 0)\n\n\n\ndp[alp][cop] = 0\n\n\ndp\n\n[[0, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]\n\n\n\nfor i in range(alp, max_alp + 1):\n    for j in range(cop,max_cop +1):\n        if i < max_alp:\n            dp[i+1][j] = min(dp[i+1][j],dp[i][j]+1)\n        if j < max_cop:\n            dp[i][j+1] = min(dp[i][j+1],dp[i][j]+1)\n        for alp_req, cop_req, alp_rwd, cop_rwd, cost in problems:\n            if i >= alp_req and j >= cop_req:\n                alp_n = min(i + alp_rwd, max_alp)\n                cop_n = min(j + cop_rwd, max_cop)\n                dp[alp_n][cop_n] = min(dp[alp_n][cop_n],dp[i][j] + cost)\nreturn dp[max_alp][max_cop]"
  },
  {
    "objectID": "posts/ct/2023-01-01-Coding_Test_Greedy.html",
    "href": "posts/ct/2023-01-01-Coding_Test_Greedy.html",
    "title": "Chapter 03 Greedy",
    "section": "",
    "text": "그리디; 현재 상황에서 가장 좋아 보이는 것만을 선택하는 알고리즘 ~ 탐욕법"
  },
  {
    "objectID": "posts/ct/2023-01-01-Coding_Test_Greedy.html#example-거스름돈",
    "href": "posts/ct/2023-01-01-Coding_Test_Greedy.html#example-거스름돈",
    "title": "Chapter 03 Greedy",
    "section": "Example 거스름돈",
    "text": "Example 거스름돈\n당신은 음식점의 계산을 도와주는 점원이다. 카운터에는 거스름돈으로 사용할 500원, 100원, 50원, 10원짜리 동전이 무한히 존재한다고 가정한다. 손님에게 거슬러 줘야 할 돈이 N원일 때 거슬러 줘야 할 동전의 최소 개수를 구하라. 단, 거슬러 줘야 할 돈 N은 항상 10의 배수이다.\n해설\n\n최소 동전 개수 \\(\\to\\) ‘가장 큰 화폐 단위부터’ 돈을 거슬러 주는 것\n\n이 예제가 그리디 알고리즘이 가능한 이유\n\n가지고 있는 동전 중에서 큰 단위가 항상 작은 단위의 배수이므로 작은 단위의 동전들을 종합해 다른 해가 나올 수 없기 때문이다.\n대부분의 그리디 알고리즘 문제에서는 이처럼 문제 풀이를 위한 최소한의 아이디어를 떠올리고 이것이 정당한지 검토할 수 있어야 답을 도출할 수 있다.\n\n\nAnswer\n\nn = 1260\ncount = 0\n\n# 큰 단위 화폐부터 차례대로 확인\ncoin_types = [500, 100, 50, 10]\n\nfor coin in coin_types:\n    count += n // coin # 해당 화폐로 거슬러 줄 수 있는 동전의 개수 세기\n    n %= coin\n    \nprint(count)\n\n6\n\n\n\n\nstep\n\ncount = 0;count\n\n0\n\n\n\nn = 1260;n\n\n1260\n\n\n\ncount = count + n//500;count\n\n2\n\n\n\nn %= 500;n\n\n260\n\n\n\ncount = count + n//100;count\n\n4\n\n\n\nn %= 100 ; n\n\n60\n\n\n\ncount = count + n//50;count\n\n5\n\n\n\nn %= 50 ; n\n\n10\n\n\n\ncount = count + n//10;count\n\n6\n\n\n\nn %= 10;n\n\n0"
  },
  {
    "objectID": "posts/ct/2023-01-01-Coding_Test_Greedy.html#큰-수의-법칙",
    "href": "posts/ct/2023-01-01-Coding_Test_Greedy.html#큰-수의-법칙",
    "title": "Chapter 03 Greedy",
    "section": "큰 수의 법칙",
    "text": "큰 수의 법칙\n’큰 수의 법칙’은 일반적으로 통계 분야에서 다루어지는 내용이지만 동빈이는 본인만의 방식으로 다르게 사용하고 있다. 동빈이의 큰 수의 법칙은 다양한 수로 이루어진 배열이 있을 때 주어진 수들을 M번 더하여 가장 큰 수를 만드는 법칙이다. 단 배열의 특정한 인덱스(번호)에 해당하는 수가 연속해서 K번 초과하여 더해질 수 없는 것이 이 법칙의 특징이다.\n배열의 크기 N, 숫자가 더해지는 횟수 M, 그리고 K가 주어질 떄 동빈이의 큰 수의 법칙에 따른 결과를 출력하시오.\n입력 조건\n\n첫쨰 줄에 \\(N(2\\le N \\le 1,000), M(1 \\le M \\le 10,000), K(1 \\le K \\le 10,000)\\)의 자연수가 주어지며, 각 자연수는 공백으로 구분한다.\n둘째 줄에 N개의 자연수가 주어진다. 각 자연수는 공백으로 구분한다. 단, 각각의 자연수는 1 이상 10,000 이하의 수로 주어진다.\n입력으로 주어지는 K는 항상 M보다 작거나 같다.\n\n출력 조건\n\n첫째 줄에 동빈이의 큰 수의 법칙에 따라 더해진 답을 출력한다.\n\n해설\n\n’가장 큰 수를 K번 더하고 두 번째로 큰 수를 한 번 더하는 연산’을 반복\n반복되는 수열에 대해서 파악하기\n\n가장 큰 수가 더해지는 횟수 \\[int(M / (K + 1)) * K + M \\% (K + 1)\\]\n\nAnswer 1\n\n# N, M, K를 공백으로 구분하여 입력받기\nn, m, k = map(int, input().split())\n# N개의 수를 공백으로 구분하여 입력받기\ndata = list(map(int, input().split()))\n\ndata.sort() # 입력받은 수들 정렬하기\nfirst = data[n - 1] # 가장 큰 수\nsecond = data[n - 2] # 두 번째로 큰 수\n\nresult = 0\n\nwhile True:\n    for i in range(k): # 가장 큰 수를  K번 더하기\n        if m == 0: # m이 0이라면 반복문 탈출\n            break\n        result += first\n        m -= 1 # 더할 때마다 1씩 빼기\n    if m == 0: # m이 0이라면 반복문 탈출\n        break\n    result += second # 두 번째로 큰 수를 한 번 더하기\n    m -= 1 # 더할 떄마다 1씩 빼기\n    \nprint(result)\n\n 5 8 3 \n 2 4 5 4 6\n\n\n46\n\n\n\n\nstep\n\nn, m, k = map(int, input().split())\n\n 5 8 3\n\n\n\ndata = list(map(int, input().split()))\n\n 2 4 5 4 6\n\n\n\ndata.sort();data\n\n[2, 4, 4, 5, 6]\n\n\n\nrange(k)\n\nrange(0, 3)\n\n\n\nfirst = data[n-1];first\n\n6\n\n\n\nsecond = data[n-2];second\n\n5\n\n\n\nresult = 0; result\n\n0\n\n\n\nresult = result + first*3; result\n\n18\n\n\n\nm = m - 3 ;m\n\n5\n\n\n\nresult = result + second; result\n\n23\n\n\n\nm = m - 1 ;m\n\n4\n\n\n\nresult = result + first*3; result\n\n41\n\n\n\nm = m - 3 ;m\n\n1\n\n\n\nresult = result + second; result\n\n46\n\n\n\nm = m - 1 ;m\n\n0\n\n\n\nint(8 / (6 + 1)) * 3 + 8 % (6 + 1)\n\n4\n\n\n\n\nAnswer 2\n\n# N, M, K를 공백으로 구분하여 입력받기\nn, m, k = map(int, input().split())\n# N개의 수를 공백으로 구분하여 입력받기\ndata = list(map(int, input().split()))\n\ndata.sort() # 입력받은 수 정렬\nfirst = data[n - 1] # 가장 큰 수\nsecond = data[n - 2] # 두 번째로 큰 수\n\n# 가장 큰 수가 더해지는 횟수 계산\ncount = int(m / (k + 1)) * k\ncount += m % (k+1)\n\nresult = 0\nresult += count * first # 가장 큰 수 더하기\nresult += (m - count) * second # 두 번재로 큰 수 더하기\n\nprint(result)\n\n 5 8 3\n 2 4 5 4 6\n\n\n46\n\n\n\nn, m, k = map(int, input().split())\n\n 5 8 3\n\n\n\ndata = list(map(int, input().split()))\n\n 2 4 5 4 6\n\n\n\ndata.sort();data\n\n[2, 4, 4, 5, 6]\n\n\n\nfirst = data[n-1];first\n\n6\n\n\n\nsecond = data[n-2];second\n\n5\n\n\n\ncount = int(m / (k + 1)) * k ; count\n\n6\n\n\n\nm % (k+1)\n\n1\n\n\n\nresult = 0;result\n\n0\n\n\n\nresult += count * first;result\n\n36\n\n\n\nresult += (m - count) * second; result\n\n46"
  },
  {
    "objectID": "posts/ct/2023-01-01-Coding_Test_Greedy.html#숫자-카드-게임",
    "href": "posts/ct/2023-01-01-Coding_Test_Greedy.html#숫자-카드-게임",
    "title": "Chapter 03 Greedy",
    "section": "숫자 카드 게임",
    "text": "숫자 카드 게임\n숫자 카드 게임은 여러 개의 숫자 카드 중에서 가장 높은 숫자가 쓰인 카드 한 장을 뽑는 게임이다. 단, 게임의 룰을 지키며 카드를 뽑아야 하고 룰은 다음과 같다. 1. 숫자가 쓰인 카드들이 \\(N \\times M\\) 형태로 놓여 있다. 이때 N은 행의 개수를 의미하며, M은 열의 개수를 의미한다. 2. 먼저 뽑고자 하는 카드가 포함되어 있는 행을 선택한다. 3. 그다음 선택된 행에 초함된 카드들 중 가장 숫자가 낮은 카드를 뽑아야 한다. 4. 따라서 처음에 카드를 골라낼 행을 선택할 때, 이후에 해당 행에서 가장 숫자가 낮은 카드를 뽑을 것을 고려하여 최종적으로 가장 높은 숫자의 카드를 뽑을 수 있도록 정략을 세워야 한다.\n입력 조건\n\n첫째 줄에 숫자 카드들이 놓인 행의 개수 N과 열의 개수 M이 공백을 기준으로 하여 각각 자연수로 주어진다.(\\(1 \\le N, M \\le 100\\))\n둘째 줄부터 N개의 줄에 걸쳐 각 카드에 적힌 숫자가 주어진다. 각 숫자는 1 이상 10,000 이햐의 자연수이다.\n\n출력 조건\n\n첫째 줄에 게임의 룰에 맞게 선택한 카드에 적힌 숫자를 출력한다.\n\n문제 해설\n\n’각 행마다 가장 작을 수를 찾은 뒤에 그 수 중에서 가장 큰 수’를 찾는 것\n\n\nAnswer 1\n\nmin() 함수를 이용하는 답안 예시\n\n\n# N, M을 공백으로 구분하여 입력받기\nn, m = map(int, input().split())\n\nresult = 0\n# 한 줄씩 입력받아 확인\nfor i in range(n): \n    data = list(map(int, input().split()))\n    # 현재 줄에서 '가장 작은 수' 찾기\n    min_value = min(data)\n    # '가장 작은 수'들 중에서 가장 큰 수 찾기\n    result = max(result, min_value)\n    \nprint(result)\n\n 3 3\n 3 1 2\n 4 1 4\n 2 2 2\n\n\n2\n\n\n\nn, m = map(int, input().split())\n\n 3 3\n\n\n\nresult = 0; result\n\n0\n\n\n\nrange(3)\n\nrange(0, 3)\n\n\n\ndata = list(map(int, input().split()))\n\n 3 1 2\n\n\n\nmin_value = min(data);min_value\n\n1\n\n\n\nresult = max(result, min_value);result\n\n1\n\n\n\ndata = list(map(int, input().split()))\n\n 4 1 4\n\n\n\nmin_value = min(data);min_value\n\n1\n\n\n\nresult = max(result, min_value);result\n\n1\n\n\n\ndata = list(map(int, input().split()))\n\n 2 2 2\n\n\n\nmin_value = min(data);min_value\n\n2\n\n\n\nresult = max(result, min_value);result\n\n2\n\n\n\n\nAnswer 2\n\n2중 반복문 구조를 이용하는 답안 예시\n\n\n# N, M을 공백으로 구분하여 입력받기\nn, m = map(int, input().split())\n\nresult = 0\n# 한 줄씩 입력받아 확인\nfor i in range(n):\n    data = list(map(int, input().split()))\n    # 현재 줄에서 '가장 작은 수'찾기\n    min_value = 10001\n    for a in data:\n        min_value = min(min_value,a)\n    # '가장 작은 수'들 중에서 가장 큰 수 찾기\n    result = max(result, min_value)\n    \nprint(result)\n\n 2 4\n 7 3 1 8\n 3 3 3 4\n\n\n3\n\n\n\nn, m = map(int, input().split())\n\n 2 4\n\n\n\nresult = 0\n\n\nrange(2)\n\nrange(0, 2)\n\n\n\ndata = list(map(int, input().split()))\n\n 7 3 1 8\n\n\n\nmin_value = 10001;min_value\n\n10001\n\n\n\nmin_value = min(min_value,7,3,1,8);min_value\n\n1\n\n\n\nresult = max(result, min_value);result\n\n1\n\n\n\ndata = list(map(int, input().split()))\n\n 3 3 3 4\n\n\n\nmin_value = 10001;min_value\n\n10001\n\n\n\nmin_value = min(min_value,3,3,3,4);min_value\n\n3\n\n\n\nresult = max(result, min_value);result\n\n3"
  },
  {
    "objectID": "posts/ct/2023-01-01-Coding_Test_Greedy.html#이-될-때까지",
    "href": "posts/ct/2023-01-01-Coding_Test_Greedy.html#이-될-때까지",
    "title": "Chapter 03 Greedy",
    "section": "1이 될 때까지",
    "text": "1이 될 때까지\n어떠한 수 N이 1이 될 떄까지 다음의 두 과정 중 하나를 반복적으로 선택하여 수행하려고 한다. 단, 두 번째 연산은 N이 K로 나누어 쩔어질 때만 선택할 수 있다. 1. N에서 1을 뺀다. 2. N을 K로 나눈다.\nN과 K가 주어질 때 N이 1이 될 때까지 1번 혹은 2번의 과정을 수행해야 하는 최소 횟수를 구하는 프로그램을 작성하시오.\n입력 조건\n\n첫째 줄에 \\(N(2 \\le N \\le 100,000)\\)과 \\(K(2 \\le K \\le 100,000)\\)가 공백으로 구분되며 각각 자연수로 주어진다. 이때 입력으로 주어지는 N은 항상 K보다 크거나 같다.\n\n출력 조건\n\n첫째 줄에 N이 1이 될 때까지 1번 혹은 2번의 과정을 수행해야 하는 횟수의 최솟값을 출력한다.\n\n문제 해설\n\n‘최대한 많이 나누기’\n\n\nAnswer 1\n\n단순하기 푸는 답안 예시\n\n\nn, k = map(int, input().split())\nresult = 0\n\n# N이 K 이상이라면 K로 계속 나누기\nwhile n >= k:\n    # N이 K로 나누어 떨어지지 않는다면 N에서 1씩 빼기\n    while n % k != 0:\n        n -= 1\n        result += 1\n    # K 로 나누기\n    n //= k\n    result += 1\n    \n# 마지막으로 남은 수에 대하여 1씩 빼기\nwhile n > 1:\n    n -= 1\n    result += 1\n\nprint(result)\n\n 25 5\n\n\n2\n\n\n\nn, k = map(int, input().split())\n\n 25 5\n\n\n\nresult = 0; result\n\n0\n\n\n\nn%k\n\n0\n\n\n\nn //= k;n\n\n5\n\n\n\nresult += 1 ;result\n\n1\n\n\n\nn%k\n\n0\n\n\n\nn // k ; n\n\n5\n\n\n\nresult += 1 ; result\n\n2\n\n\n\n\nAnswer 2\n\n답안 예시\n\n\n# N, K를 공백으로 구분하여 입력받기\nn, k = map(int, input().split())\nresult = 0\n\nwhile True:\n    # (N == K로 나누어 쩔어지는 수)가 될 때까지 1씩 빼기\n    target = (n // k) * k\n    result += (n - target)\n    n = target\n    # N이 K보다 작을 때(더 이상 나눌 수 없을 떄) 반복문 탈출\n    if n < k:\n        break\n    # K로 나누기\n    result += 1\n    n //= k\n    \n# 마지막으로 남은 수에 대하여 1씩 빼기\nresult += ( n - 1 )\nprint(result)\n\n 25 5\n\n\n2\n\n\n\nn, k = map(int, input().split())\n\n 25 5\n\n\n\nresult = 0\n\n\ntarget = (n // k) * k; target\n\n25\n\n\n\nn = target ; n\n\n25\n\n\n\nresult += 1;result\n\n1\n\n\n\nn //= k ;n\n\n5\n\n\n\ntarget = (n // k) * k; target\n\n5\n\n\n\nn = target ; n\n\n5\n\n\n\nresult += 1;result\n\n2\n\n\n\nn //= k ;n\n\n1"
  },
  {
    "objectID": "posts/ct/index.html",
    "href": "posts/ct/index.html",
    "title": "Coding Test",
    "section": "",
    "text": "Let’s prepare the coding test."
  },
  {
    "objectID": "posts/ct/2023-03-05-Coding_Test_Stack.html",
    "href": "posts/ct/2023-03-05-Coding_Test_Stack.html",
    "title": "Stack",
    "section": "",
    "text": "Stack"
  },
  {
    "objectID": "posts/ct/2023-03-05-Coding_Test_Stack.html#stack을-통해-사용하는-기능",
    "href": "posts/ct/2023-03-05-Coding_Test_Stack.html#stack을-통해-사용하는-기능",
    "title": "Stack",
    "section": "Stack을 통해 사용하는 기능",
    "text": "Stack을 통해 사용하는 기능\n\npush: 데이터를 스택에 추가한다.(스택의 하단부터 상단으로 차곡차곡 쌓는다.\npop: 스택의 최상단 데이터를 삭제한다.\ntop: 스택에 데이터가 몇 개 들어있는지 확인한다.\nsize: 스택에 데이터가 몇 개 들어있는지 확인한다.\nempty: 스택이 비어 있는지 확인한다.(데이터가 없는지 확인)"
  },
  {
    "objectID": "posts/ct/2023-03-05-Coding_Test_Stack.html#쇠막대기",
    "href": "posts/ct/2023-03-05-Coding_Test_Stack.html#쇠막대기",
    "title": "Stack",
    "section": "쇠막대기",
    "text": "쇠막대기\n여러 개의 쇠막대기를 레이저로 절단하려고 한다. 효율적인 작업을 위해서 쇠막대기를 아래에서 위로 겹쳐 놓고, 레이저를 위에서 수직으로 발사하여 쇠막대기들을 자른다. 쇠막대기와 레이저의 배치는 다음 조건을 만족한다.\n\n쇠막대기는 자신보다 긴 쇠막대기 위에만 놓일 수 있다.\n\n쇠막대기를 다른 쇠막대기 위에 놓는 경우 완전히 포함되도록 놓되, 끝점은 겹치지 않도록 놓는다.\n\n각 쇠막대기를 자르는 레이저는 적어도 하나 존재한다.\n레이저는 어떤 쇠막대기릐 양 끝점과도 겹치지 않는다.\n\n아래 그림은 위 조건을 만족하는 예를 보여준다. 수평으로 그려진 굵은 실선은 쇠막대기이고, 점은 레이저의 위치, 수직으로 그려진 점선 화살표는 레이저의 발사 방향이다.\n\n\n\nimage.png\n\n\n이러한 레이저와 쇠막대기의 배치는 다음과 같이 괄호를 이용하여 왼쪽부터 순서대로 표현할 수 있다.\n레이저는 여는 괄호와 닫는 괄호의 인접한 쌍 ‘( )’ 으로 표현된다. 또한, 모든 ‘( ) ’는 반드시 레이저를 표현한다. 쇠막대기의 왼쪽 끝은 여는 괄호 ‘ (’ 로, 오른쪽 끝은 닫힌 괄호 ‘)’ 로 표현된다. 위 예의 괄호 표현은 그림 위에 주어져 있다.\n쇠막대기는 레이저에 의해 몇 개의 조각으로 잘려지는데, 위 예에서 가장 위에 있는 두 개의 쇠막대기는 각각 3개와 2개의 조각으로 잘려지고, 이와 같은 방식으로 주어진 쇠막대기들은 총 17개의 조각으로 잘려진다.\n쇠막대기와 레이저의 배치를 나타내는 괄호 표현이 주어졌을때, 잘려진 쇠막대기 조각의 총 개수를 구하는 프로그램을 작성하시오."
  },
  {
    "objectID": "posts/ct/2023-03-05-Coding_Test_Stack.html#크게-만들기",
    "href": "posts/ct/2023-03-05-Coding_Test_Stack.html#크게-만들기",
    "title": "Stack",
    "section": "크게 만들기",
    "text": "크게 만들기\nN 자리 숫자가 주어졌을때, 여기서 숫자 K개를 얻을 수 있는 가장 큰 수를 구하는 프로그램을 작성하시오.\n- 맨 왼쪽에 있는 수가 가장 큰 수를 선택했을때 결과도 크겠지?\n- array보다는 stack으로 시간복잡도를 줄일 수 있다.\n\nn, k = map(int, input().split())\nnumber = list(input())\n\nanswer = []\ncnt = k\nfor num in number: \n    while answer and cnt>0 and answer[-1] <num: \n        # 만약 answer에 값이 있고 지울 수 있는 수 k가 남아있고, answer의 마지막 값이 num보다 작다면?\n        del answer[-1]\n        # answer 마지막 값 삭제하고\n        cnt-=1\n        # 지울 수 있는 k 수도 줄인다.\n    answer.append(num)\n    \nprint(''.join(answer[:n-k]))\n\n 4 2\n 1924\n\n\n94"
  },
  {
    "objectID": "posts/ct/2023-03-12-Coding_Test_Queue.html",
    "href": "posts/ct/2023-03-12-Coding_Test_Queue.html",
    "title": "Queue",
    "section": "",
    "text": "Queue"
  },
  {
    "objectID": "posts/ct/2023-03-12-Coding_Test_Queue.html#queue을-통해-사용하는-기능",
    "href": "posts/ct/2023-03-12-Coding_Test_Queue.html#queue을-통해-사용하는-기능",
    "title": "Queue",
    "section": "Queue을 통해 사용하는 기능",
    "text": "Queue을 통해 사용하는 기능\n\npush: 데이터를 큐에 추가한다.(큐의 전단부터 후단으로 차곡차곡 쌓는다.\npop: 큐의 가장 앞(전단) 데이터를 삭제한다.\nsize: 큐에 데이터가 몇 개 들어있는지 확인한다.\nempty: 큐가 비어 있는지 확인한다.(데이터가 없는지 확인)\nfront: 큐의 가장 앞(전단) 데이터가 무엇인지 확인한다.\nback: 큐의 가장 뒤(후단) 데이터가 무엇인지 확인한다.\n\n\\(\\star\\) 파이썬에서 queue를 사용하기 위해 deque라는 라이브러리를 이용한다. \\(\\to\\) stack과 queue의 기능을 합친 것이다."
  },
  {
    "objectID": "posts/ct/2023-01-21-Coding_Test_Q1.html",
    "href": "posts/ct/2023-01-21-Coding_Test_Q1.html",
    "title": "성격 유형 검사하기(Done)",
    "section": "",
    "text": "2022:KAKAO TECH INTERNSHIP\n\n\n문제 설명\n나만의 카카오 성격 유형 검사지를 만들려고 합니다.\n성격 유형 검사는 다음과 같은 4개 지표로 성격 유형을 구분합니다. 성격은 각 지표에서 두 유형 중 하나로 결정됩니다.\n\n\n\n지표 번호\n성격 유형\n\n\n\n\n1번 지표\n라이언형(R), 튜브형(T)\n\n\n2번 지표\n콘형(C), 프로도형(F)\n\n\n3번 지표\n제이지형(J), 무지형(M)\n\n\n4번 지표\n어피치형(A), 네오형(N)\n\n\n\n4개의 지표가 있으므로 성격 유형은 총 16(=2 x 2 x 2 x 2)가지가 나올 수 있습니다. 예를 들어, “RFMN”이나 “TCMA”와 같은 성격 유형이 있습니다.\n검사지에는 총 n개의 질문이 있고, 각 질문에는 아래와 같은 7개의 선택지가 있습니다.\n\n\n\n매우 비동의\n\n\n\n\n비동의\n\n\n약간 비동의\n\n\n모르겠음\n\n\n약간 동의\n\n\n동의\n\n\n매우 동의\n\n\n\n각 질문은 1가지 지표로 성격 유형 점수를 판단합니다.\n예를 들어, 어떤 한 질문에서 4번 지표로 아래 표처럼 점수를 매길 수 있습니다.\n\n\n\n선택지\n성격 유형 점수\n\n\n\n\n매우 비동의\n네오형 3점\n\n\n비동의\n네오형 2점\n\n\n약간 비동의\n네오형 1점\n\n\n모르겠음\n어떤 성격 유형도 점수를 얻지 않습니다\n\n\n약간 동의\n어피치형 1점\n\n\n동의\n어피치형 2점\n\n\n매우 동의\n어피치형 3점\n\n\n\n이때 검사자가 질문에서 약간 동의 선택지를 선택할 경우 어피치형(A) 성격 유형 1점을 받게 됩니다. 만약 검사자가 매우 비동의 선택지를 선택할 경우 네오형(N) 성격 유형 3점을 받게 됩니다.\n위 예시처럼 네오형이 비동의, 어피치형이 동의인 경우만 주어지지 않고, 질문에 따라 네오형이 동의, 어피치형이 비동의인 경우도 주어질 수 있습니다.\n하지만 각 선택지는 고정적인 크기의 점수를 가지고 있습니다.\n\n매우 동의나 매우 비동의 선택지를 선택하면 3점을 얻습니다.\n동의나 비동의 선택지를 선택하면 2점을 얻습니다.\n약간 동의나 약간 비동의 선택지를 선택하면 1점을 얻습니다.\n모르겠음 선택지를 선택하면 점수를 얻지 않습니다.\n\n검사 결과는 모든 질문의 성격 유형 점수를 더하여 각 지표에서 더 높은 점수를 받은 성격 유형이 검사자의 성격 유형이라고 판단합니다. 단, 하나의 지표에서 각 성격 유형 점수가 같으면, 두 성격 유형 중 사전 순으로 빠른 성격 유형을 검사자의 성격 유형이라고 판단합니다.\n질문마다 판단하는 지표를 담은 1차원 문자열 배열 survey와 검사자가 각 질문마다 선택한 선택지를 담은 1차원 정수 배열 choices가 매개변수로 주어집니다. 이때, 검사자의 성격 유형 검사 결과를 지표 번호 순서대로 return 하도록 solution 함수를 완성해주세요.\n제한사항\n\n1 ≤ survey의 길이 ( = n) ≤ 1,000\nsurvey의 원소는 “RT”, “TR”, “FC”, “CF”, “MJ”, “JM”, “AN”, “NA” 중 하나입니다.\nsurvey[i]의 첫 번째 캐릭터는 i+1번 질문의 비동의 관련 선택지를 선택하면 받는 성격 유형을 의미합니다.\nsurvey[i]의 두 번째 캐릭터는 i+1번 질문의 동의 관련 선택지를 선택하면 받는 성격 유형을 의미합니다.\nchoices의 길이 = survey의 길이\n\nchoices[i]는 검사자가 선택한 i+1번째 질문의 선택지를 의미합니다.\n\n1 ≤ choices의 원소 ≤ 7\n\n\n\n\nchoices\n뜻\n\n\n\n\n1\n매우 비동의\n\n\n2\n비동의\n\n\n3\n약간 비동의\n\n\n4\n모르겠음\n\n\n5\n약간 동의\n\n\n6\n동의\n\n\n7\n매우 동의\n\n\n\n입출력 예\n\n\n\nsurvey\nchoices\nresult\n\n\n\n\n[“AN”, “CF”, “MJ”, “RT”, “NA”]\n[5, 3, 2, 7, 5]\n“TCMA”\n\n\n[“TR”, “RT”, “TR”]\n[7, 1, 3]\n“RCJA”\n\n\n\nanswer\n\ndef solution(survey, choices):\n    answer = ''\n    a = {'R':0,'T':0,'F':0,'C':0,'M':0,'J':0,'N':0,'A':0}\n    \n    for i in range(len(survey)):\n        if choices[i] < 4:\n            if choices[i] == 1:\n                a[survey[i][0]] = a[survey[i][0]] + 3\n            elif choices[i] == 2:\n                a[survey[i][0]] = a[survey[i][0]] + 2\n            elif choices[i] == 3:\n                a[survey[i][0]] = a[survey[i][0]] + 1\n        if choices[i] > 4:\n            if choices[i] == 5:\n                a[survey[i][1]] = a[survey[i][1]] + 1\n            elif choices[i] == 6:\n                a[survey[i][1]] = a[survey[i][1]] + 2\n            elif choices[i] == 7:\n                a[survey[i][1]] = a[survey[i][1]] + 3\n                \n    # for i in range(len(choices)):\n    #     if choices[i] > 4:\n    #         choices[i] = choices[i] - 4\n\n    if a['T']>a['R']:\n        answer += 'T'\n    else: answer += 'R'\n        \n    if a['F']>a['C']:\n        answer += 'F'\n    else: answer += 'C'\n        \n    if a['M']>a['J']:\n        answer += 'M'\n    else: answer += 'J'\n        \n    if a['N']>a['A']:\n        answer += 'N'\n    else: answer += 'A'\n    \n    return answer\n\n\nsolution([\"AN\", \"CF\", \"MJ\", \"RT\", \"NA\"],[5, 3, 2, 7, 5])\n\n'TCMA'\n\n\n\nsolution([\"TR\", \"RT\", \"TR\"],[7,1,3])\n\n'RCJA'"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_Algorithm.html",
    "href": "posts/ct/2023-01-15-Coding_Test_Algorithm.html",
    "title": "Algorithm",
    "section": "",
    "text": "Algorithm"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_Algorithm.html#순열",
    "href": "posts/ct/2023-01-15-Coding_Test_Algorithm.html#순열",
    "title": "Algorithm",
    "section": "순열",
    "text": "순열\n\nimport itertools\n\ndata = [2,3,4]\n\nfor x in itertools.permutations(data,2):\n    print(list(x))\n\n[2, 3]\n[2, 4]\n[3, 2]\n[3, 4]\n[4, 2]\n[4, 3]"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_Algorithm.html#조합",
    "href": "posts/ct/2023-01-15-Coding_Test_Algorithm.html#조합",
    "title": "Algorithm",
    "section": "조합",
    "text": "조합\n\nimport itertools\n\ndata = [2,4,5,2]\n\nfor x in itertools.combinations(data,2):\n    print(list(x),end = '')\n\n[2, 4][2, 5][2, 2][4, 5][4, 2][5, 2]"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html",
    "title": "내장함수",
    "section": "",
    "text": "주요 라이브러리 문법"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#sum",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#sum",
    "title": "내장함수",
    "section": "sum",
    "text": "sum\n\nsum([1,2])\n\n3"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#min",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#min",
    "title": "내장함수",
    "section": "min",
    "text": "min\n\nmin([14,5,6,0,3])\n\n0"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#max",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#max",
    "title": "내장함수",
    "section": "max",
    "text": "max\n\nmax(1,3,6,3,33)\n\n33"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#eval",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#eval",
    "title": "내장함수",
    "section": "eval",
    "text": "eval\n수학 수식이 문자열 형식으로 들어오면 해당 수식을 계산한 결과를 반환\n\neval(\"3+4\")\n\n7\n\n\n\neval(\"3*4\")\n\n12"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#sorted",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#sorted",
    "title": "내장함수",
    "section": "sorted",
    "text": "sorted\n\nsorted([1,2,3,2,3])\n\n[1, 2, 2, 3, 3]\n\n\n\nsorted([1,2,3,2,3],reverse=True)\n\n[3, 3, 2, 2, 1]\n\n\n\\(\\star\\) List 형식이어야 함\n\nsorted(1,2,3,2,3)\n\nTypeError: sorted expected 1 argument, got 5\n\n\ngroup 가능\n\nsorted([('a',3),('b',4)],reverse=True)\n\n[('b', 4), ('a', 3)]\n\n\n사실 list 는 iterable 객체라 기본으로 sort()함수 가지고 있음\n\na = [4,1,3,4]\na.sort()\nprint(a)\n\n[1, 3, 4, 4]"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#itertools",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#itertools",
    "title": "내장함수",
    "section": "itertools",
    "text": "itertools\n\npermutations 순열(중복 허용하지 않음)\n\nfrom itertools import permutations\n\ndata = [1,3,5]\n\nresult = list(permutations(data,3))\n\nprint(result)\n\n[(1, 3, 5), (1, 5, 3), (3, 1, 5), (3, 5, 1), (5, 1, 3), (5, 3, 1)]\n\n\n\n\ncombinations 조합(중복 허용하지 않음)\n\nfrom itertools import combinations\n\ndata = ['a','r','t','e']\nresult = list(combinations(data,2))\n\nprint(result)\n\n[('a', 'r'), ('a', 't'), ('a', 'e'), ('r', 't'), ('r', 'e'), ('t', 'e')]\n\n\n\n\nproduct 순열(중복 허용)\n\nfrom itertools import product\n\ndata = [1,3,5]\n\nresult = list(product(data,repeat=3))\n\nprint(result)\n\n[(1, 1, 1), (1, 1, 3), (1, 1, 5), (1, 3, 1), (1, 3, 3), (1, 3, 5), (1, 5, 1), (1, 5, 3), (1, 5, 5), (3, 1, 1), (3, 1, 3), (3, 1, 5), (3, 3, 1), (3, 3, 3), (3, 3, 5), (3, 5, 1), (3, 5, 3), (3, 5, 5), (5, 1, 1), (5, 1, 3), (5, 1, 5), (5, 3, 1), (5, 3, 3), (5, 3, 5), (5, 5, 1), (5, 5, 3), (5, 5, 5)]\n\n\n\n\ncombinations_with_replacement(중복 허용)\n\nfrom itertools import combinations_with_replacement\n\ndata = ['a','r','t','e']\nresult = list(combinations_with_replacement(data,2))\n\nprint(result)\n\n[('a', 'a'), ('a', 'r'), ('a', 't'), ('a', 'e'), ('r', 'r'), ('r', 't'), ('r', 'e'), ('t', 't'), ('t', 'e'), ('e', 'e')]"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#heapq",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#heapq",
    "title": "내장함수",
    "section": "heapq",
    "text": "heapq\n다익스트라 최단 경로 알고리즘을 포함해 다양한 알고리즘에서 우선순위 큐 기능을 구현하고자 할 때 사용\n\nimport heapq\n\ndef heapsort(iterable):\n    h = []\n    result = []\n    for value in iterable:\n        heapq.heappush(h,value)\n    for _ in range(len(h)):\n        result.append(heapq.heappop(h))\n    return result\n    \nresult = heapsort([1,4,5,6,2,2,77,3,25])\nprint(result)\n\n[1, 2, 2, 3, 4, 5, 6, 25, 77]\n\n\n\na = []\nheapq.heappush(a,5)\n\n\na\n\n[5]\n\n\n\nheapq.heappush(a,3)\na\n\n[3, 5]\n\n\n\nheapq.heappush(a,2)\na\n\n[2, 5, 3]\n\n\n\nheapq.heappop(a)\n\n2\n\n\n\nmax heap\n\nimport heapq\n\ndef heapsort(iterable):\n    h = []\n    result = []\n    for value in iterable:\n        heapq.heappush(h,-value)\n    for _ in range(len(h)):\n        result.append(-heapq.heappop(h))\n    return result\n    \nresult = heapsort([1,4,5,6,2,2,77,3,25])\nprint(result)\n\n[77, 25, 6, 5, 4, 3, 2, 2, 1]\n\n\n\na = []\nheapq.heappush(a,-5)\n\n\na\n\n[-5]\n\n\n\nheapq.heappush(a,-3)\na\n\n[-5, -3]\n\n\n\nheapq.heappush(a,-2)\na\n\n[-5, -3, -2]\n\n\n\n-heapq.heappop(a)\n\n5"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#bisect",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#bisect",
    "title": "내장함수",
    "section": "bisect",
    "text": "bisect\n이진 탐색 구현\n\n정렬된 상태여야 함\n\n\nbisect_left(a,x)\n정렬된 순서를 유지하면서 리스트 a에 데이터 x를 삽입할 가장 왼쪽 인덱스를 찾는 메서드\n\n\nbisect_right(a,x)\n정렬된 순서를 유지하면서 리스트 a에 데이터 x를 삽입할 가장 오른쪽 인덱스를 찾는 메서드\n\nfrom bisect import bisect_left, bisect_right\n\na = [1,2,5,6,33,3]\nx = 3\nb = sorted(a)\nprint(b)\nprint(bisect_left(b,x))\nprint(bisect_right(b,x))\n\n[1, 2, 3, 5, 6, 33]\n2\n3\n\n\n\nfrom bisect import bisect_left, bisect_right\n\ndef count_by_range(a,left_value,right_value):\n    right_index = bisect_right(a,right_value)\n    left_index = bisect_left(a,left_value)\n    return right_index - left_index\n\na = [1,4,5,7,5,3,6,7,9,99,2,22]\nb = sorted(a)\n\nprint(count_by_range(b,5,5))\n\nprint(count_by_range(b,-1,3))\n\n2\n3"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#collections",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#collections",
    "title": "내장함수",
    "section": "collections",
    "text": "collections\n\ndeque\n\nfrom collections import deque\n\ndata = deque([2,5,4,6,3])\ndata.appendleft(3)\ndata.append(1)\n\nprint(data)\n\nprint(list(data))\n\ndeque([3, 2, 5, 4, 6, 3, 1])\n[3, 2, 5, 4, 6, 3, 1]\n\n\n\ndata.pop()\n\n1\n\n\n\ndata\n\ndeque([3, 2, 5, 4, 6, 3])\n\n\n\ndata.popleft()\n\n3\n\n\n\ndata\n\ndeque([2, 5, 4, 6, 3])\n\n\n\n\nCounter\n등장 횟수 세는 기능\n\nfrom collections import Counter\n\ncounter = Counter(['d','d','d','a','e','q','d'])\n\nprint(counter['d'])\nprint(counter['a'])\nprint(dict(counter),\"\\n사전자료형으로 반환\")\n\n4\n1\n{'d': 4, 'a': 1, 'e': 1, 'q': 1} \n사전자료형으로 반환\n\n\n\ncounter\n\nCounter({'d': 4, 'a': 1, 'e': 1, 'q': 1})"
  },
  {
    "objectID": "posts/ct/2023-01-15-Coding_Test_interfunction.html#math",
    "href": "posts/ct/2023-01-15-Coding_Test_interfunction.html#math",
    "title": "내장함수",
    "section": "math",
    "text": "math\n\nimport math\nprint(math.factorial(4))\n\n24\n\n\n\n4*3*2*1\n\n24\n\n\n\nimport math\nprint(math.sqrt(25))\n\n5.0\n\n\n최대 공약수\n\nimport math\nprint(math.gcd(30,25))\n\n5\n\n\n\nimport math\nprint(math.pi)\nprint(math.e)\n\n3.141592653589793\n2.718281828459045"
  },
  {
    "objectID": "posts/ct/Untitled.html",
    "href": "posts/ct/Untitled.html",
    "title": "Map",
    "section": "",
    "text": "Map\n\n\nMap"
  },
  {
    "objectID": "posts/ct/2023-01-23-Coding_Test_Q2.html",
    "href": "posts/ct/2023-01-23-Coding_Test_Q2.html",
    "title": "두 큐 합 같게 만들기(Done)",
    "section": "",
    "text": "2022:KAKAO TECH INTERNSHIP\n\n문제 설명\n길이가 같은 두 개의 큐가 주어집니다.\n하나의 큐를 골라 원소를 추출(pop)하고, 추출된 원소를 다른 큐에 집어넣는(insert) 작업을 통해 각 큐의 원소 합이 같도록 만들려고 합니다.\n이때 필요한 작업의 최소 횟수를 구하고자 합니다.\n한 번의 pop과 한 번의 insert를 합쳐서 작업을 1회 수행한 것으로 간주합니다.\n큐는 먼저 집어넣은 원소가 먼저 나오는 구조입니다.\n이 문제에서는 큐를 배열로 표현하며, 원소가 배열 앞쪽에 있을수록 먼저 집어넣은 원소임을 의미합니다.\n즉, pop을 하면 배열의 첫 번째 원소가 추출되며, insert를 하면 배열의 끝에 원소가 추가됩니다.\n예를 들어 큐 [1, 2, 3, 4]가 주어졌을 때, pop을 하면 맨 앞에 있는 원소 1이 추출되어 [2, 3, 4]가 되며, 이어서 5를 insert하면 [2, 3, 4, 5]가 됩니다.\n다음은 두 큐를 나타내는 예시입니다.\nqueue1 = [3, 2, 7, 2]\nqueue2 = [4, 6, 5, 1]\n두 큐에 담긴 모든 원소의 합은 30입니다.\n따라서, 각 큐의 합을 15로 만들어야 합니다.\n예를 들어, 다음과 같이 2가지 방법이 있습니다.\n\nqueue2의 4, 6, 5를 순서대로 추출하여 queue1에 추가한 뒤, queue1의 3, 2, 7, 2를 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [4, 6, 5], queue2는 [1, 3, 2, 7, 2]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 7번 수행합니다.\nqueue1에서 3을 추출하여 queue2에 추가합니다. 그리고 queue2에서 4를 추출하여 queue1에 추가합니다. 그 결과 queue1은 [2, 7, 2, 4], queue2는 [6, 5, 1, 3]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 2번만 수행하며, 이보다 적은 횟수로 목표를 달성할 수 없습니다.\n따라서 각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수는 2입니다.\n\n길이가 같은 두 개의 큐를 나타내는 정수 배열 queue1, queue2가 매개변수로 주어집니다.\n각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수를 return 하도록 solution 함수를 완성해주세요. 단, 어떤 방법으로도 각 큐의 원소 합을 같게 만들 수 없는 경우, -1을 return 해주세요.\n제한사항\n\n1 ≤ queue1의 길이 = queue2의 길이 ≤ 300,000\n1 ≤ queue1의 원소, queue2의 원소 ≤ 109\n주의: 언어에 따라 합 계산 과정 중 산술 오버플로우 발생 가능성이 있으므로 long type 고려가 필요합니다.\n\n입출력 예\n\n\n\nqueue1\nqueue2\nresult\n\n\n\n\n[3, 2, 7, 2]\n[4, 6, 5, 1]\n2\n\n\n[1, 2, 1, 2]\n[1, 10, 1, 2]\n7\n\n\n[1, 1]\n[1, 5]\n1\n\n\n\n입출력 예 설명\n입출력 예 #1\n문제 예시와 같습니다.\n입출력 예 #2\n두 큐에 담긴 모든 원소의 합은 20입니다. 따라서, 각 큐의 합을 10으로 만들어야 합니다. queue2에서 1, 10을 순서대로 추출하여 queue1에 추가하고, queue1에서 1, 2, 1, 2와 1(queue2으로부터 받은 원소)을 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [10], queue2는 [1, 2, 1, 2, 1, 2, 1]가 되며, 각 큐의 원소 합은 10으로 같습니다. 이때 작업 횟수는 7회이며, 이보다 적은 횟수로 목표를 달성하는 방법은 없습니다. 따라서 7를 return 합니다.\n입출력 예 #3\n어떤 방법을 쓰더라도 각 큐의 원소 합을 같게 만들 수 없습니다. 따라서 -1을 return 합니다.\nanswer\n\nfrom collections import deque\n\ndef solution(queue1, queue2):\n    answer = -1\n    q1, q2 = deque(queue1),deque(queue2)\n    s2 = sum(q1 + q2)/2\n    s1 = sum(q1)\n    count = 0\n    if max(q1) > s2 or max(q2)>s2:\n        return -1\n        \n    while q1 and q2:\n        if s1 > s2:\n            s1 -= q1.popleft()\n            count += 1\n        elif s1 < s2:\n            add = q2.popleft()\n            q1.append(add)\n            s1 += add\n            count += 1\n        elif s1 == s2:\n            return count\n    return answer\n\n\nqueue1 = [3, 2, 7, 2]\n\n\nqueue2 = [4, 6, 5, 1]\n\n\nq1, q2 = deque(queue1),deque(queue2)\n\n\ns2 = sum(q1 + q2)/2\ns2\n\n15.0\n\n\n\ns1 = sum(q1)\ns1\n\n14\n\n\n\ncount = 0\n\n\n# s1>s2\n\n\ns1 -= q1.popleft()\ns1\n\n11\n\n\n\ncount+=1\ncount\n\n1\n\n\n\n# s1<s2\n\n\nadd = q2.popleft()\nadd\n\n4\n\n\n\nq1.append(add)\n\n\ns1 += add\ns1\n\n15\n\n\n\ncount += 1\ncount\n\n2\n\n\n\ns1==s2\n\nTrue\n\n\n\ncount\n\n2"
  },
  {
    "objectID": "posts/ct/2023-02-12-Coding_Test.html",
    "href": "posts/ct/2023-02-12-Coding_Test.html",
    "title": "ArrayList & LinkedList",
    "section": "",
    "text": "ArrayList & LinkedList"
  },
  {
    "objectID": "posts/ct/2023-02-12-Coding_Test.html#max-min",
    "href": "posts/ct/2023-02-12-Coding_Test.html#max-min",
    "title": "ArrayList & LinkedList",
    "section": "max & min",
    "text": "max & min\n\nn = input(list())\n\n[] \n\n\n\narray_list = list(map(int,input().split()))\n\n 20 10 35 30 7\n\n\n\nmax_num = array_list[0]\nmin_num = array_list[0]\n\n\nfor num in array_list:\n    if num > max_num:\n        max_num = num\n    if num < min_num:\n        min_num = num\n\n\nprint(min_num,max_num)\n\n7 35"
  },
  {
    "objectID": "posts/ct/2023-02-12-Coding_Test.html#dimension-arraylist",
    "href": "posts/ct/2023-02-12-Coding_Test.html#dimension-arraylist",
    "title": "ArrayList & LinkedList",
    "section": "2 dimension arraylist",
    "text": "2 dimension arraylist\n\nhuman = [list(map(int,input().split())) for _ in range(5)]\n\n 5 4 4 5\n 5 4 4 4 \n 5 5 4 4 \n 5 5 5 4\n 4 4 4 5\n\n\n\nhumanscore = [0]*5\n\n\nscore = 0\n\n\nfor i in range(5):\n    sum = 0\n    for j in range(4):\n        sum += human[i][j]\n    humanscore[i] = sum\n    score = max(score,sum)\n\n\nhumanscore\n\n[18, 17, 18, 19, 17]\n\n\n\nscore\n\n19\n\n\n\nfor i in range(5):\n    if humanscore[i] == score:\n        print(i+1,score)\n        break\n\n4 19\n\n\n\\(\\star\\) 0 부터 시작하니까 1 더해주기"
  },
  {
    "objectID": "posts/ct/2023-02-12-Coding_Test.html#add-delete",
    "href": "posts/ct/2023-02-12-Coding_Test.html#add-delete",
    "title": "ArrayList & LinkedList",
    "section": "add & delete",
    "text": "add & delete\n삽입과 삭제가 많은 문제를 접랬을때는 ArrayList 사용하게 되면 시간복잡도가 높아진다.\n따라서 스택을 이용하는 게 적절하다. 다른 장에 추가 서술 예정"
  },
  {
    "objectID": "posts/ct/2023-03-22-Coding_Test_Tree.html",
    "href": "posts/ct/2023-03-22-Coding_Test_Tree.html",
    "title": "Tree",
    "section": "",
    "text": "Tree\n\n\nTree\n전위 순회(preorder traversal)\n\n노드를 방문한다.\n왼쪽 서브 트리를 전위 순회한다.\n오른쪽 서브 트리를 전위 순회한다.\n\n(쉽게 생각하면 중간->왼쪽->오른쪽 순)\n\ndef pre_order(data, left_node, right_node):\n    print(data,end='')\n    if left_node != '.' :\n        pre_order(tree[left_node])\n    if right_node != '.' :\n        pre_order(tree[right_node])\n\n\npre_order()\n\n중위 순회(inorder traversal)\n\n왼쪽 서브 트리를 중위 순회한다.\n노드를 방문한다.\n오른쪽 서브 트리를 중위 순회한다.\n\n(쉽게 생각하면 왼쪽->중간->오른쪽 순)\n\ndef in_order(data, left_node, right_node):\n    if left_node != '.' :\n        pre_order(tree[left_node])\n    print(data,end='')\n    if right_node != '.' :\n        pre_order(tree[right_node])\n\n휘위 순회(postorder traversal)\n\n왼쪽 서브 트리를 후위 순회한다.\n오른쪽 서브 트리를 후위 순회한다.\n노드를 방문한다.\n\n(쉽게 생각하면 왼쪽->오른쪽->중간 순)\n\ndef post_order(data, left_node, right_node):\n    if left_node != '.' :\n        pre_order(tree[left_node])\n    if right_node != '.' :\n        pre_order(tree[right_node])\n    print(data,end='')\n\n\n\n이진 검색 트리\n이진 검색 트리는 다음과 같은 세 가지 조건을 만족하는 이진 트리이다.\n노드의 왼쪽 서브트리에 있는 모든 노드의 키는 노드의 키보다 작다.\n노드의 오른쪽 서브트리에 있는 모든 노드의 키는 노드의 키보다 크다.\n왼쪽, 오른쪽 서브트리도 이진 검색 트리이다.\n\n\n\nimage.png\n\n\n전위 순회 (루트-왼쪽-오른쪽)은 루트를 방문하고, 왼쪽 서브트리, 오른쪽 서브 트리를 순서대로 방문하면서 노드의 키를 출력한다. 후위 순회 (왼쪽-오른쪽-루트)는 왼쪽 서브트리, 오른쪽 서브트리, 루트 노드 순서대로 키를 출력한다. 예를 들어, 위의 이진 검색 트리의 전위 순회 결과는 50 30 24 5 28 45 98 52 60 이고, 후위 순회 결과는 5 28 24 45 30 60 52 98 50 이다.\n이진 검색 트리를 전위 순회한 결과가 주어졌을 때, 이 트리를 후위 순회한 결과를 구하는 프로그램을 작성하시오.\n\nglobal something\n위 global 에 대한 교수님 수업 자료임!!\n\\(\\star\\)\n(원칙1) global 에서 정의된 이름은 local 에서 정의된 이름이 없을 경우 그를 대신할 수 있다 (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다)\n(원칙2) local과 global에서 같은 이름 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하며 서로 영향을 주지 않는다. (독립적이다)\n만약에 local이 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 새롭게 같은 이름의 변수가 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다.\n\n\nuser_input = input(\"dd: \")\nprint(\"dd{}dd\".format(user_input))\n\ndd:  d\n\n\nddddd\n\n\n\nclass Node:\n    def __init__(self,data):\n        self.data = data\n        self.left = None\n        self.right = None\n\n\nclass Tree:\n    def __init__(self):\n        self.root = None\n        \n    def add(self,data):\n        if(self.root == None):\n            self.root = Node(data)\n            \n        else:\n            current = self.root\n            while(True):\n                if (current.data > data):\n                    if(current.left == None):\n                        current.left = Node(data)\n                        break\n                    current = current.left\n                \n                if (current.data < data):\n                    if(current.right == None):\n                        current.right = Node(data)\n                        break\n                    current = current.right\n    def postorder(self,node=None):\n        global answer\n        if node == None:\n            node = self.root\n        if node.left != None:\n            self.postorder(node.left)\n        if node.right != None:\n            self.postorder(node.right)\n        answer.append(node.data)   \n\n\n# import sys\n# sys.setrecursionlimit(200000)\n\n# input = sys.stdin.readline()\ninput = input()\n\ntree = Tree()\n\nwhile True:\n    try:\n        tree.add(int(input()))\n    except:\n        break\n\nanswer = []\n\ntree.postorder()\nprint('\\n'.join(map(str,answer)))\n\n예제 입력 값\n50\n30\n24\n5\n28\n45\n98\n52\n60\n\ntree = Tree()\n\n\nwhile True:\n    try:\n        tree.add(int(input()))\n    except:\n        break"
  },
  {
    "objectID": "posts/ts/index.html",
    "href": "posts/ts/index.html",
    "title": "Theoritical Statistics",
    "section": "",
    "text": "Those are posts of Theoritical Statistics.\n[참고(https://seoyeonc.github.io/chch/theoritical%20statistics/2022/04/17/ts-3%EC%9E%A5%EA%B3%BC%EC%A0%9C.html)"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html",
    "href": "posts/ts/2023-01-09-ts_HW4.html",
    "title": "Theoritical Statistics HW4",
    "section": "",
    "text": "랜덤표본"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section",
    "title": "Theoritical Statistics HW4",
    "section": "1.",
    "text": "1.\n\\(X_1 \\sim \\chi^2_m\\), \\(X_2 \\sim \\chi^2_n\\) 이고 서로 독립이면 \\(X_1 + X_2 \\sim \\chi^2_{m+n}\\)\nanswer\n\\(Y = X_1 + X_2\\)\n\\(M_Y(t) = M_{X_1 + X_2}(t) = M_{X_1}(t) \\times M_{X_2}(t) = (1-2t)^{-\\frac{m+n}{2}}\\)\n적률생성함수의 유일성에 의하여 \\(X_1 + X_2 \\sim \\chi^2_{m+n}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-1",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-1",
    "title": "Theoritical Statistics HW4",
    "section": "2.",
    "text": "2.\n서로 독립인 \\(X_1\\)과 \\(X_2\\)에 대하여 \\(Y = X_1 + X_2\\) 라고 할 떄 \\(Y \\sim \\chi^2_m\\), \\(X_1 \\sim \\chi^2_n\\)이면 \\(X_2 \\sim \\chi^2_{m-n}\\)\nanswer\n\\(X_2 = Y - X_1\\)\n\\(M_Y (t) = (1-2t)^{\\frac{m}{2}}\\)\n\\(M_{X_1}(t) = (1-2t)^{-\\frac{n}{2}}\\)\n\\(M_{X_2}(t) = M_{Y-X_1}(t) = E(e^{(y-X_1)t}) = E(e^{Yt} \\times e^{-X_1t}) = \\frac{M_Y(t)}{M_{X_1}(t)} = (1-2t)^{-\\frac{m-n}{2}}\\)\n\\(\\therefore X_2 \\sim \\chi^2_{m-n}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-3",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-3",
    "title": "Theoritical Statistics HW4",
    "section": "(1)",
    "text": "(1)\n\\(P(\\bar{X}_5 < c) = 0.90\\)을 만족하는 상수 \\(c\\)값을 구하라.\nanswer\n\\(\\bar{X}_5 \\sim N(0,5)\\)\n\\(P(\\bar{X}_t <c) = 0.90\\)\n\\(c = 2.8656\\)\n\nqnorm(0.9,0,1)*sqrt(5)\n\n2.865636417229"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-4",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-4",
    "title": "Theoritical Statistics HW4",
    "section": "(2)",
    "text": "(2)\n\\(P(\\frac{1}{5}\\sum^5_{i=1} X^2_i <c) = 0.90\\)을 만족하는 상수 \\(c\\)값을 구하라.\nanswer\n\\(P(\\frac{1}{5} \\sum^5_{i=1} X^2_i <c) = 0.90\\)\n\\(\\star\\)\n\\(\\sum^5_{i=1} X^2_i \\sim \\chi^2_5\\)\n\\(\\frac{1}{5} \\sum^5_{i=1} X^2_i \\sim \\chi^2_1\\)\n\\(\\star\\)\n\\(E(\\frac{1}{5}\\sum^5_{i=1} X^2_i ) =\\frac{1}{5} \\times 5 = 1\\)\n\\(var(\\frac{1}{5}\\sum^5_{i=1} X^2_i) = 2\\)\n\\(\\therefore c = 2.7055\\)\n\nqchisq(0.9,df=1)\n\n2.70554345409542"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-9",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-9",
    "title": "Theoritical Statistics HW4",
    "section": "(1)",
    "text": "(1)\n\\(\\bar{X}_{16} - \\bar{Y}_{25}\\)의 분포를 구하라.\nanswer\n\\(E(\\bar{X}_{16} - \\bar{Y}_{25}) = E(0-2) = -2\\)\n\\(Var(\\bar{X}_{16} - \\bar{Y}_{25}) = \\frac{9}{16} + \\frac{26}{25} = \\frac{481}{400}\\)\n\\(M_{\\bar{X}_{16} - \\bar{Y}_{25}}(t) = exp(t(0-2) + \\frac{t^2}{2}(\\frac{9}{16} + \\frac{16}{25}))\\)\n\\(\\bar{X}_{16} - \\bar{Y}_{25} \\sim N(-2,\\frac{481}{400})\\)"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-10",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-10",
    "title": "Theoritical Statistics HW4",
    "section": "(2)",
    "text": "(2)\n\\(P(\\bar{X}_{16} - \\bar{Y}_{25} >0)\\)를 계산하라.\nanswer\n\\(P(\\bar{X}_{16} - \\bar{Y}_{25} > 0) = 1-P(\\bar{X}_{16} - \\bar{Y}_{25} <0) = 0.0341\\)\n\n1-pnorm(0,-2,sqrt(481/400))\n\n0.0340879047226217"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-12",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-12",
    "title": "Theoritical Statistics HW4",
    "section": "(1)",
    "text": "(1)\n\\(P(X_1 - X_2 <2)\\)을 계산하라.\nanswer\n\\(P(X_1 - X_2 < 2)\\), \\(X_1 - X_2 \\sim N(0,2)\\)\n\\(= P(\\frac{(X_1 - X_2) - 0}{\\sqrt{2}} <\\sqrt{2}) = P(z<\\sqrt{2}) = 0.9214\\)\n\npnorm(sqrt(2),0,1)\n\n0.921350396474857"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-13",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-13",
    "title": "Theoritical Statistics HW4",
    "section": "(2)",
    "text": "(2)\n\\(P(X_1 + X_2 <2)\\)을 계산하라.\nanswer\n\\(P(X_1 + X_2 < 2)\\), \\(X_1 + X_2 \\sim N(0,2)\\)\n\\(= P(\\frac{(X_1 + X_2) - 0}{\\sqrt{2}} <\\sqrt{2}) = P(z<\\sqrt{2}) = 0.9214\\)\n\npnorm(sqrt(2),0,1)\n\n0.921350396474857"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-14",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-14",
    "title": "Theoritical Statistics HW4",
    "section": "(3)",
    "text": "(3)\n\\(P(X^2_1 + \\dots + X^2_{50} <60)\\)을 계산하라.\nanswer\n\\(P(X^2_1 + \\dots + X^2_{50}<60 ) = 0.8428\\)\n\\(\\star X^2_1 + \\dots X^2_{50} \\sim \\chi^2_{50}\\)\n\npchisq(60,df=50)\n\n0.842757972761609"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-15",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-15",
    "title": "Theoritical Statistics HW4",
    "section": "(4)",
    "text": "(4)\n\\(P(40<X^2_1 + \\dots + X^2_{50} <60)\\)을 계산하라.\nanswer\n\\(P(40<X^2_1 + X^2_{50} <60) = P(X^2_1 + \\dots + X^2_{50}<60) - P(X^2_1 + \\dots X^2_{50} <40) = 0.6860\\)\n\npchisq(60,df=50) - pchisq(40,df=50)\n\n0.685985350935371"
  },
  {
    "objectID": "posts/ts/2023-01-09-ts_HW4.html#section-16",
    "href": "posts/ts/2023-01-09-ts_HW4.html#section-16",
    "title": "Theoritical Statistics HW4",
    "section": "(5)",
    "text": "(5)\n\\(P(X^2_1 + \\dots + X^2_{50} + c) = 0.9\\) 를 만족하는 상수 \\(c\\)를 구하라.\nanswer\n\\(P(X^2_1 + \\dots + X^2_{50} < 50+ c) = 0.9\\), \\(50+c = 63.1671\\), \\(c=13.1671\\)\n\nqchisq(0.9,df=50)\n\n63.1671210057263"
  },
  {
    "objectID": "posts/ts/2022-12-31-ts_1.html",
    "href": "posts/ts/2022-12-31-ts_1.html",
    "title": "확률변수와 확률분포",
    "section": "",
    "text": "확률변수와 확률분포"
  },
  {
    "objectID": "posts/ts/2022-12-31-ts_1.html#확률",
    "href": "posts/ts/2022-12-31-ts_1.html#확률",
    "title": "확률변수와 확률분포",
    "section": "확률",
    "text": "확률\n\n표본공간; 모든 관찰 가능한 조합\n사건; 표본 공간의 부분 집합\n확률의 공리적 정의\n\n\\(P(S) = 1\\), 사건이 일어날 확률은 1\n\\(0<P(A)<1\\), 표본공간 안에서 사건 A의 확률은 0과 1 사이\n\\(A_1,A_2, \\dots\\)사건들 중 임의의 두 사건을 뽑았을 때, 공집합 이어야 함 \\(\\to\\) 상호배반\n\n\\(P(A_i \\cap A_j) = \\varnothing\\)\n\\(P(\\sum A) = \\sum P(A)\\)\n\n\n\nExample\n\n동전 3회 던지는 실험\n\n\\(S\\{ (H,H), (H,T), (T,H), (T,T) \\}\\) \\(\\to\\) 배반사건\n\n\\(P((H,H)) = \\frac{1}{9}\\)\n\\(P((H,T)) = \\frac{2}{9}\\)\n\\(P((T,H)) = \\frac{2}{9}\\)\n\\(P((T,T)) = \\frac{4}{9}\\)\n\n\\(P(S) = 1\\)\n\\(P(\\sum A_i) = \\sum P(A_i)\\)\n\n어떤 기계의 수명시간 측정"
  },
  {
    "objectID": "posts/ts/2022-12-31-ts_1.html#확률변수",
    "href": "posts/ts/2022-12-31-ts_1.html#확률변수",
    "title": "확률변수와 확률분포",
    "section": "확률변수",
    "text": "확률변수\n확률변수 : 표본공간 S에 정의된 실수값을 가지는 합수(real-valued function), X 영문 대문자\n\n사건을 수치화, 실험 결과에 따라 변하는 변수\n사건의 가능성을 확률적으로 결정\n확률밀도함수 = 확률질량함수\n\\(P(X = x)\\)라는 사건을 가징 확률 \\(P(X=x)\\)는 \\(f(x)\\)에 속해 있다.\n\n이산형\n\n\\(f(1) = P(X = 1)\\)\n\n연속형\n\n\\(f(1)\\) 이렇게 나타낼 수는 없지만, 넓이로 표현; 사건을 구간으로 표현\n\n\\(\\star\\) 확률변수는 대문자로, 관측값은 소문자로 표현"
  },
  {
    "objectID": "posts/ts/2022-12-31-ts_1.html#확률밀도함수-및-확률분포함수",
    "href": "posts/ts/2022-12-31-ts_1.html#확률밀도함수-및-확률분포함수",
    "title": "확률변수와 확률분포",
    "section": "확률밀도함수 및 확률분포함수",
    "text": "확률밀도함수 및 확률분포함수\n확률밀도함수(probablity density function) - \\(f(x)\\)\n확률분포함수(probablity distribution function) - \\(F(x)\\)\n확률변수의 분포형태를 나타내는 데 사용\n이산형 확률변수의 확률밀도함수\n다음 조건 만족\n\n모든 실수 \\(x\\)애 대하여 \\(f(x) \\ge 0\\)\n확률변수가 가질 수 있는 값 \\(x_1, x_2, \\dots\\)에 대하여\n\n\\(f(x_i) > 0, \\sum_{all x_i} f(x_i) = 1\\)\n\n확률질량함수(probablity mass function)으로 부르기도 함\n\\(f(x) = P(X = x)\\)\n\n연속형 확률변수의 확률밀도함수\n\n셀 수 없이 무한히 많은 가능한 값 하나하나에 확률을 부여하지 않고 구간에 확률 부여, 즉 \\(P(X=x)=0\\)\n\n다음 조건 만족\n\n모든 실수 \\(x\\)에 대하여 \\(f(x) \\ge 0\\)\n\\(\\int_{-\\inf}^\\inf f(x) dx = 1\\)\n\n\\(P(a \\le X \\le b) = \\int_a^b f(x) dx\\)\n\\[P(X \\in A) = \\begin{cases} \\sum_{x_i \\in A} f(x_i) & \\text{discrete  } X \\\\ \\int_A f(x) dx & \\text{continuous  } X \\end{cases}\\]\nExample\n구간 (0,3)에서 정의된 함수\n\n\\(f(x) = \\frac{x^2}{9}\\)\n\n\\(\\frac{x^2}{9} \\ge 0\\)\n\\(\\int_0^3 \\frac{x^2}{9} dx = \\frac{1}{9} \\begin{bmatrix} \\frac{x^2}{3} \\end{bmatrix}^3_0 = 1\\)\n\n\n\\(\\therefore\\) 확률밀도함수가 맞다.\n(누적)분포함수(cumulative distribution function): 확률변수 \\(X\\)가 주어진 점 \\(x\\) 이하인 값을 가질 확률 \\[F(x) = P(X \\le x)\\]\n참고\n\n\\(X \\sim f(x)\\) : 확률변수 \\(X\\)가 확률밀도함수 \\(f(x)\\)를 가짐\n\\(X \\sim F(x)\\) : 확률변수 \\(X\\)가 확률분포함수 \\(F(x)\\)를 가짐\n\n\\[F(x) = \\begin{cases} \\sum_{x_i \\le x} f(x_i) & \\text{discrete  } X \\\\ \\int_{-\\inf}^x f(t)dt & \\text{continuous  } X \\end{cases}\\]\n확률뷴포함수의 성질\n함수 \\(F(x)\\)가 어떤 확률변수 \\(X\\)의 누적분포함수가 되는 필요충분조건\n\n\\(lim_{x \\to -\\inf} F(x) = 0\\)\n\\(lim_{x \\to \\inf} F(x) = 1\\)\n\\(lim_{h \\to 0+} F(x+h) = F(x)\\)\n\\(a < b\\)이면 \\(F(a) \\le F(b)\\)\n\n\\[P(a < X \\le b) = F(b) - F(a)\\]\n\\(\\star\\) 연속형이라면? 각 점에서 확률이 0이니까\n\n\\(P(a \\le X \\le b) = P(a \\le X < b) = P(a< X \\le b) = P(a< X < b)\\)\n\n\\(\\star\\) 이산형이라면?\n\nprobability density function 미분하면 step function 을 가지는 probability distribution function\n\nExample\n앞면 나올 확률이 \\(\\frac{1}{2}\\)인 동전 3회 던지는 실험에서 관심있는 변수 \\(X\\) = 앞면의 수일때 \\(f(x)\\)와 \\(F(x)\\)는?\n\n각각 독립이다.\n\\(f(3) = P(X = 3) = P(A)P(B)P(C) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\\)\n\\(f(2) = P(X = 2) = P(A)P(B)P(C) = \\frac{1}{8} \\times 3\\)\n\\(f(1) = P(X = 1) = P(A)P(B)P(C) = \\frac{1}{8} \\times 3\\)\n\\(f(0) = P(X = 0) = P(A)P(B)P(C) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\\)\n\\(f(3) + f(2) + f(1) + f(0) = 1\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_HW6.html",
    "href": "posts/ts/2023-01-14-ts_HW6.html",
    "title": "Theoritical Statistics HW6",
    "section": "",
    "text": "4장 모수의 추정"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_HW6.html#section-6",
    "href": "posts/ts/2023-01-14-ts_HW6.html#section-6",
    "title": "Theoritical Statistics HW6",
    "section": "(1)",
    "text": "(1)\n\\(T_1(X)\\)와 \\(T_2(X)\\)의 비평향성을 점검하라.\nanswer\n\\(E(T_1(X)) = E(X) = p \\to T_1(X)\\) 비편향성 만족\n\\(E(T_2(X)) = E(\\frac{1}{2}) = \\frac{1}{2} \\to T_2(X)\\) 비편향성 불만족"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_HW6.html#section-7",
    "href": "posts/ts/2023-01-14-ts_HW6.html#section-7",
    "title": "Theoritical Statistics HW6",
    "section": "(2)",
    "text": "(2)\n\\(T_1(X)\\)와 \\(T_2(X)\\)의 평균제곱오차를 비교하라.\nanswer\n\\(MSE(T_1(X)) = MSE(X)\\)\n\\(= E(X-P)^2 = E(X^2 - 2PX + P^2)\\)\n\\(= P(1-P) + P^2 -2P^2 + P^2 = P-P^2+P^2-2P^2+P^2 = P-P^2 = P(1-P)\\)\n\\(MSE(T_2(X)) = MSE(\\frac{1}{2} - E(\\frac{1}{2} - P)^2) = (\\frac{1}{2}-P)^2\\)\n\\(\\star\\)\n\\(P-P^2 = \\frac{1}{4}-2P+P^2\\)\n\\(2P^2 -3P+\\frac{1}{4} = 0\\)\n\\(P = \\frac{3 \\pm \\sqrt{9-1}}{4} = \\frac{3 \\pm \\sqrt{7}}{4}\\)\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_HW6.html#section-9",
    "href": "posts/ts/2023-01-14-ts_HW6.html#section-9",
    "title": "Theoritical Statistics HW6",
    "section": "(1)",
    "text": "(1)\n\\(\\hat{\\lambda}_1\\)과 \\(\\hat{\\lambda}_2\\)의 분산을 구하라.\nanswer\n\\(E(X_i) = \\lambda, Var(X_i) = \\lambda^2\\)\n\\(E(\\hat{\\lambda}_1) = E(\\bar{X}) = \\lambda \\to\\), 비편향추정량이다.\n\\(Var(\\hat{\\lambda}_1) = E(\\bar{X}) = \\frac{\\lambda^2}{n}\\)\n\\(E(\\hat{\\lambda_2}) = E(\\frac{n\\bar{X}_n}{n+1}) = \\frac{n}{n+1}E(\\bar{X}_n) = \\frac{n}{n+1}\\lambda \\to\\)비편향 추정량이 아니다., 즉, 분산에 bias존재\n\\(Var(\\hat{\\lambda_2}) = Var(\\frac{n}{n+1}\\bar{X}_n) = E(\\frac{n}{n+1}\\bar{X} - \\lambda)^2\\)\n\\(= E(\\frac{n}{n+1} \\bar{X} - \\frac{n}{n+1} \\lambda + \\frac{n}{n+1}\\lambda -\\lambda)^2\\)\n\\(= E(\\frac{n}{n+1}(\\bar{X}-\\lambda) - \\frac{1}{n+1}\\lambda)^2\\)\n\\(= (\\frac{n}{n+1})^2E(\\bar{X} - \\lambda)^2 + \\frac{1}{(n+1)^2}\\lambda^2 - \\frac{2n}{(n+1)^2}\\lambda(\\bar{X} - \\lambda)\\)\n\\(= (\\frac{n}{n+1})^2 \\lambda^2 + \\frac{1}{(n+1)^2}\\lambda^2\\)\n\\(= \\frac{n^2+1}{(n+1)^2}\\lambda^2\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_HW6.html#section-10",
    "href": "posts/ts/2023-01-14-ts_HW6.html#section-10",
    "title": "Theoritical Statistics HW6",
    "section": "(2)",
    "text": "(2)\n\\(\\hat{\\lambda}_1\\)과 \\(\\hat{\\lambda}_2\\)의 평균제곱오차를 구하라.\nanswer\n\\(MSE(\\hat{\\lambda}_1) = E(\\hat{\\lambda}_1 - \\lambda)^2 = var(\\hat{\\lambda}_1) = \\frac{\\lambda^2}{n}\\)\n\\(\\star\\) 비편향추정량이라 분산과 일치하는 \\(\\hat{\\lambda}_1\\)\n\\(MSE(\\hat{\\lambda}_2) = E(\\hat{\\lambda}_2 - \\lambda)^2 = var(\\hat{\\lambda}_2) + (bias(\\hat{\\lambda}_2))^2\\)\n\\(\\star\\) 비편향추정량이 아니라 bias까지 고려해줘야 하는 \\(\\hat{\\lambda}_2\\)\n\\(var(\\hat{\\lambda}_2) = \\frac{n^2+1}{(n+1)^2}\\lambda^2\\)\n\\(bias(\\hat{\\lambda}_2) = E(\\hat{\\lambda}_2) - \\lambda = \\frac{n}{n+1}\\lambda - \\lambda = -\\frac{1}{n+1}\\lambda\\)\n\\(\\star var(\\hat{\\lambda}_2) + (bias(\\hat{\\lambda}_2))^2\\)\n\\(= \\frac{n^2+1}{(n+1)^2}\\lambda^2 + \\frac{1}{(n+1)^2}\\lambda^2 = \\frac{n^2+2}{(n+1)^2}\\lambda^2\\)"
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html",
    "href": "posts/ts/2023-03-03-graduation_test.html",
    "title": "Theoritical Statistics GT",
    "section": "",
    "text": "GT"
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#a",
    "href": "posts/ts/2023-03-03-graduation_test.html#a",
    "title": "Theoritical Statistics GT",
    "section": "(a)",
    "text": "(a)\n이 랜덤표본의 결합확률밀도함수를 기술하시오."
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#b",
    "href": "posts/ts/2023-03-03-graduation_test.html#b",
    "title": "Theoritical Statistics GT",
    "section": "(b)",
    "text": "(b)\n\\(p(1-p)\\)에 대한 최대가능도추정량을 구하시오."
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#c",
    "href": "posts/ts/2023-03-03-graduation_test.html#c",
    "title": "Theoritical Statistics GT",
    "section": "(c)",
    "text": "(c)\n\\(X_1(1-X_2)\\)의 기댓값을 구하시오."
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#d",
    "href": "posts/ts/2023-03-03-graduation_test.html#d",
    "title": "Theoritical Statistics GT",
    "section": "(d)",
    "text": "(d)\n\\(E[X_1(1-X_2)|\\sum^n_{i=1} X_i = t]\\)을 구하시오."
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#e",
    "href": "posts/ts/2023-03-03-graduation_test.html#e",
    "title": "Theoritical Statistics GT",
    "section": "(e)",
    "text": "(e)\n\\(p=\\frac{1}{2}\\)일 때의 점근분포를 구체적으로 제시하라."
  },
  {
    "objectID": "posts/ts/2023-03-03-graduation_test.html#f",
    "href": "posts/ts/2023-03-03-graduation_test.html#f",
    "title": "Theoritical Statistics GT",
    "section": "(f)",
    "text": "(f)\n가설 \\(H_0 : p = \\frac{1}{2}\\) vs \\(H_1 : p \\neq \\frac{1}{2}\\) 일때, 일반화 가능도비를 이용하여 기각 영역을 제시하라."
  },
  {
    "objectID": "posts/ts/2023-03-03-ts-final_qanda.html",
    "href": "posts/ts/2023-03-03-ts-final_qanda.html",
    "title": "Theoritical Statistics Final term 6 Explanation",
    "section": "",
    "text": "Final term 질문\n참고"
  },
  {
    "objectID": "posts/ts/2023-03-03-ts-final_qanda.html#alpha-구해보자.-시뮬레이션",
    "href": "posts/ts/2023-03-03-ts-final_qanda.html#alpha-구해보자.-시뮬레이션",
    "title": "Theoritical Statistics Final term 6 Explanation",
    "section": "\\(\\alpha\\) 구해보자. (시뮬레이션)",
    "text": "\\(\\alpha\\) 구해보자. (시뮬레이션)\n\nθ=2 \nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 0.6687906085363371\n 1.5350253919744037\n\n\n\nT(x)\n\n0.7524758613118447\n\n\n\nTs = [rand(Exponential(θ),2) |> T for i in 1:1400000]\nmean(Ts .< 1/2)\n\n0.15347"
  },
  {
    "objectID": "posts/ts/2023-03-03-ts-final_qanda.html#beta를-구해보자.시뮬레이션",
    "href": "posts/ts/2023-03-03-ts-final_qanda.html#beta를-구해보자.시뮬레이션",
    "title": "Theoritical Statistics Final term 6 Explanation",
    "section": "\\(\\beta\\)를 구해보자.(시뮬레이션)",
    "text": "\\(\\beta\\)를 구해보자.(시뮬레이션)\n\nθ=1\nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 3.8233540141024713\n 1.6552057743570938\n\n\n\nTs = [rand(Exponential(θ),2) |> T for i in 1:1400000]\nmean(Ts .> 1/2)\n\n0.5970164285714286"
  },
  {
    "objectID": "posts/ts/2023-03-03-ts-final_qanda.html#alpha를-구해보자.-이론",
    "href": "posts/ts/2023-03-03-ts-final_qanda.html#alpha를-구해보자.-이론",
    "title": "Theoritical Statistics Final term 6 Explanation",
    "section": "\\(\\alpha\\)를 구해보자. (이론)",
    "text": "\\(\\alpha\\)를 구해보자. (이론)\n\\(T(X_1,X_2) = \\frac{0.25 exp(-0.5 X_1 - 0.5 X_2)}{exp(-X_1 - X_2)} = 0.25 exp(0.5 X_1 + 0.5 X_2)\\)\n\\(T(X_1,X_2) < \\frac{1}{2} \\iff exp(0.5 X_1 + 0.5 X_2) < 2 \\iff X_1 + X_2 < 2 ln2\\)\n그런데 \\(X_1 + X_2 \\sim \\chi^2(4)\\) under \\(H_0\\)\n\\(P(X_1 + X_2 < 2 ln 2) = \\int^{2ln2}_{0} \\frac{1}{4 \\Gamma(2)} x e^{-x/2} dx = \\int^{ln2}_{0} t e^{-t} dt = [ t(-e^{-t}) -e^{-t}]^{ln 2}_{0}\\)\n\nt = log(2) \nu = t*(-exp(-t)) - exp(-t)\nt = 0\nl = t*(-exp(-t)) - exp(-t)\n\n-1.0\n\n\n\nu-l\n\n0.1534264097200273"
  },
  {
    "objectID": "posts/ts/2023-03-03-ts-final_qanda.html#beta를-구해보자.-이론",
    "href": "posts/ts/2023-03-03-ts-final_qanda.html#beta를-구해보자.-이론",
    "title": "Theoritical Statistics Final term 6 Explanation",
    "section": "\\(\\beta\\)를 구해보자. (이론)",
    "text": "\\(\\beta\\)를 구해보자. (이론)\n\\(T(X_1,X_2) > \\frac{1}{2} \\iff exp(0.5 X_1 + 0.5 X_2) < 2 \\iff 2(X_1 + X_2) < 4 ln2\\)\n그런데 \\(2(X_1 + X_2) \\sim \\chi^2(4)\\) under \\(H_0\\)\n\\(P(2(X_1 + X_2) < 4 ln 2) = \\int_{4ln2}^{\\infty} \\frac{1}{4 \\Gamma(2)} x e^{-x/2} dx = \\int_{2ln2}^{\\infty} t e^{-t} dt = [ t(-e^{-t}) -e^{-t}]_{2ln 2}^{0}\\)\n\nu = 0\nt = 2*log(2)\nl = t*(-exp(-t)) - exp(-t)\n\n-0.5965735902799727\n\n\n\nu-l\n\n0.5965735902799727"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html",
    "href": "posts/ts/2023-01-14-ts_Mid term.html",
    "title": "Theoritical Statistics Mid term",
    "section": "",
    "text": "중간고사"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#a",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#a",
    "title": "Theoritical Statistics Mid term",
    "section": "(a)",
    "text": "(a)\n\\(T_1(X)\\)과 \\(T_2(X)\\)의 기댓값을 각각 구하시오.\nanswer\n- \\(E(T_1(X))\\)\n\\(= E(\\frac{1}{n}\\sum^n_{i=1}(X_i - \\mu)^2) = \\frac{E((X-\\mu)^2)}{n} = \\frac{n\\sigma^2}{n} = \\sigma^2\\)\n- \\(E(T_2(X))\\)\n\\(= E(\\frac{1}{n-1}\\sum^n_{i=1}(X_i - \\bar{X})^2)\\)\n\\(\\star\\)\n\\(\\sum(X-\\mu)^2 = \\sum(X - \\bar{X} + \\bar{X} - \\mu)^2\\)\n\\(= \\sum((X-\\bar{X})^2 + (\\bar{X} - \\mu)^2 + 2(X-\\bar{X})(\\bar{X}-\\mu))\\)\n\\(= \\sum(X - \\bar{X})^2.+ n(\\bar{X} - \\mu)^2\\)\n\\(\\star\\)\n\\(= E(\\frac{\\sum(X-\\mu)^2 - n(\\bar{X}-\\mu)}{n-1}^2) = \\frac{1}{n-1}[n\\sigma^2 - n\\frac{\\sigma^2}{n}] = \\frac{(n-1)\\sigma^2}{n-1} = \\sigma^2\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#b",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#b",
    "title": "Theoritical Statistics Mid term",
    "section": "(b)",
    "text": "(b)\n\\(T_1(X)\\)과 \\(T_2(X)\\)의 분산을 각각 구하시오.\nanswer\n- \\(Var(T_1(X))\\)\n\\(= Var(\\frac{\\sigma^2}{n} \\frac{1}{\\sigma^2}\\sum^n_{i=1}(X_i - \\mu)^2)\\)\n\\(= \\frac{\\sigma^4}{n^2}2n\\)\n\\(= (\\frac{2\\sigma^4}{n})\\)\n\\(\\star\\)\n\\(V = \\sum(\\frac{X_i - \\mu}{\\sigma})^2 \\sim chi^2(n)\\)\n\\(\\star\\)\n- \\(Var(T_2(X))\\)\n\\(= Var(\\frac{1}{n-1}\\sum^n_{i=1}(X_i - \\bar{X})^2)\\)\n\\(\\star\\)\n\\(S_n^2 = \\sum^n_{i=1}\\frac{(X_i - \\bar{X}_n)^2}{n-1}\\)\n\\(\\frac{(n-1)S_n^2}{\\sigma^2} = \\frac{\\sum^n_{i=1}(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\)\n\\(\\star\\)\n\\(= Var(\\frac{\\sigma^2}{n-1} \\times \\frac{(n-1)S^2}{\\sigma^2})\\)\n\\(\\frac{\\sigma^4}{(n-1)^2} \\times 2(n-1)\\)\n\\(= \\frac{2\\sigma^4}{n-1}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#a-1",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#a-1",
    "title": "Theoritical Statistics Mid term",
    "section": "(a)",
    "text": "(a)\n\\(\\sqrt{n}(\\bar{X}^2_n - \\lambda^2)\\)의 극한 분포를 구하시오.\nanswer\nDelta Method\n\\(\\sqrt{n}(\\bar{X}^2_n - \\mu^2) \\xrightarrow[]{d} N(0,4\\mu^2\\sigma^2)\\) 단, \\(\\mu \\neq 0\\)\n\\(\\mu = \\lambda\\), \\(\\sigma^2 = \\lambda\\)\n\\(\\sqrt{n}(\\bar{X}^2_n - \\lambda^2) \\xrightarrow[]{d} N(0,4\\lambda^4)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#b-1",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#b-1",
    "title": "Theoritical Statistics Mid term",
    "section": "(b)",
    "text": "(b)\n\\(E(\\sum^n_{i=1}a_i X_i) = \\lambda\\)가 되기 위한 \\(a_i\\)의 조건은 무엇인가?\nanswer\n\\(E(\\sum^n_{i=1}a_iX_i) = \\lambda\\)\n\\(= E(a_1X_1 + a_2X_2 + \\dots a_nX_n) = a_1E(X_1) + a_2E(X_2) + \\dots + a_nE(X_n)\\)\n\\(= a_1\\lambda + a_2\\lambda + \\dots + a_n\\lambda = \\sum^n_{i=1}a_i \\lambda\\)\n\\(\\therefore \\sum^n_{i=1} a_i= 1\\)이어야 한다."
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#c",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#c",
    "title": "Theoritical Statistics Mid term",
    "section": "(c)",
    "text": "(c)\n\\(\\sum^n_{i=1}a_iX_i\\)의 분산을 구하시오.\nanswer\n\\(Var(\\sum^n_{i=1}a_iX_i) = Var(a_1X_1 + a_2X_2 + \\dots + a_n X_n)\\)\n\\(= a_1^2\\lambda + a_2^2\\lambda + \\dots + a_n^2\\lambda = \\sum^n_{i=1} a_i^2 \\lambda\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#a-2",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#a-2",
    "title": "Theoritical Statistics Mid term",
    "section": "(a)",
    "text": "(a)\n이 랜덤표본의 결합확률밀도함수를 기술하시오.\nanswer\n\\(f(X_1)f(X_2)f(X_3)f(X_4)\\)\n\\(= (1-p)^{1-X_1}p^{X_1}(1-p)^{1-X_2}p^{X_2}(1-p)^{1-X_3}p^{X_3}(1-p)^{1-X_4}p^{X_4}\\)\n\\(= (1-p)^{4-X_1-X_2-X_3-X_4} p^{X_1+X_2+X_3+X_4}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#b-2",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#b-2",
    "title": "Theoritical Statistics Mid term",
    "section": "(b)",
    "text": "(b)\n\\(X_1(1-X_2)\\)의 기댓값을 구하시오.\nanswer\n\\(E(X_1(1-X_2))\\)\n\\(= E(X_1 - X_1X_2) = E(X_1) - E(X_1)E(X_2)\\)\n\\(= p - p^2 = p(1-p)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#c-1",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#c-1",
    "title": "Theoritical Statistics Mid term",
    "section": "(c)",
    "text": "(c)\n\\(E[X_1(1-X_2)|\\sum^4_{i=1}X_i = t]\\)을 구하시오.\nanswer\n\\(\\frac{p(1-p)}{4p} = \\frac{1-p}{4}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-14-ts_Mid term.html#d",
    "href": "posts/ts/2023-01-14-ts_Mid term.html#d",
    "title": "Theoritical Statistics Mid term",
    "section": "(d)",
    "text": "(d)\n\\(E[(\\frac{d}{dp}log p^X (1-p)^{1-p})^2]\\)을 구하시오.\nanswer\n\\(E[(\\frac{d}{dp} log p^x (1-p)^{1-X})^2]\\)\n\\(= E((\\frac{d}{dp}(X log p - (1-X) log(1-p)))^2)\\)\n\\(= E((\\frac{X}{p} - \\frac{1-X}{1-p})^2)\\)\n\\(= E((\\frac{X-Xp - p + pX}{p(1-p)})^2)\\)\n\\(= E((\\frac{X-p}{p(1-p)})^2)\\)\n\\(= \\frac{1}{p^2(1-p)^2}(E(X^2)-p^2)\\)\n\\(= \\frac{p-p^2+p^2+p^2}{p^2(1-p)^2} = \\frac{p(1-p)}{p^2(1-p)^2} = \\frac{1}{p(1-p)}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-18-ts-HW8.html",
    "href": "posts/ts/2023-01-18-ts-HW8.html",
    "title": "Theoritical Statistics HW8",
    "section": "",
    "text": "9장 최강력 검정법"
  },
  {
    "objectID": "posts/ts/2023-01-18-ts-HW8.html#section-2",
    "href": "posts/ts/2023-01-18-ts-HW8.html#section-2",
    "title": "Theoritical Statistics HW8",
    "section": "(1)",
    "text": "(1)\n기각영역의 형태가 \\(C = \\{(X_1,X_2,\\dots,X_{16}) | \\bar{X}_{16} < a\\}\\)로 주어질때 상수 \\(a\\)의 값을 구하라.\nanswer\n\\(X_1, \\dots, X_{16} \\sim N(\\mu,4)\\)\n\\(\\bar{X}_{16} \\sim N(\\mu,(\\frac{1}{2})^2)\\)\n\\(\\alpha = P(\\text{Reject } H_0 | H_0 True)\\)\n\\(0.05 = P(\\bar{X}_{16}<a | \\mu = 100)\\)\n\\(0.05 = P(\\frac{\\bar{X}_{16} - 100}{1/2} < \\frac{a-100}{1/2} | \\mu = 100)\\)\n\\(0.05 = P(Z < 2(a-100))\\)\n\n\n\nimage.png\n\n\n\\(a = \\frac{Z_{0.05}}{2} + 100\\)\n\\(= \\frac{-1.645}{2} + 100 = -0.8225 + 100 = 99.1775\\)"
  },
  {
    "objectID": "posts/ts/2023-01-18-ts-HW8.html#section-3",
    "href": "posts/ts/2023-01-18-ts-HW8.html#section-3",
    "title": "Theoritical Statistics HW8",
    "section": "(2)",
    "text": "(2)\n대립가설이 \\(H_1: \\mu = 103\\)인 경우 (1)에서 구한 기각영역에 대하여 제 2종오류를 범할 확률을 구하라.\nanswer\n\\(\\beta = P(\\text{Type 2 error}) = P(\\text{Not Reject }H_0 | H_0 \\text{False})\\)\n\\(= P(\\bar{X}_{16} > a | \\mu = 103)\\)\n\\(= P(\\frac{\\bar{X}_{16} - 103} {1/2} > \\frac{1-103}{1/2} | \\mu = 103)\\)\n\\(= P(Z > -7.645) = 1- \\phi (-7.645)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-18-ts-HW8.html#section-4",
    "href": "posts/ts/2023-01-18-ts-HW8.html#section-4",
    "title": "Theoritical Statistics HW8",
    "section": "(3)",
    "text": "(3)\n대립가설이 \\(H_1 : \\mu = 97(\\)일 경우 (1)에서 구한 기각 영역에 대하여 제 2종 오류를 범할 확률을 구하라.\nanswer\n\\(\\beta = P(\\text{Type 2 error}) = P(\\text{Not Reject }H_0 | H_0 \\text{False})\\)\n\\(= P(\\bar{X}_{16} > a | \\mu = 97)\\)\n\\(= P(\\frac{\\bar{X}_{16} - 97} {1/2} > \\frac{1-97}{1/2} | \\mu = 97)\\)\n\\(= P(Z > 4.355) = 1- \\phi (4.355)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW1.html",
    "href": "posts/ts/2023-01-05-ts_HW1.html",
    "title": "Theoritical Statistics HW1",
    "section": "",
    "text": "확률변수와 확률분포"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW1.html#a-x의-주변분포를-구하시오.",
    "href": "posts/ts/2023-01-05-ts_HW1.html#a-x의-주변분포를-구하시오.",
    "title": "Theoritical Statistics HW1",
    "section": "(a) \\(X\\)의 주변분포를 구하시오.",
    "text": "(a) \\(X\\)의 주변분포를 구하시오.\nAnswer\n\\(F_X(X)\\)\n\n\\(P(X=0) = \\frac{1}{9} + \\frac{1}{9} = \\frac{2}{9}\\)\n\\(P(X=1) = \\frac{1}{6} + \\frac{1}{6} =\\frac{2}{6} = \\frac{1}{3}\\)\n\\(P(X=2) = \\frac{2}{9} + \\frac{2}{9} = \\frac{4}{9}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW1.html#b-y의-주변분포를-구하시오.",
    "href": "posts/ts/2023-01-05-ts_HW1.html#b-y의-주변분포를-구하시오.",
    "title": "Theoritical Statistics HW1",
    "section": "(b) \\(Y\\)의 주변분포를 구하시오.",
    "text": "(b) \\(Y\\)의 주변분포를 구하시오.\nAnswer\n\\(F_Y(Y)\\)\n\n\\(P(Y=10) = \\frac{1}{9} + \\frac{1}{6} = \\frac{15}{54} = \\frac{5}{18}\\)\n\\(P(Y=20) = \\frac{1}{9} + \\frac{2}{9} =\\frac{3}{9} = \\frac{1}{3}\\)\n\\(P(Y=30) = \\frac{1}{6} + \\frac{2}{9} = \\frac{21}{54} = \\frac{7}{18}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW1.html#c-y10-일-때-x의-조건부분포를-구하시오",
    "href": "posts/ts/2023-01-05-ts_HW1.html#c-y10-일-때-x의-조건부분포를-구하시오",
    "title": "Theoritical Statistics HW1",
    "section": "(c) \\(Y=10\\) 일 때 \\(X\\)의 조건부분포를 구하시오",
    "text": "(c) \\(Y=10\\) 일 때 \\(X\\)의 조건부분포를 구하시오\nAnswer\n\\(F_X(Y=10|X=0) = \\frac{1/9}{5/18} = \\frac{2}{5}\\)\n\\(F_X(Y=10|X=1) = \\frac{1/6}{5/18} = \\frac{3}{5}\\)\n\\(F_X(Y = 10|X = 2) = \\frac{0}{5/18} = 0\\)"
  },
  {
    "objectID": "posts/ts/2023-01-18-ts_HW7.html",
    "href": "posts/ts/2023-01-18-ts_HW7.html",
    "title": "Theoritical Statistics HW7",
    "section": "",
    "text": "8장 검정의 기본요소\n\n\n수업시간 과제\n\\(X_1, X_2, \\dots X_n\\) 이 다음 분포로부터의 랜덤샘플일 때 \\(\\theta\\)의 추정량 \\(\\hat{\\theta}=\\bar{X}\\)이 비편향추정량 중에서 분산이 가장 작은 추정량임을 보여라.\n\n\\(Poisson(\\theta)\\)\n\nanswer\n\\(X \\sim Poisson(\\lambda) \\to f(x| \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{\\lambda!}\\)\n\\(E(X) = \\lambda\\), \\(Var(X) = \\lambda\\)\n\\(Var(\\bar{X}) = \\frac{1}{nI(\\theta)}\\)일까?\n\\(I(\\theta) = E(\\frac{\\partial}{\\partial \\lambda} log f(x|\\lambda)^2)\\)\n\\(= E(-1 + \\frac{X}{\\lambda})^2 = E(\\frac{X-\\lambda}{\\lambda})^2 = \\frac{E(X^2) - 2\\lambda E(X) + \\lambda^2}{\\lambda^2} = \\frac{\\lambda + \\lambda^2-2\\lambda^2 + \\lambda^2}{\\lambda^2} = \\frac{1}{\\lambda}\\)\n\\(\\frac{1}{nI(\\theta)} = \\frac{\\lambda}{n}\\)\n\\(Var(\\bar{X}) = \\frac{\\lambda}{n}\\)\n\\(\\therefore \\bar{X}\\)는 MVUE다.\n\n\n수업시간 과제\n\\(X_1,X_2, \\dots, X_n\\)가 \\(Bernoulli(p)\\)로부터의 랜덤샘플이라고 할 때, \\(Bernoulli(p)\\)의 분산의 최대가능도추정량의 점근분포를 구하시오.\nanswer\n델타 방법 이용\n\\(\\sqrt{n}(g(\\hat{\\theta})-g(\\theta_0) \\xrightarrow[]{d} N(0,\\frac{g'(\\theta_0)^2}{I(\\theta_0)})\\)\n\\(g(\\theta_0) = p(1-p)\\)\n\\(g'(\\theta_0) = 1-2p\\)\n\\(I(p) = \\frac{1}{p(1-p)}\\)\n\\(\\therefore \\frac{(1-2p)^2}{I(p)} = p(1-p)(1-2p)^2\\)\n\\(\\sqrt{n}(\\frac{p(1-p)}{n} - p(1-p) \\xrightarrow[]{d} N(0,p(1-p)(1-2p)^2)\\)\n\n\n4장 23.\n\\(X_1,X_2,X_3\\)이 \\(f_X(x:\\theta)=\\frac{1}{\\theta}exp(-\\frac{x}{\\theta}),x>0\\)으로부터 얻은 랜덤표본이라고 하자. 모수 \\(\\theta\\)를 추정함에 있어 \\(\\frac{X_1 + 2X_2 + X_3}{4}\\)의 \\(\\bar{X}_3\\)에 대한 효율을 구하라.\nanswer\n\\(\\sim \\frac{(X_1 + 2X_2 + X_3)/4}{\\bar{X}_3}\\)\n\\(\\text{eff}((X_1 + 2X_2 + X_3)/4,\\bar{X}_3) = \\frac{1/Var((X_1+2X_2+X_3)/4)}{1/Var(\\bar{X}_3)}\\)\n\\(E(\\frac{X_1 + 2X_2 + X_3}{4}) = \\frac{\\theta + 2\\theta + \\theta}{4} = \\theta \\to\\)비편향추정량\n\\(E(\\bar{X}_3) = \\theta \\to\\) 비편향추정량\n\\(Var(\\frac{X_1 + 2X_2 + X_3}{4}) = \\frac{\\theta^2 + 4\\theta^2 + \\theta^2}{16} = \\frac{6\\theta^2}{16} = \\frac{3\\theta^2}{8}\\)\n\\(Var(\\bar{X}_3) = \\frac{\\theta^2}{3}\\)\n\\(\\therefore \\frac{8}{3\\theta^2} \\times \\frac{\\theta^2}{3} = \\frac{8}{9}\\)\n\\((X_1 + 2X_2 + X_3)/4\\) 가 \\(\\bar{X}_3\\)에 비해 \\(\\frac{8}{9}\\)만큼 효율을 가진다.\n\n\n4장 25.\n\\(X_1,X_2,\\dots,X_n\\)을 베르누이 \\((p)\\)로부터 얻은 랜덤 표본이라고 하자.\n분산 \\(p(1-p)\\)에 대한 비편향추정량의 크래머-라오 하한값을 구하라.\nanswer\n\\(f_X(x|p) = p^x(1-p)^{1-x}\\)\n\\(I(p) = E(\\frac{\\partial}{\\partial p} log f_X(x|p))^2\\)\n\\(= E(\\frac{\\partial}{\\partial p} X log p + (1-x) log(1-p))^2\\)\n\\(= E(\\frac{x}{p} - \\frac{1-x}{1-p})^2\\)\n\\(\\frac{E((x-p)^2}{o(1-p)^2}\\)\n\\(\\star E((x-p)^2 = Var \\sim p(1-p)\\)\n\\(= \\frac{E(x-p)^2}{p^2(1-p)^2} = \\frac{1}{p(1-p)}\\)\nCRLB: \\(\\frac{(g'(p))^2}{nI(p)} = \\frac{p(1-p)(1-2p)^2}{n}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html",
    "href": "posts/ts/2023-01-25-ts-final term.html",
    "title": "Theoritical Statistics Final term",
    "section": "",
    "text": "Final term"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#a",
    "href": "posts/ts/2023-01-25-ts-final term.html#a",
    "title": "Theoritical Statistics Final term",
    "section": "(a)",
    "text": "(a)\n\\(\\hat{\\theta}_{a_1,a_2}\\)이 비편향추정량이 될 \\(a_1\\)과 \\(a_2\\)의 조건을 구하시오\nanswer\n비편향 추정량 \\(\\theta_1\\), \\(\\theta_2\\), \\(E(\\hat{\\theta}_1) = \\bar{\\theta}\\), \\(E(\\hat{\\theta}_2) = \\bar{\\theta}\\)\n\\(E(\\hat{\\theta}_{a_1,a_2}) = E(a_1\\hat{\\theta}_1 + a_2\\hat{\\theta}_2) = a_1E(\\hat{\\theta}_1) + a_2E(\\hat{\\theta}_2) = a_1\\bar{\\theta} + a_2\\bar{\\theta} = \\bar{\\theta}(a_1 + a_2)\\)\n\\(\\bar{\\theta} = \\bar{\\theta}(a_1 + a_2)\\), 즉, \\(a_1 + a_2 = 1\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#b",
    "href": "posts/ts/2023-01-25-ts-final term.html#b",
    "title": "Theoritical Statistics Final term",
    "section": "(b)",
    "text": "(b)\n비편향추정량인 \\(\\hat{\\theta}_{a_1,a_2}\\) 중에서 가장 작은 분산을 가지는 추정량을 구하시오.\nanswer\n\\(Var(\\hat{\\theta}_1) = \\sigma^2_1\\), \\(Var(\\hat{\\theta}_2) = \\sigma^2_2\\)\n\\(Var(\\hat{\\theta}_{a_1,a_2}) = Var((a_1 + a_2)\\bar{\\theta}) = (a_1 + a_2)^2 Var(\\hat{\\theta})\\)\n\\((a_1 + a_2)^2\\)이 최소이면서 \\(a_1 + a_2=1\\)일 때, 즉 \\(a_1 = 0.5, a_2 = 0.5\\)일 때 가장 작은 분산을 가진다."
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#a-1",
    "href": "posts/ts/2023-01-25-ts-final term.html#a-1",
    "title": "Theoritical Statistics Final term",
    "section": "(a)",
    "text": "(a)\n\\(p(1-p)\\)에 대한 비편향추정량의 크래머-라오 하한값을 구하시오.\nanswer\nbias = \\(E(\\hat{p}(1-\\hat{p})) - p(1-p) = 0\\)\n\\(E(\\bar{X}) = np\\), \\(Var(\\bar{X}) = np(1-p)\\)\n\\(E(\\hat{p}(1-\\hat{p})) = E(\\frac{\\bar{X}}{n}(1-\\frac{\\bar{X}}{n})) = E(\\frac{\\bar{X}}{n} - (\\frac{\\bar{X}}{n})^2) = p - p^2\\)\n\\(f(x) = p^x(1-p)^{1-x}\\)\n\\(logf(x) = xlogp + (1-x)log(1-p)\\)\n\\(\\frac{\\partial log f(x)}{\\partial p} = \\frac{x}{p} - \\frac{1-x}{1-p}\\)\n\\(\\frac{\\partial^2 log f(x)}{\\partial^2 p} = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}\\)\n\\(\\star I(\\theta) = E[(\\frac{\\partial}{\\partial \\theta} log f(C; \\theta))^2)]\\)\n\\(I(p) = -E(-\\frac{x}{p^2}-\\frac{1-x}{(1-p)^2}) = \\frac{1}{p} + \\frac{1}{1-p} = \\frac{1}{p(1-p)}\\)\n\\(CRLB = \\frac{g'(p)^2}{nI(p)} = \\frac{(1-2p)^2p(1-p)}{n}\\)\n\\(\\star g(p) = p(1-p)\\), \\(g'(p) = 1-2p\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#b-1",
    "href": "posts/ts/2023-01-25-ts-final term.html#b-1",
    "title": "Theoritical Statistics Final term",
    "section": "(b)",
    "text": "(b)\n\\(X_1(1-X_2)\\)의 기댓값을 구하시오.\nanswer\n\\(E(X_1(1-X_2)) = E(X_1 - X_1X_2) = E(X_1) - E(X_1X_2) = E(X_1) - E(X_1)E(X_2) = p - p^2 = p(1- p)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#c",
    "href": "posts/ts/2023-01-25-ts-final term.html#c",
    "title": "Theoritical Statistics Final term",
    "section": "(c)",
    "text": "(c)\n\\(p(1-p)\\)에 대한 최소분산 비편향 추정량을 구하시오.\nanswer\n\\(E(\\bar{X}_n(1 - \\bar{X}_n)) = \\frac{(n-1)p(1-p)}{n}\\)\n\\(p(1-p) = \\frac{n\\bar{X}_n(1-\\bar{X}_n)}{n-1}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#d",
    "href": "posts/ts/2023-01-25-ts-final term.html#d",
    "title": "Theoritical Statistics Final term",
    "section": "(d)",
    "text": "(d)\n\\(p(1-p)\\)에 대한 적률추정량을 구하시오.\nanswer\n\\(M_1 = E(\\bar{X}) = p\\)\n\\(M_2 = Var(\\bar{X}) + E(\\bar{X})^2 = p(1-p) + p^2 = p\\)\n\\(p(1-p)\\)의 적률추정량 \\((p(1-p))^{MME} = M_2 - M^2_1 = p - p^2 = p(1-p)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#e",
    "href": "posts/ts/2023-01-25-ts-final term.html#e",
    "title": "Theoritical Statistics Final term",
    "section": "(e)",
    "text": "(e)\n\\(p(1-p)\\)에 대한 최대가능도추정량을 구하시오.\nanswer\n\\(f(x) = p^x(1-p)^{1-x}, x=0,1, 0<p<1\\)\n\\(L(p) = f(x_1|p)\\dots f(x_n|p) = p^{x_1}(1-p)^{1-x_1}\\dots p^{x_n}(1-p)^{1-x_n} = p^{\\sum x_i}(1-p)^{n-\\sum x_i}\\)\n\\(l(p) = \\sum x_i log p + (n-\\sum x_i) log (1-p)\\)\n\\(l'(p) = \\frac{\\sum x_i}{p} - \\frac{n-\\sum x_i}{1-p} = 0\\)\n\\(\\hat{p} = \\frac{\\sum x_i}{n} = \\bar{X}\\)\n\\((\\hat{p}(1-\\hat{p}))^{MLE} = \\bar{X}(1-\\bar{X})\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#a-2",
    "href": "posts/ts/2023-01-25-ts-final term.html#a-2",
    "title": "Theoritical Statistics Final term",
    "section": "(a)",
    "text": "(a)\n\\(\\theta\\)에 대한 적절한 추축변량을 구하고, 해당 추축변량의 분포를 명시하시오.\nanswer\n\\(X_i \\sim exp(\\frac{1}{\\theta})\\)\n\\(2X_i \\theta \\sim exp(2)\\)\n\\(2n\\bar{X}\\theta \\sim \\chi^2 (2n)\\)\n참고"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#b-2",
    "href": "posts/ts/2023-01-25-ts-final term.html#b-2",
    "title": "Theoritical Statistics Final term",
    "section": "(b)",
    "text": "(b)\n\\(\\theta\\)에 대한 95%신뢰구간을 구하시오.\nanswer\n\\((\\chi^2_{0.025} (2n) \\le 2n \\bar{X} \\theta \\le \\chi^2_{0.975} (2n)) = 0.95\\)\n\\(\\theta_{0.95} \\to (\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}} , \\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}})\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#c-1",
    "href": "posts/ts/2023-01-25-ts-final term.html#c-1",
    "title": "Theoritical Statistics Final term",
    "section": "(c)",
    "text": "(c)\n\\(P(X>1)\\)에 대한 95% 신뢰구간을 구하시오.\nanswer\n\\(P(X>1) = \\int^{\\infty}_1 \\theta e^{-x\\theta} dx = [ e^{x\\theta}]^{\\infty}_1 = e^{-\\theta}\\)\n\\(P(X>1)\\) \\(95\\)% CI : \\((exp(\\frac{\\chi^2_{0.9755}(2n)}{2n\\bar{X}} , exp(\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}}))\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#d-1",
    "href": "posts/ts/2023-01-25-ts-final term.html#d-1",
    "title": "Theoritical Statistics Final term",
    "section": "(d)",
    "text": "(d)\n가설에서 고려하고 있는 \\(\\theta\\)의 전체 모수공간 \\(\\Omega\\)와 귀무가설 하에서의 모수공간 \\(\\Omega_0\\)을 구하시오.\nanswer\n\\(\\Omega = \\{ \\theta : \\theta > 0 \\}\\)\n\\(\\Omega_0 = \\{ \\theta: \\theta = 2\\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#e-1",
    "href": "posts/ts/2023-01-25-ts-final term.html#e-1",
    "title": "Theoritical Statistics Final term",
    "section": "(e)",
    "text": "(e)\n\\(\\theta\\)의 가능도 함수를 기술하시오.\nanswer\n\\(L(\\theta) = \\theta^n e^{-\\theta n \\bar{X}}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#f",
    "href": "posts/ts/2023-01-25-ts-final term.html#f",
    "title": "Theoritical Statistics Final term",
    "section": "(f)",
    "text": "(f)\n\\(\\theta\\)의 \\(\\Omega\\)에서의 최대가능도 추정량과 \\(\\Omega_0\\)에서의 최대가능도 추정량을 구하시오.\nanswer\n\\(\\hat{\\theta}^{\\Omega} = \\frac{1}{\\bar{X}}\\)\n\\(\\hat{\\theta}^{\\Omega_0} = 2\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#g",
    "href": "posts/ts/2023-01-25-ts-final term.html#g",
    "title": "Theoritical Statistics Final term",
    "section": "(g)",
    "text": "(g)\n일반화 가능도 비 \\(\\Lambda\\)을 구하시오.\nanswer\n\\(\\frac{L(\\hat{\\theta}^{\\Omega_0})}{L(\\hat{\\theta}^{\\Omega})} = \\frac{\\hat{\\theta}^{n,\\Omega_0} e^{-\\hat{\\theta} n \\bar{X}}}{\\hat{\\theta}^{n,\\Omega} e^{-\\hat{\\theta} n \\bar{X}}} = (\\frac{2}{n})^2 e^{n\\bar{x}(\\theta-2)}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#h",
    "href": "posts/ts/2023-01-25-ts-final term.html#h",
    "title": "Theoritical Statistics Final term",
    "section": "(h)",
    "text": "(h)\n유의수준 \\(\\alpha\\)인 가능도비 검정법의 기각역을 \\(\\chi^2\\)분포의 분위수를 사용하여 표현하시오.\nanswer\n7장 예제 7.2.3. 참고\n\\(2(l(\\hat{\\theta}^{\\Omega}) - l(\\hat{\\theta}^{\\Omega_0})) = 2n(\\bar{x}\\theta_0 - 1 -log(\\bar{X}\\theta_0))\\)\n최대가능도비 검정의 기각역 형태 \\(2n(\\bar{x}\\theta_0 - 1 -log(\\bar{X}\\theta_0))\\ge c\\)\n기각역\n\\(\\begin{cases} \\bar{x}\\theta_0 \\le c_1 \\text{ 또는 } \\bar{x}\\theta_0\\ge c_2 \\\\ c_1 log c_1 = c_2 - log c_2\\end{cases}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#a-3",
    "href": "posts/ts/2023-01-25-ts-final term.html#a-3",
    "title": "Theoritical Statistics Final term",
    "section": "(a)",
    "text": "(a)\n가설 \\(H_0:\\sigma^2 = 4\\) vs \\(H_1: \\sigma^2 = 9\\)에 대한 최강력 검정의 기각역은\n\\[C = \\{ (x_1,\\dots,x_n):\\sum^n_{i=1}x_i^2 \\ge c\\}\\]\n의 꼴로 주어짐을 보이시오.\nanswer\n\\(L(\\sigma^2) = \\Pi^n_{i=1} f(x_2 : \\sigma^2) = \\Pi^n_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x_i^2}{2\\sigma^2}} * \\mu=0\\)\n\\(= (\\frac{1}{2\\pi\\sigma^2})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1}x_i^2}{2\\sigma^2}}\\)\n네이만 피어슨 정의에 의하면, \\(LR = \\frac{L(H_0)}{L(H_1)} = \\frac{L(4)}{L(9)} \\le k\\)\n\\(LR = \\frac{L(4)}{L(9)} = \\frac{(\\frac{1}{2\\pi 4})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1} x_i^2}{2 \\times 4}}}{(\\frac{1}{2\\pi 9})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1} x_i^2}{2 \\times 9}}}\\)\n\\(= (\\frac{9}{4})^{\\frac{n}{2}}e^{-\\sum^n_{i=1}x^2_i(\\frac{1}{8}-\\frac{1}{18})}\\)\n\\(= (\\frac{9}{4})^{\\frac{n}{2}}e^{-\\frac{5}{72}\\sum^n_{i=1}x^2_i} \\le k\\)\n\\(\\to e^{-\\frac{5}{72}\\sum^n_{i=1}x^2_i} \\le k\\)\n\\(\\to -\\frac{5}{72}\\sum^n_{i=1}x^2_i \\le k\\)\n\\(\\to \\sum^n_{i=1}x^2_i \\ge k\\)\n기각역: \\(\\therefore c = \\{ (x_1, \\dots ,x_n) : \\sum^n_{i=1} x^2_i \\ge c)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#b-3",
    "href": "posts/ts/2023-01-25-ts-final term.html#b-3",
    "title": "Theoritical Statistics Final term",
    "section": "(b)",
    "text": "(b)\n표본의 크기가 \\(n=20\\)일 때 유의수준이 \\(\\alpha=0.05\\)이기 위한 상수 \\(c\\)의 값을 \\(\\chi^2\\)분포의 분위수를 사용하여 표현하시오.\nanswer\n\\(\\alpha = P(\\text{Reject } H_0 | H_0 \\text{True})\\)\n\\(= P(\\sum^n_{i=1}x^2_i \\ge k | \\sigma^2 = 4)\\)\n\\(= P(\\sum^n_{i=1}\\frac{\\chi^2_i}{\\sigma^2} \\ge \\frac{k}{\\sigma^2}|\\sigma^2 = 4)\\)\n\\(= P(\\sum^{20}_{i=1} \\frac{\\chi^2_i}{4} \\ge \\frac{k}{4} | \\sigma^2 = 4)\\)\n\n\n\nimage.png\n\n\n\\(\\frac{k}{4} = \\chi^2_{0.05(20)}\\)\n\nqchisq(0.95,20)\n\n31.4104328442309\n\n\n\nround(4*qchisq(0.95,20),2)\n\n125.64\n\n\n\\(k = 4\\chi^2_{0.05(20)} = 125.64\\)\n\\(c = \\{(x_1,\\dots,x_n) : \\sum^n_{i=1}\\ge 125.64\\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#c-2",
    "href": "posts/ts/2023-01-25-ts-final term.html#c-2",
    "title": "Theoritical Statistics Final term",
    "section": "(c)",
    "text": "(c)\n표본의 크기가 \\(n=20\\)일 때 (b)에서 찾은 기각역에 대한 제2종오류를 범할 확률을 구하시오.\nanswer\n\\(\\beta = P(\\text{Not Reject } H_0 | H_1 \\text{True})\\)\n\\(= P(\\sum^n_{i=1}x^2_i \\le k | \\sigma^2 = 9)\\)\n\\(\\star\\)\n모집단 분포 \\(X_i \\sim N(0,\\sigma^2)\\)\n표준화 \\(\\frac{X_i}{\\sigma} \\sim N(0,1)\\)\n표분화 제곱 분포는 카이제곱 \\((\\frac{X_i^2}{\\sigma})^2 \\sim \\chi^2_1, i=1,2,\\dots, n\\)\n카이제곱의 합의 자유도 합 \\(\\sum^n_{i=1}(\\frac{X_i}{\\sigma})^2 \\sim \\chi^2_{(n)}\\)\n\\(\\star\\)\n\\(= P(\\sum^n_{i=1}\\frac{\\chi^2_i}{\\sigma^2} \\le \\frac{k}{\\sigma^2}|\\sigma^2 = 9)\\)\n\\(= P(\\sum^{20}_{i=1} \\frac{\\chi^2_i}{9} \\le \\frac{k}{9} | \\sigma^2 = 9)\\)\n\\(\\frac{k}{9} = \\chi^2_{0.05(20)}\\)\n\nqchisq(0.95,20)\n\n31.4104328442309\n\n\n\nround(9*qchisq(0.95,20),2)\n\n282.69\n\n\n\\(k = 9\\chi^2_{0.05(20)} = 282.69\\)\n\\(c = \\{(x_1,\\dots,x_n) : \\sum^n_{i=1}\\le 282.69\\}\\)\n랜덤표본\\(X_1,X_2,\\dots,X_n\\)의 분포가 확률밀도함수 \\(f(x;\\theta), \\theta\\in\\Omega\\)를 따른다고 하자. 이때 표본과 모수 \\(\\theta\\)의 함수인 확률변량 \\(T(X_1,X_2,\\dots, X_n;\\theta)\\)의 분포가 모수 \\(\\theta\\)에 의존하지 않으면 이를 추축변량이라 한다."
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#a-4",
    "href": "posts/ts/2023-01-25-ts-final term.html#a-4",
    "title": "Theoritical Statistics Final term",
    "section": "(a)",
    "text": "(a)\n적절한 추축변량을 이용하여 \\(\\sigma^2\\)에 대한 \\(100(1-\\alpha)%\\) 신뢰구간을 구하시오.\nanswer\n\\(\\frac{\\sum(X_i - \\bar{X})2}{\\sigma^2} = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{(n-1)}^2\\)\n\\(P[\\chi_L \\le \\frac{(n-1)S^2}{\\sigma^2} \\le \\chi_U] = 1-\\alpha\\)\n\\(P[\\chi_{\\alpha/2} \\le \\frac{(n-1)S^2}{\\sigma^2} \\le \\chi_{1-\\alpha/2}] = 1-\\alpha\\)\n\\(P[\\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2}} \\le \\sigma^2 \\le \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}}] = 1-\\alpha\\)\n\\(\\therefore (\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}},\\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2}})\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#b-4",
    "href": "posts/ts/2023-01-25-ts-final term.html#b-4",
    "title": "Theoritical Statistics Final term",
    "section": "(b)",
    "text": "(b)\n유의수준 \\(\\alpha\\)인 일반화 가능도비 검정 기각역을 구하시오.\nanswer\n모평균 \\(\\mu\\)의 최대가능도 추정량은 가설에 관계없이 언제나 \\(\\bar{X}\\)\n모분산 \\(\\sigma^2\\)의 최대가능도 추정량은 귀무가설 하에서는 \\(4\\)이며, 전체 모수공간\\(\\Omega\\)내에서는 \\(\\hat{\\sigma}^2 = \\frac{\\sum^n_{i=1}(X - \\bar{X})^2}{n}\\)\n일반화 가능도비 \\(\\Lambda(X_1,X_2,\\dots,X_n) = (\\frac{\\sum^n_{i=1} (X_i - \\bar{X}_n)^2}{4n})^{n/2} \\times exp[-\\frac{1}{2} \\{ \\frac{\\sum^n_{i=1}\\{(X_i - \\bar{X}_i)}{4} \\}^2 + \\frac{n}{2}]\\)\n\\(= (\\frac{\\hat{\\sigma}^2}{4})^{n/2} exp\\{ -(\\frac{n}{2})(\\frac{\\hat{\\sigma^2}}{4}) + \\frac{n}{2} \\}\\)\n\\(\\Lambda = \\frac{\\hat{\\sigma}^2}{4}^{n/2} exp(-\\frac{n}{2}\\frac{\\hat{\\sigma}^2}{4} + \\frac{n}{2})\\)\n\\(\\frac{\\hat{\\sigma}^2}{4} exp(-\\frac{\\hat{\\sigma}^2}{4}) \\le (\\lambda^*)^{2/n} exp(-1) = c^*\\)\n\\(\\frac{\\hat{\\sigma}^2}{4}<1\\)일 때는 단조증가, \\(\\frac{\\hat{\\sigma}^2}{4}>1\\)일 때는 단조감소\n기각영역 ; \\(C = \\{ (x_1,x_2,\\dots,x_n):(\\frac{\\hat{\\sigma}^2}{4} \\le a\\) 또는 \\(\\frac{\\hat{\\sigma}^2}{4} \\ge b \\}\\)\n\\(n\\frac{\\hat{\\sigma}^2}{4} = \\frac{\\sum^n_{i=1}(X_i - \\bar{X}_n)^2}{4} \\sim \\chi^2(n-1)\\)\n\\(P[\\frac{\\sum^n_{i=1}(X_i - \\bar{X}_n)^2}{4} \\le \\chi^2_{1-\\alpha/2}(n-1)|H_0]\\)\n\\(= P[\\frac{\\sum^n_{i=1}(X_i - \\bar{X}_n)^2}{4} \\ge \\chi^2_{\\alpha/2}(n-1)|H_0]\\)\n\\(= \\frac{\\alpha}{2}\\)\n그러므로 카이제곱분포의 양쪽꼬리에서 \\(\\alpha/2\\)씩 고려한 일반화 가능도비 검정법의 근사꼴로서 기각영역은\n\\(\\frac{\\sum^n_{i=1}(X_i - \\bar{X}_n)^2}{4} \\le \\chi^2_{1-\\alpha/2}(n-1)\\)\n또는 \\(\\frac{\\sum^n_{i=1}(X_i - \\bar{X}_n)^2}{4} \\ge \\chi^2_{\\alpha/2}(n-1)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-25-ts-final term.html#c-3",
    "href": "posts/ts/2023-01-25-ts-final term.html#c-3",
    "title": "Theoritical Statistics Final term",
    "section": "(c)",
    "text": "(c)\n\\(n=10\\)이고 표본분산 \\(S^2\\)의 관측값은 \\(6\\)이라고 한다. (b)의 일반화 가능도비 검정법으로 유의확률 (\\(p-value\\))을 구하시오.\nanswer\n\n1-pchisq(6/4*9,9)\n\n0.14125582649328"
  },
  {
    "objectID": "posts/ts/2023-01-12-ts_HW5.html",
    "href": "posts/ts/2023-01-12-ts_HW5.html",
    "title": "Theoritical Statistics HW5",
    "section": "",
    "text": "확률변수의 극한"
  },
  {
    "objectID": "posts/ts/2023-01-12-ts_HW5.html#section-3",
    "href": "posts/ts/2023-01-12-ts_HW5.html#section-3",
    "title": "Theoritical Statistics HW5",
    "section": "(1)",
    "text": "(1)\n\\(\\sqrt{n}(\\bar{X}_n^2 - \\mu^2) \\xrightarrow[]{d} N(0,4 \\mu^2 \\sigma^2)\\)임을 보여라, 단 \\(\\mu \\neq 0\\)\nanswer\nDelta Method\n\\(\\sqrt{n}(g(X_n) - g(\\mu) ) \\xrightarrow[]{d} N(0,\\sigma^2 g'(\\mu)^2)\\)\n\\(g(X_n) = \\bar{X}_n^2\\), \\(g'(X_n) = 2\\bar{X}_n\\)\n\\(g(\\mu) = \\mu^2\\), \\((g'(\\mu))^2 = 4\\mu^2\\)\n\\(\\sqrt{n}(\\bar{X}_n^2 - \\mu^2) \\xrightarrow[]{d} N(0,4\\mu^2 \\sigma^2)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-12-ts_HW5.html#section-4",
    "href": "posts/ts/2023-01-12-ts_HW5.html#section-4",
    "title": "Theoritical Statistics HW5",
    "section": "(2)",
    "text": "(2)\n(1)에서 \\(\\mu=0\\)인 경우에 대해 설명하라.\nanswer\n\\(\\mu=0\\)\n\\(\\sqrt{n} \\bar{X}^2_n \\xrightarrow[]{d} N(0,0)\\) 분산 0\n\\(\\sqrt{n}\\bar{X}^2_n \\xrightarrow[]{p} 0\\) 0으로 확률 수렴한다."
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html",
    "href": "posts/ts/2023-01-05-ts_HW2.html",
    "title": "Theoritical Statistics HW2",
    "section": "",
    "text": "기댓값"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-3",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-3",
    "title": "Theoritical Statistics HW2",
    "section": "(1)",
    "text": "(1)\n\\(x\\)에 대한 \\(Y\\)의 조건부 기댓값 \\(E(Y|x)\\)를 구하라.\nAnswer\n\\(E(Y|x) = \\int^\\infty_{-\\infty} y f_{Y|X} (y) dy = \\int^\\infty_{-\\infty} y \\frac{f(X,y)}{f_X(X)}dy\\)\n\\(\\star\\)\n\\(f_X(X) = \\int f(x,y) dy = \\int^\\infty_{-\\infty} 4I(0<x<1,1<y<2x)dy\\)\n\\(= 4I(0<x<1)\\int_{-\\infty < y < \\infty , 1<y<2x} I(1<y<2x)dy\\)\n\\(= 4I(0<x<1)\\int^{2x}_1 a dy = 4I(0<x<1)[y]^{2x}_1\\)\n\\(= 4(2x-1)I(0<x<1)\\)\n\\(\\star\\)\n\\(= \\int^\\infty_{-\\infty} y \\frac{4I(0<x<1,1<y<2x)}{4(2x-1)I(0<x<1)} dy\\)\n\\(= \\int_{-\\infty < y< \\infty, 1<y<2x} y \\frac{I(0<x<1)}{2x-1} dy = \\frac{I(0<x<1)}{2x-1} \\int^{2x}_1 y dy\\)\n\\(= \\frac{I(0,x<1)}{2x-1} [\\frac{1}{2} y^2]^{2x}_1\\)\n\\(= \\frac{I(0<x<1)}{2x-1}(\\frac{1}{2}(4x^2-1))\\)\n\\(\\star 4x^2 - 1 = (2x+1)(2x-1)\\)\n\\(= \\frac{2x+1}{2}I(0<x<1)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-4",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-4",
    "title": "Theoritical Statistics HW2",
    "section": "(2)",
    "text": "(2)\n\\(y\\)에 대한 \\(X\\)의 조건부 기댓값 \\(E(X|y)\\)를 구하라.\nAnswer\n\\(E(X|Y=y) = \\int^\\infty_{-\\infty} x f_{X|Y}(x|y) dx = \\int^\\infty_{-\\infty} x\\frac{f(x,y}{f(y)} dx\\)\n\\(\\star\\)\n\\(f_Y(Y) = \\int^\\infty_{-\\infty} f(x,y) dy = \\int^\\infty_{-\\infty} 4I(0<x<1,1<y<2x) dx\\)\n\\(= 4\\int_{-\\infty <x<\\infty, 0<x<1, 1<y<2x} 1 dx = \\int^1_{\\frac{y}{2}} 4I(1<y<2) dx [4x]^1_{\\frac{y}{2}}\\)\n\\(= (4-2y)I(1<y<2)\\)\n\\(\\star\\)\n\\(= \\int^\\infty_{-\\infty} x \\frac{4I(0<x<1,1<y<2x)}{(4-2y)I(1<y<2)} dx\\)\n\\(= \\frac{4}{4-2y} \\int_{-\\infty x<\\infty, 0<x<1,1<y<2x,1<y<2} x dx = \\frac{4}{4-2y} \\int^1_{\\frac{y}{2}} xI(1<y<2) dx\\)\n\\(= \\frac{4}{4-2y}[\\frac{1}{2}x^2]^1_{\\frac{y}{2}} = \\frac{4}{4-2y} (\\frac{1}{2} - \\frac{y^2}{8})\\)\n\\(= \\frac{4}{2(2-y)}(\\frac{4-y^2}{8})\\)\n\\(\\star 4-y^2 = (2+y)(2-y)\\)\n\\(= 4(2+y)I(1<y<2)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-5",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-5",
    "title": "Theoritical Statistics HW2",
    "section": "(3)",
    "text": "(3)\n\\(x\\)에 관한 \\(Y\\)의 조건부 분산 \\(E[Y - E(Y|x)|x]^2\\)을 구하라.\nAnswer\n\\(E[(Y - E(Y|X=x))^2|X=x] = E(Y^2|X=x) - \\{ E(Y|X=x) \\}^2\\)\n\\(\\star\\)\n\\(E(Y^2|X=x) = \\int Y^2 f_{Y|X}(y) dy = \\int^\\infty_{-\\infty} y^2 \\frac{f(x,y)}{f_X(X)}dy\\)\n\\(= \\int^\\infty_{-\\infty} y^2 \\frac{4I(0<x<1,1<y<2x)}{4(2x-1)I(0<x<1)} dy\\)\n\\(= \\frac{1}{2x-1} I(0<x<0 \\int_{-\\infty < y<\\infty, 1<y<2x} y^2 dy\\)\n\\(= \\frac{1}{2x-1} I(0<x<1)\\int^{2x}_1 y^2 dy\\)\n\\(= \\frac{1}{2x-1}I(0<x<1) [\\frac{1}{3} y^2]^{2x}_1\\)\n\\(= \\frac{1}{2x-1} I (0<x<1)\\frac{1}{3}(8x^3-1)\\)\n\\(= \\frac{4x^2 + 2x+1}{3} I(0<x<1)\\)\n\\(\\star\\)\n\\(= (\\frac{4x^2 + 2x + 1}{3} - \\frac{(2x+ 1)^2}{2^2}) I(0<x<1)\\)\n\\(\\frac{(2x-1)^2}{12}I(0<x<1)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-9",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-9",
    "title": "Theoritical Statistics HW2",
    "section": "(1)",
    "text": "(1)\n확률변수들 \\(X_i\\)의 적률생성함수를 구하라.\nAnswer\n\\(M_X(t) = E(e^{tx})\\)\n\\(\\sum^1_{x=0}e^{tx}f(x) = e^0 f(0) + e^tf(1) = 1-p+e^tp\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-10",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-10",
    "title": "Theoritical Statistics HW2",
    "section": "(2)",
    "text": "(2)\n(1)의 결과를 이용하여 확률변수 \\(X_i\\)의 분산을 구하라.\nAnswer\n\\(M_X^{'} (t) = e^tp \\to M_X^{'}(0) = p\\)\n\\(M_X^{''} (t) = e^tp \\to M_X^{''}(0) = p\\)\n\\(var(X_i) = E(X^2) - (E(X))^2\\)\n\\(= M_X^{''}(0) - \\{ M_X^{'}(0)\\}^2\\)\n\\(= p - p^2\\)\n\\(= p(1-p)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-11",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-11",
    "title": "Theoritical Statistics HW2",
    "section": "(3)",
    "text": "(3)\n(1)의 결과를 이용하여 확률변수 \\(\\sum^n_{i=1} X_i\\)의 적률생성함수를 구하라.\nAnswer\n\\(M_{\\sum^n_{i=1} X_i}(t) = E(e^{t\\sum^n_{i=1} X_i})\\)\n\\(= E(e^{tX_1 + tX_2 + \\dots + tX_n})\\)\n\\(= E(e^{tX_1} e^{tX_2} \\dots e^{tX_n})\\)\n\\(= E(e^{tx})E(e^{tX_2})\\dots E(e^{tX_n})\\)\n\\(= M_{X_1}(t) M_{X_2}(t) \\dots M_{X_n}(t)\\)\n\\(= (1-p+pe^t)^n \\to\\) 이항분포의 적률생성함수"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-13",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-13",
    "title": "Theoritical Statistics HW2",
    "section": "(1)",
    "text": "(1)\n\\(P(X = 3)\\)을 계산하라.\nAnswer\n\\(P(X=1) = \\frac{1}{9}\\)\n\\(P(X=3) = \\frac{3}{9}\\)\n\\(P(X=5) = \\frac{5}{9}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW2.html#section-14",
    "href": "posts/ts/2023-01-05-ts_HW2.html#section-14",
    "title": "Theoritical Statistics HW2",
    "section": "(2)",
    "text": "(2)\n\\(X\\)의 확륢밀도함수를 구하라.\nAnswer\n\\(M_X(t) = E(e^{tx}) = \\sum^n_{i=1} e^{tx} f(x) = \\frac{1}{9}e^t + \\frac{3}{9} e^{3t} + \\frac{5}{9} e^{5t}\\)\n\\(\\therefore f(x) = \\frac{x}{9}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html",
    "href": "posts/ts/2023-01-21-ts-HW9.html",
    "title": "Theoritical Statistics HW9",
    "section": "",
    "text": "5장 이표본 검정법"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-1",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-1",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n가설 \\(H_0 : \\sigma^4 = 4\\) vs \\(H_1 : \\sigma^2 = 9\\)에 대한 최강력 검정의 기각역은\n\\[C = \\{ (x_1, \\dots, x_n) : \\sum^n_{i=1} x^2_i \\ge c \\}\\]\n의 꼴로 주어짐을 보이시오.\nanswer\n\\(L(\\sigma^2) = \\Pi^n_{i=1} f(x_2 : \\sigma^2) = \\Pi^n_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x_i^2}{2\\sigma^2}} * \\mu=0\\)\n\\(= (\\frac{1}{2\\pi\\sigma^2})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1}x_i^2}{2\\sigma^2}}\\)\n네이만 피어슨 정의에 의하면, \\(LR = \\frac{L(H_0)}{L(H_1)} = \\frac{L(4)}{L(9)} \\le k\\)\n\\(LR = \\frac{L(4)}{L(9)} = \\frac{(\\frac{1}{2\\pi 4})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1} x_i^2}{2 \\times 4}}}{(\\frac{1}{2\\pi 9})^{\\frac{n}{2}} e^{-\\frac{\\sum^n_{i=1} x_i^2}{2 \\times 9}}}\\)\n\\(= (\\frac{9}{4})^{\\frac{n}{2}}e^{-\\sum^n_{i=1}x^2_i(\\frac{1}{8}-\\frac{1}{18})}\\)\n\\(= (\\frac{9}{4})^{\\frac{n}{2}}e^{-\\frac{5}{72}\\sum^n_{i=1}x^2_i} \\le k\\)\n\\(\\to e^{-\\frac{5}{72}\\sum^n_{i=1}x^2_i} \\le k\\)\n\\(\\to -\\frac{5}{72}\\sum^n_{i=1}x^2_i \\le k\\)\n\\(\\to \\sum^n_{i=1}x^2_i \\ge k\\)\n기각역: \\(\\therefore c = \\{ (x_1, \\dots ,x_n) : \\sum^n_{i=1} x^2_i \\ge c)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-2",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-2",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n표본의 크기가 \\(n=20\\)일 때 유의수준이 \\(\\alpha=0.05\\)이기 위한 상수 \\(c\\)의 값을 구하시오.\nanswer\n\\(\\alpha = P(\\text{Reject } H_0 | H_0 \\text{True})\\)\n\\(= P(\\sum^n_{i=1}x^2_i \\ge k | \\sigma^2 = 4)\\)\n\\(\\star\\)\n모집단 분포 \\(X_i \\sim N(0,\\sigma^2)\\)\n표준화 \\(\\frac{X_i}{\\sigma} \\sim N(0,1)\\)\n표분화 제곱 분포는 카이제곱 \\((\\frac{X_i^2}{\\sigma})^2 \\sim \\chi^2_1, i=1,2,\\dots, n\\)\n카이제곱의 합의 자유도 합 \\(\\sum^n_{i=1}(\\frac{X_i}{\\sigma})^2 \\sim \\chi^2_{(n)}\\)\n\\(\\star\\)\n\\(= P(\\sum^n_{i=1}\\frac{\\chi^2_i}{\\sigma^2} \\ge \\frac{k}{\\sigma^2}|\\sigma^2 = 4)\\)\n\\(= P(\\sum^{20}_{i=1} \\frac{\\chi^2_i}{4} \\ge \\frac{k}{4} | \\sigma^2 = 4)\\)\n\n\n\nimage.png\n\n\n\\(\\frac{k}{4} = \\chi^2_{0.05(20)}\\)\n\nqchisq(0.95,20)\n\n31.4104328442309\n\n\n\n4*qchisq(0.95,20)\n\n125.641731376924\n\n\n\\(k = 4\\chi^2_{0.05(20)} = 125.64\\)\n\\(c = \\{(x_1,\\dots,x_n) : \\sum^n_{i=1}\\ge 125.64\\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-3",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-3",
    "title": "Theoritical Statistics HW9",
    "section": "(3)",
    "text": "(3)\n표본의 크기가 \\(n=20\\)일 때 (2)에서 찾은 기각역에 대한 제 2종 오류를 범할 확률\n\\[\\beta = P(\\sum^n_{i=1} X^2_i \\le c| \\sigma^2 = 9)\\]\n의 값은?\nanswer\n\\(\\beta = P(\\sum^n_{i=1}\\chi^2_i \\le c|\\sigma^2 = 9)\\)\n\\(= P(\\sum^n_{i=1} \\chi^2_i \\le 125.64 | \\sigma^2 = 9)\\)\n\\(= P(\\sum^n_{i=1} \\frac{\\chi^2_i-0}{\\sigma^2} \\le \\frac{125.64-0}{\\sigma^2} | \\sigma^2 = 9)\\) 표준화, 평균은 0\n\\(= P(\\sum^n_{i=1} \\frac{\\chi^2_i}{9} \\le \\frac{125.64}{9} | \\sigma^2 = 9)\\)\n\\(= P(\\sum^n_{i=1}\\frac{\\chi^2_i}{9} \\le 13.96) = 0.20\\)\n\n125.64/9\n\n13.96\n\n\n\npchisq(13.96,20)\n\n0.167481777665076\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-5",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-5",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n유의수준 \\(\\alpha \\le \\frac{1}{2}\\)인 검정법의 기각역을 모두 제시하라.\nanswer\n모집단 \\(B(2,p)\\)\n\\(f(x:p) = \\begin{pmatrix} 2 \\\\ x \\end{pmatrix} p^x (1-p)^{2-x}\\)\n\\(\\alpha = P(X \\in c | p =\\frac{1}{2}) \\le \\frac{1}{2}\\), \\(f(x: \\frac{1}{2}) = \\begin{pmatrix} 2 \\\\ x \\end{pmatrix} (\\frac{1}{2})^2\\)\n\\(x = 0,1,2\\)\n8가지 경우 존재\n\n\\(\\{0 \\}\\), \\(P(x = 0 | p = \\frac{1}{2}) = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} (\\frac{1}{2})^2 = \\frac{1}{4} \\to \\le \\frac{1}{2} \\therefore\\) 가능\n\\(\\{1 \\}\\), \\(P(x = 1 | p = \\frac{1}{2}) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} (\\frac{1}{2})^2 = \\frac{1}{2} \\to \\le \\frac{1}{2} \\therefore\\) 가능\n\\(\\{2 \\}\\), \\(P(x = 2 | p = \\frac{1}{2}) = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} (\\frac{1}{2})^2 = \\frac{1}{4} \\to \\le \\frac{1}{2} \\therefore\\) 가능\n\\(\\{0,1 \\}\\), \\(P(x = 2 | p = \\frac{1}{2}) = f(0:\\frac{1}{2}) + f(1:\\frac{1}{2}) = \\frac{1}{4} + \\frac{1}{2} = \\frac{3}{4} \\to \\text{ NOT } \\le \\frac{1}{2} \\therefore\\) 불가능\n\\(\\{0,2 \\}\\), \\(P(x = 2 | p = \\frac{1}{2}) = f(0:\\frac{1}{2}) + f(2:\\frac{1}{2}) = \\frac{1}{4} + \\frac{1}{4} =\\frac{1}{2} \\to \\le \\frac{1}{2} \\therefore\\) 가능\n\\(\\{1,2 \\}\\), \\(P(x = 2 | p = \\frac{1}{2}) = f(1:\\frac{1}{2}) + f(2:\\frac{1}{2}) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4} \\to \\text{ NOT } \\le \\frac{1}{2} \\therefore\\) 불가능\n\\(\\{0,1,2 \\}\\), \\(P(x = 2 | p = \\frac{1}{2}) = f(0:\\frac{1}{2}) + f(1:\\frac{1}{2}) + f(2:\\frac{1}{2})= \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} =1 \\to \\text{ NOT } \\le \\frac{1}{2} \\therefore\\) 불가능\n\\(\\{\\phi\\}\\) 사건이 전혀 발생하지 않는 것이니 제외"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-6",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-6",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n(1)의 기각역들 중 \\(\\alpha + \\beta\\)를 최소화하는 기각역을 구하라.\nanswer\n\\(\\beta = P(\\text{Not Reject } H_0 | H_1 \\text{True})\\)\n\\(= P(X \\notin c | P = \\frac{2}{3})\\)\n\\(= 1-P(X \\in c | P = \\frac{2}{3})\\)\n\n\\(\\{0 \\}\\), \\(\\alpha = \\frac{1}{4}\\)\n\n\n\\(\\beta = 1-f(0:\\frac{2}{3}) = 1-\\begin{pmatrix} 2 \\\\ 0\\end{pmatrix} (\\frac{2}{3})^0 (\\frac{1}{3})^2 = \\frac{8}{9}\\)\n\\(\\alpha + \\beta = \\frac{1}{4} + \\frac{8}{9} = \\frac{41}{36}\\)\n\n\n\\(\\{1 \\}\\), \\(\\alpha = \\frac{1}{2}\\)\n\n\n\\(\\beta = 1-f(1:\\frac{2}{3}) = 1-\\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} (\\frac{2}{3})^1 (\\frac{1}{3})^1 = 1- \\frac{4}{9} = \\frac{5}{9}\\)\n\\(\\alpha + \\beta = \\frac{1}{2} + \\frac{5}{9} = \\frac{19}{18} = \\frac{38}{36}\\)\n\n\n\\(\\{2 \\}\\), \\(\\alpha = \\frac{1}{4}\\)\n\n\n\\(\\beta = 1-f(2:\\frac{2}{3}) = 1-\\begin{pmatrix} 2 \\\\ 2\\end{pmatrix} (\\frac{2}{3})^2 (\\frac{1}{3})^0 = 1-\\frac{4}{9}=\\frac{5}{9}\\)\n\\(\\alpha + \\beta = \\frac{1}{4} + \\frac{5}{9} = \\frac{29}{36}\\)\n\n\n\\(\\{0,2 \\}\\), \\(\\alpha = \\frac{1}{2}\\)\n\n\n\\(\\beta = f(1:\\frac{2}{3}) = \\begin{pmatrix} 2 \\\\ 1\\end{pmatrix} (\\frac{2}{3})^1 (\\frac{1}{3})^1 = \\frac{4}{9}\\)\n\\(\\alpha + \\beta = \\frac{1}{2} + \\frac{4}{9} = \\frac{17}{18} = \\frac{34}{36}\\)\n\n\\(\\therefore\\) 제일 작은 기각역인 3번\\(\\{ 2\\}\\)가 기각역일 때 \\(c = ( x \\in \\{2\\})\\)가 \\(\\alpha + \\beta\\)값이 최솟값이 된다."
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-8",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-8",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n유의수준을 구하라.\nanswer\n\\(\\alpha = P(\\text{Reject } H_0 | H_0 \\text{ True})\\)\n= 귀무가설 하에서 \\(p\\)값의 범위 중 기각할 확률의 최댓값\n= \\({max}_{p\\le\\frac{1}{2}} P(\\text{Reject } H_0 | p) \\to \\Pi(p)\\)\n\\(= \\Pi(p) = P(\\text{Reject } H_0 | p)\\)\n\\(= P(X \\ge 7|p)\\)\n\\(= \\sum_{x \\ge7} f(x:p) = \\sum_{x \\ge 7} \\begin{pmatrix} 10 \\\\ x \\end{pmatrix}p^x (1-p)^{10-x}\\)\n\\(\\star p <1\\) 구간에서 증가함수다.\n\\(\\Pi(\\frac{1}{2}) = \\sum_{x\\ge7} \\begin{pmatrix} 10 \\\\ x \\end{pmatrix} (\\frac{1}{2})^2 = \\{ \\begin{pmatrix} 10 \\\\ 7 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 8 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 9 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 10 \\end{pmatrix} \\} \\{ (\\frac{1}{2})^{8}\\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-9",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-9",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n검정력함수를 구하라.\nanswer\n검정력 \\(= P(\\text{Reject } H_0 | H_1 \\text{ True})\\)\n= 대립가설 하에서 \\(p\\)값의 범위 중 기각할 확률의 최댓값\n= \\({max}_{p>\\frac{1}{2}} P(\\text{Reject } H_0 | p) \\to \\Pi(p)\\)\n\\(= \\Pi(p) = P(\\text{Reject } H_0 | p)\\)\n\\(= P(X \\ge 7|p)\\)\n\\(= \\sum_{x \\ge7} f(x:p) = \\sum_{x \\ge 7} \\begin{pmatrix} 10 \\\\ x \\end{pmatrix}p^x (1-p)^{10-x}\\)\n\\(\\star \\frac{1}{2}<p <1\\) 구간에서 증가함수다.\n\\(\\Pi(\\frac{1}{2}) = \\sum_{x\\ge7} \\begin{pmatrix} 10 \\\\ x \\end{pmatrix} (\\frac{1}{2})^2 = \\{ \\begin{pmatrix} 10 \\\\ 7 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 8 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 9 \\end{pmatrix} + \\begin{pmatrix} 10 \\\\ 10 \\end{pmatrix} \\} \\{ (\\frac{1}{2})^{10}\\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-11",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-11",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n\\(H_0: \\theta = \\theta_0\\) 대 \\(H_1:\\theta < \\theta_0\\)에 대한 균일 최강력검정의 기각영역을 구하기\nanswer\n\\(H_0 : \\theta = \\theta_0\\), \\(H_1 : \\theta = \\theta_0\\), \\((\\theta_1<\\theta_0)\\) 기각역은 \\(c = \\{ (x_1,\\dots,x_n):\\frac{L(\\theta_0)}{L(\\theta_1)} \\le k\\}\\)\n\\(L(\\theta) = \\Pi^n_{i=1} f(x:\\theta) = \\Pi^n_{i=1} \\frac{1}{\\theta} I(0<x_i < \\theta) = \\frac{1}{\\theta^n}I(0<x_{(1)}<x_{(n)} <\\theta)\\) 모든 값들이 0과 1사이에 존재\n가능도비 \\(LR = \\frac{L(\\theta_0)}{L(\\theta_1)} = \\frac{\\frac{1}{\\theta^2_0} I(0<x_{(n)} < \\theta_0)}{\\frac{1}{\\theta^n_1}I(0<x_{(n)} < \\theta_1)} \\le k\\)\n\\(\\begin{cases} X_{(n)} < \\theta_1 , & LR = (\\frac{\\theta_1}{\\theta_0})^n & \\to \\text{분자, 분모 범위가 모두 1이 됨!} \\le k \\text{이 부분만 남고} \\\\ \\theta_1 \\le X_{(n)} < \\theta_0 & LR = \\frac{1}{0} = \\infty & \\to \\text{ k보다 작을 수 없음} \\\\ X_{(n)} \\ge \\theta_0 & LR = \\frac{0}{0} & \\to \\text{ 고려하지 않을 것이다.어차피 제외될 걸??} \\end{cases}\\)\n\\(c = \\{ x_{(n)} \\le c\\text{ 상수}\\}\\)\n기각역의 모양이 \\(X_{(n)}\\le c\\)일 때, \\(\\le k\\)가 성립\n\\(c = \\{ X_{(n)} \\le c \\}\\)\n\\(\\alpha = P(X_{(n)} \\le c | \\theta = \\theta_0 \\} = \\int^c_{-\\infty} f_n (x : \\theta_0)dx\\)\n\\(\\star\\)\n최댓값\\(X_{(n)}\\)의 확률밀도함수 \\(\\to f_n(x:\\theta) = n(\\{ F(x:\\theta)\\}^{n-1} f(x:\\theta) = n(\\frac{x}{\\theta})^{n-1} \\frac{1}{\\theta} I(0<x<\\theta)\\)\n\\(= \\int^c_{-\\infty} n(\\frac{x}{\\theta_0})^{n-1} \\frac{1}{\\theta_0} I(0<x<\\theta_0)dx \\to c>\\theta_0\\text{ 면 무조건 1이다.}\\)\n\\(\\int^c_0 n(\\frac{x}{\\theta_0})^{n-1} \\frac{1}{\\theta_0}dx = (\\frac{x}{\\theta_0})^n \\to c = \\alpha^{\\frac{1}{n}} \\theta_0\\)\n유의수준 \\(\\alpha\\)인 최강력 기각역 \\(X_{(n))} < \\theta_0 \\alpha^{\\frac{1}{n}} \\to \\theta_1\\text{ 과 관련이 없다.} \\to \\text{모든 }\\theta \\text{에 대한 가설의 균일 최강력 기각역}\\)\n\\(H_0: \\theta=\\theta_0,H_1: \\theta = \\theta_1(\\theta < \\theta_1)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-12",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-12",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n귀무가설을 \\(H_0: \\theta \\ge \\theta_0\\)으로 택했을 때도 역시 균일 최강력검정이 됨을 보이기\nanswer\n\\(H_0:\\theta \\ge \\theta_0\\) 귀무가설이 복합가설일 때\n\\(\\alpha = {max}_{\\theta\\ge\\theta_0} \\Pi (\\theta)\\) 검정력 함수의 maximum 이다.\n\\(= {max}_{\\theta\\ge\\theta_0} P(\\text{Reject } H_0|\\theta)\\)\n\\(= {max}_{\\theta\\ge\\theta_0} P((x_1, \\dots, x_n) \\in x| \\theta)\\)\n\\(= {max}_{\\theta\\ge\\theta_0} P(x_{(n)} \\le \\theta_0 \\alpha^{\\frac{1}{n}}|\\theta)\\)\n\\(\\star\\)\n\\(\\Pi(\\theta) = P(X_{(n)} \\le c)\\)\n\\(= P(x_{(n)} \\le \\alpha^{\\frac{1}{n}} \\theta_0 | \\theta) = \\int^{\\alpha^{1/n}\\theta_0}_{-\\infty} f_n (x:\\theta) dx\\)\n\\(= \\int^{\\alpha^{1/n}\\theta_0}_{-\\infty} n (\\frac{x}{\\theta})^{n-1} \\frac{1}{\\theta} I (0<x< \\theta)\\)\n\\(= \\int^{\\alpha^{q/n}\\theta_0}_0 n (\\frac{x}{\\theta})^{n-1} \\frac{1}{\\theta} dx = (\\frac{\\alpha^{\\frac{1}{n}}\\theta_0}{\\theta})^n = \\alpha(\\frac{\\theta_0}{\\theta})^n\\)\n\\(\\star\\)\n\\(\\therefore \\alpha = {max}_{\\theta \\ge \\theta_0}\\Pi(\\theta) = {max}_{\\theta \\ge \\theta_0} P(X_{(n)} \\le \\theta_0 \\alpha ^{\\frac{1}{n}}| \\theta)\\)\n\\(= {max}_{\\theta \\ge \\theta_0} \\alpha(\\frac{\\theta_0}{\\theta})^n = \\alpha \\to \\theta\\)가 작을 수록 크다., \\(\\theta_0\\)일 때 \\(\\alpha\\)가 크다.\n복합가설일 때도 균일 최강력 기각역이 된다."
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-14",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-14",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n최강력 검정의 기각역을 구하라.\nanswer\nNeyman-Pearson 정리 \\(c = \\{ (x_1,\\dots , x_n) : \\frac{L(\\theta_0)}{L(\\theta_1)} \\le k\\}\\)\n\\(f(x:\\theta) = (\\theta + 1) x^{\\theta} I (0\\le x \\le 1)\\)\n\\(L(\\theta) = \\Pi^n_{i=1}f(x_i : \\theta) = (\\theta+1)^n \\Pi^n_{i=1}x_i^{\\theta} I(0\\le x_i \\le 1)\\)\n\\(\\frac{L(\\theta_0)}{L(\\theta_1)} = \\frac{(\\theta_0 + 1)^n(\\Pi^n_{i=1}x_i)^{\\theta_0} \\Pi^n_{i=1}I(0<x_i <1)}{(\\theta_1+1)^n(\\Pi^n_{i=1}x_i)^{\\theta_1}\\Pi^n_{i=1}I(0<x_i<1)}\\)\n\\(\\star \\uparrow \\Pi^n_{i=1}I(0<x_i<1) \\to \\theta\\)랑 무관한 영역이라 무시\n\\((\\Pi^n_{i=1}x_i)^{\\theta_0 - \\theta_1} \\le k\\)\n\\(log(\\theta_0 - \\theta_1)\\sum^n_{i=1} x_i \\le k\\)\n\\((\\theta_0 - \\theta_1) \\sum^n_{i=1} x_i \\le k\\)\n\\(\\star \\theta_1 > \\theta_0 \\to \\theta_0 - \\theta_1 < 0 \\therefore\\) 부등호 방향 바뀜\n\\(c = \\{ (x_1,\\dots, x_n:\\sum^n_{i=1}log x_i \\ge k \\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-15",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-15",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n(1)에서 구한 검정이 대립가설 \\(\\theta>\\theta_0\\)에 대하여 균일최강력 검정이 되는지를 밝혀라.\nanswer\n기각역 \\(\\sum^n_{i=1} x_i \\ge k\\)는 \\(\\theta_1\\)과 무관하다.\n\\(\\theta > \\theta_0\\)에 대해 \\(\\sum^n_{i=1} x_i \\ge k\\)는 최강력 기각역이다.\n\\(H_0 : \\theta = \\theta_0\\) vs \\(H_1 : \\theta > \\theta_0\\)의 균일 최강력 기각역이 될 수 있다."
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-18",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-18",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n최강력 검정의 기각역이 \\(\\{(x_1,\\dots,x_n:x_{(n)}>1,\\) 또는 \\(\\sum^n_{i=1} x_i \\le c\\}\\)임을 보여라.\nanswer\n\\(\\star\\) 모집단을 한가지로 정의 가능해서 단순가설 \\(\\to\\) 네이만 피어슨 정리 사용 가능\n\\(LR = \\frac{L_0}{L_1} = \\frac{\\Pi^n_{i=1}f_0(x_i)}{\\Pi^n_{i=1}f_1(x_i)} = \\frac{\\Pi^n_{i=1}I(0<x_i<1)}{\\Pi^n_{i=1}e^{-x_i}I(x_i>0)}\\le k\\)\n\\(= \\frac{I(0<x_{(1)}<x_{(n)}<1)}{e^{-\\sum^n_{i=1}x_i}I(x_{(1)}>0)}\\)\n\\(= \\frac{I(0<x_{(1)} <1)I(0<x_{(n)}<1)}{e^{-\\sum^n_{i=1}x_i}I(x_{(1)}>0)}\\)\n귀무가설을 기각할 기각역 \\(\\to e^{\\sum^n_{i=1}x_i}I(0<x_{(n)}<1)\\le k\\)\n\\(\\star x_{(n)}>1\\)이면 \\(I(0<x_{(n)}<1) \\to 0\\)\n\\(\\star x_{(n)}<1\\)이면 \\(I(0<x_{(n)}<1) \\to 1\\)\n\\(\\star \\sum^n_{i=1}x_i \\le k\\)\n\\(\\therefore \\{ x_{(n)}>1 \\text{ or } \\sum^n_{i=1}x_i \\le k \\}\\)이 기각역"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-19",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-19",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n유의수준이 \\(\\alpha = 0.05\\)이고 \\(n=50\\)일 때 중심극한정리를 이용하여 \\(c\\)의 값을 구하라.\nanswer\n\\(P(x_{(n)}>1 \\text{ or } \\sum^n_{i=1}x_i \\le c | H_0 ) = 0.05\\)\n\\(H_0\\)는 \\(f(x) = I(0<x<1)\\)이었다. 즉, \\(x\\)는 \\(1\\)보다 클 수 없고, 따라서 \\(x_{(n)}\\)도 \\(1\\)보다 클 수 없다.\n\\(P(x_{(n)} >1 | H_0) = 0\\)\n\\(P(\\sum^n_{i=1}x_i \\le c|H_0)\\)\n\\(\\star\\)\n\\(n=50\\), \\(x\\)가 균등분포\n균등분포 평균, 분산 이용한 중심극한정리 \\(\\to \\frac{\\bar{x} - \\frac{1}{2}}{\\sqrt{1/12}/\\sqrt{50}} \\sim N(0,1)\\)\n\\(\\star\\)\n\\(P(\\frac{\\sum^n_{i=1}x_i}{50} = \\bar{x} \\le \\frac{c}{50}|H_0)\\)\n\\(-z_{0.05} = \\frac{c/50 - 1/2}{1/\\sqrt{600}}\\)\n\\(-z_{0.05} = -1.645\\), \\(c=21.6422\\)\n\n\n\nimage.png\n\n\n\nqnorm(0.95)\n\n1.64485362695147\n\n\n\n((-qnorm(0.95)) * 1/sqrt(600) + 1/2 ) * 50\n\n21.6424565936689\n\n\n\\(c = \\{ \\sum^{50}_{i=1}x_i \\le 21.6422 \\}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-21",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-21",
    "title": "Theoritical Statistics HW9",
    "section": "(1)",
    "text": "(1)\n모분산 \\(\\sigma^2\\)가 알려져 있지 않은 경우 \\(H_0:\\mu \\le \\mu_0 \\text{ vs } H_1 : \\mu > \\mu_0\\)에 대한 유의수준 \\(\\alpha\\)인 일반화 가능도비 검정법을 구하라.\nanswer\n모평균 가설을 검정할때 \\(\\sigma^2\\)이 알려져 있다면, 균일 최강력 검정이 존재한다. 하지만 알려져 있지 않다. \\(\\to\\) 장애요소\n\\(\\Lambda(x_1,\\dots,x_n) = \\frac{L(\\hat{\\mu}_0,\\hat{\\sigma}_0^2)}{L(\\hat{\\mu},\\hat{\\sigma}^2)}\\) 귀무가설 하 모수가능도 최대치\n\\(L(\\mu, \\hat{\\sigma}^2) = (\\frac{1}{2\\pi \\sigma^2})^{\\frac{n}{2}}e^{-\\frac{\\sum^n_{i=1}(x_i - \\mu)^2}{2\\sigma^2}}\\)\n\\({max}_{\\Omega_0} L(\\mu,\\sigma^2)\\) 최대는 \\(\\bar{x}\\)이긴 하지만, 귀무가설 하 \\(\\mu \\le \\mu_0\\)이라는 제약이 있어서 이 조건이 필요하다.\n최대가능도 추정량\n\n\\(\\Omega_0 = \\{ (\\mu,\\sigma^2):\\mu \\le \\mu_0, \\sigma^2>0\\}\\)\n\n\\(\\hat{\\mu}_0 = {min}(\\bar{x},\\mu_0)\\)\n\\(\\hat{\\sigma}^2_0 = \\frac{1}{n}\\sum^n_{i=1}{x_i}(x_i - \\hat{\\mu}_0)^2\\)\n\n\\(\\Omega = \\{(\\mu,\\sigma^2) : -\\infty \\le \\mu \\le \\infty, \\sigma^2 > 0\\}\\)\n\n\\(\\hat{\\mu} = \\bar{x}\\)\n\\(\\hat{\\sigma}^2 = \\frac{1}{n} \\sum^n_{i=1}(x_i- \\bar{x})^2\\)\n\n\n\\(\\Lambda = \\frac{(\\frac{1}{2\\pi\\hat{\\sigma}^2_0})^{\\frac{n}{2}}e^{-\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu}_0)^2}{2\\hat{\\sigma}^2_0}}}{(\\frac{1}{2\\pi\\hat{\\sigma}^2})^{\\frac{n}{2}}e^{-\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu})^2}{2\\hat{\\sigma}^2}}} \\le k\\)\n\\(\\uparrow\\) 각 분자, 분모의 \\(exp\\)에 있는 \\(\\hat{\\sigma}\\)들에 각 값을 대입하면\n\n\\(\\hat{\\sigma}_0\\)대입하면 \\(\\to -\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu}_0)^2}{2\\frac{1}{n}\\sum^n_{i=1}(x_i - \\hat{\\mu}_0)^2} = \\frac{n}{2}\\)\n\\(\\hat{\\sigma}\\) 대입하면 \\(\\to -\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu})^2}{2\\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar{x})}^2 = -\\frac{n}{2}\\)\n\n\\(\\to (\\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2_0})^{\\frac{n}{2}} \\le k\\)\n$ k $\n대입하면 \\(\\frac{\\sum^n_{i=1}(x_i - \\bar{X})^2}{\\sum^n_{i=1}(x_i - \\hat{\\mu}_0)^2} \\le k\\)\n\\(\\hat{\\mu}_o = {min}(\\bar{X}, \\mu_0)\\)\n\n\\(\\bar{x} < \\mu_0 \\to \\hat{\\mu}_0 = \\bar{x} \\to 1 \\le k\\)\n\n\n\\(k\\)는 1보다 작아야 한다.(일반화 가능도비 기본 성질)\n\\(\\therefore\\) 성립하지 않는다.\n\n\n\\(\\bar{x} > \\mu_0 \\to \\frac{\\sum(x_i - \\bar{x})^2}{\\sum(x_i - \\mu_0)^2} \\le k\\)\n\n\\(c = \\{ (x_1,\\dots,x_n)|\\bar{x} > \\mu_0, \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{\\sum^n_{i=1}(x_i - \\mu_0)^2} \\le k\\)\n\\(\\sum^n_{i=1}(x_i - \\mu_0)^2 = \\sum^n_{i=1}(x_i - \\bar{x} + \\bar{x} - \\mu_0)^2\\)\n\\(= \\sum^n_{i=1}(x_i - \\bar{x})^2 + 2(\\bar{x}-\\mu_0)\\sum(x_i - \\bar{x}) + n(\\bar{x} -\\mu_0)^2\\)\n\\(\\star \\sum(x_i - \\bar{x}) = 0\\)\n\\(= \\sum^n_{i=1}(x_i - \\bar{x})^2 + n(\\bar{x} - \\mu_0)^2\\)\n\\(c = \\{ (x_1,\\dots,x_n)|\\bar{x} > \\mu_0, \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2 + n(\\bar{x} - \\mu_0)^2} \\le k\\)\n\\(\\to \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2 + n(\\bar{x} - \\mu_0)^2} \\le k\\)\n\\(\\to \\frac{1}{1+\\frac{n(\\bar{x} - \\mu_0)^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}} \\le k\\)\n\\(\\to \\frac{n(\\bar{x} - \\mu_0)^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\ge k\\)\n\\(c = \\{ (x_1,\\dots,x_n)|\\bar{x} > \\mu_0, \\frac{n(\\bar{x} - \\mu_0)^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\ge k\\)\n\\(\\to \\sum^n_{i=1}(x_i - \\bar{x})^2 = (n-1)S^2\\)\n\\(\\to \\frac{n(\\bar{x} - \\mu_0)^2}{(n-1)S^2} = \\frac{n(\\bar{x} - \\mu_0)^2}{S^2} \\ge k\\)\n\\(\\star N(\\mu, \\sigma^2) \\to \\frac{\\bar{x} -\\mu}{\\sigma/\\sqrt{n}} \\to \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t_{(n-1)}\\)\n\\(c = \\{ (x_1,\\dots, x_n) | \\bar{x} > \\mu_0, |\\frac{\\sqrt{n}(\\bar{x}-\\mu_0)}{S}| \\ge k \\}\\)\n\\(= \\{ (x_1,\\dots, x_n) | \\bar{x} > \\mu_0, \\text{ and } (\\bar{x} > \\mu_0 + \\frac{S}{\\sqrt{n}}k \\text{ or } \\bar{x} \\le \\mu_0 - \\frac{X}{\\sqrt{n}k})\\}\\)\n\\(= \\{ (x_1,\\dots, x_n) | \\bar{x} > \\mu_0 + \\frac{S}{\\sqrt{n}}k\\}\\)\n\\(\\alpha = {max}_{\\Omega_0} \\Pi(\\mu)\\)\n\\(\\Pi(\\mu) = P((x_1,\\dots,x_n) \\in c | \\mu)\\)\n\\(= P(\\bar{x} \\ge \\mu_0 + \\frac{S}{\\sqrt{n}}k | \\mu)\\)\n\\(= P(\\frac{\\bar{x}-\\mu}{S/\\sqrt{n}} \\ge \\frac{\\mu_0 - \\mu}{S/\\sqrt{n}} + k | \\mu)\\)\n\\(= P(t \\ge \\frac{\\mu_0 - \\mu}{S/\\sqrt{n}}+k) \\to \\mu\\)가 커질수록 작아진다 \\(\\to \\mu\\)에 대해 증가 함수.\n\\(\\therefore \\alpha = {max}_{\\mu \\le \\mu_0} \\Pi(\\mu) = \\Pi(\\mu_0) = P(t \\ge 0 + k) = t_{\\alpha(n-1)}\\)\n\\(\\therefore c = \\{ (x_1,\\dots,x_n) | \\bar{x} \\ge \\mu_0 + \\frac{S}{\\sqrt{n}} t_{\\alpha(n-1)}\\}\\)\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/ts/2023-01-21-ts-HW9.html#section-22",
    "href": "posts/ts/2023-01-21-ts-HW9.html#section-22",
    "title": "Theoritical Statistics HW9",
    "section": "(2)",
    "text": "(2)\n모평균 \\(\\mu\\)가 알려져 있지 않은 경우 \\(H_0:\\sigma^2 = \\sigma^2_0 \\text{ vs }H_1 : \\sigma^2 > \\sigma^2_0\\)에 대한 유의수준 \\(\\alpha\\)인 일반화 가능도비 검정법을 구하라.\nanswer\n\\(\\mu\\)가 장애요소\n\\(L(\\mu, \\sigma^2( = (\\frac{1}{2\\pi \\sigma^2})^{\\frac{n}{2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}})\\)\n\\(\\Lambda = \\frac{L(\\hat{\\mu}_0,\\hat{\\sigma}^2_0)}{L(\\hat{\\mu},\\hat{\\sigma}^2)}\\)\n\n\\(\\Omega_0 = \\{ (\\mu,\\sigma^2) : -\\infty < \\mu < \\infty , \\sigma^2 = \\sigma^2_0\\}\\)\n\n\\(\\hat{\\mu}_0 = {argmax}_{(\\mu,\\sigma^2)\\in\\Omega_0} L(\\mu,\\sigma^2) = \\bar{x}\\)\n\\(\\hat{\\sigma}^2_0 = \\sigma^2_0\\)\n\n\\(\\Omega = \\{ (\\mu, \\sigma^2) : -\\infty < \\mu < \\infty , \\sigma^2 \\ge \\sigma^2_0, \\sigma^2 > 0 \\}\\)\n\n\\(\\hat{\\mu} = \\bar{x}\\)\n\\(\\hat{\\sigma}^2 = {max}(\\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar{x})^2, \\sigma^2_0)\\)\n\n왼쪽은 \\(\\sigma^2\\)보다 클때, 오른쪽은 \\(\\sigma^2\\)보다 작을때\n\n\n\n\\(\\Lambda = \\frac{L(\\hat{\\mu}_0, \\hat{\\sigma}^2_0)}{L(\\hat{\\mu},\\hat{\\sigma}^2)}= \\frac{(\\frac{1}{2\\pi\\hat{\\sigma}^2_0})^{\\frac{n}{2}} e^{-\\frac{\\sum(x_i - \\bar{x})^2}{2\\hat{\\sigma}^2_0}}}{(\\frac{1}{2\\pi\\hat{\\sigma}^2})^{\\frac{n}{2}} e^{-\\frac{\\sum(x_i - \\bar{x})^2}{2\\hat{\\sigma}^2}}}\\)\n\\(\\to (\\frac{1}{2\\pi\\hat{\\sigma}^2_0}) = \\frac{1}{2\\pi \\sigma^2}\\)\n\\(\\to -\\frac{\\sum(x_i - \\bar{x})^2}{2\\hat{\\sigma}^2_0} = -\\frac{\\sum(x_i - \\bar{x})^2}{2\\sigma^2_0}\\)\n\\(= (\\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2_0})^{\\frac{n}{2}} e^{-\\frac{1}{2}\\sum^n_{i=1}(x_i - \\bar{x})^2 (\\frac{\\hat{\\sigma}^2 - \\sigma^2_0}{\\sigma^2_0\\hat{\\sigma}^2})} \\le k\\)\n\n\\(\\frac{1}{n} \\sum^n_{i=1}(x_i - \\bar{x})^2 < \\sigma^2_0 \\to \\sigma^2_0 \\to \\Lambda = 1\\)\n\n\n\\(k=1\\)이 되어 의미가 없어짐\n\n\n\\(\\frac{1}{n}\\sum^n_{i=1}(x_i-\\bar{x})^2 > \\sigma^2_0 \\to \\hat{\\sigma}^2 = \\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar{x})^2\\)\n\n\n예 5.24에서 다룬 식, \\(\\Lambda\\)는 \\(\\frac{\\hat{\\sigma}}{\\sigma^2_0}\\)의 감소함수이다.\n\n기각역 : \\(\\frac{1}{n}\\sum(x_i - \\bar{x})^2 \\ge k\\)\n\\(\\alpha = P(\\sum^n_{i=1}(x_i - \\bar{x})^2 \\ge k | \\sigma^2 = \\sigma^2_0)\\)\n\\(\\star \\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}\\)\n\\(= P(\\frac{\\sum^n_{i=1}(x_i - \\bar{x})^2}{\\sigma^2_0} \\ge \\frac{k}{\\sigma^2_0} | \\sigma^2 = \\sigma^2_0)\\)\n\\(\\frac{k}{\\sigma^2_0} = \\chi^2_{0.05(n-1)}\\)\n\\(k = \\sigma^2_0 \\chi^2_{0.05(n-1)}\\)\n\n\n\nimage.png\n\n\n\\(c = \\{ (x_1,\\dots,x_n):\\sum^n_{i=1}(x_i - \\bar{x})^2 \\ge \\sigma^2_0 \\chi^2_{0.05(n-1)}\\)\n유의수준 \\(\\alpha\\) 일반화 가능도비 검정법의 기각역이다."
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html",
    "href": "posts/ts/2023-01-05-ts_HW3.html",
    "title": "Theoritical Statistics HW3",
    "section": "",
    "text": "자주 사용되는 확률분포"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#수업-과제-1.",
    "href": "posts/ts/2023-01-05-ts_HW3.html#수업-과제-1.",
    "title": "Theoritical Statistics HW3",
    "section": "수업 과제 1.",
    "text": "수업 과제 1.\n기하분포: \\(X \\sim Geometric(p)\\)\n\\(X=\\)서로 독립인 베르누이 시행을 반복할 때 첫번째 성공이 나올 때까지 시행횟수\n\\[f(x) = (1-p)^{x-1}p, x=1,2,\\dots\\]\n\n\\(\\sum^{\\infty}_{x=1}(1-p)^{x-1}p = 1\\)\n아래 증명\n\\(M(t) = \\frac{pe^t}{1-qe^t} \\text{ for } t< - ln q\\)\n\\(E(X) = \\frac{1}{p}, Var(X) = \\frac{q}{p^2}\\)\n\nanswer\n\\(M(t) = E(e^{tx}) = \\sum^{\\infty}_{x=1} e^{tx} (1-p)^{x-1}p\\)\n\\(= \\frac{p}{1-p}\\sum^{\\infty}_{x=1}\\{e^t(1-p)\\}^x\\)\n\\(= \\frac{p}{1-p}\\frac{e^t - pe^t}{1-e^t + pe^t}\\)\n\\(= \\frac{pe^t}{1-e^t+pe^t}\\)\n\\(M^{'}_X(t) = \\frac{pe^t(1-e^t + pe^t) - pe^t(-e^t + pe^t)}{(1-e^t + pe^t)^2} = \\frac{pe^t}{(1-e^t+pe^t)^2}\\)\n\\(M^{'}_X(0) = E(X) = \\frac{1}{p}\\)\n\\(M^{''}_X(t) = pe^t(1-e^t+pe^t)^{-2}+pe^t(-2)(1-e^t+pe^t)^{-3}(-e^t+pe^t)\\)\n\\(= pe^t(1-e^t+pe^t)^{-3}(1-e^t+pe^t-2(-e^t+pe^t))\\)\n\\(= pe^t(1-e^t+pe^t)^{-3}(1+e^t-pe^t)\\)\n\\(M^{''}_X(0) = E(X^2)=p(1-1+p)^{-3}(1+1-p)=\\frac{2-p}{p^2} = \\frac{1+q}{p^2}\\)\n\\(var(X) = E(X^2) - (E(X))^2 = \\frac{1+q}{p^2} - \\frac{1}{p^2} = \\frac{q}{p^2}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#수업과제-2.",
    "href": "posts/ts/2023-01-05-ts_HW3.html#수업과제-2.",
    "title": "Theoritical Statistics HW3",
    "section": "수업과제 2.",
    "text": "수업과제 2.\n\\(\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\) 증명\n\\(\\Gamma(k) = \\int^{\\infty}_{0} t^{k-1} e^{-t} dt\\)\nanswer\n\\(\\Gamma(\\frac{1}{2}) = \\int^{\\infty}_{0} x^{\\frac{1}{2} - 1} e^{-x} dx\\)\n변수 변환 \\(y = x^{\\frac{1}{2}}\\), \\(x = y^2\\), \\(dx = 2y dy\\), \\(y>0\\)\n\\(= \\int^{\\infty}_0 \\frac{1}{y} e^{-y^2} 2y dy = 2 \\int^{\\infty}_0 e^{-y^2} dy\\)\n변수 변환 \\(u = r\\cos \\theta\\), \\(v=r \\sin \\theta\\), \\(J = \\begin{bmatrix} \\cos \\theta & r \\sin \\theta \\\\ \\sin \\theta & r \\cos \\theta \\end{bmatrix} = r\\),\n\\(0<u<\\infty \\to 0<r<\\infty\\),\n\\(0<v<\\infty \\to 0< \\theta < \\frac{\\pi}{2}\\)\n\\(\\Gamma \\{ (\\frac{1}{2}) \\}^2 = 4\\int^{\\infty}_0 e^{-u^2} du \\int^{\\infty}_0 e^{-v^2} dv = 4\\int^{\\infty}_0 \\int^{\\infty}_0 e^{-(u^2 + v^2)} du dv\\)\n\\(= 4\\int^{\\infty}_0 \\int^{\\frac{\\pi}{2}}_0 e^{-r^2} r d \\theta dr\\)\n\\(= 4 \\frac{\\pi}{2} \\int^{\\infty}_0 e^{-r^2} r dr = 2\\pi \\frac{1}{2} e^{-r^2} |^{\\infty}_0 = \\pi\\)\n\\(\\to \\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#수업-과제-3.",
    "href": "posts/ts/2023-01-05-ts_HW3.html#수업-과제-3.",
    "title": "Theoritical Statistics HW3",
    "section": "수업 과제 3.",
    "text": "수업 과제 3.\n\\(P(X \\le t) = 1-P(X>t) = 1-P(n\\text{번째 사건이 발생하는 시간}>t) = 1-P([0,t) \\text{사이에 발생하는 사건의 수} \\le n-1)\\)\nanswer\ncdf : \\(= 1-\\sum^{n-1}_{y=0} \\frac{e^{-\\lambda t}(\\lambda t)^y} {y!}\\)\n\\(f(x) = \\frac{1}{\\lambda} exp(-\\frac{x}{\\lambda})\\)\npdf : \\(=(1-\\sum^{n-1}_{y=0}\\frac{e^{-\\lambda t} (\\lambda t)^y}{y!} )\\frac{d}{dy}\\)\n\\(= \\frac{d}{dy}(1-e^{-\\lambda t} -\\sum^{n-1}_{y=1} \\frac{e^{-\\lambda t}(\\lambda t)^y}{y!})\\)\n\\(= \\lambda e^{-\\lambda t} - \\sum^{n-1}_{y=1} \\frac{1}{y!} (t(\\lambda t)^{y-1} \\lambda e^{-\\lambda t} - \\lambda (\\lambda t)^y e^{-\\lambda t})\\)\n\\(= \\lambda e^{-\\lambda t} - \\lambda e^{-\\lambda t} \\sum^{n-1}_{y=1} \\frac{1}{y!} (y (\\lambda t)^{y-1} - (\\lambda t)^y)\\)\n\\(= \\lambda e^{-\\lambda t} + \\lambda e^{-\\lambda t} \\sum^{n-1}_{y=1} (\\frac{(\\lambda t)^{y-1}}{(y-1)!} - \\frac{(\\lambda t)^{y-1}}{(y-1)!})\\)\n\\(= \\lambda e^{-\\lambda t} + \\lambda e^{-\\lambda t} (\\lambda t - 1 + \\frac{\\lambda t^2}{2!} - \\lambda t + \\dots \\frac{\\lambda t^2}{3!} - \\frac{\\lambda t^2}{2!} + \\dots + \\frac{(\\lambda t)^{y-1}}{(y-1)!} - \\frac{(\\lambda t)^{n-2}}{(y-2)!}\\)\n\\(= \\lambda e^{-\\lambda t} - \\lambda e^{-\\lambda t}(\\frac{(\\lambda t)^{y-1}}{(y-1)!} - 1)\\)\n\\(= \\frac{\\lambda}{\\Gamma(y)}(\\lambda t)^{y-1} e^{-\\lambda t} \\dots\\)\n\\(\\star \\Gamma(t) = (y-1)!\\)\n\\(= \\frac{t^{y-1} e^{-\\lambda t}}{\\lambda^{-y} \\Gamma(y)}\\)\n\\(y\\)에 \\(\\alpha\\)를 놓으면\n\\(\\therefore f(x) = \\frac{t^{\\alpha -1}e^{-\\lambda t}}{\\lambda^{-\\alpha}\\Gamma(\\alpha)}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#수업과제-4.",
    "href": "posts/ts/2023-01-05-ts_HW3.html#수업과제-4.",
    "title": "Theoritical Statistics HW3",
    "section": "수업과제 4.",
    "text": "수업과제 4.\n\\(E(S^2) = \\sigma^2\\) 증명\nanswer\n표본분산 \\(S^2 = \\frac{1}{n-1}\\sum^n_{i=1} (X_i - \\bar{X})^2\\)\n\\(E(S^2) = E(\\frac{1}{n-1} \\sum^n_{i=1}(X_i - \\bar{X})^2)\\)\n\\(= \\frac{1}{n-1} E(\\sum^n_{i=1}(X_i - \\bar{X})^2)\\)\n\\(\\star\\)\n\\(\\sum^n_{i=1}(X_i - \\mu)^2 = \\sum^n_{i=1}(X_i - \\bar{X}_n + \\bar{X}_n -\\mu)^2\\)\n\\(\\sum^n_{i=1}[(X_i - \\bar{X}_n)^2 + 2(X_i - \\bar{X}_n)(\\bar{X}_n - \\mu) + (\\bar{X}_n - \\mu)^2]\\)\n\\(\\sum^n_{i=1}(X_i - \\bar{X}_n)^2 + n(\\bar{X}_n - \\mu)^2\\)\n\\(\\star\\)\n\\(= \\frac{1}{n-1} E(\\sum^n_{i=1}(X_i - \\mu)^2 - n(\\bar{X}_n - \\mu)^2)\\)\n\\(= \\frac{1}{n-1}(\\sum^n_{i=1} E(X_i-\\mu)^2 - nE(\\bar{X}_n - \\mu)^2)\\)\n\\(= \\frac{1}{n-1}(n \\sigma^2 - n \\frac{\\sigma^2}{n})\\)\n\\(= \\sigma^2\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section",
    "title": "Theoritical Statistics HW3",
    "section": "19.",
    "text": "19.\n확률변수 \\(X\\)의 확률밀도함수가 \\(f_X(x) = 2x,0<x<1\\)일 때\n\n(1)\n\\(Y = 3X+2\\)의 확률밀도함수를 구하라.\nanswer\n\\(g(X) = 3X+2\\)\n\\(g^{-1}(Y) = \\frac{Y-2}{3}\\)\n\\(f_Y(y) = f_X(g^{-1}(y))|\\frac{dg^{-1}(y)}{dy}| = f_X(\\frac{x-2}{3}) |\\frac{1}{3}|\\)\n\\(= \\frac{2}{3}\\times \\frac{y-2}{3} = \\frac{2y-4}{9}, 2<y<5\\)\n\n\n(2)\n\\(Y = X^2\\)의 확률밀도함수를 구하라.\nanswer\n\\(g(X) = X^2\\)\nas \\(x\\) is greater than 0, \\(g^{-1}(Y) = \\sqrt{y}\\)\n\\(= f_Y(y) = f_X(g^{-1}(y))\\frac{dg^{-1}(y)}{dy}\\)\n\\(= f_X(\\sqrt{y})\\frac{1}{2\\sqrt{y}}\\)\n\\(= 2\\sqrt{y}\\frac{1}{2\\sqrt{y}}\\)\n\\(= I(0<y<1)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-3",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-3",
    "title": "Theoritical Statistics HW3",
    "section": "20.",
    "text": "20.\n확률변수 \\(X\\)의 확률밀도함수가 \\(f_X(x) = 3e^{-3x},x>0\\)일 때\n\n(1)\n\\(Y = 2X+5\\)의 확률밀도함수와 확률분포함수를 구하라.\nanswer\n\\(F_Y(y) = P(Y \\le y) = P(X \\le \\frac{y-5}{2}) = F_X(\\frac{y-5}{2})\\)\n\\(f_Y(y) = \\frac{d}{dy} F_y(y) = \\frac{1}{2}f_X(\\frac{y-5}{2}) = \\frac{3}{2}exp(-\\frac{3y-15}{2})I(y>5)\\)\n\n\n(2)\n\\(Y = \\frac{1}{X}\\)의 확률밀도함수와 확률분포함수를 구하라.\nanswer\n\\(F_Y(y) = P(Y \\le y) = P(X \\le \\frac{1}{y}) = F_X(\\frac{1}{y})\\)\n\\(f_y(y) = \\frac{d}{dy}F_y(y) = \\frac{1}{y^2}f_X(\\frac{1}{y}) = \\frac{3}{y^2}exp(-\\frac{3}{y})I(y>0)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-6",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-6",
    "title": "Theoritical Statistics HW3",
    "section": "42.",
    "text": "42.\n\\(X=B(n,p)\\)일 때 \\(Y = n-X\\)의 분포를 구하라.\nanswer\n\\(M_X(t) = E(e^{tx})=(1-p+pe^t)^n\\)\n\\(M_Y(t) = E(e^{ty}) = E(e^{(n-x)t}) = \\sum^n_{x=0}e^{(n-x)t}\\begin{pmatrix} n \\\\ n-x \\end{pmatrix} p^{n-x}(1-p)^x\\)\n\\(= \\sum^n_{x=0} e^{nt}\\begin{pmatrix} n \\\\ n-x \\end{pmatrix}(pe^{-t})^x(1-p)^x\\)\n\\(= e^{nt}(1-p+pe^{-t})^n \\to\\) mgf of \\(B(n,1-p)\\)\n\\(\\therefore n-x \\sim B(n,1-p)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-7",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-7",
    "title": "Theoritical Statistics HW3",
    "section": "46.",
    "text": "46.\n\\(X \\sim POI(\\lambda)\\)일 때 \\(E(1+X)^2\\)을 계산하라.\nanswer\n\\(E(X) = \\lambda\\)\n\\(var(X) = \\lambda = E(X^2) - \\{E(X)\\}^2\\)\n\\(E(X^2) = var(X) + \\{E(X)\\}^2 = \\lambda + \\lambda^2\\)\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{X!}\\)\n\\(E(1+X) = 1+E(X) = 1+\\lambda\\)\n\\(E(1+X)^2 = E(1+2X + X^2) = 1+2E(X) + E(X^2) = 1+2\\lambda + \\lambda + \\lambda^2 = 1+ 3\\lambda + \\lambda^2\\)\n\\(E(1+X)^2 = \\sum^{\\infty}_{x=0}(1+2x+ x^2)\\frac{e^{-\\lambda}\\lambda^x}{x!}\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-8",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-8",
    "title": "Theoritical Statistics HW3",
    "section": "52.",
    "text": "52.\n어느 전자제품의 수명이 확률변수 \\(X \\sim EXP(100)\\)라고 할 때,\n\\(\\lambda = \\frac{1}{100}\\)\n\\(E(X) = 100\\)\n\\(var(X) = 100^2\\)\n\n(1)\n\\(P(X>30)\\)을 구하라.\nanswer\n\\(P(X>30) = \\int^{\\infty}_{-\\infty} \\lambda e^{-\\lambda x} dx = \\int^{\\infty}_{30} \\frac{1}{100}e^{-\\frac{x}{100}}dx = [-e^{-\\frac{\\lambda}{100}}]^{\\infty}_{30} = e^{-0.3}\\)\n\n\n(2)\n\\(P(X>110)\\)을 구하라.\nanswer\n\\(P(X>110) = \\int^{\\infty}_{110} \\frac{1}{100} e^{-\\frac{x}{100}}dx = [-e^{-\\frac{\\lambda}{100}}]^{\\infty}_{110} = e^{-1.1}\\)\n\n\n(3)\n\\(P(X>110|X>80)\\)을 구하고 (1)의 결과와 비교하라.\nanswer\n\\(P(X>110|X>80) = \\frac{P(X>110)}{P(X>80)} = \\frac{e^{-1.1}}{e^{-0.8}} = e^{-1.1+0.8} = e^{-0.3}\\)\n(1)의 결과와 (3)의 결과가 같다."
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-12",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-12",
    "title": "Theoritical Statistics HW3",
    "section": "54.",
    "text": "54.\n\\(X \\sim N(\\mu, \\sigma^2)\\)일 때,\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n(1)\n\\(Y = |X-\\mu|\\)의 확률밀도함수를 구하라.\nanswer\n\\(F_Y(y) = P(Y \\le y) = P(|X-\\mu| \\le y) = P(\\mu-y \\le X \\le \\mu+y) = F_X(\\mu + y) - F_X(\\mu - y)\\)\n\\(f_y(y) = \\frac{d}{dy}\\{F_X(\\mu + y) - F_X(\\mu - y) \\} = f_X (\\mu + y) + f_X(\\mu - y) = \\frac{2}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{y^2}{2\\sigma^2}) I(y>0)\\)\n\n\n(2)\n\\(Y = exp(X)\\)의 확률밀도함수를 구하라.(이를 로그 정규확률밀도함수라고 한다.)\nanswer\n\\(Y = e^X\\)\n\\(g(y) = e^X\\)\n\\(g^{-1}(y) = ln(y)\\)\n\\((g^{-1}(y))^{' }= \\frac{1}{y}\\)\n\\(f_y(y) = f_X(g^{-1}(y))(\\frac{d g^{-1}(y) }{dy} ) I(y>0) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(lny - \\mu)^2}{2\\sigma^2}} \\times \\frac{1}{y}\\)\n\\(= \\frac{1}{y\\sigma\\sqrt{2\\pi}}exp(-\\frac{(lny - \\mu)^2}{2\\sigma^2})I(y>0)\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-15",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-15",
    "title": "Theoritical Statistics HW3",
    "section": "65.",
    "text": "65.\n확률변수들 \\(X_i(i=1,2,\\dots,k)\\)는 서로 독립이며 \\(B(n,p)\\)분포를 따른다고 하자,\n이떄 \\(\\sum^k_{i=1}X_i\\)의 적률생성함수를 구하고 그의 분포가 \\(B(nk,p)\\)임을 보여라.\nanswer\n\\(M_X(t) = E(e^{tx}) = (e^t + q)^n\\)\n\\(M_{\\sum^k_{i=1}X_i}(t) = E(e^{tX_1 + tX_2 + \\dots+tX_k}) = E(e^{tX_1}e^{tX_2}\\dots e^{tX_k})\\)\nBecause those are independent, \\(= E(e^{tX_1})E(e^{tX_2})\\dots E(e^{tX_k})\\)\n\\(= (pe^{tx} + q)^n(pe^t+q)^n \\dots (pe^t+q)^n = (pe^{tx} + q)^{kn}\\)\n\\(M_{\\sum^k_{i=1}X_i}^{'}(t) = kn(pe^{tx}+q)^{kn-1}pe^{tx}\\)\n\\(M_{\\sum^k_{i=1}X_i}^{'}(0) = kn(p+q)^{kn-1}p = knp = E(\\sum^k_{i=1}X_i)\\)\n\\(M_{\\sum^k_{i=1}X_i}^{''}(t) = kn(kn-1)(pe^{tx}+q)^{kn-2}p^2e^{2tx} + kn(pe^{tx} + 1)^{kn-1}pe^{tx}\\)\n\\(M_{\\sum^k_{i=1}X_i}^{''}(0) = kn(kn-1)p^2 + knp = k^2n^2p^2 - knp^2 + knp\\)\n\\(var(\\sum^k_{i=1}X_i) = k^2n^2p^2 - knp^2 + knp - k^2n^2p^2 = knp(1-p) = knpq\\)"
  },
  {
    "objectID": "posts/ts/2023-01-05-ts_HW3.html#section-16",
    "href": "posts/ts/2023-01-05-ts_HW3.html#section-16",
    "title": "Theoritical Statistics HW3",
    "section": "68.",
    "text": "68.\n확률변수 \\(X\\)가 \\(GAM(k,\\theta)\\)를 따르면 임의의 상수 \\(c\\)에 대해 \\(cX\\)는 \\(GAM(k,c\\theta)\\)를 따름을 보여라.\nanswer\n\\(E(X) = k \\theta\\), \\(var(X) = k \\theta^2\\), \\(f(x;k,\\theta) = \\frac{1}{\\theta^2 \\Gamma(k)} x^{k-1} e^{-\\frac{x}{\\theta}}\\)\n\\(E(cX) = ck\\theta\\), \\(var(cX) = c^2 var(X) = c^2k\\theta^2\\)\n\\(g(y) = \\frac{Y}{c}\\)\n\\(f_Y(y) = f_X(x) |\\frac{dx}{dy}|I(x>0)\\)\n\\(= f_X(g^{-1}(y)|\\frac{dg^{-1}(y)}{dy}|I(g^{-1}(y) >0)\\)\n\\(= \\frac{1}{\\theta^k \\Gamma(k)} (\\frac{y}{c})^{k-1} e^{-\\frac{x}{c\\theta}}\\frac{1}{c}\\)\n\\(= \\frac{1}{c^k\\theta^k \\Gamma(k)} y^{k-1}e^{-\\frac{x}{c\\theta}} \\sim GAM(k,c\\theta)\\)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, there :)\nLateX_refers"
  }
]