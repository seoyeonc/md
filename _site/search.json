[
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "",
    "text": "(5주차) 10월5일 [딥러닝의 기초 - 로지스틱(2), 깊은신경망(1)]"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#imports",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#imports",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\n시각화를 위한 준비함수들\n준비1 loss_fn을 plot하는 함수\n\ndef plot_loss(loss_fn,ax=None):\n    if ax==None:\n        fig = plt.figure()\n        ax=fig.add_subplot(1,1,1,projection='3d')\n        ax.elev=15;ax.azim=75\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) \n    ax.scatter(-1,5,l(-1,5),s=200,marker='*') # 실제로 -1,5에서 최소값을 가지는건 아님.. \n\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\) 에서 생성된 데이터 한정하여 손실함수가 그려지게 되어있음.\n\n준비2: for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는 함수\n\ndef learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    what_history = [] \n\n    for epoc in range(1000): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        optimizr.zero_grad() \n\n        ## record \n        if epoc % 20 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            what_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    return yhat_history, loss_history, what_history\n\n\n20에폭마다 yhat, loss, what을 기록\n\n준비3: 애니메이션을 만들어주는 함수\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\ndef show_lrpr2(net,loss_fn,optimizr,suptitle=''):\n    yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7,2.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ax1.set_xticks([]);ax1.set_yticks([])\n    ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([])\n    ax2.elev = 15; ax2.azim = 75\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,v,'--')\n    ax1.scatter(x,y,alpha=0.05)\n    line, = ax1.plot(x,yhat_history[0],'--') \n    plot_loss(loss_fn,ax2)\n    fig.suptitle(suptitle)\n    fig.tight_layout()\n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\n준비1에서 그려진 loss 함수위에, 준비2의 정보를 조합하여 애니메이션을 만들어주는 함수"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#logistic-intro-review-alpha",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#logistic-intro-review-alpha",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "Logistic intro (review + \\(\\alpha\\))",
    "text": "Logistic intro (review + \\(\\alpha\\))\n- 모델: \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n- toy example\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함\ny = torch.bernoulli(v) \n\n\nnote: \\((w_0,w_1)\\)의 true는 \\((-1,5)\\)이다. -> \\((\\hat{w}_0, \\hat{w}_1)\\)을 적당히 \\((-1,5)\\)근처로 추정하면 된다는 의미\n\n\nplt.scatter(x,y,alpha=0.05)\nplt.plot(x,v,'--r')\n\n\n\n\n- step1: yhat을 만들기\n(방법1)\n\nx.shape\n\ntorch.Size([2000, 1])\n\n\n뒤의 1이 input feature로서 입력\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)\na1 = torch.nn.Sigmoid() \nyhat = a1(l1(x))\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n\nl1.weight,l1.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n(방법2)\n\\(x \\overset{l1}{\\to} u \\overset{a1}{\\to} v = \\hat{y}\\)\n\\(x \\overset{net}{\\to} \\hat{y}\\)\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(1,1)\na1 = torch.nn.Sigmoid() \nnet = torch.nn.Sequential(l1,a1) \nyhat = net(x)\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n(방법3)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nyhat = net(x)\nyhat\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n중간과정보기 힘들다.\n\n\nlen(net)\n\n2\n\n\n\nnet[0]\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nnet[0](x)\n\ntensor([[-0.5003],\n        [-0.5007],\n        [-0.5010],\n        ...,\n        [-1.1930],\n        [-1.1934],\n        [-1.1937]], grad_fn=<AddmmBackward0>)\n\n\n\nnet[1]\n\nSigmoid()\n\n\n\nnet[1](net[0](x))\n\ntensor([[0.3775],\n        [0.3774],\n        [0.3773],\n        ...,\n        [0.2327],\n        [0.2327],\n        [0.2326]], grad_fn=<SigmoidBackward0>)\n\n\n\nl1, a1 = net\n\n\nl1\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\na1\n\nSigmoid()\n\n\n\nid(l1),id(net[0])\n\n(140123913611200, 140123913611200)\n\n\n\n- step2: loss (일단 MSE로..)\n(방법1)\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(0.2823, grad_fn=<MeanBackward0>)\n\n\n(방법2)\n\nloss_fn = torch.nn.MSELoss()\nloss = loss_fn(yhat,y) # yhat을 먼저쓰자!\nloss\n\ntensor(0.2823, grad_fn=<MseLossBackward0>)\n\n\n- step3~4는 동일\n- 반복 (준비+for문)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#로지스틱bceloss",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#로지스틱bceloss",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "로지스틱–BCEloss",
    "text": "로지스틱–BCEloss\n- BCEloss로 바꾸어서 적합하여 보자.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.01)\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y)  #. -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) \n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,v,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 왜 잘맞지? -> “linear -> sigmoid” 와 같은 net에 BCEloss를 이용하면 손실함수의 모양이 convex 하기 때문에 - “linear -> sigmoid” 로 \\(\\hat{y}\\)을 구하고 BCEloss로 loss를 계산하면 그 모영아 convex하므로\nBCSloss에는 local error에 빠지지 않아, loss는 있아.\n\nplot_loss 함수소개 = 이 예제에 한정하여 \\(\\hat{w}_0,\\hat{w}_1,loss(\\hat{w}_0,\\hat{w}_1)\\)를 각각 \\(x,y,z\\) 축에 그려줍니다.\n\n\nplot_loss(torch.nn.MSELoss())\n\n\n\n\n\nplot_loss(torch.nn.BCELoss())\n\n\n\n\n\n시각화1: MSE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\n\n\nl1.bias.data,l1.weight.data\n\n(tensor([0.6245]), tensor([[-0.2593]]))\n\n\n\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nl1.bias.data,l1.weight.data\n\n(tensor([-3.]), tensor([[-1.]]))\n\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, bad_init // SGD')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#로지스틱adam-국민옵티마이저",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#로지스틱adam-국민옵티마이저",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "로지스틱–Adam (국민옵티마이저)",
    "text": "로지스틱–Adam (국민옵티마이저)\n\n시각화1: MSE, 좋은초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05)  ## <-- 여기를 수정!\n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화2: MSE, 나쁜초기값 –> 이걸 아담으로!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSEloss, bad_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화3: BCE, 좋은초기값 –> 이걸 아담으로! (혼자해봐요..)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-3.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, good_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n시각화4: BCE, 나쁜초기값 –> 이걸 아담으로! (혼자해봐요..)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.05) \n\n\nl1,a1 = net\nl1.bias.data = torch.tensor([-10.0])\nl1.weight.data = torch.tensor([[-1.0]])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'BCEloss, bad_init // Adam')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(참고) Adam이 우수한 이유? SGD보다 두 가지 측면에서 개선이 있었음. 1. 그런게 있음.. 2. 가속도의 개념을 적용!!"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망로지스틱-회귀의-한계",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망로지스틱-회귀의-한계",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "깊은신경망–로지스틱 회귀의 한계",
    "text": "깊은신경망–로지스틱 회귀의 한계\n\n신문기사 (데이터의 모티브)\n- 스펙이 높아도 취업이 안된다고 합니다..\n중소·지방 기업 “뽑아봤자 그만두니까”\n중소기업 관계자들은 고스펙 지원자를 꺼리는 이유로 높은 퇴직률을 꼽는다. 여건이 좋은 대기업으로 이직하거나 회사를 관두는 경우가 많다는 하소연이다. 고용정보원이 지난 3일 공개한 자료에 따르면 중소기업 청년취업자 가운데 49.5%가 2년 내에 회사를 그만두는 것으로 나타났다.\n중소 IT업체 관계자는 “기업 입장에서 가장 뼈아픈 게 신입사원이 그만둬서 새로 뽑는 일”이라며 “명문대 나온 스펙 좋은 지원자를 뽑아놔도 1년을 채우지 않고 그만두는 사원이 대부분이라 우리도 눈을 낮춰 사람을 뽑는다”고 말했다.\n\n\n가짜데이터\n- 위의 기사를 모티브로 한 데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nplt.plot(df.x,df.y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n\n로지스틱 회귀로 적합\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n#yhat=net(x)\n\n\nloss_fn = torch.nn.BCELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nloss = loss_fn(net(x),y) \n# loss = loss_fn(yhat,y) \n# loss = -torch.mean((y)*torch.log(yhat)+(1-y)*torch.log(1-yhat))\nloss\n\ntensor(0.6403, grad_fn=<BinaryCrossEntropyBackward0>)\n\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n- 이건 epoc=6억번으로 설정해도 못 맞출 것 같다 (증가하다가 감소하는 underlying을 설계하는 것이 불가능) \\(\\to\\) 모형의 표현력이 너무 낮다.\n\n\n해결책\n- sigmoid 넣기 전의 상태가 꺽인 그래프 이어야 한다.\n\nsig = torch.nn.Sigmoid()\n\n\nfig,ax = plt.subplots(4,2,figsize=(8,8))\nu1 = torch.tensor([-6,-4,-2,0,2,4,6])\nu2 = torch.tensor([6,4,2,0,-2,-4,-6])\nu3 = torch.tensor([-6,-2,2,6,2,-2,-6])\nu4 = torch.tensor([-6,-2,2,6,4,2,0])\nax[0,0].plot(u1,'--o',color='C0');ax[0,1].plot(sig(u1),'--o',color='C0')\nax[1,0].plot(u2,'--o',color='C1');ax[1,1].plot(sig(u2),'--o',color='C1')\nax[2,0].plot(u3,'--o',color='C2');ax[2,1].plot(sig(u3),'--o',color='C2')\nax[3,0].plot(u4,'--o',color='C3');ax[3,1].plot(sig(u4),'--o',color='C3')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망dnn을-이용한-해결",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망dnn을-이용한-해결",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "깊은신경망–DNN을 이용한 해결",
    "text": "깊은신경망–DNN을 이용한 해결\n- 목표: 아래와 같은 벡터 \\({\\boldsymbol u}\\)를 만들어보자.\n\\({\\boldsymbol u} = [u_1,u_2,\\dots,u_{2000}], \\quad u_i = \\begin{cases} 9x_i +4.5& x_i <0 \\\\ -4.5x_i + 4.5& x_i >0 \\end{cases}\\)\n\n꺽인 그래프를 만드는 방법1\n\nu = [9*xi+4.5 if xi <0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()]\nplt.plot(u,'--')\n\n\n\n\n\n\n꺽인 그래프를 만드는 방법2\n- 전략: 선형변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환\n(예비학습) ReLU 함수란?\n\\(ReLU(x) = \\max(0,x)\\)\n\nrelu=torch.nn.ReLU()\nplt.plot(x,'--r')\nplt.plot(relu(x),'--b')\n\n\n\n\n\n빨간색: x, 파란색: relu(x)\n\n예비학습끝\n우리 전략 다시 확인: 선형변환1 -> 렐루 -> 선형변환2\n(선형변환1)\n\nplt.plot(x);plt.plot(-x)\n\n\n\n\n(렐루)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.5)\nplt.plot(relu(x),'--',color='C0');plt.plot(relu(-x),'--',color='C1')\n\n\n\n\n(선형변환2)\n\nplt.plot(x,alpha=0.2);plt.plot(-x,alpha=0.2)\nplt.plot(relu(x),'--',color='C0',alpha=0.2);plt.plot(relu(-x),'--',color='C1',alpha=0.2)\nplt.plot(-4.5*relu(x)-9.0*relu(-x)+4.5,'--',color='C2') \n\n\n\n\n이제 초록색선에 sig를 취하기만 하면?\n\nplt.plot(sig(-4.5*relu(x)-9.0*relu(-x)+4.5),'--',color='C2')\n\n\n\n\n정리하면!\n\nfig = plt.figure(figsize=(8, 4))\nspec = fig.add_gridspec(4, 4)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title('x'); ax1.plot(x,'--',color='C0')\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title('-x'); ax2.plot(-x,'--',color='C1')\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title('relu(x)'); ax3.plot(relu(x),'--',color='C0')\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title('relu(-x)'); ax4.plot(relu(-x),'--',color='C1')\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title('u'); ax5.plot(-4.5*relu(x)-9*relu(-x)+4.5,'--',color='C2')\nax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title('yhat'); ax6.plot(sig(-4.5*relu(x)-9*relu(-x)+4.5),'--',color='C2')\nfig.tight_layout()\n\n\n\n\n\n이런느낌으로 \\(\\hat{\\boldsymbol y}\\)을 만들면 된다.\n\n\n\ntorch.nn.Linear()를 이용한 꺽인 그래프 구현\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True) \na1 = torch.nn.ReLU()\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True) \na2 = torch.nn.Sigmoid() \n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2) \n\n\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[-0.3467],\n         [-0.8470]], requires_grad=True),\n Parameter containing:\n tensor([0.3604, 0.9336], requires_grad=True),\n Parameter containing:\n tensor([[ 0.2880, -0.6282]], requires_grad=True),\n Parameter containing:\n tensor([0.2304], requires_grad=True))\n\n\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\nl2.weight.data = torch.tensor([[ -4.5, -9.0]])\nl2.bias.data= torch.tensor([4.5])\nl1.weight,l1.bias,l2.weight,l2.bias\n\n(Parameter containing:\n tensor([[ 1.],\n         [-1.]], requires_grad=True),\n Parameter containing:\n tensor([0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[-4.5000, -9.0000]], requires_grad=True),\n Parameter containing:\n tensor([4.5000], requires_grad=True))\n\n\n\nplt.plot(l1(x).data)\n\n\n\n\n\nplt.plot(a1(l1(x)).data)\n\n\n\n\n\nplt.plot(l2(a1(l1(x))).data,color='C2')\n\n\n\n\n\nplt.plot(a2(l2(a1(l1(x)))).data,color='C2')\n#plt.plot(net(x).data,color='C2')\n\n\n\n\n- 수식표현\n\n\\({\\bf X}=\\begin{bmatrix} x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}\\)\n\\(l_1({\\bf X})={\\bf X}{\\bf W}^{(1)}\\overset{bc}{+} {\\boldsymbol b}^{(1)}=\\begin{bmatrix} x_1 & -x_1 \\\\ x_2 & -x_2 \\\\ \\dots & \\dots \\\\ x_n & -x_n\\end{bmatrix}\\)\n\n\\({\\bf W}^{(1)}=\\begin{bmatrix} 1 & -1 \\end{bmatrix}\\)\n\\({\\boldsymbol b}^{(1)}=\\begin{bmatrix} 0 & 0 \\end{bmatrix}\\)\n\n\\((a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big)=\\begin{bmatrix} \\text{relu}(x_1) & \\text{relu}(-x_1) \\\\ \\text{relu}(x_2) & \\text{relu}(-x_2) \\\\ \\dots & \\dots \\\\ \\text{relu}(x_n) & \\text{relu}(-x_n)\\end{bmatrix}\\)\n\\((l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\\\ =\\begin{bmatrix} -4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5 \\\\ -4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\\\ \\dots \\\\ -4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\end{bmatrix}\\)\n\n\\({\\bf W}^{(2)}=\\begin{bmatrix} -4.5 \\\\ -9 \\end{bmatrix}\\)\n\\(b^{(2)}=4.5\\)\n\n\\(net({\\bf X})=(a_2 \\circ l_2 \\circ a_1\\circ l_1)({\\bf X})=\\text{sig}\\Big(\\text{relu}\\big({\\bf X}{\\bf W}^{(1)}\\overset{bc}{+}{\\boldsymbol b}^{(1)}\\big){\\bf W}^{(2)}\\overset{bc}{+}b^{(2)}\\Big)\\\\=\\begin{bmatrix} \\text{sig}\\Big(-4.5\\times\\text{relu}(x_1) -9.0 \\times \\text{relu}(-x_1) +4.5\\Big) \\\\ \\text{sig}\\Big(-4.5\\times\\text{relu}(x_2) -9.0 \\times\\text{relu}(-x_2) + 4.5 \\Big)\\\\ \\dots \\\\ \\text{sig}\\Big(-4.5\\times \\text{relu}(x_n) -9.0 \\times\\text{relu}(-x_n)+4.5 \\Big)\\end{bmatrix}\\)\n\n- 차원만 따지자\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nStep1 ~ Step4\n- 준비\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2), #u1=l1(x), x:(n,1) --> u1:(n,2) \n    torch.nn.ReLU(), # v1=a1(u1), u1:(n,2) --> v1:(n,2) \n    torch.nn.Linear(in_features=2,out_features=1), # u2=l2(v1), v1:(n,2) --> u2:(n,1) \n    torch.nn.Sigmoid() # v2=a2(u2), u2:(n,1) --> v2:(n,1) \n) \n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters()) # lr은 디폴트값으로..\n\n- 반복\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\nplt.title(\"before\")\n\nText(0.5, 1.0, 'before')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 3000 epochs\")\n\nText(0.5, 1.0, 'after 3000 epochs')\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## step1 \n    yhat = net(x) \n    ## step2 \n    loss = loss_fn(yhat,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--',color='C1')\nplt.title(\"after 6000 epochs\")\n\nText(0.5, 1.0, 'after 6000 epochs')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "href": "posts/ml/2022-10-05-ml-(5주차)_10월05일.html#깊은신경망dnn으로-해결가능한-다양한-예제",
    "title": "기계학습 특강 (5주차) 10월5일",
    "section": "깊은신경망–DNN으로 해결가능한 다양한 예제",
    "text": "깊은신경망–DNN으로 해결가능한 다양한 예제\n\n예제1\n- 언뜻 생각하면 방금 배운 기술은 sig를 취하기 전이 꺽은선인 형태만 가능할 듯 하다. \\(\\to\\) 그래서 이 역시 표현력이 부족할 듯 하다. \\(\\to\\) 그런데 생각보다 표현력이 풍부한 편이다. 즉 생각보다 쓸 만하다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex1.csv')\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\n이거 시그모이드 취하기 직전은 step이 포함된 듯 \\(\\to\\) 그래서 꺽은선으로는 표현할 수 없는 구조임 \\(\\to\\) 그런데 사실 대충은 표현가능\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=16), # x:(n,1) --> u1:(n,16) \n    torch.nn.ReLU(), # u1:(n,16) --> v1:(n,16)\n    torch.nn.Linear(in_features=16,out_features=1), # v1:(n,16) --> u2:(n,1) \n    torch.nn.Sigmoid() # u2:(n,1) --> v2:(n,1) \n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,16)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,16)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(20000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()    \n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제2\n- 사실 꺽은선의 조합으로 꽤 많은걸 표현할 수 있거든요? \\(\\to\\) 심지어 곡선도 대충 맞게 적합된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex2.csv')\n\n\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\n\n\n\n\n\nx=torch.tensor(df.x).float().reshape(-1,1)\ny=torch.tensor(df.y).float().reshape(-1,1)\n\n(풀이1)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(20000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n(풀이2) – 풀이1보다 좀 더 잘맞음. 잘 맞는 이유? 좋은초기값 (=운)\n\ntorch.manual_seed(5)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32), # x:(n,1) --> u1:(n,32)\n    torch.nn.ReLU(), # u1:(n,32) --> v1:(n,32) \n    torch.nn.Linear(in_features=32,out_features=1) # v1:(n,32) --> u2:(n,1)\n)\n\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.MSELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(6000): \n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(df.x,df.underlying,'-b')\nplt.plot(x,net(x).data,lw=4)\n\n\n\n\n\n풀이1에서 에폭을 많이 반복하면 풀이2의 적합선이 나올까? –> 안나옴!! (local min에 빠졌다)\n\n\n\n예제3\n\nimport seaborn as sns\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex3.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)})\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n\nx1 = torch.tensor(df.x1).float().reshape(-1,1) \nx2 = torch.tensor(df.x2).float().reshape(-1,1) \nX = torch.concat([x1,x2],axis=1) \ny = torch.tensor(df.y).float().reshape(-1,1) \n\n\nX.shape\n\ntorch.Size([2000, 2])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,2)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss() \n\n\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(3000):\n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ndf2 = df.assign(yhat=yhat.reshape(-1).detach().tolist())\ndf2\n\n\n\n\n\n  \n    \n      \n      x1\n      x2\n      y\n      yhat\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n      0.342171\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n      0.599576\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n      0.106441\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n      0.916042\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n      0.910025\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n      0.253134\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n      0.480342\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n      0.397069\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n      0.268198\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n      0.102882\n    \n  \n\n2000 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow')\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 결과시각화\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nsns.scatterplot(data=df,x='x1',y='x2',hue='y',alpha=0.5,palette={0:(0.5, 0.0, 1.0),1:(1.0,0.0,0.0)},ax=ax[0])\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat',alpha=0.5,palette='rainbow',ax=ax[1])\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n- 교훈: underlying이 엄청 이상해보여도 생각보다 잘 맞춤"
  },
  {
    "objectID": "posts/ml/2022-09-07-ml.html",
    "href": "posts/ml/2022-09-07-ml.html",
    "title": "기계학습 특강 (1주차) 9월7일 Intro",
    "section": "",
    "text": "Intro\n\n딥러닝 프레임워크\n딥러닝 슈퍼스타: 힌튼, 얀르쿤, 요수아 벤지오, 응\n요수아벤지오가 티아노를 만들었음.\n\n2010 티아노 (요수아벤지오)\n2013 caffe\n2015 체이너\n2015 텐서플로우 (구글)\n2015 케라스 (프랑소와 숄레)\n2016 파이토치 (페이스북)\n\n2017 티아노 개발 중지 선언 - 사실 개인 개발자나 대학연구팀이 구글과 메타(페이스북)을 이기는 건 불가능 - 사실상 텐서플로우와 파이토치가 양분해야하는게 맞는데? - 케라스가 안죽어..\n\n\n케라스의 시대\n케라스의 시대가 왔다 왜?? 쓰기가 쉬워요..\n누가 쓰기쉽냐\n\n비 컴퓨터공학 출신\n딥러닝의 딥도 모르는 사람\n\n–> 인공지능구현을 거의 엑셀다루는 수준으로 내림..\n–> 잘 모르겠지만 이렇게 이미지 넣으면 개 고양이 구분해줍니다!\n다른 언어는 어떠냐\n\n텐서플로우: 이건 진짜 못된 언어임 (높은 수준의 파이썬 숙련도가 필요, 컴공위주로 만들어졌음)\n파이토치: 프로그래밍 숙련도는 높지 않은데, 딥러닝의 알고리즘이 머리속에 있어야 함\n\n케라스가 압도..\n결말: 구글이 케라스를 삽니다.\n\n\n케라스와 파이토치의 시대\n그런데 지금은 파이토치랑 케라스(텐서플로우)가 비등비등해요,\n그런데 점점 파이토치를 쓰는 추세 왜?"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "",
    "text": "(11주차) 11월16일 [순환신경망– abc예제, abdc예제, abcde예제, AbAcAd예제]"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#import",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#import",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "import",
    "text": "import\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#define-some-funtions",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#define-some-funtions",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "Define some funtions",
    "text": "Define some funtions\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam4-abacad-2",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam4-abacad-2",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "Exam4: AbAcAd (2)",
    "text": "Exam4: AbAcAd (2)\n\ndata\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))\n\n\n\n\n순환신경망 구현1 (손으로 직접구현) – 리뷰\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1160dcaa50>\n\n\n\n\n\n\n\n순환신경망 구현2 (with RNNCell, hidden node 2)\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n구현1과 같은 초기값 (확인용)\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell() # 숙성담당 네트워크 \n\n\nrnncell = torch.nn.RNNCell(4,2)\n\n\ninput_size = 4\nhindden_soze = 2\n\nrNNCell() 는 사실 torch.nn.RNNCell()와 같은 동작을 하도록 설계를 하였음.\n같은동작을 하는지 확인하기 위해서 동일한 초기상태에서 rNNCell()에 의하여 학습된 결과와 torch.nn.RNNCell()에 의하여 학습된 결과를 비교해보자.\n\nrnncell.weight_ih.shape\n\ntorch.Size([2, 4])\n\n\n\nrnncell.bias_ih.shape\n\ntorch.Size([2])\n\n\n\nrnncell.weight_hh.shape\n\ntorch.Size([2, 2])\n\n\n\nrnncell.bias_hh.shape \n\ntorch.Size([2])\n\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data\nrnncell.bias_ih.data = _rnncell.i2h.bias.data\nrnncell.weight_hh.data = _rnncell.h2h.weight.data\nrnncell.bias_hh.data = _rnncell.h2h.bias.data\n\n_rnncell 초기값을 rnncell에 넣어주기\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nhidden = torch.zeros(T,2) \n\n\n# t=0 \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\n# t=1~T \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat[:15].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123dd2650>\n\n\n\n\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11237d65d0>\n\n\n\n\n\n뒷부분갈수록 잘 맞음\n\n\n새로운 초기값\n(1) 숙성네트워크\n\ntorch.manual_seed(43052)\ntorch.nn.RNNCell(4,2)\n\nRNNCell(4, 2)\n\n\n앞 부분은 잘 맞지 않고 뒷부분은 잘 맞을 것!\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\nx -> h -> o -> yhat(softmax(o))\n네트워크만 학습된 상태, 따라서 hiddenlayer를 재구성해줘야 한다.\n\n\n\n순환신경망 구현3 (with RNN, hidden node 2) – 성공\n(예비학습)\n- 네트워크학습이후 yhat을 구하려면 번거로웠음\nhidden = torch.zeros(T,2) \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\nyhat = soft(cook(hidden))\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water), hidden\n\n((tensor([[-0.1271,  0.9965],\n          [-1.0000, -0.9987],\n          [ 0.9962,  1.0000],\n          ...,\n          [ 0.9962,  1.0000],\n          [-1.0000, -0.9897],\n          [ 0.9959,  1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[0.9959, 1.0000]], grad_fn=<SqueezeBackward1>)),\n tensor([[-0.2232,  0.9769],\n         [-0.9999, -0.9742],\n         [ 0.9154,  0.9992],\n         ...,\n         [ 0.9200,  0.9992],\n         [-0.9978, -0.0823],\n         [-0.9154,  0.9965]], grad_fn=<IndexPutBackward0>))\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[6.0688e-02, 4.2958e-01, 2.9885e-01, 2.1088e-01],\n        [9.9700e-01, 8.2096e-04, 1.3663e-03, 8.1039e-04],\n        [2.2782e-03, 3.3203e-01, 3.3260e-01, 3.3309e-01],\n        ...,\n        [2.2779e-03, 3.3203e-01, 3.3260e-01, 3.3309e-01],\n        [9.9692e-01, 8.4633e-04, 1.4012e-03, 8.3072e-04],\n        [2.2803e-03, 3.3206e-01, 3.3260e-01, 3.3306e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\nrnn = torch.nn.RNN(4,2)\n\n가중치초기화\n\ntorch.manual_seed(43052)\n_rnncell = torch.nn.RNNCell(4,2)\n\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,2) \nfor epoc in range(5000):\n    ## 1 \n    hidden,hT = rnn(x,_water)\n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n결과는 같을 것\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat.data[:15],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123908710>\n\n\n\n\n\n\n처음은 좀 틀렸음 ㅎㅎ\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11238cac90>\n\n\n\n\n\n\n뒤에는 잘맞음\n\n실전팁: _water 대신에 hT를 대입 (사실 큰 차이는 없음)\nhT는 이미 값이 저장되어 있잖아\n\nrnn(x[:6],_water),rnn(x[:6],hT)\n\n((tensor([[-0.9912, -0.9117],\n          [ 0.0698, -1.0000],\n          [-0.9927, -0.9682],\n          [ 0.5761, -1.0000],\n          [-0.9960, -0.0173],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)),\n (tensor([[-0.9713, -1.0000],\n          [ 0.0535, -1.0000],\n          [-0.9925, -0.9720],\n          [ 0.5759, -1.0000],\n          [-0.9960, -0.0180],\n          [ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[ 0.9960, -1.0000]], grad_fn=<SqueezeBackward1>)))\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123881510>\n\n\n\n\n\n\n히든노드의 해석이 어려움.\n\nhidden layer->layer 더 있음 좋겠다\n\n\n순환신경망 구현4 (with RNN, hidden node 3) – 성공\n(1) 숙성네트워크~ (2) 조리네트워크\n\ntorch.manual_seed(2) #1 \nrnn = torch.nn.RNN(4,3) \ncook = torch.nn.Linear(3,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,3) \nfor epoc in range(5000):\n    ## 1\n    hidden,hT = rnn(x,_water) \n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123e41f10>\n\n\n\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1123a3fd10>\n\n\n\n\n\n\n세번째 히든노드 = 대소문자(a/A)를 구분\n1,2 히든노드 = bcd를 구분"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#gpu-실험",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#gpu-실험",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "GPU 실험",
    "text": "GPU 실험\n\n20000 len + 20 hidden nodes\ncpu\n\nimport time\n\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n228.14399337768555\n\n\n왕 느려\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n6.0587098598480225\n\n\n\n왜 빠른지?\n\n\n\n20000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n28.451032400131226\n\n\nloss 미분할 때 시간 많이 잡음ㅁ\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.6839873790740967\n\n\n\n\n2000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n10.069071292877197\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n1.7792012691497803\n\n\n\n\n2000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n2.7720658779144287\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n0.2116107940673828\n\n\n\n\n2000 len + 5000 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n34.20541262626648\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n4.586683511734009\n\n\n\n\n2000 len + 5000 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n8.695780992507935\n\n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n2.3646719455718994\n\n\n\n\n실험결과 요약\n\n\n\nlen\n# of hidden nodes\nbackward\ncpu\ngpu\nratio\n\n\n\n\n20000\n20\nO\n93.02\n3.26\n28.53\n\n\n20000\n20\nX\n18.85\n1.29\n14.61\n\n\n2000\n20\nO\n6.53\n0.75\n8.70\n\n\n2000\n20\nX\n1.25\n0.14\n8.93\n\n\n2000\n1000\nO\n58.99\n4.75\n12.41\n\n\n2000\n1000\nX\n13.16\n2.29\n5.74"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam5-abcabc",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam5-abcabc",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "Exam5: abcabC",
    "text": "Exam5: abcabC\n\ndata\n\ntxt = list('abcabC')*100\ntxt[:8]\n\n['a', 'b', 'c', 'a', 'b', 'C', 'a', 'b']\n\n\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])\n\n\n\n\nRNN\n\nbc\nbC\nb의 수준이 2개\nabc\namC\n문맥 고려해서 \\(\\to\\) hiddenlayer = 3\n\n\ntorch.manual_seed(43052) \nrnn = torch.nn.RNN(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+ list(linr.parameters()))\n\n\nrnn.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\n어차피 시각화하려면 cpu에 있어야해\n나중 기억!\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f116f95fd10>\n\n\n\n\n\n- 6000 epochs\n\n3: a일 확률\n4: b일 확률\n5: c일 확률\n6: C일 확률\n\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f1162b1ad10>\n\n\n\n\n\n- 9000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11627385d0>\n\n\n\n\n\n- 12000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11626b9950>\n\n\n\n\n\n- 15000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-12:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f11624ce950>\n\n\n\n\n\n\n15,000번 정도 하니 c와 C를 구분하는 모습\nhidden layer(0,1,2)의 색 순서에 따라 문맥상 다른 것을 알 수 있고 학습도 되는 모습을 볼 수 있다.\n\n\n\nLSTM\n- LSTM\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()))\n\n\nlstm.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f116245a750>\n\n\n\n\n\n\n하얀부분이 0 파란 부분이 -1 빨간 부분이 +1\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f11623e0a90>\n\n\n\n\n\n\nrnn에 비해 lstm은 조금 돌려도 어느정도 비교 잘 해낸다\n\n\n\nRNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n\n\n\n\nlstm이 rnn보다 이런 상황에서는 더 잘 학습해낸다.\nlinear 의 hiddenlayer로 구분되어 있다."
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam6-abcdabcd",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#exam6-abcdabcd",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "Exam6: abcdabcD",
    "text": "Exam6: abcdabcD\n\ndata\n\ntxt = list('abcdabcD')*100\ntxt[:8]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'D']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'c':2, 'd':3, 'D':4}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx=x.to(\"cuda:0\")\ny=y.to(\"cuda:0\")\n\n\n\nRNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n- 관찰1: LSTM이 확실히 장기기억에 강하다.\n- 관찰2: LSTM은 hidden에 0이 잘 나온다.\n\n사실 확실히 구분되는 특징을 판별할때는 -1,1 로 히든레이어 값들이 설정되면 명확하다.\n히든레이어에 -1~1사이의 값이 나온다면 애매한 판단이 내려지게 된다.\n그런데 이 애매한 판단이 어떻게 보면 문맥의 뉘앙스를 이해하는데 더 잘 맞다.\n그런데 RNN은 -1,1로 셋팅된 상황에서 -1~1로의 변화가 더디다는 것이 문제임."
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#lstm의-계산과정",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#lstm의-계산과정",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "LSTM의 계산과정",
    "text": "LSTM의 계산과정\n\ndata: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nab\naB\n로서 a의 수준이 2개로 나뉨 \\(\\to\\) hidden node = 2\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'B':2}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\n\n1 epoch ver1 (with torch.nn.LSTMCell)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt)\n    loss = loss / T\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\n데이터 적으니까 cpu로 할 것임\n\n\nht,ct \n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))\n\n\n\nhidden node가 많고 len 이 클수록 GPU가 효율이 좋다\n\n\n\n1 epoch ver2 (완전 손으로 구현)\n\nt=0 \\(\\to\\) t=1\n- lstm_cell 을 이용한 계산 (결과비교용)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(1):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n이런결과를 어떻게 만드는걸까?\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n\n\\(i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})\\)\n\\(f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})\\)\n\\(g_t = \\tanh (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg})\\)\n\\(o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{hg})\\)\n\\(o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})\\)\n\\(c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\\)\n\\(h_t = o_t \\odot \\tanh (c_t)\\)\n\\(\\sigma = \\text{ Sigmoid }\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ }\\underrightarrow{sig} \\text{ }i_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\star \\text{ } \\underrightarrow{tanh} \\text{ }g_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ } \\triangleleft \\text{ }\\underrightarrow{sig} \\text{ }o_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t\\)\n\n위에 나온 W가 어떻게 계산되나\n\nweight_ih_l[k] – the learnable input-hidden weights of the \\(\\text{k}^{th}\\) layer \\((W_ii|W_if|W_ig|W_io)\\), of shape (4hidden_size, input_size) for \\(k = 0\\). Otherwise, the shape is (4hidden_size, num_directions * hidden_size). If proj_size > 0 was specified, the shape will be (4hidden_size, num_directions  proj_size) for \\(k > 0\\)\nweight_hh_l[k] – the learnable hidden-hidden weights of the ^{th}$layer \\((W_hi|W_hf|W_hg|W_ho)\\), of shape (4hidden_size, hidden_size). If proj_size > 0 was specified, the shape will be (4hidden_size, proj_size).\n- 직접계산\n\n\\(o_t\\) = output_gate\n\\(f_t\\) = forget_gate\n\\(i_t\\) = input_gate\n\n\nht = torch.zeros(1,2)\nct = torch.zeros(1,2)\n\n\n_ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n_ifgo\n\ntensor([[ 0.0137,  0.1495,  0.0879,  0.6436, -0.2615,  0.3974, -0.3506, -0.4550]],\n       grad_fn=<AddBackward0>)\n\n\n$ $각 두 개씩\n\ninput_gate = sig(_ifgo[:,0:2])\nforget_gate = sig(_ifgo[:,2:4])\ngt = tanh(_ifgo[:,4:6])\noutput_gate = sig(_ifgo[:,6:8])\n\n\nct = forget_gate * ct + input_gate * gt\nht = output_gate * tanh(ct)\n\n\nht,ct\n\n(tensor([[-0.0812,  0.1327]], grad_fn=<MulBackward0>),\n tensor([[-0.1991,  0.3563]], grad_fn=<AddBackward0>))\n\n\n\n\nt=0 \\(\\to\\) t=T\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        \n        ## lstm_cell step1: calculate _ifgo \n        _ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        input_gate = sig(_ifgo[:,0:2])\n        forget_gate = sig(_ifgo[:,2:4])\n        gt = tanh(_ifgo[:,4:6])\n        output_gate = sig(_ifgo[:,6:8])\n        ## lstm_cell step3: calculate ht,ct \n        ct = forget_gate * ct + input_gate * gt\n        ht = output_gate * tanh(ct)\n        \n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))\n\n\n\n\n\n1 epoch ver3 (with torch.nn.LSTM)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2)\nlinr = torch.nn.Linear(2,3) \n\n\nlstm = torch.nn.LSTM(3,2) \n\n\nlstm.weight_hh_l0.data = lstm_cell.weight_hh.data \nlstm.bias_hh_l0.data = lstm_cell.bias_hh.data \nlstm.weight_ih_l0.data = lstm_cell.weight_ih.data \nlstm.bias_ih_l0.data = lstm_cell.bias_ih.data \n\n\n초기화된 가중치값들로 덮어씌우기\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()), lr=0.1) \n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    # ## step2\n    # loss = loss_fn(output,y) \n    # ## step3\n    # loss.backward()\n    # ## step4 \n    # optimizr.step()\n    # optimizr.zero_grad() \n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.0975,  0.7134]], grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#lstm은-왜-강한가",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#lstm은-왜-강한가",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "LSTM은 왜 강한가?",
    "text": "LSTM은 왜 강한가?\n\ndata: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nn_words = 3\n\n\nmapping = {'a':0, 'b':1, 'B':2}\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:10],txt_y[:10]\n\n(['a', 'b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b'],\n ['b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b', 'a'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.],\n         ...,\n         [1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.]]),\n tensor([[0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.],\n         ...,\n         [0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.]]))\n\n\n\n\n1000 epoch\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(3,2) \nlinr = torch.nn.Linear(2,3) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1000): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    ## step2\n    loss = loss_fn(output,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\n\n시각화\n\nT = len(x)\ninput_gate = torch.zeros(T,2)\nforget_gate = torch.zeros(T,2)\noutput_gate = torch.zeros(T,2)\ng = torch.zeros(T,2)\ncell = torch.zeros(T,2)\nh = torch.zeros(T,2) \n\n\n변수를 담을 빈 셋 설정\n\n\nfor t in range(T): \n    ## 1: calculate _ifgo \n    _ifgo = x[[t]] @ lstm.weight_ih_l0.T + h[[t]] @ lstm.weight_hh_l0.T + lstm.bias_ih_l0 + lstm.bias_hh_l0 \n    ## 2: decompose _ifgo \n    input_gate[[t]] = sig(_ifgo[:,0:2])\n    forget_gate[[t]] = sig(_ifgo[:,2:4])\n    g[[t]] = tanh(_ifgo[:,4:6])\n    output_gate[[t]] = sig(_ifgo[:,6:8])\n    ## 3: calculate ht,ct \n    cell[[t]] = forget_gate[[t]] * cell[[t]] + input_gate[[t]] * g[[t]]\n    h[[t]] = output_gate[[t]] * tanh(cell[[t]])\n\n\ncombinded1 = torch.concat([input_gate,forget_gate,output_gate],axis=1)\ncombinded2 = torch.concat([g,cell,h,soft(output)],axis=1)\n\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n\n\n\n\n상단그림은 게이트의 값들만 시각화, 하단그림은 게이트 이외의 값들을 시각화\n\n\n\n시각화의 해석I\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\n\n\n\n\n- input_gate, forget_gate, output_gate는 모두 0~1 사이의 값을 가진다.\n- 이 값들은 각각 모두 \\({\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}, \\tanh({\\boldsymbol c}_t)\\)에 곱해진다. 따라서 input_gate, forget_gate, output_gate 는 gate의 역할로 비유가능하다. (1이면 통과, 0이면 차단)\n\ninput_gate: \\({\\boldsymbol g}_t\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\nforget_gate: \\({\\boldsymbol c}_{t-1}\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\noutput_gate: \\(\\tanh({\\boldsymbol c}_t)\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\n\n(서연 필기)\n\n값들이 0과 1사이의 값을 가진다\n파 -1 흰 0 빨 1\n0 곱하면 어떤 값이든 0이 되니까 차단한다 표현\n\n\n\n시각화의 해석II\n\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n- 결국 \\({\\boldsymbol g}_t\\to {\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\to \\hat{\\boldsymbol y}\\) 의 느낌이다. (\\({\\boldsymbol h}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_t\\)가 필요했고 \\({\\boldsymbol c}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_{t-1}\\)과 \\({\\boldsymbol g}_t\\)가 필요했음)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol c}_t) \\odot {\\boldsymbol o}_t\\)\n\\({\\boldsymbol c}_t ={\\boldsymbol c}_{t-1} \\odot {\\boldsymbol f}_t + {\\boldsymbol g}_{t} \\odot {\\boldsymbol i}_t\\)\n\n- \\({\\boldsymbol g}_t,{\\boldsymbol c}_t,{\\boldsymbol h}_t\\) 모두 \\({\\boldsymbol x}_t\\)의 정보를 숙성시켜 가지고 있는 느낌이 든다.\n- \\({\\boldsymbol g}_t\\) 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어 있다. (마치 RNN의 hidden node처럼!)\n\n\\(\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})\\)\n\n- \\({\\boldsymbol c}_t\\) 특징: \\({\\boldsymbol g}_t\\)와 매우 비슷하지만 약간 다른값을 가진다. 그래서 \\({\\boldsymbol g}_t\\)와는 달리 -1,1 이외의 값도 종종 등장.\n\nprint(\"first row: gt={}, ct={}\".format(g[-8].data, cell[-8].data))\nprint(\"second row: gt={}, ct={}\".format(g[-7].data, cell[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373])\n\n\n- \\({\\boldsymbol h}_t\\) 특징: (1) \\({\\boldsymbol c}_t\\)의 느낌이 있음 하지만 약간의 변형이 있음. (2) -1~1 사이에의 값을 훨씬 다양하게 가진다. (tanh때문)\n(서연 필기)\n\ncomparison of g,c part\n\n보니까 빨간 색은 1에 가까운 값, 파란색은 -1에 가까운 값들을 띄었다.\n그리고 연한 빨간색인 부분은 0.3592로 낮았고, g부분과 c부분이 열별로 보았을 때 달랐다\n\n\n\nprint(\"first row: gt={}, ct={}, ht={}\".format(g[-8].data, cell[-8].data,h[-8].data))\nprint(\"second row: gt={}, ct={}, ht={}\".format(g[-7].data, cell[-7].data,h[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984]), ht=tensor([ 0.7370, -0.3323])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373]), ht=tensor([ 0.0604, -0.6951])\n\n\n(서연 필기)\n\ncomparison of c,h part\n\nh는 c와 무관해보이지 않는다.\n단지 어떤 변형이 있는 것 같다.\n\n\n- 예전의문 해결\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문\n\n\n\nLSTM의 알고리즘 리뷰 I (수식위주)\n(step1) calculate \\({\\tt ifgo}\\)\n\\({\\tt ifgo} = {\\boldsymbol x}_t \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1} \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias\\)\n\\(=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias\\)\n참고: 위의 수식은 아래코드에 해당하는 부분\nifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n(step2) decompose \\({\\tt ifgo}\\) and get \\({\\boldsymbol i}_t\\), \\({\\boldsymbol f}_t\\), \\({\\boldsymbol g}_t\\), \\({\\boldsymbol o}_t\\)\n\\({\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )\\)\n\\({\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )\\)\n\\({\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )\\)\n\\({\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )\\)\n(step3) calculate \\({\\boldsymbol c}_t\\) and \\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)\\)\n\n\nLSTM의 알고리즘 리뷰 II (느낌위주)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ } \\triangleleft \\text{ }\\underrightarrow{sig} \\text{ }o_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t\\)\n\n이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n\n- 느낌1: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은 콩물에서 간장을 3차로 나누어 숙성하는 느낌이다.\n\n콩물: \\({\\boldsymbol x}_t\\)\n1차숙성: \\({\\boldsymbol g}_t\\)\n2차숙성: \\({\\boldsymbol c}_t\\)\n3차숙성: \\({\\boldsymbol h}_t\\)\n\n- 느낌2: \\({\\boldsymbol g}_t\\)에 대하여\n\n계산방법: \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)를 \\({\\bf W}_{ig}, {\\bf W}_{hg}\\)를 이용해 선형결합하고 \\(\\tanh\\)를 취한 결과\nRNN에서 간장을 만들던 그 수식에서 \\(h_t\\)를 \\(g_t\\)로 바꾼것\n크게 2가지의 의미를 가진다 (1) 과거와 현재의 결합 (2) 활성화함수 \\(\\tanh\\)를 적용\n\n(서연 필기)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ }\\underrightarrow{sig} \\text{ }i_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t\\)\n\\(x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\star \\text{ } \\underrightarrow{tanh} \\text{ }g_t\\)\n를 풀어서 쓰면\n\\(\\tanh(x_t W_{ig} + h_{t-1} W_{hg} + bias)\\)\nRNN: \\(h_t = \\tanh(x_t W + h_{t-1} W + bias)\\)\nLSTM: \\(g_t = \\tanh(x_t W + h_{t-1} W + bias)\\) - 과거\\(h_{t-1}\\)와 현재\\(x_t\\)의 결합\n- 느낌3: \\({\\boldsymbol c}_t\\)에 대하여 (1)\n\n계산방법: \\({\\boldsymbol g}_{t}\\)와 \\({\\boldsymbol c}_{t-1}\\)를 요소별로 선택하고 더하는 과정\n\\(g_t\\)는 (1) 과거와 현재의 결합 (2) 활성화함수 tanh를 적용으로 나누어지는데 이중에서 (1) 과거와 현재의 정보를 결합하는 과정만 해당한다. 차이점은 요소별 선택 후 덧셈\n\n\\(g_t\\)는 선형 결합\n\n이러한 결합을 쓰는 이유? 게이트를 이용하여 과거와 현재의 정보를 제어 (일반적인 설명, 솔직히 내가 좋아하는 설명은 아님)\n\n(서연 필기)\n\\(c_t = g_t \\odot Input + c_{t-1} \\odot Forget\\)\n- 느낌4: \\({\\boldsymbol c}_t\\)에 대하여 (2) // \\({\\boldsymbol c}_t\\)는 왜 과거와 현재의 정보를 제어한다고 볼 수 있는가?\n\\(t=1\\) 시점 계산과정관찰\n\ninput_gate[1],g[1],forget_gate[1],cell[0]\n\n(tensor([0.9065, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.0014], grad_fn=<SelectBackward0>),\n tensor([ 0.3592, -0.9373], grad_fn=<SelectBackward0>))\n\n\n\\([0.9,1.0] \\odot {\\boldsymbol g}_t + [1.0,0.0] \\odot {\\boldsymbol c}_{t-1}\\)\n(서연 필기)\n여기서 곱은 element별 곱 - \\([0.9,1.0] \\odot g_t = [0.9,1.0]\\odot [g_1,g_2] = [0.9g_1(현재)_,1.0g_2(과거)]\\) - 여기서 0이 현재에 곱해지면 현재를 기억하지 않고 과거에 0이 곱해지면 과거를 기억하지 않도록 조정할 수 있음\n\\(\\star\\) gate없으면 조정 못 하나??\\(\\to\\) no, weigjht로도 조정할 수 있지 않을까?\n\nforget_gate는 \\(c_{t-1}\\)의 첫번째 원소는 기억하고, 두번째 원소는 잊으라고 말하고 있음 // forget_gate는 과거(\\(c_{t-1}\\))의 정보를 얼마나 잊을지 (= 얼마나 기억할지) 를 결정한다고 해석할 수 있다.\ninput_gate는 \\(g_{t}\\)의 첫번째 원소와 두번째 원소를 모두 기억하되 두번째 원소를 좀 더 중요하게 기억하라고 말하고 있음 // input_gate는 현재(\\(g_{t}\\))의 정보를 얼만큼 강하게 반영할지 결정한다.\n이 둘을 조합하면 \\({\\boldsymbol c}_t\\)가 현재와 과거의 정보중 어떠한 정보를 더 중시하면서 기억할지 결정한다고 볼 수 있다.\n\n\n이 설명은 제가 좀 싫어해요, 싫어하는 이유는 (1) “기억의 정도를 조절한다”와 “망각의 정도를 조절한다”는 사실 같은말임. 그래서 forget_gate의 용어가 모호함. (2) 기억과 망각을 조정하는 방식으로 꼭 gate의 개념을 사용해야 하는건 아님\n\n- 느낌5: \\({\\boldsymbol c}_t\\)에 대하여 (3)\n\n사실상 LSTM 알고리즘의 꽃이라 할 수 있음.\nLSTM은 long short term memory의 약자임. 기존의 RNN은 장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘 활용함.\nLSTM이 장기기억을 잘 활용하는 비법은 바로 \\({\\boldsymbol c}_t\\)에 있다.\n\n(서연필기) \\(c_t\\)로 과거, 현재 기억 조절할 수 있기 때문에\n- 느낌6: \\({\\boldsymbol h}_t\\)에 대하여 - 계산방법: \\(\\tanh({\\boldsymbol c}_t)\\)를 요소별로 선택\n데이터 다 가져와서 선택하는 방식\n- RNN, LSTM의 변수들 비교 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n과거정보\n현재정보\n과거와 현재의 결합방식\n활성화\n느낌\n비고\n\n\n\n\nRNN-\\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) W \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n간장\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM-\\({\\boldsymbol g}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\)W \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n1차간장\n\n\n\nLSTM-\\({\\boldsymbol c}_t\\)\n\\({\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol g}_t\\)\n\\(\\odot\\) W\\(\\to\\) \\(+\\)\nNone\n2차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\nLSTM-\\({\\boldsymbol h}_t\\)\nNone\n\\({\\boldsymbol c}_t\\)\nNone\n\\(\\tanh\\), \\(\\odot\\)\n3차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\n\n\nRNN은 기억할 과거정보가 \\({\\boldsymbol h}_{t-1}\\) 하나이지만 LSTM은 \\({\\boldsymbol c}_{t-1}\\), \\({\\boldsymbol h}_{t-1}\\) 2개이다.\n\n- 알고리즘리뷰 :\n\n콩물\\(x_t\\),과거3차간장\\(h_{t-1}\\) \\(\\overset{\\times,+,\\tanh}{\\longrightarrow}\\) 현재1차간장\\(g_t\\)\n현재1차간장\\(c_{t-1}\\), 과거2차간장 \\(\\overset{\\odot,+,\\tanh}{\\longrightarrow}\\) 현재2차간장\n현재2차간장\\(c_t\\) \\(\\overset{\\tanh,\\odot}{\\longrightarrow}\\) 현재3차간장\\(h_t\\)\n\n\n\nLSTM이 강한이유\n- LSTM이 장기기억에 유리함. 그 이유는 input, forget, output gate 들이 과거기억을 위한 역할을 하기 때문.\n\n비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭 LSTM같은 구조일 필요는 없음. (왜 3차간장을 만들때 tanh를 써야하는지? 게이트는 꼭3개이어야 하는지?)\n\n- 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다.\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문\n\n문잭적으로 이해 ->유리하다 칭함"
  },
  {
    "objectID": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#참고자료들",
    "href": "posts/ml/2022-11-21-ml-(11주차)_11월16일.html#참고자료들",
    "title": "기계학습 특강 (11주차) 11월16일",
    "section": "참고자료들",
    "text": "참고자료들\n\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\nhttps://arxiv.org/abs/1402.1128"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html",
    "href": "posts/ml/2022-10-19-ml_(7주차).html",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "",
    "text": "(7주차) 10월19일 [딥러닝의 기초 - 드랍아웃, 이미지자료분석]"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#imports",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#imports",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "imports",
    "text": "imports\n\nimport torch\nfrom fastai.vision.all import *\nimport matplotlib.pyplot as plt\n\nimport torchvision\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#깊은신경망-오버피팅",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#깊은신경망-오버피팅",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "깊은신경망– 오버피팅",
    "text": "깊은신경망– 오버피팅\n\n데이터\n- model: \\(y_i = (0\\times x_i) + \\epsilon_i\\)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)\n\n\n\n\n\n\n모든 데이터를 사용하여 적합 (512, relu, 1000 epochs)\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y)\nplt.plot(x,net(x).data, '--')\n\n\n\n\n\n\n전체데이터를 8:2로 나누어서 8만을 학습\n- 데이터를 8:2로 나눈다\n\nxtr = x[:80]\nytr = y[:80] \nxtest = x[80:] \nytest = y[80:] \n\n\nx.shape, xtr.shape, xtest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\ny.shape, ytr.shape, ytest.shape\n\n(torch.Size([100, 1]), torch.Size([80, 1]), torch.Size([20, 1]))\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\n\n\n\n\n- (xtr,ytr) 만 가지고 net를 학습시킨다.\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    # yhat\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \n#plt.plot(xtr,net(xtr).data,'--k') \n#plt.plot(xtest,net(xtest).data,'--k') \n\n\n\n\n(서연 필기) 오차항이 너무 잘 따라가면 영향을 미칠 수 있다.\n데이터에 비해 노드 수가 많으면 오버피팅의 가능성 - 한 변수로 모든 변수 맞추는 우연을 마주한다면? - 모델에 비해 feature가 너무 클때? - 위를 예로 들면 input은 1이었는데 output은 512렸다\n차원의 저주"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#깊은신경망-드랍아웃",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#깊은신경망-드랍아웃",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "깊은신경망– 드랍아웃",
    "text": "깊은신경망– 드랍아웃\n\n오버피팅의 해결\n- 오버피팅의 해결책: 드랍아웃\n동등한 초기값에서 시작한다고 설명 - manual_seed 정해준거\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    #\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n계속 바뀌는 plot\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in training mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in training mode')\n\n\n\n\n\n- 올바른 사용법\n\nnet.training\n\nTrue\n\n\nevaliation method 사용\n\nnet.eval()\nnet.training\n\nFalse\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(r\"network is in evaluation mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in evaluation mode')\n\n\n\n\n\n\n\n드랍아웃 레이어\n\n_x = torch.linspace(0,1,101) \n_x \n\ntensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900, 1.0000])\n\n\n\ndout = torch.nn.Dropout(0.9)\ndout(_x)\n\ntensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 1.3000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 2.9000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.1000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.9000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.1000,\n        0.0000, 0.0000, 0.0000, 0.0000, 7.6000, 7.7000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.9000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000])\n\n\n\n90%의 드랍아웃: 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 0으로 만든다. + 그리고 0이 되지않고 살아남은 값들은 10배 만큼 값이 커진다.\n\n- 드랍아웃레이어 정리 - 구조: 입력 -> 드랍아웃레이어 -> 출력 - 역할: (1) 입력의 일부를 임의로 0으로 만드는 역할 (2) 0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정 - 효과: 오버피팅을 억제하는 효과가 있음 (왜??) - 추측일뿐! - 의미: each iteration (each epoch x) 마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨. - 느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전반적으로 개선됨\n(서연 필기) 지배적인 예측 값들보다 비지배적인 예측값을 건들려고 하면 의미가 없음."
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-data",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-data",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "이미지자료분석– data",
    "text": "이미지자료분석– data\n- download data\n\npath = untar_data(URLs.MNIST)\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-예비학습",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-예비학습",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "이미지자료분석– CNN 예비학습",
    "text": "이미지자료분석– CNN 예비학습\n\n기존의 MLP 모형\n- 교재의 모형\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: Sigmoid\"\n}\n''')\n\n\n\n\n- 왜 28$$28 이미지를 784개의 벡터로 만든 다음에 모형을 돌려야 하는가?\n- 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어서 이미지자료를 분석하는 느낌\n- observation의 차원은 \\(784\\)가 아니라 \\(1\\times (28\\times 28)\\)이 되어야 맞다.\n\n\n새로운 아키텍처의 제시\n- 예전\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\\(l_1\\): 선형변환, feature를 뻥튀기하는 역할\n\n\\(\\sim\\) 꺾인 선이 많아진다\n\n\\(relu\\): 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화\n\\(l_2\\): 선형변환, 뻥튀기된 feature를 요약 하는 역할 (=데이터를 요약하는 역할)\n\n- 새로운 아키텍처 - \\(conv\\): feature를 뻥튀기하는 역할 (2d ver \\(l_1\\) 느낌) - \\(relu\\): - \\(pooling\\): 데이터를 요약하는 역할\n\n\nCONV 레이어 (선형변환의 2D 버전)\n- 우선 연산하는 방법만 살펴보자.\n(예시1)\n\ntorch.manual_seed(43052)\n_conv = torch.nn.Conv2d(1,1,(2,2)) # 입력1, 출력1, (2,2) window size\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[-0.1733, -0.4235],\n           [ 0.1802,  0.4668]]]]),\n tensor([0.2037]))\n\n\n\n_X = torch.arange(4).reshape(1,1,2,2).float()\n_X\n\ntensor([[[[0., 1.],\n          [2., 3.]]]])\n\n\n\n(-0.1733)*0 + (-0.4235)*1 +\\\n(0.1802)*2 + (0.4668)*3 + 0.2037\n\n1.541\n\n\n\n_conv(_X)\n\ntensor([[[[1.5410]]]], grad_fn=<ThnnConv2DBackward0>)\n\n\n\ntorch.__version__\n\n'1.10.1'\n\n\n(예시2) 잘하면 평균도 계산하겠다?\n\n_conv.weight.data = torch.tensor([[[[1/4, 1/4],[1/4,1/4]]]])\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data,_conv.bias.data\n\n(tensor([[[[0.2500, 0.2500],\n           [0.2500, 0.2500]]]]),\n tensor([0.]))\n\n\n\n_conv(_X) , (0+1+2+3)/4\n\n(tensor([[[[1.5000]]]], grad_fn=<ThnnConv2DBackward0>), 1.5)\n\n\n(예시3) 이동평균?\n\n_X = torch.arange(0,25).float().reshape(1,1,5,5) \n_X\n\ntensor([[[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]])\n\n\n\n_conv(_X)\n\ntensor([[[[ 3.,  4.,  5.,  6.],\n          [ 8.,  9., 10., 11.],\n          [13., 14., 15., 16.],\n          [18., 19., 20., 21.]]]], grad_fn=<ThnnConv2DBackward0>)\n\n\n(예시4) window size가 증가한다면? (2d의 이동평균느낌)\n\n_conv = torch.nn.Conv2d(1,1,(3,3)) # 입력1, 출력1, (3,3) window size\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data = torch.tensor([[[[1/9,1/9,1/9],[1/9,1/9,1/9],[1/9,1/9,1/9]]]])\n\n(3,3)이나~ 3이나~\n\n_X,_conv(_X)\n\n(tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n           [ 5.,  6.,  7.,  8.,  9.],\n           [10., 11., 12., 13., 14.],\n           [15., 16., 17., 18., 19.],\n           [20., 21., 22., 23., 24.]]]]),\n tensor([[[[ 6.0000,  7.0000,  8.0000],\n           [11.0000, 12.0000, 13.0000],\n           [16.0000, 17.0000, 18.0000]]]], grad_fn=<ThnnConv2DBackward0>))\n\n\n\n(1+2+3+6+7+8+11+12+13)/9\n\n7.0\n\n\n(예시5) 피처뻥튀기\n\n_X = torch.tensor([1.0,1.0,1.0,1.0]).reshape(1,1,2,2)\n_X\n\ntensor([[[[1., 1.],\n          [1., 1.]]]])\n\n\n\n_conv = torch.nn.Conv2d(1,8,(2,2))\n_conv.weight.data.shape,_conv.bias.data.shape\n\n(torch.Size([8, 1, 2, 2]), torch.Size([8]))\n\n\n\n_conv(_X).shape\n\ntorch.Size([1, 8, 1, 1])\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n\ntorch.sum(_conv.weight.data[0,...])+_conv.bias.data[0],\\\ntorch.sum(_conv.weight.data[1,...])+_conv.bias.data[1]\n\n(tensor(-0.3464), tensor(0.2739))\n\n\n결국 아래를 계산한다는 의미\n\ntorch.sum(_conv.weight.data,axis=(2,3)).reshape(-1)+ _conv.bias.data\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345])\n\n\n\n_conv(_X).reshape(-1)\n\ntensor([-0.3464,  0.2739,  0.1069,  0.6105,  0.0432,  0.8390,  0.2353,  0.2345],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n(잔소리) axis 사용 익숙하지 않으면 아래 꼭 들으세요..\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6주차)-4월11일.html , numpy공부 4단계: 축\n\n\n\nReLU (2d)\n\n_X = torch.randn(25).reshape(1,5,5)\n_X\n\ntensor([[[ 0.2656,  0.0780,  3.0465,  1.0151, -2.3908],\n         [ 0.4749,  1.6519,  1.5454,  1.0376,  0.9291],\n         [-0.7858,  0.4190,  2.6057, -0.4022,  0.2092],\n         [ 0.9594,  0.6408, -0.0411, -1.0720, -2.0659],\n         [-0.0996,  1.1351,  0.9758,  0.4952, -0.5475]]])\n\n\n\na1=torch.nn.ReLU()\n\n\na1(_X)\n\ntensor([[[0.2656, 0.0780, 3.0465, 1.0151, 0.0000],\n         [0.4749, 1.6519, 1.5454, 1.0376, 0.9291],\n         [0.0000, 0.4190, 2.6057, 0.0000, 0.2092],\n         [0.9594, 0.6408, 0.0000, 0.0000, 0.0000],\n         [0.0000, 1.1351, 0.9758, 0.4952, 0.0000]]])\n\n\n\n\nMaxpooling 레이어\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n\n\n_X = torch.arange(16).float().reshape(1,4,4) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]),\n tensor([[[ 5.,  7.],\n          [13., 15.]]]))\n\n\n가장 중요한 특징만 남게 될 것이다.\n\n_X = torch.arange(25).float().reshape(1,5,5) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.,  8.],\n          [16., 18.]]]))\n\n\n버려지는 데이터\n\n_X = torch.arange(36).float().reshape(1,6,6) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10., 11.],\n          [12., 13., 14., 15., 16., 17.],\n          [18., 19., 20., 21., 22., 23.],\n          [24., 25., 26., 27., 28., 29.],\n          [30., 31., 32., 33., 34., 35.]]]),\n tensor([[[ 7.,  9., 11.],\n          [19., 21., 23.],\n          [31., 33., 35.]]]))"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-구현-cpu",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-구현-cpu",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "이미지자료분석– CNN 구현 (CPU)",
    "text": "이미지자료분석– CNN 구현 (CPU)\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\n(1) Conv2d\n\nc1 = torch.nn.Conv2d(1,16,(5,5))\nprint(X.shape)\nprint(c1(X).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(2) ReLU\n\na1 = torch.nn.ReLU()\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(3) MaxPool2D\n\nm1 =  torch.nn.MaxPool2d((2,2)) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\n\n\n\n\n(4) 적당히 마무리하고 시그모이드 태우자\n- 펼치자.\n(방법1)\n\nm1(a1(c1(X))).reshape(-1,2304).shape\n\ntorch.Size([12665, 2304])\n\n\n\n16*12*12 \n\n2304\n\n\n(방법2)\n\nflttn = torch.nn.Flatten()\n\n\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\n\n\n- 2304 \\(\\to\\) 1 로 차원축소하는 선형레이어를 설계\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\n\n\n- 시그모이드\n\na2 = torch.nn.Sigmoid()\n\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\nprint(a1(l1(flttn(m1(a1(c1(X)))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\ntorch.Size([12665, 1])\n\n\n- 네트워크 설계\n\nnet = torch.nn.Sequential(\n    c1, # 2d: 컨볼루션(선형변환), 피처 뻥튀기 \n    a1, # 2d: 렐루(비선형변환)\n    m1, # 2d: 맥스풀링: 데이터요약\n    flttn, # 2d->1d \n    l1, # 1d: 선형변환\n    a2 # 1d: 시그모이드(비선형변환) \n)\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nt1= time.time()\nfor epoc in range(100): \n    ## 1\n    yhat = net(X) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4\n    optimizr.step()\n    optimizr.zero_grad()\nt2= time.time()\nt2-t1\n\n39.31594634056091\n\n\n\nplt.plot(y)\nplt.plot(net(X).data,'.')\nplt.title('Traning Set',size=15)\n\nText(0.5, 1.0, 'Traning Set')\n\n\n\n\n\n\nplt.plot(yy)\nplt.plot(net(XX).data,'.')\nplt.title('Test Set',size=15)\n\nText(0.5, 1.0, 'Test Set')"
  },
  {
    "objectID": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-구현-gpu",
    "href": "posts/ml/2022-10-19-ml_(7주차).html#이미지자료분석-cnn-구현-gpu",
    "title": "기계학습 특강 (7주차) 10월19일",
    "section": "이미지자료분석– CNN 구현 (GPU)",
    "text": "이미지자료분석– CNN 구현 (GPU)\n\n1. dls\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\nlen(X)/10\n\n1266.5\n\n\n\nlen(XX)\n\n2115\n\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\n\n2. lrnr 생성: 아키텍처, 손실함수, 옵티마이저\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn)\n\n\n\n3. 학습\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.901239\n      0.605223\n      00:00\n    \n    \n      1\n      0.660227\n      0.370985\n      00:00\n    \n    \n      2\n      0.507106\n      0.213785\n      00:00\n    \n    \n      3\n      0.393017\n      0.113283\n      00:00\n    \n    \n      4\n      0.304846\n      0.065374\n      00:00\n    \n    \n      5\n      0.238648\n      0.042887\n      00:00\n    \n    \n      6\n      0.189261\n      0.031143\n      00:00\n    \n    \n      7\n      0.152003\n      0.024236\n      00:00\n    \n    \n      8\n      0.123435\n      0.019730\n      00:00\n    \n    \n      9\n      0.101176\n      0.016531\n      00:00\n    \n  \n\n\n\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\n\n4. 예측 및 시각화\n\nnet.to(\"cpu\") \n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n- 결과를 시각화하면 아래와 같다.\n\nplt.plot(net(X).data,'.')\nplt.title(\"Training Set\",size=15)\n\nText(0.5, 1.0, 'Training Set')\n\n\n\n\n\n\nplt.plot(net(XX).data,'.')\nplt.title(\"Test Set\",size=15)\n\nText(0.5, 1.0, 'Test Set')\n\n\n\n\n\n1/10만 사용했는데 잘 training된 것 같다\n- 빠르고 적합결과도 좋음\n\n\nLrnr 오브젝트\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nnet\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nid(lrnr.model), id(net)\n\n(140021490006720, 140021490006720)\n\n\n\nlrnr.model(X)\n\ntensor([[4.5555e-05],\n        [1.2910e-03],\n        [6.6828e-04],\n        ...,\n        [9.8670e-01],\n        [9.8576e-01],\n        [9.9344e-01]], grad_fn=<SigmoidBackward0>)\n\n\n\nnet(X)\n\ntensor([[4.5555e-05],\n        [1.2910e-03],\n        [6.6828e-04],\n        ...,\n        [9.8670e-01],\n        [9.8576e-01],\n        [9.9344e-01]], grad_fn=<SigmoidBackward0>)\n\n\n같은 결과\n20221026 수업\n\n\nBCEWithLogitsLoss\n- BCEWithLogitsLoss = Sigmoid + BCELoss - 왜 써요? 수치적으로 더 안정\ntorch.nn.BCEWithLogitsLoss - This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n- 사용방법\n\ndls 만들기\n\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\ntorch.utils.data.TensorDataset?\n\n\nInit signature: torch.utils.data.TensorDataset(*args, **kwds)\nDocstring:     \nDataset wrapping tensors.\nEach sample will be retrieved by indexing tensors along the first dimension.\nArgs:\n    *tensors (Tensor): tensors that have the same size of the first dimension.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/utils/data/dataset.py\nType:           type\nSubclasses:     \n\n\n\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수입니다\n\n\nlrnr생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    #torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCEWithLogitsLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.956781\n      0.642780\n      00:00\n    \n    \n      1\n      0.709626\n      0.419758\n      00:00\n    \n    \n      2\n      0.554641\n      0.248010\n      00:00\n    \n    \n      3\n      0.431661\n      0.118707\n      00:00\n    \n    \n      4\n      0.331514\n      0.059536\n      00:00\n    \n    \n      5\n      0.256312\n      0.035956\n      00:00\n    \n    \n      6\n      0.200917\n      0.025288\n      00:00\n    \n    \n      7\n      0.159611\n      0.019510\n      00:00\n    \n    \n      8\n      0.128254\n      0.015889\n      00:00\n    \n    \n      9\n      0.104057\n      0.013373\n      00:00\n    \n  \n\n\n\n\n예측 및 시각화\n\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n)\n\n\n시각화 위해서 cpu로 옮겨주기\n\nnet(X)\n\ntensor([[-9.4061],\n        [-6.7910],\n        [-7.9819],\n        ...,\n        [ 4.3685],\n        [ 4.4061],\n        [ 5.4793]], grad_fn=<AddmmBackward0>)\n\n\nsigmoid 취하기 전이지 우리는 bcewithlogiticsLoss 썼잖아, 그래서 0~1사이 아님\n\na2(torch.tensor(0))\n\ntensor(0.5000)\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).data,',',color=\"C1\")\nax[1].plot(y)\nax[1].plot(a2(net(X)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(a2(net(XX)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html",
    "href": "posts/ml/2022-10-05-ml-HW.html",
    "title": "기계학습 특강 (6주차) 10월5일 Homework",
    "section": "",
    "text": "HW"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html#imports",
    "href": "posts/ml/2022-10-05-ml-HW.html#imports",
    "title": "기계학습 특강 (6주차) 10월5일 Homework",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport torchvision\nfrom fastai.data.all import *\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-10-05-ml-HW.html#숙제-해설-및-풀이는-여기참고",
    "href": "posts/ml/2022-10-05-ml-HW.html#숙제-해설-및-풀이는-여기참고",
    "title": "기계학습 특강 (6주차) 10월5일 Homework",
    "section": "숙제 (해설 및 풀이는 여기참고)",
    "text": "숙제 (해설 및 풀이는 여기참고)\n\n숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라\n\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,64)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,64)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n위에서 \\(a_1\\)은 relu를, \\(a_2\\)는 sigmoid를 의미한다.\n\n“y=0”은 숫자0을 의미하도록 하고 “y=1”은 숫자1을 의미하도록 설정하라.\n\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\n\n\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\n\n\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\n\n\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28).float()\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라.\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\nyhat.data\n\ntensor([[nan],\n        [nan],\n        [nan],\n        ...,\n        [nan],\n        [nan],\n        [nan]])\n\n\n학습이 잘 되지 않았다.\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!)\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nX = X/255\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr=torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래와 같은 수식을 이용하여 accuracy를 계산하라.\n\n\\(\\text{accuracy}=\\frac{1}{n}\\sum_{i=1}^n I(\\tilde{y}_i=y_i)\\) - \\(\\tilde{y}_i = \\begin{cases}  1 & \\hat{y}_i > 0.5 \\\\  0 & \\hat{y}_i \\leq 0.5 \\end{cases}\\) - \\(I(\\tilde{y}_i=y_i) = \\begin{cases} 1 & \\tilde{y}_i=y_i \\\\ 0 & \\tilde{y}_i \\neq y_i \\end{cases}\\)\n단, \\(n\\)은 0과 1을 의미하는 이미지의 수\n\nytilde = (yhat > 0.5) * 1\n\n\nytilde\n\ntensor([[0],\n        [0],\n        [0],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\n(ytilde == y) * 1\n\ntensor([[1],\n        [1],\n        [1],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\ntorch.sum((ytilde == y) * 1)\n\ntensor(12661)\n\n\n\ntorch.sum((ytilde == y) * 1)/len(y)\n\ntensor(0.9997)\n\n\n\nprint(\"accuraccy: \",torch.sum((ytilde == y) * 1)/len(y))\n\naccuraccy:  tensor(0.9997)"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "",
    "text": "(4주차) 9월28일 [회귀분석(2)–step1~4, step1의 다른표현, step4의 다른표현, 로지스틱 intro]"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#imports",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#imports",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "imports",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#numpy-torch-선택학습",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#numpy-torch-선택학습",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "numpy, torch (선택학습)",
    "text": "numpy, torch (선택학습)\n\nnumpy, torch는 엄청 비슷해요\n- torch.tensor() = np.array() 처럼 생각해도 무방\n\nnp.array([1,2,3]), torch.tensor([1,2,3])\n\n(array([1, 2, 3]), tensor([1, 2, 3]))\n\n\n- 소수점의 정밀도에서 차이가 있음 (torch가 좀 더 쪼잔함)\n\nnp.array([3.123456789])\n\narray([3.12345679])\n\n\n\ntorch.tensor([3.123456789])\n\ntensor([3.1235])\n\n\n(서연필기)tensor는 gpu에 저장하기 때문에 메모리 아끼기 위해 정밀도가 낮은 경향이 있다.\n- 기본적인 numpy 문법은 np 대신에 torch를 써도 무방 // 완전 같지는 않음\n\nnp.arange(10), torch.arange(10)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n\n\nnp.linspace(0,1,10), torch.linspace(0,1,10)\n\n(array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n         1.0000]))\n\n\n\nnp.random.randn(10)\n\narray([-0.90388568,  0.51779102,  0.73699131, -0.88030899,  1.71668715,\n       -0.70735651, -0.29752154,  1.10432159,  0.23592126,  0.91669421])\n\n\n\ntorch.randn(10)\n\ntensor([ 0.6896,  1.8534, -0.3807,  1.3676,  0.0515,  0.4350,  0.6051, -1.5075,\n         0.1474,  0.3098])\n\n\n\n\nlength \\(n\\) vector, \\(n \\times 1\\) col-vector, \\(1 \\times n\\) row-vector\n브로드캐스팅 길이가 3인 벡터와 1인벡터를 더하면 오류 뜨지 않고 더해줌\n- 길이가 3인 벡터 선언방법\n\na = torch.tensor([1,2,3])\na.shape\n\ntorch.Size([3])\n\n\n- 3x1 col-vec 선언방법\n(방법1)\n\na = torch.tensor([[1],[2],[3]])\na.shape\n\ntorch.Size([3, 1])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(3,1)\na.shape\n\ntorch.Size([3, 1])\n\n\n- 1x3 row-vec 선언방법\n(방법1)\n\na = torch.tensor([[1,2,3]])\na.shape\n\ntorch.Size([1, 3])\n\n\n(방법2)\n\na = torch.tensor([1,2,3]).reshape(1,3)\na.shape\n\ntorch.Size([1, 3])\n\n\n- 3x1 col-vec 선언방법, 1x3 row-vec 선언방법에서 [[1],[2],[3]] 혹은 [[1,2,3]] 와 같은 표현이 이해안되면 아래링크로 가셔서\nhttps://guebin.github.io/STBDA2022/2022/03/14/(2주차)-3월14일.html\n첫번째 동영상 12:15 - 22:45 에 해당하는 분량을 학습하시길 바랍니다.\n\n\ntorch의 dtype\n- 기본적으로 torch는 소수점으로 저장되면 dtype=torch.float32 가 된다. (이걸로 맞추는게 편리함)\n\ntsr = torch.tensor([1.23,2.34])\ntsr\n\ntensor([1.2300, 2.3400])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n- 정수로 선언하더라도 dtype를 torch.float32로 바꾸는게 유리함\n(안 좋은 선언예시)\n\ntsr = torch.tensor([1,2])\ntsr \n\ntensor([1, 2])\n\n\n\ntsr.dtype\n\ntorch.int64\n\n\n(좋은 선언예시1)\n\ntsr = torch.tensor([1,2],dtype=torch.float32)\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(좋은 선언예시2)\n\ntsr = torch.tensor([1,2.0])\ntsr \n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n(사실 int로 선언해도 나중에 float으로 바꾸면 큰 문제없음)\n\ntsr = torch.tensor([1,2]).float()\ntsr\n\ntensor([1., 2.])\n\n\n\ntsr.dtype\n\ntorch.float32\n\n\n- 왜 정수만으로 torch.tensor를 만들때에도 torch.float32로 바꾸는게 유리할까? \\(\\to\\) torch.tensor끼리의 연산에서 문제가 될 수 있음\n별 문제 없을수도 있지만\n\ntorch.tensor([1,2])-torch.tensor([1.0,2.0]) \n\ntensor([0., 0.])\n\n\n아래와 같이 에러가 날수도 있다\n(에러1)\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]]) \n\nRuntimeError: expected scalar type Float but found Long\n\n\n(에러2)\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]])\n\nRuntimeError: expected scalar type Long but found Float\n\n\n(해결1) 둘다 정수로 통일\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1],[2]])\n\ntensor([[1],\n        [2]])\n\n\n(해결2) 둘다 소수로 통일 <– 더 좋은 방법임\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1.0],[2.0]])\n\ntensor([[1.],\n        [2.]])\n\n\n\n\nshape of vector\n- 행렬곱셈에 대한 shape 조심\n\nA = torch.tensor([[2.00,0.00],[0.00,3.00]]) \nb1 = torch.tensor([[-1.0,-5.0]])\nb2 = torch.tensor([[-1.0],[-5.0]])\nb3 = torch.tensor([-1.0,-5.0])\n\n\nA.shape,b1.shape,b2.shape,b3.shape\n\n(torch.Size([2, 2]), torch.Size([1, 2]), torch.Size([2, 1]), torch.Size([2]))\n\n\n- A@b1: 계산불가, b1@A: 계산가능\n\nA@b1\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x2 and 1x2)\n\n\n\nb1@A\n\ntensor([[ -2., -15.]])\n\n\n- A@b2: 계산가능, b2@A: 계산불가\n\nA@b2\n\ntensor([[ -2.],\n        [-15.]])\n\n\n\nb2@A\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2)\n\n\n- A@b3: 계산가능, b3@A: 계산가능\n\n(A@b3).shape ## b3를 마치 col-vec 처럼 해석\n\ntorch.Size([2])\n\n\n\n(b3@A).shape ## b3를 마지 row-vec 처럼 해석\n\ntorch.Size([2])\n\n\n\n뒤에 놓으면 b3를 컬럼벡터로 인식\n앞에 놓으면 b3를 로우벡터로 인식\n\n- 브로드캐스팅\n\na = torch.tensor([1,2,3])\na - 1\n\ntensor([0, 1, 2])\n\n\n\nb = torch.tensor([[1],[2],[3]])\nb - 1\n\ntensor([[0],\n        [1],\n        [2]])\n\n\n계산이 되지 않아야 맞지 않나\n\na - b # a를 row-vec 로 해석\n\ntensor([[ 0,  1,  2],\n        [-1,  0,  1],\n        [-2, -1,  0]])\n\n\n잘못 계싼할 수 있으니 dimension 명시해주자"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#review-step14",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#review-step14",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "Review: step1~4",
    "text": "Review: step1~4\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \ndf\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -2.482113\n      -8.542024\n    \n    \n      1\n      -2.362146\n      -6.576713\n    \n    \n      2\n      -1.997295\n      -5.949576\n    \n    \n      3\n      -1.623936\n      -4.479364\n    \n    \n      4\n      -1.479192\n      -4.251570\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      95\n      2.244400\n      10.325987\n    \n    \n      96\n      2.393501\n      12.266493\n    \n    \n      97\n      2.605604\n      13.098280\n    \n    \n      98\n      2.605658\n      12.546793\n    \n    \n      99\n      2.663240\n      13.834002\n    \n  \n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df.x, df.y,'o')\n\n\n\n\n\ntorch.tensor(df.x)\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632], dtype=torch.float64)\n\n\n(서연필기)\nfloat64 숫자 정밀 저장\nfloat32이면 dtype=torch.float64)꼬리표가 붙지 않음\n_trt = torch.tensor(df.x).float()\n_trt = torch.tensor(df.x,dtype=float30)\n같은 역할, 메모리 적게 쓰기 위해 타입 바꿔주자\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)\n컬럼형식으로 받아주기 위해 변경\n\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)\ny= torch.tensor(df.y,dtype=torch.float32).reshape(100,1)\n_1= torch.ones([100,1])\nX = torch.concat([_1,x],axis=1)\n\ntorch.ones([100,1])\ntorch.tensor([[1]*100,x]).T\n같은 셋\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\nrequires_grad=True \nreshape 미분 가능 옵션 주기 전에 shape 정해주자\n\nplt.plot(x,y,'o')\n#plt.plot(x,-5+10*x,'--')\nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nver1: loss = sum of squares error\n\nalpha = 1/1000\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.sum((y-yhat)**2)\n    # step3: 미분 \n    loss.backward()\n    # step4: update \n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,X@What.data,'--')\n\n\n\n\n\nnote: 왜 What = What - alpha*What.grad 는 안되는지?\n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n\nWhat.data\n\ntensor([[2.4290],\n        [4.0144]])\n\n\nWhat과 What.data는 달라요, requires_grad=True 미분 가능 꼬리표가 붙지 않기 때문!\n\n\nver2: loss = mean squared error = MSE\n\nalpha = 1/10\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nfor epoc in range(30): \n    # step1: yhat \n    yhat = X@What \n    # step2: loss \n    loss = torch.mean((y-yhat)**2)\n    # step3: 미분 \n    loss.backward()\n    # step4: update \n    What.data = What.data - alpha * What.grad \n    What.grad = None # \n\n\nWhat\n\ntensor([[2.4290],\n        [4.0144]], requires_grad=True)\n\n\n(서연필기)mean 정의 - 데이터를 더 효율적으로 학습 가능, 데이터 수만큼 안 해도 돼, 계산 덜 해도 돼"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step1의-다른버전-net-설계만",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step1의-다른버전-net-설계만",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "step1의 다른버전 – net 설계만",
    "text": "step1의 다른버전 – net 설계만\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n\ntorch.nn.Linear?\n\n\nInit signature:\ntorch.nn.Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n) -> None\nDocstring:     \nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\nArgs:\n    in_features: size of each input sample\n    out_features: size of each output sample\n    bias: If set to ``False``, the layer will not learn an additive bias.\n        Default: ``True``\nShape:\n    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n    - Output: :math:`(*, H_{out})` where all but the last dimension\n      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\nAttributes:\n    weight: the learnable weights of the module of shape\n        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n        :math:`k = \\frac{1}{\\text{in\\_features}}`\n    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n            If :attr:`bias` is ``True``, the values are initialized from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\nExamples::\n    >>> m = nn.Linear(20, 30)\n    >>> input = torch.randn(128, 20)\n    >>> output = m(input)\n    >>> print(output.size())\n    torch.Size([128, 30])\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/nn/modules/linear.py\nType:           type\nSubclasses:     NonDynamicallyQuantizableLinear, LazyLinear, Linear, Linear\n\n\n\n\ninput 잡는 법 - x의 컬럼 부분을 input이라고 생각하자\n\nx.shape\n\ntorch.Size([100, 1])\n\n\noutput 잡는 법 - y의 컬럼 부분을 output이라고 생각하자\n\ny.shape\n\ntorch.Size([100, 1])\n\n\n\n_net =  torch.nn.Linear(in_features=1, out_features=1, bias=True) \n\n\n_net(x).shape\n\ntorch.Size([100, 1])\n\n\n\n_net.bias # w0\n\nParameter containing:\ntensor([-0.1281], requires_grad=True)\n\n\n\n_net.weight # w1\n\nParameter containing:\ntensor([[0.1433]], requires_grad=True)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1, out_features=1, bias=True) \n\n\nnet.bias, net.weight\n\n(Parameter containing:\n tensor([-0.8470], requires_grad=True),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\nw0hat = -0.847\nw1hat = -0.3467\nplt.plot(x,w0hat+w1hat*x,'--')\n\n\n\n\n출력결과 같음을 확인\n- net에서 \\(\\hat{w}_0, \\hat{w}_1\\) 의 값은?\n\nnet.weight # w1 \n\nParameter containing:\ntensor([[-0.3467]], requires_grad=True)\n\n\n\nnet.bias # w0 \n\nParameter containing:\ntensor([-0.8470], requires_grad=True)\n\n\n\n_yhat = -0.8470 + -0.3467*x \n\n\nplt.plot(x,y,'o')\nplt.plot(x, _yhat,'--')\nplt.plot(x,net(x).data,'-.')\n\n\n\n\n- 수식표현: \\(\\hat{y}_i = \\hat{w}_0 + \\hat{w}_1 x_i = \\hat{b} + \\hat{w}x_i = -0.8470 + -0.3467 x_i\\) for all \\(i=1,2,\\dots,100\\).\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 입력이 x가 아닌 X를 넣고 싶다면? (보통 잘 안하긴 해요, 왜? bias=False로 주는게 귀찮거든요) - X는 바이어스가 고려된 상황\n\nnet(X) ## 그대로 쓰면 당연히 에러\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (100x2 and 1x1)\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2, out_features=1, bias=False) \n\n\nnet(X).shape\n\ntorch.Size([100, 1])\n\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n위에 \\(w_0,w_1\\) 순\n\nnet.bias\n\nbias 없음을 확인\n\nplt.plot(x,y,'o') \nplt.plot(x,net(X).data, '--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]]), '-.')\n\n\n\n\n- 수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix}\\)\n\n\n잘못된사용1\n\n_x = x.reshape(-1)\n\n\n_x\n\ntensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632])\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=1,out_features=1) \n\n\nnet(_x)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x100 and 1x1)\n\n\nnet(_x.reshape(100,1))\n과 같이 정의\n\n\n잘못된사용2\n\ntorch.manual_seed(43052)\nnet = torch.nn.Linear(in_features=2,out_features=1) # bias=False를 깜빡..\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.2451, -0.5989]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.2549], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\nplt.plot(x,X@torch.tensor([[-0.2451],[-0.5989]])+0.2549,'-.')\n\n\n\n\n\n수식표현: \\(\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} + \\hat{b}= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\begin{bmatrix} -0.2451 \\\\ -0.5989 \\end{bmatrix} + 0.2549\\)"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step1의-다른버전-끝까지",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step1의-다른버전-끝까지",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "step1의 다른버전 – 끝까지",
    "text": "step1의 다른버전 – 끝까지\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1,bias=True)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\nnet.weight,net.bias\n\n(Parameter containing:\n tensor([[10.]], requires_grad=True),\n Parameter containing:\n tensor([-5.], requires_grad=True))\n\n\n- step1\n\nyhat = net(x) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\nplt.plot(x,-5+10*x,'--')\n\n\n\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad,net.weight.grad\n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n- step4\n(업데이트전)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True),\n Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(tensor([-13.4225]), tensor([[11.8893]]))\n\n\n(업데이트)\n\nnet.bias.data = net.bias.data - 0.1*net.bias.grad \nnet.weight.data = net.weight.data - 0.1*net.weight.grad \n\n\nnet.bias.grad = None \nnet.weight.grad = None \n\n(업데이트후)\n\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-3.6577], requires_grad=True),\n Parameter containing:\n tensor([[8.8111]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n- 반복\n\nnet = torch.nn.Linear(1,1)\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\n\n\nfor epoc in range(30):\n    yhat = net(x) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.bias.data = net.bias.data - 0.1*net.bias.grad\n    net.weight.grad = None\n    net.bias.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 준비\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n- step1\n\nyhat = net(X)\n\n- step2\n\nloss = torch.mean((y-yhat)**2)\n\n- step3\n(미분전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n(미분)\n\nloss.backward()\n\n(미분후)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n- step4\n(업데이트전)\n\nnet.weight\n\nParameter containing:\ntensor([[-5., 10.]], requires_grad=True)\n\n\n\nnet.weight.grad\n\ntensor([[-13.4225,  11.8893]])\n\n\n(업데이트)\n\nnet.weight.data = net.weight.data - 0.1*net.weight.grad\n\n\nnet.weight.grad = None\n\n(업데이트후)\n\nnet.weight\n\nParameter containing:\ntensor([[-3.6577,  8.8111]], requires_grad=True)\n\n\n\nnet.weight.grad\n\n- 반복\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, 10.0]])\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = net(X) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.weight.grad = None\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step4의-다른버전-옵티마이저",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#step4의-다른버전-옵티마이저",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "step4의 다른버전: 옵티마이저!",
    "text": "step4의 다른버전: 옵티마이저!\n\nver1: net = torch.nn.Linear(1,1,bias=True)\n- 준비\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]]) \nnet.bias.data = torch.tensor([[-5.0]]) \n\n\ntorch.optim.SGD?\n\n\nInit signature:\ntorch.optim.SGD(\n    params,\n    lr=<required parameter>,\n    momentum=0,\n    dampening=0,\n    weight_decay=0,\n    nesterov=False,\n)\nDocstring:     \nImplements stochastic gradient descent (optionally with momentum).\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\\:nesterov\\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: nesterov                                                \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t-1} + \\mu \\textbf{b}_t                             \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                    \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float): learning rate\n    momentum (float, optional): momentum factor (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    dampening (float, optional): dampening for momentum (default: 0)\n    nesterov (bool, optional): enables Nesterov momentum (default: False)\nExample:\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> optimizer.zero_grad()\n    >>> loss_fn(model(input), target).backward()\n    >>> optimizer.step()\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et. al. and implementations in some other frameworks.\n    Considering the specific case of Momentum, the update can be written as\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n    This is in contrast to Sutskever et. al. and\n    other frameworks which employ an update of the form\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n    The Nesterov version is analogously modified.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/optim/sgd.py\nType:           type\nSubclasses:     \n\n\n\n\nStocastic Gradiant Decscent\n\nnet.parameters()\n\n<generator object Module.parameters at 0x7f5f0d522740>\n\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n- step1~3\n\nyhat = net(x)     \n\n\nloss = torch.mean((y-yhat)**2) \n\n\nloss.backward() \n\n- step4\n(update 전)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 전\n\n(tensor([[10.]]), tensor([[-5.]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 청소전 \n\n(tensor([[11.8893]]), tensor([[-13.4225]]))\n\n\n(update)\n\noptimizr.step() \noptimizr.zero_grad() \n\n(update 후)\n\nnet.weight.data, net.bias.data ## 값은 업데이트 되었음 \n\n(tensor([[8.8111]]), tensor([[-3.6577]]))\n\n\n\nnet.weight.grad, net.bias.grad ## 미분값은 0으로 초기화하였음 \n\n(tensor([[0.]]), tensor([[0.]]))\n\n\n- 반복\n\nnet = torch.nn.Linear(1,1) \nnet.weight.data = torch.tensor([[10.0]])\nnet.bias.data = torch.tensor([-5.0])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(30): \n    yhat = net(x)\n    loss = torch.mean((y-yhat)**2) \n    loss.backward() \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nver2: net = torch.nn.Linear(2,1,bias=False)\n- 바로 반복하겠습니다..\n\nnet = torch.nn.Linear(2,1,bias=False) \nnet.weight.data = torch.tensor([[-5.0, 10.0]])\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(30): \n    yhat = net(X)\n    loss = torch.mean((y-yhat)**2) \n    loss.backward() \n    optimizr.step(); optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(X).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#appendix-net.parameters의-의미-선택학습",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#appendix-net.parameters의-의미-선택학습",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "Appendix: net.parameters()의 의미? (선택학습)",
    "text": "Appendix: net.parameters()의 의미? (선택학습)\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters)) & {'__iter__'}\n\nset()\n\n\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 무슨의미?\n\n_generator = net.parameters()\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\n_generator.__next__()\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n\n_generator.__next__()\n\nStopIteration: \n\n\n- 이건 이런느낌인데?\n\n_generator2 = iter([net.weight,net.bias])\n\n\n_generator2\n\n<list_iterator at 0x7f5f0d2cdeb0>\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([[-0.4277]], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nParameter containing:\ntensor([-0.0629], requires_grad=True)\n\n\n\n_generator2.__next__()\n\nStopIteration: \n\n\n- 즉 아래는 같은코드이다.\n### 코드1\n_generator = net.parameters() \ntorch.optim.SGD(_generator,lr=1/10) \n### 코드2\n_generator = iter([net.weight,net.bias])\ntorch.optim.SGD(_generator,lr=1/10) \n### 코드3 (이렇게 써도 코드2가 실행된다고 이해할 수 있음)\n_iterator = [net.weight,net.bias]\ntorch.optim.SGD(_iterator,lr=1/10) \n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트(iterable object)로 만드는 함수라 이해할 수 있다.\n- 응용예제1\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step();optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n- 응용예제2\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10)\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    yhat = b+ w*x \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step(); optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#logistic-regression",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#logistic-regression",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 증가함.\n- (X,y)는 어떤모양?\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n_df \n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -6\n      0\n    \n    \n      1\n      -5\n      0\n    \n    \n      2\n      -4\n      0\n    \n    \n      3\n      -3\n      0\n    \n    \n      4\n      -2\n      0\n    \n    \n      5\n      -1\n      0\n    \n    \n      6\n      0\n      1\n    \n    \n      7\n      1\n      0\n    \n    \n      8\n      2\n      1\n    \n    \n      9\n      3\n      1\n    \n    \n      10\n      4\n      1\n    \n    \n      11\n      5\n      1\n    \n    \n      12\n      6\n      1\n    \n  \n\n\n\n\n\nplt.plot(_df.x,_df.y,'o')\n\n\n\n\n- (예비학습) 시그모이드라는 함수가 있음\n\nxx = torch.linspace(-6,6,100)\ndef f(x):\n    return torch.exp(x)/(1+torch.exp(x))\n\n\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))\nplt.plot(xx,f(2.5*xx-1.2)) # 영향을 크게 받을 때 + 운적인 요소 영향 받을 때(절편) -> 모델링하는 과정\n\n\n\n\n베르누이 특정 확률로 0 또는 1 뽑기\n\n\nmodel\n- \\(x\\)가 커질수록 \\(y=1\\)이 잘나오는 모형은 아래와 같이 설계할 수 있음 <— 외우세요!!!\n\n$y_i Ber(_i),$ where \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\\(\\hat{y}_i= \\hat{pi}_\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\) <— 외우세요!!\n\n\\(y_i=1\\) \\(\\hat{y_i} = 1\\)loss가 0 근처 \\(\\hat{y_i} = 0\\) loss가- 무한대\n\\(y_i = 0\\) $ = 0 $loss가 0근처 \\(\\hat{y_i} = 1\\) loss가 1\n\n\ntoy example\n- 예제시작\n\ntorch.bernoulli?\n\n\nDocstring:\nbernoulli(input, *, generator=None, out=None) -> Tensor\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\nThe :attr:`input` tensor should be a tensor containing probabilities\nto be used for drawing the binary random number.\nHence, all values in :attr:`input` have to be in the range:\n:math:`0 \\leq \\text{input}_i \\leq 1`.\nThe :math:`\\text{i}^{th}` element of the output tensor will draw a\nvalue :math:`1` according to the :math:`\\text{i}^{th}` probability value given\nin :attr:`input`.\n.. math::\n    \\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})\nThe returned :attr:`out` tensor only has values 0 or 1 and is of the same\nshape as :attr:`input`.\n:attr:`out` can have integral ``dtype``, but :attr:`input` must have floating\npoint ``dtype``.\nArgs:\n    input (Tensor): the input tensor of probability values for the Bernoulli distribution\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\nExample::\n    >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n    >>> a\n    tensor([[ 0.1737,  0.0950,  0.3609],\n            [ 0.7148,  0.0289,  0.2676],\n            [ 0.9456,  0.8937,  0.7202]])\n    >>> torch.bernoulli(a)\n    tensor([[ 1.,  0.,  0.],\n            [ 0.,  0.,  0.],\n            [ 1.,  1.,  1.]])\n    >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n    >>> torch.bernoulli(a)\n    tensor([[ 1.,  1.,  1.],\n            [ 1.,  1.,  1.],\n            [ 1.,  1.,  1.]])\n    >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n    >>> torch.bernoulli(a)\n    tensor([[ 0.,  0.,  0.],\n            [ 0.,  0.,  0.],\n            [ 0.,  0.,  0.]])\nType:      builtin_function_or_method\n\n\n\n\n\ntorch.bernoulli(torch.tensor([0.5]*100)) # 0.5의 확률ㄹ 0 또는 1 뽑아\n\ntensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n        0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n        1., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n\n\n\nx=torch.linspace(-1,1,2000).reshape(2000,1)\nw0= -1 \nw1= 5 \nu = w0+x*w1 \nv = torch.exp(u)/(1+torch.exp(u)) # v=πi, 즉 확률을 의미함\ny = torch.bernoulli(v) \n\n\nv\n\ntensor([[0.0025],\n        [0.0025],\n        [0.0025],\n        ...,\n        [0.9818],\n        [0.9819],\n        [0.9820]])\n\n\n\nu\n\ntensor([[-6.0000],\n        [-5.9950],\n        [-5.9900],\n        ...,\n        [ 3.9900],\n        [ 3.9950],\n        [ 4.0000]])\n\n\n\n#plt.scatter(x,y,alpha=0.05)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\n\n\n\n\n\n우리의 목적: \\(x\\)가 들어가면 빨간선 \\(\\hat{y}\\)의 값을 만들어주는 mapping을 학습해보자.\n\n\nw0hat = 10\nw1hat = 3\nyhat = f(w0hat + w1hat*x)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat,'--r')\n\n\n\n\n\nl1 = torch.nn.Linear(1,1)\n\n\nl1.bias.data = torch.tensor([-1.0])\nl1.weight.data = torch.tensor([[1.0]])\n\n\na1 = torch.nn.Sigmoid()\n\n\nw0hat = -1\nw1hat = 3\nyhat = a1(w0hat + w1hat*x)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat,'--r')\n\n\n\n\n\nfor epoc in range(6000):\n    ## step1 \n    yhat = a1(l1(x))\n    ## step2 \n    loss = torch.mean((y-yhat)**2) ## loss 를 원래 이렇게 하는건 아니에요.. \n    ## step3 \n    loss.backward()\n    ## step4 \n    l1.bias.data = l1.bias.data - 0.1 * l1.bias.grad \n    l1.weight.data = l1.weight.data - 0.1 * l1.weight.grad \n    l1.bias.grad = None \n    l1.weight.grad = None \n\n\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--r')\nplt.plot(x,a1(l1(x)).data,'--r')"
  },
  {
    "objectID": "posts/ml/2022-09-29-ml_4주차_9월28일.html#숙제",
    "href": "posts/ml/2022-09-29-ml_4주차_9월28일.html#숙제",
    "title": "기계학습 특강 (4주차) 9월28일",
    "section": "숙제",
    "text": "숙제"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-중간고사_대체과제.html",
    "href": "posts/ml/2022-11-02-ml-중간고사_대체과제.html",
    "title": "기계학습 특강",
    "section": "",
    "text": "중간고사 대체과제"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#크롤링을-통한-이미지-분석-및-cam",
    "href": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#크롤링을-통한-이미지-분석-및-cam",
    "title": "기계학습 특강",
    "section": "1. 크롤링을 통한 이미지 분석 및 CAM",
    "text": "1. 크롤링을 통한 이미지 분석 및 CAM\n(1) 두 가지 키워드로 크롤링을 수행하여 이미지자료를 모아라. (키워드는 각자 마음에 드는 것으로 설정할 것)\n힌트1: hynn, iu 라는 키워드로 크롤링하여 이미지자료를 모으는 코드\n\n#\n# 크롤링에 필요한 준비작업들\n#!pip install -Uqq duckduckgo_search\nfrom duckduckgo_search import ddg_images\nfrom fastdownload import download_url\nfrom fastcore.all import *\ndef search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n# \n# 폴더만드는코드 -- 사실 손으로 만들어도 무방함.. \n!mkdir images\n!mkdir images/train\n!mkdir images/test \n!mkdir images/train/iu\n!mkdir images/train/hynn\n!mkdir images/test/iu\n!mkdir images/test/hynn\ndownload_images(dest='./images/train/iu',urls=search_images('iu',max_images=200)) # iu 라는 키워드로 200개 이미지 크롤링 -> ./images/train/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/hynn',urls=search_images('hynn',max_images=200)) # hynn 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/hynn 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/iu',urls=search_images('iu kpop',max_images=200))  # iu kpop 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/train/hynn',urls=search_images('hynn kpop',max_images=200)) # hynn kpop 이라는 키워드로 200개 이미지 크롤링 -> ./images/train/hynn 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/iu',urls=search_images('iu photo',max_images=50)) # iu photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/hynn',urls=search_images('hynn photo',max_images=50)) # hynn photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/hynn 에 저장 \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n힌트2: 불량이미지 삭제\nbad_images = verify_images(get_image_files('./images'))\nbad_images\n\n불량이미지 목록\n\nbad_images.map(Path.unlink)\n\n불량이미지는 dls를 불러올때 방해되므로 제거\n\n\n!mkdir images/train/bezos\n!mkdir images/train/musk\n!mkdir images/test/bezos\n!mkdir images/test/musk\n\n\ndownload_images(dest='./images/train/bezos',urls=search_images('jeff bezos',max_images=200))\ntime.sleep(10)\ndownload_images(dest='./images/train/musk',urls=search_images('elon musk',max_images=200)) \ntime.sleep(10)\ndownload_images(dest='./images/train/bezos',urls=search_images('jeff bezos rich',max_images=200))  \ntime.sleep(10) \ndownload_images(dest='./images/test/musk',urls=search_images('elon musk rich',max_images=200)) \ntime.sleep(10)\n\ndownload_images(dest='./images/test/bezos',urls=search_images('jeff bezos photo',max_images=50)) # iu photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/iu 에 저장\ntime.sleep(10) # 서버과부하를 위한 휴식코드 \ndownload_images(dest='./images/test/musk',urls=search_images('elon usk photo',max_images=50)) # hynn photo 라는 키워드로 50개 이미지 크롤링 -> ./images/test/hynn 에 저장 \ntime.sleep(10) # 서버과부하를 위한 휴식코드 \n\n\nbad_images = verify_images(get_image_files('./images'))\nbad_images\n\n(#25) [Path('images/train/bezos/1d39746b-ad4d-49ac-ac25-0f569a94e3f2.jpg'),Path('images/train/bezos/ee365bea-bb15-4b79-9ebc-350d5a823ca8.JPG'),Path('images/train/bezos/289cbafa-0fbc-4be9-a39f-68429c876095.jpg'),Path('images/train/bezos/21495a5a-9602-42f5-8f7b-4978856bd1e8.jpg'),Path('images/train/bezos/3bd6c738-5361-4f95-9b3b-643904f5a135.jpg'),Path('images/train/bezos/2ca0722d-cb88-4f18-bc5f-c410bd16243f.jpg'),Path('images/train/bezos/e9924f91-aac6-4a11-a235-dacb7f80d576.jpg'),Path('images/train/bezos/fcc25b56-8807-4e52-bc37-6e1718994d8a.jpg'),Path('images/train/bezos/07e554ae-1261-4c81-b908-7c8d94e94702.jpg'),Path('images/train/bezos/ea6592bc-eff0-4db6-b93c-3b6e85897877.jpg')...]\n\n\n\nbad_images.map(Path.unlink)\n\n(#25) [None,None,None,None,None,None,None,None,None,None...]\n\n\n(2) ImageDataLoaders.from_folder 를 이용하여 dls를 만들어라.\n힌트1: dls를 만드는 코드\ndls = ImageDataLoaders.from_folder(path = './images', train='train',valid='test',item_tfms=Resize(512),bs=8) \ndls.show_batch()\n\ndls = ImageDataLoaders.from_folder(path = './images', train='train',valid='test',item_tfms=Resize(512),bs=8) \n\n\ndls.show_batch()\n\n\n\n\n(3) resnet34를 이용하여 학습하라.\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n\nlrnr.fine_tune(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.623527\n      0.999150\n      0.695341\n      00:07\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.350595\n      1.298542\n      0.695341\n      00:09\n    \n    \n      1\n      0.228978\n      1.561831\n      0.759857\n      00:08\n    \n    \n      2\n      0.185354\n      1.211755\n      0.806452\n      00:08\n    \n    \n      3\n      0.210802\n      1.586954\n      0.731183\n      00:08\n    \n    \n      4\n      0.210048\n      0.952422\n      0.820789\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n(4) CAM (class activation mapping)을 이용하여 (3)의 모형의 판단근거를 시각화하라.\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nprint(net1(_X).shape)\nprint(net2[0](net1(_X)).shape)\nprint(net2[1](net2[0](net1(_X))).shape)\nprint(net2[2](net2[1](net2[0](net1(_X)))).shape)\n\ntorch.Size([8, 512, 16, 16])\ntorch.Size([8, 1024, 1, 1])\ntorch.Size([8, 1024])\ntorch.Size([8, 1024])\n\n\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -> (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -> (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -> (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy) \n\n\nlrnr2.loss_func, lrnr.loss_func\n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.473621\n      7.780233\n      0.831541\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.358073\n      1.187476\n      0.767025\n      00:08\n    \n    \n      1\n      0.378113\n      3.513460\n      0.236559\n      00:09\n    \n    \n      2\n      0.410308\n      0.585618\n      0.713262\n      00:08\n    \n    \n      3\n      0.339331\n      0.728930\n      0.713262\n      00:08\n    \n    \n      4\n      0.250342\n      0.674079\n      0.745520\n      00:08\n    \n  \n\n\n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  \"Palette images with Transparency expressed in bytes should be \"\n\n\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\npath = './images'\n\n\nfig, ax = plt.subplots(5,5) \nk=200\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_bezos = why[0,0,:,:] \n        why_musk = why[0,1,:,:] \n        bezosprob, muskprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if bezosprob>muskprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_bezos.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"bezos(%2f)\" % bezosprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_musk.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"musk(%2f)\" % muskprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#overparameterized-model",
    "href": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#overparameterized-model",
    "title": "기계학습 특강",
    "section": "2. Overparameterized Model",
    "text": "2. Overparameterized Model\n(풀이 있음)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n(1) 아래의 모형을 가정하고 \\(\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(x) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[6.2915]]), tensor([3.2051]))\n\n\n(2) 아래의 모형을 가정하고 \\(\\beta_0\\)를 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_0 + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nW0hat = torch.tensor([0.0], requires_grad=True)\n\n\nW0hat\n\ntensor([0.], requires_grad=True)\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,(0*x+W0hat).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = 0 * x + W0hat\n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    W0hat.data = W0hat.data - 0.1*W0hat.grad\n    W0hat.grad = None\n\n\nplt.plot(x,y,'.')\nplt.plot(x,(0*x+W0hat).data,'--')\n\n\n\n\n\nW0hat\n\ntensor([3.1945], requires_grad=True)\n\n\n(3) 아래의 모형을 가정하고 \\(\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\beta_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nnet = torch.nn.Linear(1,1,bias = False)\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nfor epoc in range(100):\n    yhat = net(x) \n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\nnet.weight.data\n\ntensor([[6.2754]])\n\n\n(4) 아래의 모형을 가정하고 \\(\\alpha_0,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\\(\\hat{\\alpha}_0+\\hat{\\beta}_0\\)은 얼마인가? 이 값과 문제 (1)에서 추정된 \\(\\hat{\\beta_0}\\)의 값과 비교하여 보라.\n\n_1= torch.ones([1000,1])\nX = torch.concat([_1,x],axis=1)\n\n\nnet = torch.nn.Linear(in_features=2,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(X) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[1.5093, 6.2918]]), tensor([1.6958]))\n\n\n\n1.7377 + 1.4358\n\n3.1734999999999998\n\n\n\n6.2363\n\n6.2363\n\n\n(5) 아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하라. – 이거 제가 힌트를 잘못줬어요.. 문제가 좀 어렵게나왔네요 ㅠㅠ\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\\(\\hat{\\alpha}_0+\\hat{\\beta}_0\\), \\(\\hat{\\alpha}_1 + \\hat{\\beta}_1\\)의 값은 각각 얼마인가? 이 값들을 (1) 에서 추정된 \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) 값들과 비교하라.\n\n_1= torch.ones([1000,1])\nX = torch.concat([_1,x,x],axis=1)\n\n\nnet = torch.nn.Linear(in_features=3,out_features=1)\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=1/10) \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nfor epoc in range(100):\n    yhat = net(X) \n    loss = torch.mean((yhat-y)**2)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\n\n\n\n\n\nnet.weight.data, net.bias.data\n\n(tensor([[1.2913, 3.4037, 2.8956]]), tensor([1.9138]))\n\n\n\n1.9599 + 1.2138\n\n3.1737\n\n\n\n3.2835 + 2.9585\n\n6.242\n\n\n(6) 다음은 위의 모형에 대하여 학생들이 discussion한 결과이다. 올바르게 해석한 학생을 모두 골라라.\n민정: \\((x_i,y_i)\\)의 산점도는 직선모양이고 직선의 절펴과 기울기 모두 유의미해 보이므로 \\(y_i = \\beta_0 + \\beta_1 x_i\\) 꼴을 적합하는게 좋겠다.\n슬기: 나도 그렇게 생각해. 그래서 (2)-(3)과 같이 기울기를 제외하고 적합하거나 절편을 제외하고 적합하면 underfitting의 상황에 빠질 수 있어.\n성재: (2)의 경우 사실상 \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i\\)를 추정하는 것과 같아지게 되지.\n세민: (4)의 경우 \\({\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_n \\end{bmatrix}\\) 와 같이 설정하고 네트워크를 아래와 같이 설정할 경우 얻어지는 모형이야.\nnet = torch.nn.Linear(in_features=2,out_features=1,bias=True)\n구환: 모델 (4)-(5)는 표현력은 (1)과 동일하지만 추정할 파라메터는 (1)보다 많으므로 효율적인 모델이라고 볼 수 없어.\nanswer : 민정, 슬기, 세민, 구환\n이 문제의 경우 풀이를 여기에서 확인할 수 있습니다."
  },
  {
    "objectID": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#차원축소기법과-표현학습",
    "href": "posts/ml/2022-11-02-ml-중간고사_대체과제.html#차원축소기법과-표현학습",
    "title": "기계학습 특강",
    "section": "3. 차원축소기법과 표현학습",
    "text": "3. 차원축소기법과 표현학습\n다음은 아이리스데이터를 불러오는 코드이다. (아이리스 데이터에 대한 자세한 설명은 생략한다. 잘 모르는 학생은 구글검색을 해볼 것)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/STML2022/master/_notebooks/iris.csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      Sepal Length\n      Sepal Width\n      Petal Length\n      Petal Width\n      Species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\n\nX = torch.tensor(df.drop(columns=['Species']).to_numpy(), dtype=torch.float32)\n\n(1) 아래를 만족하도록 적당한 아키텍처, 손실함수를 설계하라. (손실함수는 MSE를 이용)\n\n\\(\\underset{(150,4)}{\\bf X} \\overset{l_1}{\\to} \\underset{(150,2)}{\\bf Z} \\overset{l_2}{\\to} \\underset{(150,4)}{\\bf \\hat X}\\)\n\\({\\bf \\hat X} \\approx {\\bf X}\\)\n\n차원축소\n\nX[:5]\n\ntensor([[5.1000, 3.5000, 1.4000, 0.2000],\n        [4.9000, 3.0000, 1.4000, 0.2000],\n        [4.7000, 3.2000, 1.3000, 0.2000],\n        [4.6000, 3.1000, 1.5000, 0.2000],\n        [5.0000, 3.6000, 1.4000, 0.2000]])\n\n\n\nX.shape\n\ntorch.Size([150, 4])\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(4,2,bias=False),\n    torch.nn.Linear(2,4,bias = False)\n)\n\n\nZ = net[0](X)\n\n\nZ.type()\n\n'torch.FloatTensor'\n\n\n\nZ[:5]\n\ntensor([[-0.8808, -1.5322],\n        [-0.8771, -1.5868],\n        [-0.8091, -1.4178],\n        [-0.8198, -1.4608],\n        [-0.8528, -1.4621]], grad_fn=<SliceBackward0>)\n\n\n\nXhat = net[1](net[0](X))\n\n\nXhat.type()\n\n'torch.FloatTensor'\n\n\n\nXhat[:5]\n\ntensor([[-0.6900, -1.1176, -0.2331, -0.6170],\n        [-0.6969, -1.1495, -0.2264, -0.6290],\n        [-0.6355, -1.0328, -0.2131, -0.5692],\n        [-0.6478, -1.0610, -0.2137, -0.5826],\n        [-0.6646, -1.0692, -0.2276, -0.5922]], grad_fn=<SliceBackward0>)\n\n\n(2) 아래코드를 이용하여 \\({\\bf X}\\), \\({\\bf Z}\\), \\({\\bf \\hat{X}}\\)를 시각화 하라.\n(시각화예시)\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z,Xhat],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // before learning',size=25);\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // before learning',size=25);\n\n\n\n\n(3) 네트워크를 학습시키고 \\({\\bf X}, {\\bf Z}, {\\bf \\hat{X}}\\)를 시각화하라.\n(시각화예시)\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z,Xhat],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // after learning',size=25);\n\noptimizr= torch.optim.Adam(net.parameters())\n\n\nloss_fn= torch.nn.MSELoss()\n\n\nfor epoc in range(2000): \n    ## 1 \n    Z = net[0](X) \n    Xhat = net[1](Z) \n    ## 2 \n    loss=loss_fn(Xhat,X) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nfig,ax = plt.subplots(figsize=(10,10)) \nax.imshow(torch.concat([X,Z.data,Xhat.data],axis=1)[:10])\nax.set_xticks(np.arange(0,10)) \nax.set_xticklabels([r'$X_1$',r'$X_2$',r'$X_3$',r'$X_4$',r'$Z_1$',r'$Z_2$',r'$\\hat{X}_1$',r'$\\hat{X}_2$',r'$\\hat{X}_3$',r'$\\hat{X}_4$'])\nax.vlines([3.5,5.5],ymin=-0.5,ymax=9.5,lw=2,color='red',linestyle='dashed')\nax.set_title(r'First 10 obs of $\\bf [X, Z, \\hat{X}]$ // after learning',size=25);\n\n\n\n\n(4) (3)의 결과로 학습된 \\(Z\\)를 입력벡터로 하고 \\(Z \\to y=\\text{Species}\\) 로 향하는 적당한 네트워크를 설계한 뒤 학습하라.\nx->y가는 mapping 안 찾아도 - z->y 가는 mapping 적절히 잘 찾으면 - x->y 적용가능한 linear function 찾기 가능\n\nZ.shape\n\ntorch.Size([150, 2])\n\n\n\nnet = torch.nn.Linear(2,4)\n\n\nloss_fn= torch.nn.CrossEntropyLoss()\n\n\noptimizr= torch.optim.Adam(net.parameters())\n\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \n\n\nmapping = {'setosa':0,'versicolor':1,'virginica':2}\n\n\ny_base = list(df['Species'])\n\n\ny = torch.tensor(f(y_base,mapping))\n\n\ny.unique()\n\ntensor([0, 1, 2])\n\n\n\nfor epoc in range(2000): \n    ## 1 \n    yhat = net(Z.data)\n    ## 2 \n    loss=loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nnet.weight\n\nParameter containing:\ntensor([[ 0.1512,  1.0270],\n        [ 0.1297, -1.1137],\n        [ 0.6069, -1.1514],\n        [-0.5099, -0.4560]], requires_grad=True)\n\n\n(5) (1)~(4)의 결과를 토의한 내용이다. 적절하게 토의한 사람을 모두 고르라.\n규빈: \\({\\bf Z}\\)는 \\({\\bf X}\\)보다 적은 feature를 가지고 있다. 또한 적절한 선형변환을 하면 \\({\\bf X}\\)와 비슷한 \\({\\bf \\hat X}\\)을 만들 수 있으므로 \\({\\bf X}\\)의 정보량 대부분 유지한채로 효과적으로 차원을 줄인 방법이라 볼 수 있다.\n민정: 즉 \\({\\bf X}\\)에서 \\({\\bf y}\\)로 가는 맵핑을 학습하는 과업은 \\({\\bf Z}\\)에서 \\({\\bf y}\\)로 가는 맵핑을 학습하는 과업은 거의 동등하다고 볼 수 있다.\n성재: \\({\\bf Z}\\)의 차원을 (n,4)로 설정한다면 이론상 \\({\\bf X}\\)와 동일한 \\({\\bf \\hat X}\\)을 만들어 낼 수 있다.\n슬기: \\({\\bf Z}\\)의 차원이 (n,2)일지라도 경우에 따라서 \\({\\bf X}\\)와 동일한 \\({\\bf \\hat X}\\)을 만들어 낼 수 있다.\nanswer : 규빈, 민정, 성재, 슬기"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "",
    "text": "(10주차) 11월9일 [순환신경망– abc예제, abdc예제, abcde예제, AbAcAd예제]"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#import",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#import",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "import",
    "text": "import\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n선행학습\n\nfor i in '123':\n    print(i)\n\n1\n2\n3\n\n\niterable object?\n\na = 3\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\na__iter__()\n\nNameError: name 'a__iter__' is not defined\n\n\n\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\n이게 없어\n\na = '3'\nfor i in a:\n    print(i)\n\n3\n\n\n\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n이제 있음\n\na = [1,2,3]\nfor i in a:\n    print(i)\n\n1\n2\n3\n\n\n\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na.__iter__()\n\n<list_iterator at 0x7f0f0e2b8bd0>\n\n\n\naa = iter(a)\n\n\naa.__next__()\n\n1\n\n\n\naa.__next__()\n\n2\n\n\n\naa.__next__()\n\n3\n\n\n\naa.__next__()\n\nStopIteration: \n\n\nStopIteration iter 끝내는 옵션\n\na = range(3)\nfor i in a:\n    print(i)\n\n0\n1\n2\n\n\n\naa = a.__iter__()\n\n\naa.__next__()\n\n0\n\n\n\naa.__next__()\n\n1\n\n\n\naa.__next__()\n\n2\n\n\n\naa.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#예비학습-net.parameters의-의미",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#예비학습-net.parameters의-의미",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "예비학습: net.parameters()의 의미",
    "text": "예비학습: net.parameters()의 의미\n9월27일 강의노트 중 “net.parameters()의 의미?”를 설명한다.\n- iterator, generator의 개념필요 - https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고\n- 탐구시작: 네트워크 생성\n\nnet = torch.nn.Linear(in_features=1,out_features=1)\nnet.weight\n\nParameter containing:\ntensor([[-0.3783]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.7503], requires_grad=True)\n\n\n- torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n\nnet.parameters() ## generator = \n\n<generator object Module.parameters at 0x7f0f0d6d7cd0>\n\n\n- 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 __iter__를 가지고 있는 오브젝트를 의미)\n\nset(dir(net.parameters())) & {'__iter__'}\n\n{'__iter__'}\n\n\n- 무슨의미?\n\nfor param in net.parameters():\n    print(param)\n\nParameter containing:\ntensor([[-0.3783]], requires_grad=True)\nParameter containing:\ntensor([0.7503], requires_grad=True)\n\n\n- 그냥 이건 이런느낌인데?\n\nfor param in [net.weight,net.bias]:\n    print(param)\n\nParameter containing:\ntensor([[-0.3783]], requires_grad=True)\nParameter containing:\ntensor([0.7503], requires_grad=True)\n\n\n결론: net.parameters()는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트같은 iterable object로 만드는 함수라 이해할 수 있다.\nyhat = net(x)\n꼭 이런 식으로 정의할 필요는 없다\n- 응용예제1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \nx=torch.tensor(df.x).float().reshape(100,1)\ny=torch.tensor(df.y).float().reshape(100,1)\n\n\nb = torch.tensor(-5.0,requires_grad=True)\nw = torch.tensor(10.0,requires_grad=True)\noptimizr = torch.optim.SGD([b,w],lr=1/10) ## 이렇게 전달하면 됩니당!!\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = b+ w*x \n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(w*x+b).data,'--')\n\n\n\n\n- 응용예제2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-22-regression.csv\") \nx = torch.tensor(df.x).float().reshape(100,1)\ny = torch.tensor(df.y).float().reshape(100,1)\nX = torch.concat([torch.ones_like(x),x],axis=1)\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\noptimizr = torch.optim.SGD([What],lr=1/10) # What은 iterable 하지 않지만 [What]은 iterable 함\n\n\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = X@What \n    ## step2 \n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@What).data,'--')\n\n\n\n\n\n스스로 학습 (중간고사 대체과제)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하고자한다.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라.\n\nalpha0 = torch.tensor([0.5], requires_grad=True)\nalpha1 = torch.tensor([[0.5]], requires_grad=True)\nbeta0 = torch.tensor([0.7], requires_grad=True)\nbeta1 = torch.tensor([[0.7]], requires_grad=True)\n\n\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([alpha0,alpha1,beta0,beta1], lr=1/10)\n\n\nfor epoc in range(30):\n    ## 1\n    yhat = alpha0 + beta0 + alpha1*x + beta1*x \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nprint(alpha0+beta0)\n\ntensor([3.1279], grad_fn=<AddBackward0>)\n\n\n\n3.14 근처\n\n\nprint(alpha1+beta1)\n\ntensor([[6.0170]], grad_fn=<AddBackward0>)\n\n\n\n6.28 근처"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#define-some-funtions",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#define-some-funtions",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "Define some funtions",
    "text": "Define some funtions\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \nsoft = torch.nn.Softmax(dim=1)"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam2-abc",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam2-abc",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "Exam2: abc",
    "text": "Exam2: abc\n\ndata\n\ntxt = list('abc')*100\ntxt[:10]\n\n['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'a', 'b'], ['b', 'c', 'a', 'b', 'c'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=3,embedding_dim=1),\n    torch.nn.Tanh(),\n    #===#\n    torch.nn.Linear(in_features=1,out_features=3)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\nnet[0] = embedding\nnet[1] = tanh\nnet[2] = linear\n- 결과해석\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nplt.plot(hidden[:9],'--o')\n\n\n\n\n\na blue\nb orange\nc green\n\n\nplt.plot(net(x).data[:9],'--o')\n\n\n\n\n\nplt.plot(yhat[:9],'--o')\n\n\n\n\n\n억지로 맞추고있긴한데 파라메터가 부족해보인다.\n\n- 결과시각화1\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\n학습이 제대로 되었다면 0의 결과가 나오지 않지\nnet진행될때 거의 w만 곱한 결과가 나오는 것인데 0이 hidden에서 나오면 w만 곱해도 변화가 없잖아? 그래서 주황색 선이 나온 거고\n\n- 첫번째 그림 \\(\\to\\) 두번빼 그림\n\nnet[2].weight,net[2].bias\n\n(Parameter containing:\n tensor([[-4.6804],\n         [ 0.3071],\n         [ 5.2894]], requires_grad=True),\n Parameter containing:\n tensor([-1.5440,  0.9143, -1.3970], requires_grad=True))\n\n\n\nhidden[:9], (net[-1].weight.data).T, net[-1].bias.data\n\n(tensor([[-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896]]),\n tensor([[-4.6804,  0.3071,  5.2894]]),\n tensor([-1.5440,  0.9143, -1.3970]))\n\n\n\nhidden[:9].shape\n\ntorch.Size([9, 1])\n\n\n\nnet[-1].weight.data.shape\n\ntorch.Size([3, 1])\n\n\n\n(net[-1].weight.data.T).shape\n\ntorch.Size([1, 3])\n\n\n\nhidden[:9]@(net[-1].weight.data).T + net[-1].bias.data\n\ntensor([[-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312]])\n\n\n\n(파랑,주황,초록) 순서로 그려짐\n파랑 = hidden * (-4.6804) + (-1.5440)\n주황 = hidden * (0.3071) + (0.9143)\n초록 = hidden * (5.2894) + (-1.3970)\n\n- 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. - weight: 파랑과 초록을 구분하는 역할을 함 - weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 ’b’라고 나올 확률을 학습시킨다. \\(\\to\\) 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0)\n\n참고: torch.nn.Linear()의 비밀? - 사실 \\({\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf W}\\)와 \\({\\boldsymbol b}\\)가 저장되는게 아니다. - \\({\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf A}\\)와 \\({\\boldsymbol b}\\)가 저장된다. - \\({\\bf W} = {\\bf A}^T\\) 인 관계에 있으므로 l1.weight 가 우리가 생각하는 \\({\\bf W}\\) 로 해석하려면 사실 transpose를 취해줘야 한다.\n왜 이렇게..? - 계산의 효율성 때문 (numpy의 구조를 알아야함) - \\({\\boldsymbol x}\\), \\({\\boldsymbol y}\\) 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음)\n잠깐 딴소리!!\n(예시1)\n\n_arr = np.array(range(4)).reshape(2,2)\n_arr\n\narray([[0, 1],\n       [2, 3]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시2)\n\n_arr = np.array(range(6)).reshape(3,2)\n_arr\n\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시3)\n\n_arr = np.array(range(6)).reshape(2,3)\n_arr\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\n_arr.strides\n\n(24, 8)\n\n\n\n아래로 한칸 = 24칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n_arr = np.array(range(4),dtype=np.int8).reshape(2,2)\n_arr\n\narray([[0, 1],\n       [2, 3]], dtype=int8)\n\n\n\n_arr.strides\n\n(2, 1)\n\n\n\n아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n\n_arr = np.array(range(4),dtype=np.float64).reshape(2,2)\n_arr\n\narray([[0., 1.],\n       [2., 3.]])\n\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n_arr = np.array(range(4),dtype=np.float32).reshape(2,2)\n_arr\n\narray([[0., 1.],\n       [2., 3.]], dtype=float32)\n\n\n\n_arr.strides\n\n(8, 4)\n\n\n진짜 참고..\n\n1바이트 = 8비트\n1바이트는 2^8=256 의 정보 표현\nnp.int8은 8비트로 정수를 저장한다는 의미\n\n\n2**8\n\n256\n\n\n\nprint(np.array(55,dtype=np.int8))\nprint(np.array(127,dtype=np.int8))\nprint(np.array(300,dtype=np.int8)) # overflow \n\n55\n127\n44\n\n\n딴소리 끝!!\nweight의 transfose가 저장되는 이유 끝!\n혼자 크다고 인식하면 바꾸는…\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([299, 7])\n\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(7), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam3-abcd",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam3-abcd",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "Exam3: abcd",
    "text": "Exam3: abcd\n\ndata\n\ntxt = list('abcd')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'a'], ['b', 'c', 'd', 'a', 'b'])\n\n\n\n\n하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet[0].weight.data = torch.tensor([[-0.3333],[-2.5000],[5.0000],[0.3333]])\n\nnet[-1].weight.data = torch.tensor([[1.5000],[-6.0000],[-2.0000],[6.0000]])\nnet[-1].bias.data = torch.tensor([0.1500, -2.0000,  0.1500, -2.000])\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 9])\n\n\n\nplt.matshow(combined[:15],vmin=-15,vmax=15,cmap='bwr')\nplt.xticks(range(9), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n맞춘게 아니야. 0 근처인게 있잖아 hidden에서\n\n\n두개의 은닉노드를 이용한 풀이 – 깔끔한 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\nhidden layer 2장만으로 4가지 state 표현"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam4-abcde-스스로-공부",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam4-abcde-스스로-공부",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "Exam4: abcde (스스로 공부)",
    "text": "Exam4: abcde (스스로 공부)\n\ndata\n주어진 자료가 다음과 같다고 하자.\n\ntxt = list('abcde')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'a'])\n\n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)\n\n\n3개의 은닉노드를 이용한 풀이\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. - 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. - 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. - 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음.\n\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0]))\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5,embedding_dim=3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=3,out_features=5)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([499, 13])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam5-abacad",
    "href": "posts/ml/2022-11-09-ml-(10주차)_11월9일.html#exam5-abacad",
    "title": "기계학습 특강 (10주차) 11월9일",
    "section": "Exam5: AbAcAd",
    "text": "Exam5: AbAcAd\n\ndata\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\n\n두개의 은닉노드를 이용한 풀이 – 실패\n- 데이터정리\n\nmapping = {'A':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 2, 0]), tensor([1, 0, 2, 0, 3]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([599, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\nA 뒤는 a,b,c 3개 중 하나, 따라서 1/3 의 확률이겠네 하고 학습해버렸다.\nobservation 마다 봐야하는 x가 달라(수가 )\n실패\n\n- 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다.\n\n\n\nX\ny\n\n\n\n\nA,b\nA\n\n\nb,A\nc\n\n\nA,c\nA\n\n\nc,A\nd\n\n\nA,d\nA\n\n\nd,A\nb\n\n\nA,b\nA\n\n\nb,A\nc\n\n\n…\n…\n\n\n\n- 순진한 접근방식의 비판: - 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조 - 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. - 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름.\n이것에 대한 해결책은 순환신경망이다.\n\n\n순환망을 위하여 data 다시정리\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx[:8],y[:8]\n\n(tensor([0, 1, 0, 2, 0, 3, 0, 1]), tensor([1, 0, 2, 0, 3, 0, 1, 0]))\n\n\n- 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n숫자형태의 벡터형태라고 가정하고 학습하는 순환신경망\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))\n\n\n\n\n실패했던 풀이의 재구현1\n- 방금 실패한 풀이\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n- Tanh까지만 클래스로 바꾸어서 구현 - 클래스를 이용하는 방법: https://guebin.github.io/DL2022/2022/11/01/(9주차)-11월1일.html#로지스틱-모형을-이용한-풀이\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self,x):\n        hidden = self.tanh(self.i2h(x))\n        return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회반복\n\nfor epoc in range(20): \n    ## 1 \n    ## 2 \n    hidden = hnet(x) \n    output = linr(hidden)\n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n\n실패했던 풀이의 재구현2\n- Tanh까지 구현한 클래스\n\n#\n# class Hnet(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n#         self.tanh = torch.nn.Tanh()\n#     def forward(self,x):\n#         hidden = self.tanh(self.i2h(x))\n#         return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회 반복\n\nx[0].shape\n\ntorch.Size([4])\n\n\n\nx[[0]].shape\n\ntorch.Size([1, 4])\n\n\n\nT = len(x) \nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)\n\n\n\n4개가 필요한 원핫인코딩과 달리 hidden layer는 2개만으로 4state 표현했다\n\n\n\n순환신경망의 아이디어\n\n모티브\n(예비생각1) \\({\\boldsymbol h}\\)에 대한 이해\n\\({\\boldsymbol h}\\)는 사실 문자열 ’abcd’들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) - (why1) h는 “학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태”라고 이해가능 - (why2) 실제로 예시를 살펴보면 그러했다.\n결론: 사실 \\({\\boldsymbol h}\\)는 잘 숙성되어있는 입력정보 \\({\\bf X}\\) 그 자체로 해석 할 수 있다.\n(예비생각2) 수백년전통을 이어가는 방법\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬\n* 기존방식 - \\(\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}\\)\n* 수백년 전통의 간장맛을 유지하는 방식\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3\\)\n\n* 수백년 전통의 간장맛을 유지하면서 조리를 한다면?\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n점점 맛있는 간장계란밥이 탄생함\n* 알고리즘의 편의상 아래와 같이 생각해도 무방\n\n\\(\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\), \\(\\text{간장}_0=\\text{맹물}\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n아이디어\n* 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로?\n\n\\(\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1\\)\n\\(\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2\\)\n\\(\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3\\)\n\n이제 우리가 배울것은 (1) “\\(\\text{콩물}_{t}\\)”와 “\\(\\text{간장}_{t-1}\\)”로 “\\(\\text{간장}_t\\)”를 숙성하는 방법 (2) “\\(\\text{간장}_t\\)”로 “\\(\\text{간장계란밥}_t\\)를 조리하는 방법이다\n즉 숙성담당 네트워크와 조리담당 네트워크를 각각 만들어 학습하면 된다.\n\n\n알고리즘\n세부적인 알고리즘 (\\(t=0,1,2,\\dots\\)에 대하여 한줄 한줄 쓴 알고리즘)\n\n\\(t=0\\)\n\n\\({\\boldsymbol h}_0=[[0,0]]\\) <– \\(\\text{간장}_0\\)은 맹물로 초기화\n\n\\(t=1\\)\n\n\\({\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\) - \\({\\boldsymbol x}_1\\): (1,4) - \\({\\bf W}_{ih}\\): (4,2) - \\({\\boldsymbol h}_0\\): (1,2) - \\({\\bf W}_{hh}\\): (2,2) - \\({\\boldsymbol b}_{ih}\\): (1,2) - \\({\\boldsymbol b}_{hh}\\): (1,2)\n\\({\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)\\)\n\n\\(t=2\\) <– 여기서부터는 \\(t=2\\)와 비슷\n\n\n좀 더 일반화된 알고리즘\n(ver1)\ninit \\(\\boldsymbol{h}_0\\)\nfor \\(t\\) in \\(1:T\\)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\)\n\\({\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)\\)\n\n(ver2)\ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden)) # $H_{t-1}$\n    output = linr(hidden)\n    yt_hat = soft(output)\n\n코드상으로는 \\(h_t\\)와 \\(h_{t-1}\\)의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n\n전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음\n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)\n\n\n\n순환신경망 구현1 – 성공\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f0f04614dd0>\n\n\n\n\n\n\n아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까..\n\n\n\n순환신경망 구현2 (with RNNCell) – 성공\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n(1) 숙성네트워크\n선언\n\nrnncell = torch.nn.RNNCell(4,2)\n\n가중치초기화 (순환신경망 구현1과 동일하도록)\n\ntorch.manual_seed(43052)\n_rnncell = rNNCell()\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data \nrnncell.weight_hh.data = _rnncell.h2h.weight.data \nrnncell.bias_hh.data = _rnncell.h2h.bias.data \nrnncell.bias_ih.data = _rnncell.i2h.bias.data \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\n\n<matplotlib.image.AxesImage at 0x7f0f0cfe77d0>\n\n\n\n\n\n\n\n순환신경망 구현3 (with RNN) – 성공\n(예비학습)\n- 아무리 생각해도 yhat구하려면 좀 귀찮음\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nsoft(cook(hidden))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2) \n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.bias_hh_l0.data = rnncell.bias_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data \nrnn.bias_ih_l0.data = rnncell.bias_ih.data \n\n\n_water\n\ntensor([[0., 0.]])\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\n똑같음!\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water)\n\n(tensor([[-0.2232,  0.9769],\n         [-0.9999, -0.9742],\n         [ 0.9154,  0.9992],\n         ...,\n         [ 0.9200,  0.9992],\n         [-0.9978, -0.0823],\n         [-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.9154,  0.9965]], grad_fn=<SqueezeBackward1>))\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\ntorch.manual_seed(43052)\nrnn = torch.nn.RNN(4,2)\n\n(2) 조리네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4)\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters())) # 우리가 배울것: 숙성하는 방법 + 요리하는 방법 \n\n(4) 학습\n\nfor epoc in range(5000):\n    ## 1\n    _water = torch.zeros(1,2)\n    hidden, _ = rnn(x,_water)\n    output = cook(hidden)\n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nyhat = soft(cook(rnn(x,_water)[0]))\nyhat\n\ntensor([[1.9725e-02, 1.5469e-03, 8.2766e-01, 1.5106e-01],\n        [9.1875e-01, 1.6513e-04, 6.7703e-02, 1.3384e-02],\n        [2.0031e-02, 1.0659e-03, 8.5248e-01, 1.2642e-01],\n        ...,\n        [1.9640e-02, 1.3568e-03, 8.3705e-01, 1.4196e-01],\n        [9.9564e-01, 1.3114e-05, 3.5069e-03, 8.4108e-04],\n        [3.5473e-03, 1.5670e-01, 1.4102e-01, 6.9873e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[:15])\n\n<matplotlib.image.AxesImage at 0x7f0f159c0a10>"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html",
    "title": "기계학습 특강 (8주차) 10월26일–(2)",
    "section": "",
    "text": "(8주차) 10월26일–(2) [이미지자료분석 - Transfer Learning, CAM (설명가능한 인공지능모형, XAI)]"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#imports",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#imports",
    "title": "기계학습 특강 (8주차) 10월26일–(2)",
    "section": "imports",
    "text": "imports\n\nimport torch \nimport torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#transfer-learning",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#transfer-learning",
    "title": "기계학습 특강 (8주차) 10월26일–(2)",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\npath = untar_data(URLs.CIFAR)\n\n\npath.ls()\n\n(#3) [Path('/home/csy/.fastai/data/cifar10/train'),Path('/home/csy/.fastai/data/cifar10/labels.txt'),Path('/home/csy/.fastai/data/cifar10/test')]\n\n\n\n!ls '/home/csy/.fastai/data/cifar10/train'\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\n수제네트워크\n\ndls\n\n\ndls = ImageDataLoaders.from_folder(path,train='train',valid='test') \n\n\n_X,_y = dls.one_batch()\n_X.shape, _y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n\n!ls '/home/csy/.fastai/data/cifar10/train' # 10개의 클래스\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n\ndls.show_batch()\n\n\nlrnr 생성\n\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(3,128,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\nnet는 cpu에 있고 X는 gpu에 있으니 cpu로 불러오자\n\n#net1(_X.to(\"cpu\")).shape\n\n\nnet = torch.nn.Sequential(\n    net1, \n    torch.nn.Linear(25088,10)\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy) \n\n\nnet.to(\"cuda:0\")\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n  (1): Linear(in_features=25088, out_features=10, bias=True)\n)\n\n\n\n학습\n\n\nX,y=dls.one_batch()\n\n\nlrnr.model(X).shape\n\ntorch.Size([64, 10])\n\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.266191\n      1.226523\n      0.572100\n      00:05\n    \n    \n      1\n      1.128916\n      1.124115\n      0.609800\n      00:05\n    \n    \n      2\n      1.025027\n      1.076060\n      0.629600\n      00:05\n    \n    \n      3\n      0.956499\n      1.071469\n      0.636600\n      00:05\n    \n    \n      4\n      0.852002\n      1.033129\n      0.650600\n      00:05\n    \n    \n      5\n      0.811420\n      1.071609\n      0.641600\n      00:05\n    \n    \n      6\n      0.735469\n      1.074108\n      0.648300\n      00:04\n    \n    \n      7\n      0.703909\n      1.094982\n      0.648800\n      00:04\n    \n    \n      8\n      0.623525\n      1.132971\n      0.645000\n      00:05\n    \n    \n      9\n      0.589313\n      1.157667\n      0.637900\n      00:05\n    \n  \n\n\n\n\n이게 생각보다 잘 안맞아요.. 70넘기 힘듬\n\n\n\n전이학습 (남이 만든 네트워크)\n\nlrnr 생성\n\n학습되어 있는 파라메터까지 같이 가져오기\n\n#collapse_output\nnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\nnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\n\\(k=1000\\) 즉 1000개의 물체를 구분하는 모형임\n\n\nnet.fc = torch.nn.Linear(in_features=512, out_features=10) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=accuracy)\n\n\n학습\n\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.813206\n      0.955131\n      0.677300\n      00:21\n    \n    \n      1\n      0.636926\n      0.719258\n      0.760700\n      00:21\n    \n    \n      2\n      0.538001\n      0.802607\n      0.765500\n      00:21\n    \n    \n      3\n      0.446174\n      0.591965\n      0.804200\n      00:20\n    \n    \n      4\n      0.339985\n      0.677038\n      0.786200\n      00:20\n    \n    \n      5\n      0.283703\n      0.664880\n      0.797400\n      00:21\n    \n    \n      6\n      0.221962\n      0.734830\n      0.787000\n      00:21\n    \n    \n      7\n      0.183193\n      0.720297\n      0.798000\n      00:21\n    \n    \n      8\n      0.160181\n      0.785769\n      0.790900\n      00:21\n    \n    \n      9\n      0.144745\n      0.745676\n      0.804400\n      00:21\n    \n  \n\n\n\n\nCIFAR10을 맞추기 위한 네트워크가 아님에도 불구하고 상당히 잘맞음\n일반인이 거의 밑바닥에서 설계하는것보다 전이학습을 이용하는 것이 효율적일 경우가 많다.\n\n\n\n전이학습 다른 구현: 순수 fastai 이용\n- 예전코드 복습\n\npath = untar_data(URLs.PETS)/'images'\n\n\nfiles= get_image_files(path)\n\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/home/csy/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nlrnr = cnn_learner(dls,resnet34,metrics=accuracy)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.204861\n      0.011182\n      0.995940\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.056896\n      0.009584\n      0.996617\n      00:44\n    \n  \n\n\n\n- 사실 위의 코드가 transfer learning 이었음.\n\n#collapse_output\nlrnr.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)\n\n\n\nXAI(설명가능한 인공지능)\n딥러닝 연구의 네가지 축 - step 1. 아키텍처 - 최근 연구 특징 : 비전문가 + 블랙박스(안 보이는 의미) - 설명가능한 딥러닝에 대한 요구 - step 2. 손실함수 - step 3. 미분계산 - step 4. 옵티마이저"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#cam",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(2).html#cam",
    "title": "기계학습 특강 (8주차) 10월26일–(2)",
    "section": "CAM",
    "text": "CAM\n\nCAM이란?\n\nref: http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf\n\n- Class Activation Mapping (CAM)은 설명가능한 인공지능모형 (eXplainable Artificial Intelligence, XAI) 중 하나로 CNN의 판단근거를 시각화하는 기술\n\n\n학습에 사용할 데이터 Load\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath.ls()\n\n(#7393) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\n\nfiles= get_image_files(path)\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\n\n구현0단계– 예비학습\n\n# 하나의 이미지 선택\n\nximg = PILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0]\nx,x.shape\n\n(TensorImage([[[[0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9059, 0.9059, 0.9098,  ..., 0.9059, 0.9059, 0.9059],\n                ...,\n                [0.8745, 0.8784, 0.8824,  ..., 0.8902, 0.8863, 0.8824],\n                [0.9059, 0.8980, 0.8902,  ..., 0.8824, 0.8863, 0.8824],\n                [0.8863, 0.8863, 0.8824,  ..., 0.8784, 0.8863, 0.8863]],\n \n               [[0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                [0.9137, 0.9137, 0.9176,  ..., 0.9059, 0.9059, 0.9059],\n                ...,\n                [0.8784, 0.8824, 0.8863,  ..., 0.8745, 0.8667, 0.8588],\n                [0.9098, 0.9020, 0.8902,  ..., 0.8745, 0.8706, 0.8627],\n                [0.8902, 0.8902, 0.8784,  ..., 0.8784, 0.8745, 0.8706]],\n \n               [[0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                [0.9098, 0.9098, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n                ...,\n                [0.8863, 0.8902, 0.8980,  ..., 0.8784, 0.8706, 0.8667],\n                [0.9176, 0.9137, 0.9059,  ..., 0.8745, 0.8706, 0.8667],\n                [0.8980, 0.9020, 0.8980,  ..., 0.8745, 0.8706, 0.8667]]]],\n             device='cuda:0'),\n torch.Size([1, 3, 512, 512]))\n\n\n\n\n# AP layer\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1) \n\n\nX = torch.arange(48).reshape(1,3,4,4)*1.0 \nX\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(X)\n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\nX[0,0,...].mean(),X[0,1,...].mean(),X[0,2,...].mean()\n\n(tensor(7.5000), tensor(23.5000), tensor(39.5000))\n\n\n\n\n# torch.einsum\n(예시1)\n\ntsr = torch.arange(12).reshape(4,3)\ntsr\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n\ntorch.einsum('ij->ji',tsr)\n\ntensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])\n\n\n(예시2)\n\ntsr1 = torch.arange(12).reshape(4,3).float()\ntsr2 = torch.arange(15).reshape(3,5).float()\n\n\ntsr1 @ tsr2\n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n\ntorch.einsum('ij,jk -> ik',tsr1,tsr2) \n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n(예시3)\n\nx.to(\"cpu\").shape\n\ntorch.Size([1, 3, 512, 512])\n\n\ntorch,einsum을 사용하여 shape을 아래로 변경\n\ntorch.einsum('ocij -> ijc',x.to(\"cpu\")).shape\n\ntorch.Size([512, 512, 3])\n\n\n\nplt.imshow(torch.einsum('ocij -> ijc',x.to(\"cpu\")))\n\n<matplotlib.image.AxesImage at 0x7fe60eea0e50>\n\n\n\n\n\n\n\n\n구현1단계– 이미지분류 잘하는 네트워크 선택\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\nlrnr = cnn_learner(dls,resnet34,metrics=accuracy)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.180252\n      0.032132\n      0.989851\n      00:32\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.053625\n      0.008279\n      0.997970\n      00:44\n    \n  \n\n\n\n\n\n구현2단계– 네트워크의 끝 부분 수정\n- 모형의 분해\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\nnet1이 2d part, net1이 1d part\n- net2를 좀더 살펴보자.\n\nnet2\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n\nprint(net1(_X).shape)\nprint(net2[0](net1(_X)).shape)\nprint(net2[1](net2[0](net1(_X))).shape)\nprint(net2[2](net2[1](net2[0](net1(_X)))).shape)\n\ntorch.Size([64, 512, 16, 16])\ntorch.Size([64, 1024, 1, 1])\ntorch.Size([64, 1024])\ntorch.Size([64, 1024])\n\n\n- net2를 아래와 같이 수정하고 재학습하자 (왜?)\n\nnet2= torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1), # (64,512,16,16) -> (64,512,1,1) \n    torch.nn.Flatten(), # (64,512,1,1) -> (64,512) \n    torch.nn.Linear(512,2,bias=False) # (64,512) -> (64,2) \n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\n\n\nlrnr2= Learner(dls,net,metrics=accuracy) # loss_fn??\n\n\nlrnr2.loss_func, lrnr.loss_func ## 알아서 기존의 loss function으로 잘 들어가 있음. \n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5) # net2를 수정해서 accuracy가 안좋아지긴 했는데 그래도 쓸만함 \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.240225\n      0.521585\n      0.826793\n      00:44\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.139931\n      0.159443\n      0.940460\n      00:44\n    \n    \n      1\n      0.123673\n      0.396028\n      0.864682\n      00:44\n    \n    \n      2\n      0.094375\n      0.136513\n      0.952639\n      00:44\n    \n    \n      3\n      0.052172\n      0.057100\n      0.977673\n      00:44\n    \n    \n      4\n      0.028230\n      0.041083\n      0.985792\n      00:44\n    \n  \n\n\n\n\n\n구현3단계– 수정된 net2에서 Linear와 AP의 순서를 바꿈\n- 1개의 observation을 고정하였을 경우 출력과정 상상\n\nximg = PILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_106.jpg')\nx = first(dls.test_dl([ximg]))[0]\n\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\nprint(net1(x).shape)\nprint(net2[0](net1(x)).shape)\nprint(net2[1](net2[0](net1(x))).shape)\nprint(net2[2](net2[1](net2[0](net1(x)))).shape)\n\ntorch.Size([1, 512, 16, 16])\ntorch.Size([1, 512, 1, 1])\ntorch.Size([1, 512])\ntorch.Size([1, 2])\n\n\n- 최종결과 확인\n\nnet(x)\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n아마 모델 달라서 값이 다른 것일까..!\n\ndls.vocab\n\n['cat', 'dog']\n\n\n\nnet(x)에서 뒤쪽의 값이 클수록 ’dog’를 의미한다.\n\n- net2의 순서 바꾸기 전 전체 네트워크:\n\\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linear}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [-9.0358,  9.0926]\\]\n- 아래와 같이 순서를 바꿔서 한번 계산해보고 싶다. (왜???..)\n\\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{net_1}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{linear}{\\to} \\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}}\\right) = [−9.0358,9.0926]\\]\n\n여기에서 (1,512,16,16) -> (1,2,16,16) 로 가는 선형변환을 적용하는 방법? (16,16) each pixel에 대하여 (512 \\(\\to\\) 2)로 가는 변환을 수행\n\n- 통찰: 이 경우 특이하게도 레이어의 순서를 바꿨을때 출력이 동일함 (선형변환하고 평균내거나 평균내고 선형변환하는건 같으니까)\n\n_x =torch.tensor([1,2,3.14,4]).reshape(4,1)\n_x \n\ntensor([[1.0000],\n        [2.0000],\n        [3.1400],\n        [4.0000]])\n\n\n\n_l1 = torch.nn.Linear(1,1,bias=False)\n_l1(_x).mean() # _x -> 선형변환 -> 평균 \n\ntensor(-0.2621, grad_fn=<MeanBackward0>)\n\n\n\n_l1(_x.mean().reshape(1,1)) # _x -> 평균 -> 선형변환\n\ntensor([[-0.2621]], grad_fn=<MmBackward0>)\n\n\n- 구현해보자.\n\nnet2[2].weight.shape,net1(x).shape\n\n(torch.Size([2, 512]), torch.Size([1, 512, 16, 16]))\n\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\nnet2[0](why)\n\nTensorImage([[[[-6.7946]],\n\n              [[ 8.0881]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nnet(x)\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n\n잠깐 멈추고 생각\n- 이미지\n\nximg\n\n\n\n\n- 네트워크의 결과\n\nnet2(net1(x))\n\nTensorImage([[-6.7946,  8.0881]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n-9.0358 << 9.0926 이므로 ’ximg’는 높은 확률로 개라는 뜻이다.\n\n내거에서는 9.0926이 10.2985\n- 아래의 네트워크를 관찰\n\\[\\underset{(1,2,16,16)}{{\\bf why}}\\overset{ap}{\\to} \\underset{(1,2,1,1)}{{\\boldsymbol \\sharp}}\\overset{flatten}{\\to} \\underset{(1,2)}{\\hat{\\boldsymbol y}} = [-9.0358,9.0926]\\]\n\nnet2[0](why)\n\nTensorImage([[[[-6.7946]],\n\n              [[ 8.0881]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n더 파고들어서 분석해보자.\n\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n(why[0,0,:,:]).mean(), (why[0,1,:,:]).mean()\n\n(TensorImage(-6.7946, device='cuda:0', grad_fn=<AliasBackward0>),\n TensorImage(8.0881, device='cuda:0', grad_fn=<AliasBackward0>))\n\n\nwhy[0,0,:,:]\n\n#collapse_output\n(why[0,0,:,:]).to(torch.int64)\n\nTensorImage([[   0,    0,    0,    0,    0,    0,    0,   -1,   -1,    0,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,   -9,  -18,  -18,   -9,   -2,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -8,  -31,  -51,  -44,  -25,   -8,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -16,  -50,  -82,  -73,  -45,  -14,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,   -1,  -21,  -59,  -98, -111,  -63,  -18,\n                -1,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -17,  -53,  -94, -100,  -60,  -18,\n                -2,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,  -10,  -37,  -65,  -66,  -40,  -13,\n                -2,   -1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -6,  -25,  -43,  -34,  -16,   -4,\n                -2,   -1,   -1,    0,    0],\n             [   0,    0,    0,    0,    0,   -5,  -17,  -22,  -15,   -4,   -1,\n                -1,   -1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -4,  -11,  -11,   -7,   -1,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,   -1,   -2,   -3,   -2,    0,    0,    0,\n                 0,    1,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -1,    0,    1,    2,    1,    0,\n                 0,    0,    0,    0,    0],\n             [   0,    0,    0,    0,    0,   -3,    0,    1,    4,    3,    0,\n                -1,    0,    0,    0,    0],\n             [   0,   -1,    0,    0,   -1,   -2,   -1,    1,    3,    2,   -1,\n                -2,   -1,    0,    0,    0],\n             [  -1,   -1,    0,   -1,   -1,   -5,   -3,    0,    0,    0,   -2,\n                -2,   -2,   -1,   -1,   -1],\n             [  -1,   -1,    0,    0,   -1,   -7,   -5,    0,   -1,   -1,   -1,\n                -1,   -2,   -1,   -1,   -1]], device='cuda:0')\n\n\n\n이 값들의 평균은 -9.0358 이다. (이 값이 클수록 이 그림이 고양이라는 의미 = 이 값이 작을수록 이 그림이 고양이가 아니라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 작은값이 있어서 -9.0358이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 작은 값들은 ximg가 고양이가 아니라고 판단하는 근거가 된다.\n\nwhy[0,1,:,:]\n\n#collapse_output\n(why[0,1,:,:]).to(torch.int64)\n\nTensorImage([[  0,   0,   0,   0,   0,   0,   0,   2,   2,   1,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   1,  11,  21,  21,  11,   3,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   9,  35,  61,  53,  30,  11,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  19,  58,  95,  87,  54,  17,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,  24,  68, 114, 132,  75,  22,   1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,  20,  61, 109, 118,  72,  21,   2,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  12,  43,  75,  77,  46,  15,   3,   1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   7,  28,  50,  39,  18,   5,   2,   2,\n                1,   0,   0],\n             [  0,   0,   0,   0,   0,   6,  19,  26,  17,   5,   1,   1,   1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   5,  12,  13,   9,   2,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   2,   3,   3,   1,   0,   0,   0,  -1,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   1,   1,  -1,  -2,  -1,   1,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   1,   4,   0,  -1,  -5,  -3,   0,   1,   0,\n                0,   0,   0],\n             [  0,   1,   0,   0,   1,   3,   1,  -1,  -3,  -3,   1,   2,   1,\n                0,   0,   1],\n             [  1,   1,   1,   1,   1,   6,   4,   0,   0,   0,   2,   3,   2,\n                1,   1,   1],\n             [  1,   1,   1,   1,   1,   8,   6,   1,   1,   1,   1,   1,   2,\n                1,   1,   1]], device='cuda:0')\n\n\n\n이 값들의 평균은 9.0926 이다. (이 값이 클수록 이 그림이 강아지라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐. 다만 특정위치에서 엄청 큰 값들이 있어서 9.0926이라는 평균값이 나옴 \\(\\to\\) 특정위치에 존재하는 엄청 큰 값들은 결국 ximg를 강아지라고 판단하는 근거가 된다.\n\n- 시각화\n\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n<matplotlib.image.AxesImage at 0x7fe61ad76350>\n\n\n\n\n\n\nmagma = 검은색 < 보라색 < 빨간색 < 노란색\n왼쪽그림의 검은 부분은 고양이가 아니라는 근거, 오른쪽그림의 노란부분은 강아지라는 근거\n\n- why_cat, why_dog를 (16,16) \\(\\to\\) (512,512) 로 resize\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear')\n\n<matplotlib.image.AxesImage at 0x7fe616821f10>\n\n\n\n\n\n- 겹쳐그리기\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7fe5eac7af10>\n\n\n\n\n\n- 하니이미지 시각화\n\n!wget https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg\n\n--2022-11-02 23:41:24--  https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg\nResolving github.com (github.com)... 20.200.245.247\nConnecting to github.com (github.com)|20.200.245.247|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘2022-09-06-hani01.jpeg’\n\n2022-09-06-hani01.j     [ <=>                ] 134.25K  --.-KB/s    in 0.02s   \n\n2022-11-02 23:41:24 (8.09 MB/s) - ‘2022-09-06-hani01.jpeg’ saved [137470]\n\n\n\n\n#\n#!wget https://github.com/guebin/DL2022/blob/master/_notebooks/2022-09-06-hani01.jpeg?raw=true\nximg= PILImage.create('2022-09-07-dogs.jpeg')\nx= first(dls.test_dl([ximg]))[0]\n\n\nwhy = torch.einsum('cb,abij->acij',net2[2].weight,net1(x))\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n\n<matplotlib.image.AxesImage at 0x7fe61ae94190>\n\n\n\n\n\n- 하니이미지 시각화 with prob\n\nsftmax=torch.nn.Softmax(dim=1)\n\n\nsftmax(net(x))\n\nTensorImage([[1.1767e-09, 1.0000e+00]], device='cuda:0',\n            grad_fn=<AliasBackward0>)\n\n\n\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=1.000000')\n\n\n\n\n\n\n\n구현4단계– CAM 시각화\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\nfig, ax = plt.subplots(5,5) \nk=0 \nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=25\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=50\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(5,5) \nk=75\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "",
    "text": "교수님 풀이, 안 건들임"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#이미지자료분석",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#이미지자료분석",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "1. 이미지자료분석",
    "text": "1. 이미지자료분석\n아래를 이용하여 MNIST_SAMPLE 이미지 자료를 다운로드 받고 dls오브젝트를 만들어라.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\ndls = ImageDataLoaders.from_folder(path,suffle=False) \n\n\ndls.show_batch()\n\n\n\n\n(1) cnn_learner를 이용하여 lrnr 오브젝트를 생성하라. - arch 는 resnet34 로 설정할 것 - metrics 는 error_rate 로 설정할 것\n(풀이)\n\nlrnr = cnn_learner(dls, arch = resnet34, metrics=error_rate)\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/vision/learner.py:284: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n(2) fine_tune 을 이용하여 lrnr 오브젝트를 학습하라.\n(풀이)\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.282870\n      0.150136\n      0.049068\n      00:05\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.042991\n      0.017522\n      0.006379\n      00:05\n    \n  \n\n\n\n(3) 아래를 이용하여 X,y를 만들어라.\nX,y = dls.one_batch()\nX,y의 shape을 조사하라. X에는 몇개의 이미지가 있는가? 이미지의 size는 얼마인가?\n(풀이)\n\nX,y = dls.one_batch()\nX.shape\n\ntorch.Size([64, 3, 28, 28])\n\n\nX에는 64개의 이미지가 있고 크기는 (28,28) 이다.\n(4) 아래의 코드를 이용하여 X의 두번째 이미지가 어떠한 숫자를 의미하는지 확인하라. (그림보고 3인지 7인지 확인하여 답을 쓸 것)\nshow_image(X[0])\n그리고 show_image가 정의된 파일의 경로를 확인하고 show_image가 python 내장함수 인지, torch에서 지원하는 함수인지 fastai에서 지원하는 함수인지 파악하라.\n(풀이)\n\nshow_image(X[1]) # 두번째 이미지 \n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image?\n\n\nSignature:\nshow_image(\n    im,\n    ax=None,\n    figsize=None,\n    title=None,\n    ctx=None,\n    cmap=None,\n    norm=None,\n    *,\n    aspect=None,\n    interpolation=None,\n    alpha=None,\n    vmin=None,\n    vmax=None,\n    origin=None,\n    extent=None,\n    interpolation_stage=None,\n    filternorm=True,\n    filterrad=4.0,\n    resample=None,\n    url=None,\n    data=None,\n    **kwargs,\n)\nDocstring: Show a PIL or PyTorch image on `ax`.\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/torch_core.py\nType:      function\n\n\n\n\n\nfastai에서 지원하는 함수\n\n(5) lrnr 오브젝트를 이용하여 AI가 X[0]을 어떤 값으로 판단하는지 확인하라. 올바르게 판단하였는가? 올바르게 판단했다면 몇 프로의 확신으로 판단하였는가? <– 문제가 의도한 것과 다르게 만들어졌어요\n(풀이)\n\nshow_image(X[0]) # 첫번째 이미지\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<AxesSubplot:>\n\n\n\n\n\n\nlrnr.model(X[0].reshape(1,3,28,28))\n\nTensorBase([[ 3.4148, -5.0356]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nimport numpy as np\na=np.exp(3.4148)\nb=np.exp(-5.0356)\nprint('3일확률',a/(a+b))\nprint('7일확률',b/(a+b))\n\n3일확률 0.9997862308347155\n7일확률 0.0002137691652844868\n\n\n\n원래문제의도: lrnr.predict(X[0].to(\"cpu\"))"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#추천시스템",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#추천시스템",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "2. 추천시스템",
    "text": "2. 추천시스템\n아래를 이용하여 rcmd_anal.csv 를 다운로드 받고 dls오브젝트를 만들어라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n(1) 73번 유저가 먹은 아이템 및 평점을 출력하는 코드를 작성하라. 이를 기반으로 73번 유저가 어떠한 취향인지 파악하라.\n(풀이)\n\ndf.query('user == 73')\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      720\n      73\n      20\n      3.733853\n      홍차10\n    \n    \n      721\n      73\n      18\n      3.975004\n      홍차8\n    \n    \n      722\n      73\n      9\n      1.119541\n      커피9\n    \n    \n      723\n      73\n      13\n      3.840801\n      홍차3\n    \n    \n      724\n      73\n      2\n      0.943742\n      커피2\n    \n    \n      725\n      73\n      4\n      1.152405\n      커피4\n    \n    \n      726\n      73\n      1\n      0.887292\n      커피1\n    \n    \n      727\n      73\n      7\n      0.947641\n      커피7\n    \n    \n      728\n      73\n      6\n      0.868370\n      커피6\n    \n    \n      729\n      73\n      17\n      3.873590\n      홍차7\n    \n  \n\n\n\n\n\n홍차를 선호\n\n(2) dls와 lrnr 오브젝트를 생성하고 lrnr 오브젝트를 학습하라.\n(풀이)\n\ndls = CollabDataLoaders.from_df(df)\nlrnr = collab_learner(dls,y_range=(0,5))\n\n\nlrnr.fit(50)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.337114\n      2.258755\n      00:00\n    \n    \n      1\n      2.328897\n      2.254714\n      00:00\n    \n    \n      2\n      2.320246\n      2.237874\n      00:00\n    \n    \n      3\n      2.300545\n      2.191783\n      00:00\n    \n    \n      4\n      2.265857\n      2.104007\n      00:00\n    \n    \n      5\n      2.207397\n      1.966761\n      00:00\n    \n    \n      6\n      2.123599\n      1.783263\n      00:00\n    \n    \n      7\n      2.008980\n      1.562448\n      00:00\n    \n    \n      8\n      1.865242\n      1.317642\n      00:00\n    \n    \n      9\n      1.697832\n      1.068948\n      00:00\n    \n    \n      10\n      1.515044\n      0.833239\n      00:00\n    \n    \n      11\n      1.326496\n      0.625003\n      00:00\n    \n    \n      12\n      1.139156\n      0.453686\n      00:00\n    \n    \n      13\n      0.962462\n      0.320953\n      00:00\n    \n    \n      14\n      0.802481\n      0.223124\n      00:00\n    \n    \n      15\n      0.662327\n      0.155420\n      00:00\n    \n    \n      16\n      0.542384\n      0.110662\n      00:00\n    \n    \n      17\n      0.442099\n      0.082435\n      00:00\n    \n    \n      18\n      0.359706\n      0.064858\n      00:00\n    \n    \n      19\n      0.292656\n      0.054441\n      00:00\n    \n    \n      20\n      0.238817\n      0.048325\n      00:00\n    \n    \n      21\n      0.195901\n      0.045092\n      00:00\n    \n    \n      22\n      0.161955\n      0.043386\n      00:00\n    \n    \n      23\n      0.135049\n      0.042616\n      00:00\n    \n    \n      24\n      0.113653\n      0.042549\n      00:00\n    \n    \n      25\n      0.096877\n      0.042678\n      00:00\n    \n    \n      26\n      0.083618\n      0.043010\n      00:00\n    \n    \n      27\n      0.073081\n      0.043308\n      00:00\n    \n    \n      28\n      0.064768\n      0.043905\n      00:00\n    \n    \n      29\n      0.058133\n      0.044605\n      00:00\n    \n    \n      30\n      0.053050\n      0.044990\n      00:00\n    \n    \n      31\n      0.048904\n      0.045569\n      00:00\n    \n    \n      32\n      0.045665\n      0.045833\n      00:00\n    \n    \n      33\n      0.043033\n      0.045906\n      00:00\n    \n    \n      34\n      0.040883\n      0.046624\n      00:00\n    \n    \n      35\n      0.039263\n      0.046878\n      00:00\n    \n    \n      36\n      0.037608\n      0.047040\n      00:00\n    \n    \n      37\n      0.036450\n      0.047146\n      00:00\n    \n    \n      38\n      0.035638\n      0.047335\n      00:00\n    \n    \n      39\n      0.034883\n      0.047623\n      00:00\n    \n    \n      40\n      0.034177\n      0.048048\n      00:00\n    \n    \n      41\n      0.033486\n      0.047836\n      00:00\n    \n    \n      42\n      0.033047\n      0.048263\n      00:00\n    \n    \n      43\n      0.032634\n      0.048296\n      00:00\n    \n    \n      44\n      0.032165\n      0.048577\n      00:00\n    \n    \n      45\n      0.031884\n      0.048578\n      00:00\n    \n    \n      46\n      0.031517\n      0.048725\n      00:00\n    \n    \n      47\n      0.031158\n      0.048977\n      00:00\n    \n    \n      48\n      0.030711\n      0.048955\n      00:00\n    \n    \n      49\n      0.030465\n      0.049127\n      00:00\n    \n  \n\n\n\n(3) 아래와 같은 데이터 프레임을 생성하고 df_new 에 저장하라.\n\n#collapse\nimport IPython \n_html='<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>user</th>\\n      <th>item</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>73</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>73</td>\\n      <td>2</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>73</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>73</td>\\n      <td>4</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>73</td>\\n      <td>5</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>73</td>\\n      <td>6</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>73</td>\\n      <td>7</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>73</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>73</td>\\n      <td>9</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>73</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>73</td>\\n      <td>11</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>73</td>\\n      <td>12</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>73</td>\\n      <td>13</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>73</td>\\n      <td>14</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>73</td>\\n      <td>15</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>73</td>\\n      <td>16</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>73</td>\\n      <td>17</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>73</td>\\n      <td>18</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>73</td>\\n      <td>19</td>\\n    </tr>\\n    <tr>\\n      <th>19</th>\\n      <td>73</td>\\n      <td>20</td>\\n    </tr>\\n  </tbody>\\n</table>'\nIPython.display.HTML(_html)\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      73\n      1\n    \n    \n      1\n      73\n      2\n    \n    \n      2\n      73\n      3\n    \n    \n      3\n      73\n      4\n    \n    \n      4\n      73\n      5\n    \n    \n      5\n      73\n      6\n    \n    \n      6\n      73\n      7\n    \n    \n      7\n      73\n      8\n    \n    \n      8\n      73\n      9\n    \n    \n      9\n      73\n      10\n    \n    \n      10\n      73\n      11\n    \n    \n      11\n      73\n      12\n    \n    \n      12\n      73\n      13\n    \n    \n      13\n      73\n      14\n    \n    \n      14\n      73\n      15\n    \n    \n      15\n      73\n      16\n    \n    \n      16\n      73\n      17\n    \n    \n      17\n      73\n      18\n    \n    \n      18\n      73\n      19\n    \n    \n      19\n      73\n      20\n    \n  \n\n\n\n(풀이)\n\ndf_new=pd.DataFrame({'user':[73]*20,'item':range(1,21)})\ndf_new\n\n\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      73\n      1\n    \n    \n      1\n      73\n      2\n    \n    \n      2\n      73\n      3\n    \n    \n      3\n      73\n      4\n    \n    \n      4\n      73\n      5\n    \n    \n      5\n      73\n      6\n    \n    \n      6\n      73\n      7\n    \n    \n      7\n      73\n      8\n    \n    \n      8\n      73\n      9\n    \n    \n      9\n      73\n      10\n    \n    \n      10\n      73\n      11\n    \n    \n      11\n      73\n      12\n    \n    \n      12\n      73\n      13\n    \n    \n      13\n      73\n      14\n    \n    \n      14\n      73\n      15\n    \n    \n      15\n      73\n      16\n    \n    \n      16\n      73\n      17\n    \n    \n      17\n      73\n      18\n    \n    \n      18\n      73\n      19\n    \n    \n      19\n      73\n      20\n    \n  \n\n\n\n\n(4) 아래의 코드를 이용하여 73번 유저의 취향을 파악하라. 73번 유저가 커피3, 커피5를 먹는다면 얼마정도의 평점을 줄 것이라 예측되는가?\n_dl = dls.test_dl(df_new)\nlrnr.get_preds(dl=_dl)\n(풀이)\n\n_dl = dls.test_dl(df_new)\nlrnr.get_preds(dl=_dl)\n\n\n\n\n\n\n\n\n(tensor([0.9698, 1.0314, 1.0191, 1.0177, 1.0122, 0.9323, 1.0513, 1.0184, 1.0316,\n         0.9842, 3.8255, 3.9591, 3.8640, 3.8937, 3.9437, 3.8947, 3.8272, 3.9503,\n         3.8117, 3.8603]),\n None)\n\n\n\n커피3: 1.0191, 커피5: 1.0122"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#시퀀스자료분석",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#시퀀스자료분석",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "3. 시퀀스자료분석",
    "text": "3. 시퀀스자료분석\n아래를 이용하여 자료를 다운로드 받아라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-19-human_numbers_100.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      text\n    \n  \n  \n    \n      0\n      0\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1\n      1\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      2\n      2\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      3\n      3\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      4\n      4\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      1995\n      1995\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1996\n      1996\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1997\n      1997\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1998\n      1998\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1999\n      1999\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n  \n\n2000 rows × 2 columns\n\n\n\n(1) TextDataLoaders.from_df을 이용하여 dls오브젝트를 만들어라. - is_lm = True 로 설정할 것 - seq_len = 5 로 설정할 것\n(풀이)\n\ndls = TextDataLoaders.from_df(df,is_lm=True,seq_len=5,text_col='text')\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two ,\n      one , two , three\n    \n    \n      1\n      hundred xxbos one , two\n      xxbos one , two ,\n    \n    \n      2\n      one hundred xxbos one ,\n      hundred xxbos one , two\n    \n    \n      3\n      , one hundred xxbos one\n      one hundred xxbos one ,\n    \n    \n      4\n      nine , one hundred xxbos\n      , one hundred xxbos one\n    \n    \n      5\n      ninety nine , one hundred\n      nine , one hundred xxbos\n    \n    \n      6\n      , ninety nine , one\n      ninety nine , one hundred\n    \n    \n      7\n      eight , ninety nine ,\n      , ninety nine , one\n    \n    \n      8\n      ninety eight , ninety nine\n      eight , ninety nine ,\n    \n  \n\n\n\n(2) lrnr 오브젝트를 만들어라. - arch = AWD_LSTM 이용 - metrics = accuracy 이용\n(풀이)\n\nlrnr = language_model_learner(dls, arch= AWD_LSTM, metrics=accuracy)\n\n(3) lrnr오브젝트에서 fine_tune(3) 메소드를 이용하여 모형을 학습하라.\n(풀이)\n\nlrnr.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.534681\n      0.168856\n      0.977650\n      00:49\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.018749\n      0.003256\n      0.999205\n      00:54\n    \n    \n      1\n      0.001580\n      0.002430\n      0.999324\n      00:54\n    \n    \n      2\n      0.000651\n      0.002244\n      0.999315\n      00:54\n    \n  \n\n\n\n(4) ‘one , two ,’ 이후에 이어질 50개의 단어를 생성하라.\n(풀이)\n\nlrnr.predict('one, two,', n_words=50) \n\n\n\n\n\n\n\n\n'one , two , three , four , five , six , seven , eight , nine , ten , eleven , twelve , thirteen , fourteen , fifteen , sixteen , seventeen , eighteen , nineteen , twenty , twenty one , twenty two , twenty three , twenty four , twenty five'\n\n\n(5) ‘twenty , twenty one ,’ 이후에 이어질 50개의 단어를 생성하라.\n(풀이)\n\nlrnr.predict('twenty, twenty one,', n_words=50) \n\n\n\n\n\n\n\n\n'twenty , twenty one , twenty two , twenty three , twenty four , twenty five , twenty six , twenty seven , twenty eight , twenty nine , thirty , thirty one , thirty two , thirty three , thirty four , thirty five , thirty six , thirty seven , thirty eight ,'"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#리눅스명령어",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#리눅스명령어",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "4. 리눅스명령어",
    "text": "4. 리눅스명령어\nCollab 에서 (혹은 리눅스기반 서버에서) 아래의 명령어를 순서대로 실행해보라.\n!ls\n!ls -a \n!ls .\n!ls .. \n!ls sample\n!mkdir asdf \n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n!cp 2022-09-08-rcmd_anal.csv ./asdf \n!ls ./asdf \n!rm 2022-09-08-rcmd_anal.csv \n!rm -rf asdf \n각 명령들이 무엇을 의미하는지 간단히 서술하라.\n(풀이)\n!ls - 현재디렉토리 파일+폴더 출력 - !ls . 와 같음 - !ls ./ 와 같음\n!ls -a - 현재디렉토리 파일+폴더 출력, 숨겨진 항목까지 출력\n!ls . - 현재디렉토리 파일+폴더 출력 - !ls 와 같음 - !ls ./ 와 같음\n!ls .. - 현재디렉토리보다 상위디렉토리의 파일+폴더 출력\n!ls sample - 현재디렉토리에 sample 디렉토리 출력 - !ls ./sample 과 같음\n!mkdir asdf - 현재디렉토리에 asdf 폴더 생성 - !mkdir ./asdf 와 같음\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv - url에 있는 파일 다운로드하여 현재디렉토리에 저장\n!cp 2022-09-08-rcmd_anal.csv ./asdf - 2022-09-08-rcmd_anal.csv 파일을 ./asdf 로 복사\n!ls ./asdf - 현재디렉토리에서 asdf 디렉토리의 내용출력 - !ls asdf 와 같음\n!rm 2022-09-08-rcmd_anal.csv - 현재 디렉토리에서 2022-09-08-rcmd_anal.csv 파일삭제; - rm ./2022-09-08-rcmd_anal.csv 와 같음\n!rm -rf asdf - 현재 디렉토리에서 asdf 삭제 (asdf 폴더내에 파일이 존재하면 파일도 같이 삭제) - r은 recursively, f는 force의 약자"
  },
  {
    "objectID": "posts/ml/2022-09-19-Assignment-1-Copy1.html#appendix-ipynb---html-변환",
    "href": "posts/ml/2022-09-19-Assignment-1-Copy1.html#appendix-ipynb---html-변환",
    "title": "Assignment 1 (22.09.19) – 풀이O",
    "section": "Appendix: ipynb -> html 변환",
    "text": "Appendix: ipynb -> html 변환\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-x3HQLeyrS7GLh70Dv_54Yg"
  },
  {
    "objectID": "posts/ml/index.html",
    "href": "posts/ml/index.html",
    "title": "Special Topics in Machine Learning",
    "section": "",
    "text": "This is posts of Special Topics in Machine Learning."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "",
    "text": "(2주차) 9월14일 [추천시스템, 텍스트분석, GAN]"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#imports",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#imports",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "imports",
    "text": "imports\n\n#\nfrom fastai.collab import * ## 추천시스템\nfrom fastai.text.all import * ## 텍스트분석 \nfrom fastai.vision.all import *  ## GAN (이미지분석) \nfrom fastai.vision.gan import * ## GAN (이미지생성)\n\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#이미지-자료분석-실습-지난시간-복습",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#이미지-자료분석-실습-지난시간-복습",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "이미지 자료분석 실습 (지난시간 복습)",
    "text": "이미지 자료분석 실습 (지난시간 복습)\n\n1단계: 데이터의 정리\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath.ls()\n\n(#7393) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\nevery files’ list\n\nfnames = get_image_files(path)\n\n\nfnames\n\n(#7390) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\nimage files’ list\n\nImageDataLoaders.from_name_func??\n\n\nSignature:\nImageDataLoaders.from_name_func(\n    path,\n    fnames,\n    label_func,\n    valid_pct=0.2,\n    seed=None,\n    item_tfms=None,\n    batch_tfms=None,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    def from_name_func(cls, path, fnames, label_func, **kwargs):\n        \"Create from the name attrs of `fnames` in `path`s with `label_func`\"\n        if sys.platform == 'win32' and isinstance(label_func, types.LambdaType) and label_func.__name__ == '<lambda>':\n            # https://medium.com/@jwnx/multiprocessing-serialization-in-python-with-pickle-9844f6fa1812\n            raise ValueError(\"label_func couldn't be lambda function on Windows\")\n        f = using_attr(label_func, 'name')\n        return cls.from_path_func(path, fnames, f, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/data.py\nType:      method\n\n\n\n\ndef f(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\nf(x) = x+ 1\nlambda x : x+1\n\nfnames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nf = lambda fname: 'cat' if fname[0].isupper() else 'dog'\n\n\nf('s')\n\n'dog'\n\n\n\nf('D')\n\n'cat'\n\n\ndls = ImageDataLoaders.from_name_func(\n    path, \n    fnames,\n    lambda fname: 'cat' if fname[0].isupper() else 'dog'\n    item_tfms=Resize(224)) \n\ndls = ImageDataLoaders.from_name_func(\n    path, \n    fnames,\n    f, # f대신 (lambda fname: 'cat' if fname[0].isupper() else 'dog') 를 넣어도 가능\n    item_tfms=Resize(224)) # 사이즈가 달라서 통일\n\n\ndls.show_batch()\n\n\n\n\n\n\n2단계: lrnr 오브젝트 생성\n\ncnn_learner??\n\n\nSignature:\ncnn_learner(\n    dls,\n    arch,\n    normalize=True,\n    n_out=None,\n    pretrained=True,\n    config=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=None,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    cut=None,\n    n_in=3,\n    init=<function kaiming_normal_ at 0x7fcbc439a8b0>,\n    custom_head=None,\n    concat_pool=True,\n    lin_ftrs=None,\n    ps=0.5,\n    first_bn=True,\n    bn_final=False,\n    lin_first=False,\n    y_range=None,\n)\nSource:   \n@delegates(create_cnn_model)\ndef cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None,\n                # learner args\n                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n                # other model args\n                **kwargs):\n    \"Build a convnet style learner from `dls` and `arch`\"\n    if config:\n        warnings.warn('config param is deprecated. Pass your args directly to cnn_learner.')\n        kwargs = {**config, **kwargs}\n    meta = model_meta.get(arch, _default_meta)\n    if normalize: _add_norm(dls, meta, pretrained)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    model = create_cnn_model(arch, n_out, pretrained=pretrained, **kwargs)\n    splitter=ifnone(splitter, meta['split'])\n    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n                   moms=moms)\n    if pretrained: learn.freeze()\n    # keep track of args for loggers\n    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n    return learn\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/learner.py\nType:      function\n\n\n\n\n!cat ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/learner.py\n이 코드로 존재하는 함수의 정의 확인 가능\n어디 소속된 함수인지 확인 하기 위해\nfastai에 소속된 cnn_leaner,따라서 fastai를 import해야 나타나지.\n\nlrnr = cnn_learner(dls,resnet34,metrics=error_rate)\n\n\nlrnr.dls.show_batch()\n\n\n\n\n\nid(lrnr.dls)\n\n140510181797744\n\n\n\nid(dls)\n\n140510181797744\n\n\n주소가 같다. 같은 역할\nlrnr에 dls가 소속되어 있다고 생각(?) - 포스트잇을 위에 덧붙인다 생각\n\n\n3단계: lrnr.학습()\n학습하는 fine_tune 이외에 여러가지 있음 - fine_tune 학습된 일부는 유지하고 바꿀 부분만 학습시키는 법: transfer learning\nfor exampel: cnn의 1d에서는 끝에만 학습\n\nlrnr.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.155603\n      0.014394\n      0.006766\n      00:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.051081\n      0.008424\n      0.002706\n      00:11\n    \n  \n\n\n\n\nfine_tune()은 모든 가중치를 학습하는 것이 아니라 일부만 학습하는 것임.\nfine_tune()이외이 방법으로 학습할 수도 있음.\n\n\n\n4단계: lrnr.예측()\n(방법1) lrnr.predict() 함수를 이용\n\nlrnr.predict('2022-09-07-dogs.jpeg') # 방법1-1\n#lrnr.predict(PILImage.create('2022-09-07-dogs.jpeg')) # 방법1-2\n#lrnr.predict(path.ls()[0]) # 방법1-3\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.2932e-05, 9.9998e-01]))\n\n\n컴퓨터가 이해하기 쉬운 방법인 1-2번째 방법\n\nlrnr.predict(PILImage.create('2022-09-07-dogs.jpeg'))\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.2932e-05, 9.9998e-01]))\n\n\n\nlrnr.predict(path.ls()[1])\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.4945e-08, 1.0000e+00]))\n\n\n\ndir(lrnr.model)\ndirectory에 _call_있으면 함수처럼 사용 가능\n\n(방법2) lrnr.model(X) 를 이용: X의 shape이 (?,3,224,224)의 형태의 텐서이어야함\n\ntype(dls.one_batch())\n\ntuple\n\n\n끝에 괄호로 묶여 있으면 tuple\n\nX,y = dls.one_batch() # 방법2\nlrnr.model(X[0:1]) \n\nTensorBase([[-8.3588,  7.0462]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nX[1].shape\n\ntorch.Size([3, 224, 224])\n\n\n\nX[:2].shape\n\ntorch.Size([2, 3, 224, 224])\n\n\n\nX.shape\n\ntorch.Size([64, 3, 224, 224])\n\n\nimage의 사이즈 224 * 224 - 3개의 채널 - 64개 - image, 입력\n\ny.shape\n\ntorch.Size([64])\n\n\n\n예측값\n\n\ny[:3]\n\nTensorCategory([1, 1, 0], device='cuda:0')\n\n\nlrnr.model(X[0])\n오류 뜬다. - torch.Size([3, 224, 224]) - shape을 - torch.Size([?, 3, 224, 224]) - 이런 식으로 만들어주자, 입력\n\nlrnr.model(X[:3])\n\nTensorBase([[ -8.3605,   7.0472],\n        [ -4.4236,   5.1110],\n        [ 14.0977, -13.0582]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nlrnr.model(X)\n\nTensorBase([[ -8.3595,   7.0465],\n        [ -4.4246,   5.1111],\n        [ 14.0959, -13.0577],\n        [ -6.4868,   7.2289],\n        [ -3.4974,   2.0202],\n        [ -7.1135,   6.2276],\n        [ -4.2407,   2.9429],\n        [ -7.0260,   6.4789],\n        [ -6.5011,   5.1029],\n        [ -7.4927,   4.9038],\n        [ -5.7292,   5.0113],\n        [ -9.6244,   5.7399],\n        [ -6.8247,   3.4742],\n        [ 17.2742, -12.6829],\n        [ -4.0548,   2.6589],\n        [ 16.3894, -14.2360],\n        [ -3.8864,   5.6632],\n        [ -5.1192,   6.0355],\n        [ 11.3016, -13.4798],\n        [ -8.1850,   7.5925],\n        [  8.3147,  -5.9946],\n        [ -8.0415,   8.4349],\n        [ -9.6461,   8.3790],\n        [ -5.4923,   5.8070],\n        [ 12.1504,  -9.3661],\n        [ -7.7945,   6.7907],\n        [ -5.0291,   3.4955],\n        [ 13.8045, -11.3889],\n        [ -4.5400,   5.1561],\n        [ 16.5360, -13.3928],\n        [ -4.0467,   3.3478],\n        [ -5.8401,   7.2492],\n        [  6.9878,  -4.8408],\n        [ -8.0189,   6.0578],\n        [ -7.7578,   4.7063],\n        [ -5.0351,   4.5309],\n        [  6.0511,  -4.1623],\n        [ -8.4919,   8.1300],\n        [ -5.9893,   5.8341],\n        [ -7.0671,   6.2901],\n        [ 17.0369, -13.7746],\n        [ -6.7633,   5.5232],\n        [ -7.3533,   7.6700],\n        [ -8.3923,   6.6368],\n        [ 13.2212, -10.2649],\n        [ 14.7573, -11.7938],\n        [ -6.6409,   5.6934],\n        [ -6.5882,   4.9800],\n        [ -5.2839,   5.3899],\n        [ -5.7066,   4.9765],\n        [ -5.8099,   3.8355],\n        [ -8.5055,   7.2022],\n        [ -8.7006,   4.5980],\n        [ -5.4901,   4.5288],\n        [ -7.6612,   7.1533],\n        [ 15.9380, -16.2778],\n        [  7.9763,  -7.1954],\n        [ 13.4158, -10.9864],\n        [ -4.9234,   2.9219],\n        [ -4.0274,   4.1298],\n        [ 16.8217, -16.0985],\n        [ -8.6418,   7.1085],\n        [ -5.9216,   6.0076],\n        [ -5.3720,   3.9876]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\\(y\\) : 왼쪽이 크면 0, 오른쪽이 크면 1 - 둘다 음수인 건 없네? - 왼쪽이 양수면 0 오른쪽이 양수면 1로 생각 가능하겠다."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#프로그래밍-과정",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#프로그래밍-과정",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "프로그래밍 과정",
    "text": "프로그래밍 과정\n\n프로그래밍 과정 overview\n- overview\n\ndls 오브젝트 생성\nlrnr 오브젝트 생성\nlrnr.학습()\nlrnr.예측()\n\n\n\n이미지분석, 추천시스템, 텍스트분석, GAN 분석과정 비교\n- 비교\n\n\n\n\n\n\n\n\n\n\n\n이미지분석(CNN)\n추천시스템\n텍스트분석\nGAN\n\n\n\n\n1단계\nImageDataLoaders\nCollabDataLoaders\nTextDataLoaders\nDataBlock -> dls\n\n\n2단계\ncnn_learner()\ncollab_learner()\nlanguage_model_learner()\nGANLearner.wgan()\n\n\n3단계\nlrnr.fine_tune(1)\nlrnr.fit()\nlrnr.fit()\nlrnr.fit()\n\n\n4단계\nlrnr.predict(), lrnr.model(X)\nlrnr.model(X)\nlrnr.predict()"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#추천시스템-실습",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#추천시스템-실습",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "추천시스템 실습",
    "text": "추천시스템 실습\n\n1단계\ngithub에서 해당 파일의 raw click하여 주소 가져오기\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv\n위와 같이 wget사용하면 주소의 data 바로 다운 가능\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_view.csv')\ndf_view\n\n\n\n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns\n\n\n\n컴퓨터가 좋아하는 타입은 아님\n\nrow0 - row49 에 해당하는 유저는 커피를 선호\nrow50 - row99 에 해당하는 유저는 홍차를 선호\n\n위의 자료는 비효율적, tidy data로 바꿔주자, 아래와 같이 정리함으로써 저장할 data도 줄어든다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n\n컴퓨터는 이러한 형태를 더 분석하기 좋아한다.\n\n!cat 파일명\ndata 도 확인 가능하다\n!wget https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-09-08-rcmd_anal.csv\n!cat Real_estate_valuation_data_set.csv\n\n\ndf.item.unique(),df.user.unique()\n# 유저는 1~100 으로 아이템은 1~20으로 번호가 매겨져 있음 \n\n(array([15,  1, 11,  5,  4, 14,  6, 20, 12, 17,  8,  9, 13, 19, 18, 16,  2,\n         3, 10,  7]),\n array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n         92,  93,  94,  95,  96,  97,  98,  99, 100]))\n\n\nitem, user 번호 확인\n\n\nCollabDataLoaders.from_df??\n\n\n\nSignature:\nCollabDataLoaders.from_df(\n    ratings,\n    valid_pct=0.2,\n    user_name=None,\n    item_name=None,\n    rating_name=None,\n    seed=None,\n    path='.',\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @delegates(DataLoaders.from_dblock)\n    @classmethod\n    def from_df(cls, ratings, valid_pct=0.2, user_name=None, item_name=None, rating_name=None, seed=None, path='.', **kwargs):\n        \"Create a `DataLoaders` suitable for collaborative filtering from `ratings`.\"\n        user_name   = ifnone(user_name,   ratings.columns[0])\n        item_name   = ifnone(item_name,   ratings.columns[1])\n        rating_name = ifnone(rating_name, ratings.columns[2])\n        cat_names = [user_name,item_name]\n        splits = RandomSplitter(valid_pct=valid_pct, seed=seed)(range_of(ratings))\n        to = TabularCollab(ratings, [Categorify], cat_names, y_names=[rating_name], y_block=TransformBlock(), splits=splits)\n        return to.dataloaders(path=path, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/collab.py\nType:      method\n\n\n\n\n\ndls=CollabDataLoaders.from_df(df)\n\nbatch 데이터들의 group\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n    \n  \n  \n    \n      0\n      59\n      14\n      3.986921\n    \n    \n      1\n      23\n      3\n      4.393831\n    \n    \n      2\n      43\n      15\n      1.022492\n    \n    \n      3\n      15\n      16\n      0.857821\n    \n    \n      4\n      81\n      11\n      3.892794\n    \n    \n      5\n      8\n      1\n      4.194341\n    \n    \n      6\n      6\n      18\n      1.124469\n    \n    \n      7\n      41\n      20\n      1.019717\n    \n    \n      8\n      10\n      18\n      0.789071\n    \n    \n      9\n      2\n      4\n      3.822704\n    \n  \n\n\n\n학습 전\n\nX,y= dls.one_batch()\n\n\ntype(X)\n\ntorch.Tensor\n\n\n\ntype(y)\n\ntorch.Tensor\n\n\n\ntype(dls.one_batch())\n\ntuple\n\n\n\nX[0],y[0]\n\n(tensor([74,  5]), tensor([1.0687]))\n\n\n\n99번 user가 13번 아이템을 먹었을때 평점 4.3294\n64번 유저가 15번 아이템을 먹었을때 평점을 4.1146 주었음\n\n\n\n2단계\n\ncollab_learner??\n\n\nSignature:\ncollab_learner(\n    dls,\n    n_factors=50,\n    use_nn=False,\n    emb_szs=None,\n    layers=None,\n    config=None,\n    y_range=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7f6f5cbebca0>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7f6f7682d0d0>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n@delegates(Learner.__init__)\ndef collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, **kwargs):\n    \"Create a Learner for collaborative filtering on `dls`.\"\n    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n    if loss_func is None: loss_func = MSELossFlat()\n    if config is None: config = tabular_config()\n    if y_range is not None: config['y_range'] = y_range\n    if layers is None: layers = [n_factors]\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n    return Learner(dls, model, loss_func=loss_func, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/collab.py\nType:      function\n\n\n\n\n\nlrnr = collab_learner(dls,y_range=(0,5)) # y_range는 평점의 범위\n\ny는 평점이니까 0~5까지의 범위를 넣어주자\n\n\n3단계\n\nlrnr.fit(30) # 총 30번 정도 해야 적합이 잘된다. \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.319048\n      2.344653\n      00:00\n    \n    \n      1\n      2.308136\n      2.343053\n      00:00\n    \n    \n      2\n      2.294945\n      2.327852\n      00:00\n    \n    \n      3\n      2.277872\n      2.286632\n      00:00\n    \n    \n      4\n      2.242915\n      2.204543\n      00:00\n    \n    \n      5\n      2.190223\n      2.074773\n      00:00\n    \n    \n      6\n      2.110882\n      1.897575\n      00:00\n    \n    \n      7\n      2.002486\n      1.683303\n      00:00\n    \n    \n      8\n      1.865617\n      1.440904\n      00:00\n    \n    \n      9\n      1.705019\n      1.189762\n      00:00\n    \n    \n      10\n      1.528594\n      0.947479\n      00:00\n    \n    \n      11\n      1.343253\n      0.728591\n      00:00\n    \n    \n      12\n      1.158638\n      0.542451\n      00:00\n    \n    \n      13\n      0.982688\n      0.394331\n      00:00\n    \n    \n      14\n      0.821111\n      0.282930\n      00:00\n    \n    \n      15\n      0.678422\n      0.203212\n      00:00\n    \n    \n      16\n      0.556185\n      0.148874\n      00:00\n    \n    \n      17\n      0.453426\n      0.112210\n      00:00\n    \n    \n      18\n      0.368528\n      0.088727\n      00:00\n    \n    \n      19\n      0.299861\n      0.073288\n      00:00\n    \n    \n      20\n      0.244360\n      0.064172\n      00:00\n    \n    \n      21\n      0.200107\n      0.058580\n      00:00\n    \n    \n      22\n      0.164968\n      0.055078\n      00:00\n    \n    \n      23\n      0.137080\n      0.052871\n      00:00\n    \n    \n      24\n      0.115055\n      0.051715\n      00:00\n    \n    \n      25\n      0.097788\n      0.051180\n      00:00\n    \n    \n      26\n      0.084044\n      0.051137\n      00:00\n    \n    \n      27\n      0.073312\n      0.050811\n      00:00\n    \n    \n      28\n      0.064564\n      0.050948\n      00:00\n    \n    \n      29\n      0.057734\n      0.051064\n      00:00\n    \n  \n\n\n\nloss가 2.3에서 0.47으로 떨어지는 모습\n\n\n4단계\n- 이미 있는 데이터를 예측\n- 하나의 배치 전체를 예측\nlrnr.model(X)\n만 넣으면 에러뜬다.\n\n!nvidia-smi\n\nTue Sep 20 23:48:21 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n| 30%   51C    P2   130W / 420W |  12684MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     37424      C   ...onda3/envs/csy/bin/python     3419MiB |\n|    0   N/A  N/A    293359      C   ...onda3/envs/csy/bin/python     4789MiB |\n|    0   N/A  N/A    378609      C   ...onda3/envs/csy/bin/python     2605MiB |\n|    0   N/A  N/A    378623      C   ...onda3/envs/csy/bin/python     1869MiB |\n+-----------------------------------------------------------------------------+\n\n\nGPU 확인 가능\nGPU아님 CPU로 올리자\n\nyhat=lrnr.model(X.to(\"cuda:0\"))\nyhat\n\ntensor([1.0097, 3.9209, 4.0095, 1.0768, 1.0179, 3.9701, 1.0862, 4.0802, 4.0528,\n        1.0718, 3.9230, 0.9994, 0.9662, 0.9122, 0.9745, 4.0083, 0.9989, 4.1045,\n        4.1632, 4.0724, 3.9754, 0.9565, 4.0757, 4.0317, 4.0740, 1.0779, 3.9354,\n        0.9951, 3.9031, 1.0241, 4.0253, 4.0965, 4.0368, 4.0944, 1.0856, 4.1450,\n        4.0549, 4.0072, 0.8689, 4.0659, 3.9192, 3.9501, 4.0449, 0.9437, 1.0582,\n        0.9584, 4.0409, 4.0453, 1.0675, 0.9406, 1.0740, 0.9307, 0.9885, 3.9951,\n        3.9118, 4.1501, 0.8893, 0.8946, 3.9687, 1.0579, 4.1036, 3.9685, 1.0809,\n        1.0768], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\ny.reshape(-1)\n\ntensor([1.0687, 3.8248, 3.8609, 1.0261, 0.8480, 4.4577, 1.1617, 4.3921, 4.2632,\n        0.7725, 4.0136, 1.2705, 0.8435, 1.0225, 0.8010, 4.1617, 1.0468, 4.1678,\n        4.5026, 4.0560, 3.8630, 0.9917, 4.0591, 3.7022, 4.1746, 1.3469, 3.7943,\n        1.0213, 3.7841, 0.8235, 4.0407, 3.9853, 4.1260, 4.1900, 1.0309, 4.1798,\n        3.9636, 3.7450, 0.6707, 4.0318, 4.1648, 4.1057, 3.9359, 0.7864, 1.2067,\n        0.8126, 4.0661, 4.1786, 1.3155, 0.9504, 1.1084, 0.8396, 0.8503, 4.0655,\n        3.8489, 4.0402, 0.7891, 0.9279, 4.1935, 0.9436, 4.4777, 4.0123, 1.0577,\n        0.8246])\n\n\n\nlrnr.model()은 GPU메모리에 존재하고 X는 일반메모리에 존재하므로 X를 GPU메모리로 옮겨주어야 함\nX.to(“cuda:0”)을 통하여 X를 GPU메모리로 옮기는 작업을 수행할 수 있다.\n\n- 하나의 유저가 하나의 아이템을 선택했다고 가정하고 예측 (주어진 자료중에서 예측)\n\nX.shape\n\ntorch.Size([64, 2])\n\n\n\nX[0:1]\n\ntensor([[74,  5]])\n\n\n- 1번 user가 커피2 마셨을때 - 예상: 4점 근처\n\nlrnr.model(X[0:1].to(\"cuda:0\"))\n\ntensor([1.0097], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nlrnr.model(tensor([[1,2]]).to(\"cuda:0\"))\n\ntensor([3.9337], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\n18번 유저가 5번 아이템(커피)를 먹는다면?\n\n\nlrnr.model(X[0:1].to(\"cuda:0\"))\n\ntensor([1.0097], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\n평점은 4.1128정도 될것\n\n- 하나의 유저가 하나의 아이템을 선택했다고 가정하고 예측 (주어지지 않은 자료중에서 예측)\n\nX[0:1]\n\ntensor([[74,  5]])\n\n\n\nXnew = torch.tensor([[1,  2]])\n\n\nlrnr.model(Xnew.to(\"cuda:0\"))\n\ntensor([3.9337], device='cuda:0', grad_fn=<AddBackward0>)"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#텍스트분석-실습",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#텍스트분석-실습",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "텍스트분석 실습",
    "text": "텍스트분석 실습\ntimeseries 와 text 순서가 중요! - 가장 잘 응용할 수 있는 게 chatbot챗봇 - 나는 \\(\\to\\) 학교에 \\(\\to\\) 갔다.\ntimeseries는 뒤를 정확히 맞춰야 하지만, text는 그렇지 않..?\n\n1단계\n\ndf = pd.DataFrame({'text':['h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??']*20000})\ndf\n\n\n\n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      1\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      2\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      3\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      4\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      ...\n      ...\n    \n    \n      19995\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19996\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19997\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19998\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      19999\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n  \n\n20000 rows × 1 columns\n\n\n\n\nTextDataLoaders.from_df??\n\n\nSignature:\nTextDataLoaders.from_df(\n    df,\n    path='.',\n    valid_pct=0.2,\n    seed=None,\n    text_col=0,\n    label_col=1,\n    label_delim=None,\n    y_block=None,\n    text_vocab=None,\n    is_lm=False,\n    valid_col=None,\n    tok_tfm=None,\n    tok_text_col='text',\n    seq_len=72,\n    backwards=False,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_df(cls, df, path='.', valid_pct=0.2, seed=None, text_col=0, label_col=1, label_delim=None, y_block=None,\n                text_vocab=None, is_lm=False, valid_col=None, tok_tfm=None, tok_text_col=\"text\", seq_len=72, backwards=False, **kwargs):\n        \"Create from `df` in `path` with `valid_pct`\"\n        blocks = [TextBlock.from_df(text_col, text_vocab, is_lm, seq_len, backwards, tok=tok_tfm)]\n        if y_block is None and not is_lm:\n            blocks.append(MultiCategoryBlock if is_listy(label_col) and len(label_col) > 1 else CategoryBlock)\n        if y_block is not None and not is_lm: blocks += (y_block if is_listy(y_block) else [y_block])\n        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n        dblock = DataBlock(blocks=blocks,\n                           get_x=ColReader(tok_text_col),\n                           get_y=None if is_lm else ColReader(label_col, label_delim=label_delim),\n                           splitter=splitter)\n        return cls.from_dblock(dblock, df, path=path, seq_len=seq_len, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/text/data.py\nType:      method\n\n\n\n\nis_lm = False\n다음 자료를 예측하고 싶을때\nis_lm = True\n\nclassification을 수행하고 싶을 때\n생성에 목적\nis_lm: text의 생성에 관심이 있다면 True로 설정할 것\n\n\ndls = TextDataLoaders.from_df(df,text_col='text',is_lm=True) \n\n\n\n\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n      h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o .\n    \n    \n      1\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n    \n    \n      2\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n    \n    \n      3\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n    \n    \n      4\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n    \n    \n      5\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n    \n    \n      6\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n    \n    \n      7\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n    \n    \n      8\n      ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n    \n  \n\n\n\n위의 결과에서 xxbos는 하나의 내용이 끝나고 다른 내용이 시작된다는 의미\n\n\n2단계\n\nlanguage_model_learner??\n\n\nSignature:\nlanguage_model_learner(\n    dls,\n    arch,\n    config=None,\n    drop_mult=1.0,\n    backwards=False,\n    pretrained=True,\n    pretrained_fnames=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7fcb79d04940>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n@delegates(Learner.__init__)\ndef language_model_learner(dls, arch, config=None, drop_mult=1., backwards=False, pretrained=True, pretrained_fnames=None, **kwargs):\n    \"Create a `Learner` with a language model from `dls` and `arch`.\"\n    vocab = _get_text_vocab(dls)\n    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n    meta = _model_meta[arch]\n    learn = LMLearner(dls, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n    url = 'url_bwd' if backwards else 'url'\n    if pretrained or pretrained_fnames:\n        if pretrained_fnames is not None:\n            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n        else:\n            if url not in meta:\n                warn(\"There are no pretrained weights for that architecture yet!\")\n                return learn\n            model_path = untar_data(meta[url] , c_key='model')\n            try: fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n            except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n        learn = learn.load_pretrained(*fnames)\n    return learn\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/text/learner.py\nType:      function\n\n\n\n\n\nlrnr = language_model_learner(dls, AWD_LSTM)\n\n\n\n3단계\n\nlrnr.fit(5)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.926327\n      0.851078\n      00:15\n    \n    \n      1\n      1.030813\n      0.366239\n      00:15\n    \n    \n      2\n      0.636483\n      0.251018\n      00:15\n    \n    \n      3\n      0.478914\n      0.209533\n      00:15\n    \n    \n      4\n      0.431652\n      0.190482\n      00:15\n    \n  \n\n\n\n\n\n4단계\n\nlrnr.predict('h e',n_words=30)\n\n\n\n\n'h e l l o ? h e l l o ! h e l l o ! h e l l o ! h e l l o ? ?'"
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#gan-intro",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#gan-intro",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "GAN intro",
    "text": "GAN intro\n- 저자: 이안굿펠로우 (이름이 특이함. 좋은친구..) - 천재임 - 지도교수가 요수아 벤지오\n- 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. - https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음 \\(\\to\\) 48978회 인용..)\n- 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..)\n- 무슨내용? 생성모형\n\n생성모형이란? (쉬운 설명)\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n\n\nGAN의 응용분야\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면? - https://www.lgsl.kr/sto/stories/60/ALMA2020070001\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요…..\n\n\n\n생성모형이란? 통계학과 버전의 설명\n\n제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\ny의 평균적인 확률이 나올떄 x로 y 를 예측할 수 있다고 한단\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n분류할 수 았다는게 생성할 수 있다는 건 아니니까\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다.\n\n\nGAN의 원리\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n\n\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n\n\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n\n\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n\n\n위조범: 더 정교하게 만들자..\n\n\n경찰: 더 판별능력을 업그레이드 하자!\n\n\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "posts/ml/2022-09-14-ml_2주차_9월14일.html#gan-실습",
    "href": "posts/ml/2022-09-14-ml_2주차_9월14일.html#gan-실습",
    "title": "기계학습 특강 (2주차) 9월14일",
    "section": "GAN 실습",
    "text": "GAN 실습\n\n1단계\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\nDataBlock??\n\n\nInit signature:\nDataBlock(\n    blocks=None,\n    dl_type=None,\n    getters=None,\n    n_inp=None,\n    item_tfms=None,\n    batch_tfms=None,\n    *,\n    get_items=None,\n    splitter=None,\n    get_y=None,\n    get_x=None,\n)\nSource:        \nclass DataBlock():\n    \"Generic container to quickly build `Datasets` and `DataLoaders`\"\n    get_x=get_items=splitter=get_y = None\n    blocks,dl_type = (TransformBlock,TransformBlock),TfmdDL\n    _methods = 'get_items splitter get_y get_x'.split()\n    _msg = \"If you wanted to compose several transforms in your getter don't forget to wrap them in a `Pipeline`.\"\n    def __init__(self, blocks=None, dl_type=None, getters=None, n_inp=None, item_tfms=None, batch_tfms=None, **kwargs):\n        blocks = L(self.blocks if blocks is None else blocks)\n        blocks = L(b() if callable(b) else b for b in blocks)\n        self.type_tfms = blocks.attrgot('type_tfms', L())\n        self.default_item_tfms  = _merge_tfms(*blocks.attrgot('item_tfms',  L()))\n        self.default_batch_tfms = _merge_tfms(*blocks.attrgot('batch_tfms', L()))\n        for b in blocks:\n            if getattr(b, 'dl_type', None) is not None: self.dl_type = b.dl_type\n        if dl_type is not None: self.dl_type = dl_type\n        self.dataloaders = delegates(self.dl_type.__init__)(self.dataloaders)\n        self.dls_kwargs = merge(*blocks.attrgot('dls_kwargs', {}))\n        self.n_inp = ifnone(n_inp, max(1, len(blocks)-1))\n        self.getters = ifnone(getters, [noop]*len(self.type_tfms))\n        if self.get_x:\n            if len(L(self.get_x)) != self.n_inp:\n                raise ValueError(f'get_x contains {len(L(self.get_x))} functions, but must contain {self.n_inp} (one for each input)\\n{self._msg}')\n            self.getters[:self.n_inp] = L(self.get_x)\n        if self.get_y:\n            n_targs = len(self.getters) - self.n_inp\n            if len(L(self.get_y)) != n_targs:\n                raise ValueError(f'get_y contains {len(L(self.get_y))} functions, but must contain {n_targs} (one for each target)\\n{self._msg}')\n            self.getters[self.n_inp:] = L(self.get_y)\n        if kwargs: raise TypeError(f'invalid keyword arguments: {\", \".join(kwargs.keys())}')\n        self.new(item_tfms, batch_tfms)\n    def _combine_type_tfms(self): return L([self.getters, self.type_tfms]).map_zip(\n        lambda g,tt: (g.fs if isinstance(g, Pipeline) else L(g)) + tt)\n    def new(self, item_tfms=None, batch_tfms=None):\n        self.item_tfms  = _merge_tfms(self.default_item_tfms,  item_tfms)\n        self.batch_tfms = _merge_tfms(self.default_batch_tfms, batch_tfms)\n        return self\n    @classmethod\n    def from_columns(cls, blocks=None, getters=None, get_items=None, **kwargs):\n        if getters is None: getters = L(ItemGetter(i) for i in range(2 if blocks is None else len(L(blocks))))\n        get_items = _zip if get_items is None else compose(get_items, _zip)\n        return cls(blocks=blocks, getters=getters, get_items=get_items, **kwargs)\n    def datasets(self, source, verbose=False):\n        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n        splits = (self.splitter or RandomSplitter())(items)\n        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n    def dataloaders(self, source, path='.', verbose=False, **kwargs):\n        dsets = self.datasets(source, verbose=verbose)\n        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n    _docs = dict(new=\"Create a new `DataBlock` with other `item_tfms` and `batch_tfms`\",\n                 datasets=\"Create a `Datasets` object from `source`\",\n                 dataloaders=\"Create a `DataLoaders` object from `source`\")\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/data/block.py\nType:           type\nSubclasses:     \n\n\n\n\n\nTransformBlock\n\nfastai.data.block.TransformBlock\n\n\n\nImageBlock\n\n<function fastai.vision.data.ImageBlock(cls=<class 'fastai.vision.core.PILImage'>)>\n\n\n\ngenerate_noise\n\n<function fastai.vision.gan.generate_noise(fn, size=100)>\n\n\n\ndblock = DataBlock(blocks=(TransformBlock,ImageBlock),\n          get_x = generate_noise,\n          get_items=get_image_files,\n          item_tfms=Resize(32))\ndls = dblock.dataloaders(path) \n\n\ndls.show_batch()\n\n\n\n\n\n\n2단계\n\nbasic_generator??\n\n\nSignature:\nbasic_generator(\n    out_size,\n    n_channels,\n    in_sz=100,\n    n_features=64,\n    n_extra_layers=0,\n    ks=3,\n    stride=1,\n    padding=None,\n    bias=None,\n    ndim=2,\n    norm_type=<NormType.Batch: 1>,\n    bn_1st=True,\n    act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n    transpose=False,\n    init='auto',\n    xtra=None,\n    bias_std=0.01,\n    dilation: Union[int, Tuple[int, int]] = 1,\n    groups: int = 1,\n    padding_mode: str = 'zeros',\n    device=None,\n    dtype=None,\n)\nSource:   \n@delegates(ConvLayer.__init__)\ndef basic_generator(out_size, n_channels, in_sz=100, n_features=64, n_extra_layers=0, **kwargs):\n    \"A basic generator from `in_sz` to images `n_channels` x `out_size` x `out_size`.\"\n    cur_size, cur_ftrs = 4, n_features//2\n    while cur_size < out_size:  cur_size *= 2; cur_ftrs *= 2\n    layers = [AddChannels(2), ConvLayer(in_sz, cur_ftrs, 4, 1, transpose=True, **kwargs)]\n    cur_size = 4\n    while cur_size < out_size // 2:\n        layers.append(ConvLayer(cur_ftrs, cur_ftrs//2, 4, 2, 1, transpose=True, **kwargs))\n        cur_ftrs //= 2; cur_size *= 2\n    layers += [ConvLayer(cur_ftrs, cur_ftrs, 3, 1, 1, transpose=True, **kwargs) for _ in range(n_extra_layers)]\n    layers += [nn.ConvTranspose2d(cur_ftrs, n_channels, 4, 2, 1, bias=False), nn.Tanh()]\n    return nn.Sequential(*layers)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      function\n\n\n\n\n\nbasic_critic??\n\n\nSignature:\nbasic_critic(\n    in_size,\n    n_channels,\n    n_features=64,\n    n_extra_layers=0,\n    norm_type=<NormType.Batch: 1>,\n    ks=3,\n    stride=1,\n    padding=None,\n    bias=None,\n    ndim=2,\n    bn_1st=True,\n    act_cls=<class 'torch.nn.modules.activation.ReLU'>,\n    transpose=False,\n    init='auto',\n    xtra=None,\n    bias_std=0.01,\n    dilation: Union[int, Tuple[int, int]] = 1,\n    groups: int = 1,\n    padding_mode: str = 'zeros',\n    device=None,\n    dtype=None,\n)\nSource:   \n@delegates(ConvLayer.__init__)\ndef basic_critic(in_size, n_channels, n_features=64, n_extra_layers=0, norm_type=NormType.Batch, **kwargs):\n    \"A basic critic for images `n_channels` x `in_size` x `in_size`.\"\n    layers = [ConvLayer(n_channels, n_features, 4, 2, 1, norm_type=None, **kwargs)]\n    cur_size, cur_ftrs = in_size//2, n_features\n    layers += [ConvLayer(cur_ftrs, cur_ftrs, 3, 1, norm_type=norm_type, **kwargs) for _ in range(n_extra_layers)]\n    while cur_size > 4:\n        layers.append(ConvLayer(cur_ftrs, cur_ftrs*2, 4, 2, 1, norm_type=norm_type, **kwargs))\n        cur_ftrs *= 2 ; cur_size //= 2\n    init = kwargs.get('init', nn.init.kaiming_normal_)\n    layers += [init_default(nn.Conv2d(cur_ftrs, 1, 4, padding=0), init), Flatten()]\n    return nn.Sequential(*layers)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      function\n\n\n\n\n\ncounterfeiter = basic_generator(32,n_channels=3,n_extra_layers=1)\npolice = basic_critic(32,n_channels=3,n_extra_layers=1)\n\n\n32는 사이즈\n채널은 컬러이면 3이지만 이건 흑백이라도 3으로 표현해봄\n\n\nGANLearner.wgan??\n\n\nSignature:\nGANLearner.wgan(\n    dls,\n    generator,\n    critic,\n    switcher=None,\n    clip=0.01,\n    switch_eval=False,\n    gen_first=False,\n    show_img=True,\n    cbs=None,\n    metrics=None,\n    loss_func=None,\n    opt_func=<function Adam at 0x7fcb70042550>,\n    lr=0.001,\n    splitter=<function trainable_params at 0x7fcb79d04940>,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n)\nSource:   \n    @classmethod\n    def wgan(cls, dls, generator, critic, switcher=None, clip=0.01, switch_eval=False, **kwargs):\n        \"Create a WGAN from `data`, `generator` and `critic`.\"\n        if switcher is None: switcher = FixedGANSwitcher(n_crit=5, n_gen=1)\n        return cls(dls, generator, critic, _tk_mean, _tk_diff, switcher=switcher, clip=clip, switch_eval=switch_eval, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/gan.py\nType:      method\n\n\n\n\n\nlrnr = GANLearner.wgan(dls,counterfeiter,police) \n\n\n\n3단계\n- lrnr.fit(10) 진행\n\nlrnr.fit(10)\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.543594\n      0.420719\n      0.420719\n      -0.754515\n      00:04\n    \n    \n      1\n      -0.578704\n      0.376830\n      0.376830\n      -0.759121\n      00:04\n    \n    \n      2\n      -0.581667\n      0.335058\n      0.335058\n      -0.765852\n      00:04\n    \n    \n      3\n      -0.581913\n      0.350422\n      0.350422\n      -0.766256\n      00:04\n    \n    \n      4\n      -0.576919\n      0.239790\n      0.239790\n      -0.757070\n      00:04\n    \n    \n      5\n      -0.568187\n      0.165856\n      0.165856\n      -0.738596\n      00:04\n    \n    \n      6\n      -0.561763\n      0.312277\n      0.312277\n      -0.738768\n      00:04\n    \n    \n      7\n      -0.545816\n      0.312626\n      0.312626\n      -0.735054\n      00:04\n    \n    \n      8\n      -0.530404\n      0.315626\n      0.315626\n      -0.713279\n      00:04\n    \n    \n      9\n      -0.552664\n      0.292665\n      0.292665\n      -0.719266\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총20회\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.534434\n      0.329432\n      0.329432\n      -0.738360\n      00:04\n    \n    \n      1\n      -0.491044\n      0.241687\n      0.241687\n      -0.282900\n      00:04\n    \n    \n      2\n      -0.430823\n      0.247032\n      0.247032\n      -0.631827\n      00:04\n    \n    \n      3\n      -0.509287\n      0.228638\n      0.228638\n      -0.702186\n      00:04\n    \n    \n      4\n      -0.541639\n      0.306787\n      0.306787\n      -0.737486\n      00:04\n    \n    \n      5\n      -0.490239\n      0.270219\n      0.270219\n      -0.686973\n      00:04\n    \n    \n      6\n      -0.456657\n      0.370165\n      0.370165\n      -0.651278\n      00:04\n    \n    \n      7\n      -0.375928\n      0.254674\n      0.254674\n      -0.463629\n      00:04\n    \n    \n      8\n      -0.505262\n      0.241540\n      0.241540\n      -0.706440\n      00:04\n    \n    \n      9\n      -0.511837\n      0.264010\n      0.264010\n      -0.717528\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총30회\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.389076\n      0.203898\n      0.203898\n      -0.610006\n      00:04\n    \n    \n      1\n      -0.404953\n      0.248211\n      0.248211\n      -0.564691\n      00:04\n    \n    \n      2\n      -0.399689\n      0.157126\n      0.157126\n      -0.475484\n      00:04\n    \n    \n      3\n      -0.412959\n      0.160083\n      0.160083\n      -0.628447\n      00:04\n    \n    \n      4\n      -0.419133\n      0.140253\n      0.140253\n      -0.315640\n      00:04\n    \n    \n      5\n      -0.412665\n      0.360084\n      0.360084\n      -0.504751\n      00:04\n    \n    \n      6\n      -0.419645\n      0.331901\n      0.331901\n      -0.627747\n      00:04\n    \n    \n      7\n      -0.393825\n      0.099620\n      0.099620\n      -0.479805\n      00:04\n    \n    \n      8\n      -0.383802\n      0.332651\n      0.332651\n      -0.485545\n      00:04\n    \n    \n      9\n      -0.329964\n      0.066743\n      0.066743\n      -0.331843\n      00:04\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n- lrnr.fit(10) 추가로 진행 // 총 60회\n\nlrnr.fit(30)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.280188\n      0.083489\n      0.083489\n      -0.421842\n      00:04\n    \n    \n      1\n      -0.211743\n      0.066232\n      0.066232\n      -0.485185\n      00:04\n    \n    \n      2\n      -0.548622\n      0.439374\n      0.439374\n      -0.729976\n      00:04\n    \n    \n      3\n      -0.184136\n      0.166024\n      0.166024\n      -0.196536\n      00:04\n    \n    \n      4\n      -0.180048\n      0.283176\n      0.283176\n      -0.343643\n      00:04\n    \n    \n      5\n      -0.082062\n      -0.111767\n      -0.111767\n      -0.232821\n      00:06\n    \n    \n      6\n      -0.134064\n      -0.252754\n      -0.252754\n      -0.056792\n      00:05\n    \n    \n      7\n      -0.024693\n      -0.019944\n      -0.019944\n      -0.060462\n      00:04\n    \n    \n      8\n      -0.067052\n      -0.176633\n      -0.176633\n      -0.087246\n      00:05\n    \n    \n      9\n      -0.051849\n      0.077216\n      0.077216\n      -0.063890\n      00:04\n    \n    \n      10\n      -0.062414\n      0.576616\n      0.576616\n      0.007565\n      00:04\n    \n    \n      11\n      -0.028339\n      -0.177623\n      -0.177623\n      -0.165999\n      00:04\n    \n    \n      12\n      -0.285967\n      0.256777\n      0.256777\n      -0.486597\n      00:04\n    \n    \n      13\n      -0.040034\n      0.200722\n      0.200722\n      -0.002714\n      00:05\n    \n    \n      14\n      -0.082319\n      -0.218245\n      -0.218245\n      -0.076548\n      00:04\n    \n    \n      15\n      -0.126090\n      -0.120631\n      -0.120631\n      -0.311712\n      00:04\n    \n    \n      16\n      -0.120472\n      0.024194\n      0.024194\n      -0.165129\n      00:04\n    \n    \n      17\n      -0.213207\n      0.447029\n      0.447029\n      -0.441171\n      00:04\n    \n    \n      18\n      -0.104892\n      0.076353\n      0.076353\n      -0.305898\n      00:04\n    \n    \n      19\n      -0.077636\n      -0.229590\n      -0.229590\n      -0.206540\n      00:04\n    \n    \n      20\n      -0.037347\n      -0.169197\n      -0.169197\n      -0.070061\n      00:04\n    \n    \n      21\n      -0.063813\n      -0.283316\n      -0.283316\n      0.009801\n      00:04\n    \n    \n      22\n      -0.037806\n      -0.101751\n      -0.101751\n      -0.020896\n      00:04\n    \n    \n      23\n      -0.057209\n      -0.012665\n      -0.012665\n      -0.095574\n      00:04\n    \n    \n      24\n      -0.036946\n      0.090177\n      0.090177\n      -0.049521\n      00:04\n    \n    \n      25\n      -0.050363\n      -0.206716\n      -0.206716\n      -0.035576\n      00:04\n    \n    \n      26\n      -0.047856\n      0.052171\n      0.052171\n      -0.017636\n      00:04\n    \n    \n      27\n      -0.009292\n      -0.027788\n      -0.027788\n      -0.003629\n      00:04\n    \n    \n      28\n      -0.032223\n      -0.223866\n      -0.223866\n      -0.011261\n      00:04\n    \n    \n      29\n      -0.028316\n      -0.006388\n      -0.006388\n      -0.024545\n      00:05\n    \n  \n\n\n\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n4단계 (없음)"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "",
    "text": "(3주차) 9월21일 [딥러닝의 기초 - 회귀분석(1)–선형모형,손실함수,경사하강법]"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#imports",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#imports",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#로드맵",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#로드맵",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "로드맵",
    "text": "로드맵\n- 회귀분석 \\(\\to\\) 로지스틱 \\(\\to\\) 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN)\n- 강의계획서"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#ref",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#ref",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "ref",
    "text": "ref\n- 넘파이 문법이 약하다면? (reshape, concatenate, stack)\n\nreshape: 아래 링크의 넘파이공부 2단계 reshape 참고\n\nhttps://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html\n\nconcatenate, stack: 아래 링크의 넘파이공부 4단계 참고\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형-소개",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형-소개",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "회귀모형 소개",
    "text": "회귀모형 소개\n- model: \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n- model: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix} \\quad = \\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형에서-데이터-생성",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형에서-데이터-생성",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "회귀모형에서 데이터 생성",
    "text": "회귀모형에서 데이터 생성\n\n\ntorch.manual_seed(1)\n_r = torch.randn(100).sort() # 두번쨰는 index~\n\n\ntype(_r)\n\ntorch.return_types.sort\n\n\n\n_r[0]\n\ntensor([-3.3312, -2.5832, -2.2456, -2.1021, -1.6095, -1.6091, -1.5256, -1.4782,\n        -1.4465, -1.1608, -1.1334, -1.0703, -1.0373, -1.0233, -1.0055, -0.9962,\n        -0.9823, -0.9798, -0.9276, -0.9274, -0.8743, -0.8696, -0.8313, -0.8138,\n        -0.7981, -0.7773, -0.7735, -0.7502, -0.7479, -0.7121, -0.7040, -0.6970,\n        -0.6629, -0.6540, -0.6540, -0.6446, -0.6298, -0.6200, -0.6177, -0.6092,\n        -0.5962, -0.5601, -0.5065, -0.4757, -0.4610, -0.4370, -0.2515, -0.2223,\n        -0.2106, -0.1860, -0.1853, -0.1759, -0.1578, -0.1316, -0.1110, -0.1010,\n        -0.1002, -0.0721, -0.0288, -0.0255, -0.0075,  0.0103,  0.0457,  0.0612,\n         0.0663,  0.0998,  0.1374,  0.1530,  0.1578,  0.1938,  0.1991,  0.1991,\n         0.2284,  0.2444,  0.2927,  0.3037,  0.3434,  0.3956,  0.4415,  0.4676,\n         0.5451,  0.6155,  0.6995,  0.7317,  0.7626,  0.8073,  0.8539,  0.8657,\n         0.9386,  1.1017,  1.1120,  1.1651,  1.3851,  1.5392,  1.5748,  1.6734,\n         1.6871,  1.8793,  2.0154,  2.3571])\n\n\n\n_r[1]\n\ntensor([62, 90, 26, 92,  3,  7,  0, 94, 27, 17, 95, 98, 45, 65, 67, 74, 79,  6,\n        86, 48, 99, 61, 75, 85, 30, 10, 35,  1, 63,  8, 72, 16, 22,  2, 82, 59,\n        47, 93, 29,  5, 66, 77, 80, 39, 76, 51, 11, 12, 68, 58, 73, 25, 42, 31,\n        40, 96,  4, 33, 43, 64, 69, 71, 37, 28, 50, 81, 56, 38, 34, 89, 36, 19,\n        14, 21, 41,  9, 97, 78, 53, 15, 49, 88, 18, 83, 52, 23, 91, 20, 57, 24,\n        87, 54, 84, 60, 46, 70, 13, 32, 55, 44])\n\n\n\na,_ = _r[0],_r[1]\n\n\na\n\ntensor([-3.3312, -2.5832, -2.2456, -2.1021, -1.6095, -1.6091, -1.5256, -1.4782,\n        -1.4465, -1.1608, -1.1334, -1.0703, -1.0373, -1.0233, -1.0055, -0.9962,\n        -0.9823, -0.9798, -0.9276, -0.9274, -0.8743, -0.8696, -0.8313, -0.8138,\n        -0.7981, -0.7773, -0.7735, -0.7502, -0.7479, -0.7121, -0.7040, -0.6970,\n        -0.6629, -0.6540, -0.6540, -0.6446, -0.6298, -0.6200, -0.6177, -0.6092,\n        -0.5962, -0.5601, -0.5065, -0.4757, -0.4610, -0.4370, -0.2515, -0.2223,\n        -0.2106, -0.1860, -0.1853, -0.1759, -0.1578, -0.1316, -0.1110, -0.1010,\n        -0.1002, -0.0721, -0.0288, -0.0255, -0.0075,  0.0103,  0.0457,  0.0612,\n         0.0663,  0.0998,  0.1374,  0.1530,  0.1578,  0.1938,  0.1991,  0.1991,\n         0.2284,  0.2444,  0.2927,  0.3037,  0.3434,  0.3956,  0.4415,  0.4676,\n         0.5451,  0.6155,  0.6995,  0.7317,  0.7626,  0.8073,  0.8539,  0.8657,\n         0.9386,  1.1017,  1.1120,  1.1651,  1.3851,  1.5392,  1.5748,  1.6734,\n         1.6871,  1.8793,  2.0154,  2.3571])\n\n\n\n_ones = torch.ones(100)\n\n\nX = torch.stack([_ones,a]).T\n\n\n#같아요 _X = torch.stack([_ones,a],axis=1)\n\n\nϵ = torch.randn(100)*0.5\n\n\nx=4*2.5+ϵ\n\n\nx\n\ntensor([10.1027, 10.1526, 10.2678,  9.7844, 10.0786, 10.6270, 10.6638,  9.7523,\n         9.0098, 10.8993, 10.0509, 10.1700,  9.6777,  9.8565, 11.6606,  9.7990,\n         9.8485,  9.1191, 10.3174,  9.5978,  9.4814,  9.4665,  9.8957,  9.8922,\n        11.1476, 10.3375, 10.8567,  9.1029,  9.2396, 10.4598,  9.7258,  9.8264,\n        10.2365,  9.7857, 10.2757,  9.2263, 10.3787,  9.7966,  9.9361, 10.1402,\n        10.8730, 10.9275,  9.6468, 11.2785, 10.3853,  9.4630,  9.8992,  9.7199,\n         9.6880,  9.5114, 10.4374, 10.4936, 10.1252,  9.6035, 10.2616, 10.6118,\n         9.7983,  9.5204,  9.9974,  9.9606,  9.8054,  9.9602, 10.3802,  9.4987,\n         9.5180, 10.0708,  9.9182,  9.8209,  9.9703,  8.7540, 10.1211, 10.1442,\n        10.0516, 10.5502,  9.8292, 10.4737, 10.3112,  9.7759,  9.8572, 10.1940,\n        10.2575,  9.0763,  8.5416,  9.7163,  9.9647, 10.1735,  9.6732, 10.7793,\n        10.2000, 11.2211,  9.8091, 10.2163,  8.9914, 10.2118, 10.2865,  9.1019,\n         9.8469,  9.7899, 10.1414, 10.1821])\n\n\n\nW = torch.tensor([2.5,4])\n\n\nW.shape\n\ntorch.Size([2])\n\n\n곱하지지 않았어야하지만 곱해짐..!\n\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\ntorch.manual_seed(43052)\nones= torch.ones(100)\nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # torch.stack([ones,x],axis=1)\nW = torch.tensor([2.5,4])\nϵ = torch.randn(100)*0.5\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.5+4*x,'--')\n\n\n\n\nblue를 observe한 상태에서 orange를 measure함\n학습이 된 상태: prediction을 제시할 수 있는 상태\nunderline function을 아는 상태는 w0와 w1을 아는 상태라고 할 수 있다.\n\\(x_{new}\\)가 주어졌을때 underline function과 얼마나 떨어져 있나 보면 되니까"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형에서-학습이란",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#회귀모형에서-학습이란",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "회귀모형에서 학습이란?",
    "text": "회귀모형에서 학습이란?\n- 파란점만 주어졌을때, 주황색 점선을 추정하는것. 좀 더 정확하게 말하면 given data로 \\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)를 최대한 \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는것.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\n\\(\\hat{y} = x \\hat{W}\\)\n- 더 쉽게 말하면 아래의 그림을 보고 적당한 추세선을 찾는것이다.\n적당한?\n\nplt.plot(x,y,'o')\n\n\n\n\n- 시도: \\((\\hat{w}_0,\\hat{w}_1)=(-5,10)\\)을 선택하여 선을 그려보고 적당한지 판단.\n\nplt.plot(x,y,'o')\nplt.plot(x,-5+x*10,'--')\n\n\n\n\n\n\\(\\hat{y}_i=-5 +10 x_i\\) 와 같이 \\(y_i\\)의 값을 적합시키겠다는 의미\n\n- 벡터표현으로 주황색점선을 계산\n\nWhat = torch.tensor([-5.0,10.0])\n\n\nX.shape\n\ntorch.Size([100, 2])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'--')\n\n\n\n\ndata를 보고 architecture를 설계하는 modeling과정"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#파라메터를-학습하는-방법-적당한-선으로-업데이트-하는-방법",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)",
    "text": "파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법)\n- 이론적으로 추론 <- 회귀분석시간에 배운것\n- 컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법) <- 우리가 오늘 파이토치로 실습해볼 내용.\n- 전략: 아래와 같은 3단계 전략을 취한다.\n\nstage1: 아무 점선이나 그어본다..\nstage2: stage1에서 그은 점선보다 더 좋은 점선으로 바꾼다.\nstage3: stage1 - 2 를 반복한다.\n\n\nStage1: 첫번째 점선 – 임의의 선을 일단 그어보자\n- \\(\\hat{w}_0=-5, \\hat{w}_1 = 10\\) 으로 설정하고 (왜? 그냥) 임의의 선을 그어보자.\n\n처음에는 ${}=\n\\[\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} -5 \\\\ 10 \\end{bmatrix}\\]\n$ 를 대입해서 주황색 점선을 적당히 그려보자는 의미\n끝에 requires_grad=True는 나중에 미분을 위한 것\n\n\nWhat = torch.tensor([-5.0,10.0])\nWhat\n\ntensor([-5., 10.])\n\n\ntensor에서 tf.variable로 출력할떄롸 같은 결과임\n\nWhat = torch.tensor([-5.0,10.0],requires_grad=True)\nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n꼬리표가 생겼다.\n\nWhat.detach()\n\ntensor([-5., 10.])\n\n\n\nWhat.data\n\ntensor([-5., 10.])\n\n\n꼬리표가 사라졌다.\n꼬리표 있어도 계산은 되지만, matplot에서는 오류..\n그려보자!\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')\n\n\n\n\n\n\nStage2: 첫번째 수정 – 최초의 점선에 대한 ‘적당한 정도’를 판단하고 더 ’적당한’ 점선으로 업데이트 한다.\n- ’적당한 정도’를 판단하기 위한 장치: loss function 도입!\n\\(loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2\\)\n\\(=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})\\)\n\nloss = torch.sum((y - X@What)**2)\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n- loss 함수의 특징 - \\(y_i \\approx \\hat{y}_i\\) 일수록 loss값이 작다. - \\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0,\\hat{w}_1)\\)을 잘 찍으면 loss값이 작다. - (중요) 주황색 점선이 ‘적당할 수록’ loss값이 작다.\n- 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. - 궁극적으로는 아예 모든 조합 \\((\\hat{w}_0,\\hat{w}_1)\\)에 대하여 가장 작은 loss를 찾으면 좋겠다. (stage2에서 할일은 아님)\n- 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. - 적당해보이는 주황색 선을 찾자 \\(\\to\\) \\(loss(w_0,w_1)\\)를 최소로하는 \\((w_0,w_1)\\)의 값(정의역 set)을 찾자.\n- 수정된 목표: \\(loss(w_0,w_1)\\)를 최소로 하는 \\((w_0,w_1)\\)을 구하라. - 단순한 수학문제가 되었다. 마치 \\(loss(w)=w^2-2w+3\\) 을 최소화하는 \\(w\\)를 찾으라는 것과 같음. - 즉 “적당한 선으로 업데이트 하라 = 파라메터(\\(W\\))를 학습 하라 = 손실함수를 최소화 하라”\n- 우리의 무기: 경사하강법, 벡터미분\n\n\nStage2를 위한 경사하강법 복습\n경사하강법 아이디어 (1차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접선) <– 미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다.\n(서연필기) 접선 음수면 오른쪽으로 가고 접선 양수면 왼쪽으로 가쟈~\n경사하강법 아이디어 (2차원)\n(step 1) 임의의 점을 찍는다.\n(step 2) 그 점에서 순간기울기를 구한다. (접평면) <– 편미분\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다.\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다.\n(서연필기) x,y 다르게 정의하는~편미분\nloss를 줄이도록 \\({\\bf W}\\)를 개선하는 방법\n- $수정값 원래값 - 기울어진크기(=미분계수) $\n\n여기에서 \\(\\alpha\\)는 전체적인 보폭의 크기를 결정한다. 즉 \\(\\alpha\\)값이 클수록 한번의 update에 움직이는 양이 크다.\n\n- \\({\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)\n(서연필기) 미분계수 반대로 움직이기 위해 마이너스(-) 취해주자\n(서연필기) 알파자체가 음수면 방향이 바뀌니까 양수!\n\n마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라.\n\\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1):\\) 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라.\n\\(\\alpha\\)의 의미: 전체적인 보폭의 속도를 조절, \\(\\alpha\\)가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다.\n\n\n- 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다.\n- 경사하강법으로 loss를 줄이기 위해서는 \\(\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)\\)의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (loss.backward()로 하면된다)\n\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n\nWhat.grad\n\n\nloss.backward()\n\n\nWhat.grad\n\ntensor([-1342.2523,  1188.9307])\n\n\n(서연필기) What.grad의 결과 값이 생겼다!\n\nloss\n\ntensor(8587.6875, grad_fn=<SumBackward0>)\n\n\n(서연필기) loss 계산할때 What에있는 꼬리표가 따라와서 loss에도 꼬리표가 붙었다.\n\nloss.backward()의 의미: loss를 미분해라! 뭘로? requires_grad=True를 가진 텐서로!!\n\n\nloss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2)\n# 이었고 \nWhat=torch.tensor([-5.0,10.0],requires_grad=True)\n# 이므로 결국 What으로 미분하라는 의미. \n# 미분한 식이 나오는 것이 아니고, \n# 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. \n- 위에서 loss.backward()의 과정은 미분을 활용하여 \\((-5,10)\\)에서의 순간기울기를 구했다는 의미임.\n\nWhat,What.grad\n\n(tensor([-5., 10.], requires_grad=True), tensor([-1342.2523,  1188.9307]))\n\n\n- (-5,10)에서 loss의 순간기울기 값은 What.grad로 확인가능하다.\n\n이것이 의미하는건 \\((-5,10)\\)에서의 \\(loss(w_0,w_1)\\)의 순간기울기가 \\((-1342.2523, 1188.9307)\\) 이라는 의미\n\n- (확인1) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 손계산으로 검증하여 보자.\n\n\\(loss(w_0,w_1)=({\\bf y}-\\hat{\\bf y})^\\top ({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf XW})^\\top ({\\bf y}-{\\bf XW})\\)\n\\(\\frac{\\partial}{\\partial {\\bf W} }loss(w_0,w_1)=-2{\\bf X}^\\top {\\bf y}+2{\\bf X}^\\top {\\bf X W}\\)\n\n\n- 2 * X.T @ y + 2 * X.T @ X @ What\n\ntensor([-1342.2522,  1188.9305], grad_fn=<AddBackward0>)\n\n\n- (확인2) loss.backward()가 미분을 잘 계산해 주는 것이 맞는가? 편미분을 간단히 구현하여 검증하여 보자.\n\n\\(\\frac{\\partial}{\\partial {\\bf W} } loss(w_0,w_1)=\\begin{bmatrix}\\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\end{bmatrix}loss(w_0,w_1) =\\begin{bmatrix}\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\end{bmatrix}\\)\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n\n스칼라일때\nh = 0.01\n(loss(w+h) - loss(w)) / h\n\n_lossfn = lambda w0,w1: torch.sum((y-w0-w1*x)**2)\n_lossfn(-5,10)\n\ntensor(8587.6875)\n\n\n\nh=0.001\n(_lossfn(-5+h,10) - _lossfn(-5,10))/h,  (_lossfn(-5,10+h) - _lossfn(-5,10))/h\n\n(tensor(-1341.7968), tensor(1190.4297))\n\n\n-5,10에서의 편미분한 순간기울기\n\n약간 오차가 있지만 얼추비슷 \\(\\to\\) 잘 계산했다는 소리임\n\n(서연필기) 꼭 정확하진 않지!\n- 수정전, 수정하는폭, 수정후의 값은 차례로 아래와 같다.\n\nalpha=0.001 \nprint('수정전: ' + str(What.data)) # What 에서 미분꼬리표를 떼고 싶다면? What.data or What.detach()\nprint('수정하는폭: ' +str(-alpha * What.grad))\nprint('수정후: ' +str(What.data-alpha * What.grad))\nprint('*참값: (2.5,4)' )\n\n수정전: tensor([-5., 10.])\n수정하는폭: tensor([ 1.3423, -1.1889])\n수정후: tensor([-3.6577,  8.8111])\n*참값: (2.5,4)\n\n\n- Wbefore, Wafter 계산\n\nWbefore = What.data\nWafter = What.data- alpha * What.grad\nWbefore, Wafter\n\n(tensor([-5., 10.]), tensor([-3.6577,  8.8111]))\n\n\ndata쓰는지 grad 쓰는지 명확히\n- Wbefore, Wafter의 시각화\n\nplt.plot(x,y,'o')\nplt.plot(x,X@Wbefore,'--')\nplt.plot(x,X@Wafter,'--')\n\n\n\n\n\n\n\nStage3: Learn (=estimate \\(\\bf\\hat{W})\\)\n- 이 과정은 Stage1,2를 반복하면 된다.\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True) #\n\n\nalpha=0.001 \nfor epoc in range(30): ## 30번 반복합니다!! \n    yhat=X@What \n    loss=torch.sum((y-yhat)**2)\n    loss.backward() \n    What.data = What.data-alpha * What.grad\n    What.grad=None\n\n(서연필기) What.grad=None 해주는 이유는 grad가 미분을 누적하기 때문에 막아주기 위해서\n\n원래 철자는 epoch이 맞아요\n\n- 반복결과는?! (최종적으로 구해지는 What의 값은?!) - 참고로 true\n\nWhat.data ## true인 (2.5,4)와 상당히 비슷함\n\ntensor([2.4290, 4.0144])\n\n\n- 반복결과를 시각화하면?\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.data,'--')"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#파라메터의-학습과정-음미-학습과정-모니터링",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "파라메터의 학습과정 음미 (학습과정 모니터링)",
    "text": "파라메터의 학습과정 음미 (학습과정 모니터링)\n\n학습과정의 기록\n- 기록을 해보자.\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.detach().tolist())\n    What.grad=None\n\n(서연필기) list 그대로 받으니까 꼬리표 삭제\n- \\(\\hat{y}\\) 관찰 (epoch=3, epoch=10, epoch=15)\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[2],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[9],'--')\n\n\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat_history[14],'--')\n\n\n\n\n\nlen(yhat_history[0])\n\n100\n\n\n- \\(\\hat{\\bf W}\\) 관찰\n\nWhat_history\n\n[[-3.657747745513916, 8.81106948852539],\n [-2.554811716079712, 7.861191749572754],\n [-1.649186372756958, 7.101552963256836],\n [-0.9060714244842529, 6.49347448348999],\n [-0.29667866230010986, 6.006272315979004],\n [0.2027742564678192, 5.615575313568115],\n [0.6119104623794556, 5.302003383636475],\n [0.9469034671783447, 5.050129413604736],\n [1.2210699319839478, 4.847657680511475],\n [1.4453645944595337, 4.684779167175293],\n [1.6287915706634521, 4.553659439086914],\n [1.778746247291565, 4.448036193847656],\n [1.90129816532135, 4.3628973960876465],\n [2.0014259815216064, 4.294229507446289],\n [2.0832109451293945, 4.238814353942871],\n [2.149996757507324, 4.194070339202881],\n [2.204521894454956, 4.157923698425293],\n [2.249027729034424, 4.128708839416504],\n [2.285348415374756, 4.105085849761963],\n [2.31498384475708, 4.0859761238098145],\n [2.339160442352295, 4.070511341094971],\n [2.3588807582855225, 4.057991027832031],\n [2.3749637603759766, 4.0478515625],\n [2.3880786895751953, 4.039637088775635],\n [2.3987717628479004, 4.032979965209961],\n [2.40748929977417, 4.027583599090576],\n [2.414595603942871, 4.023208141326904],\n [2.4203879833221436, 4.019659042358398],\n [2.4251089096069336, 4.016779899597168],\n [2.4289560317993164, 4.014443874359131]]\n\n\n- loss 관찰\n\nplt.plot(loss_history)\n\n\n\n\n\n\n학습과정을 animation으로 시각화\n\nfrom matplotlib import animation\n\n\nplt.rcParams['figure.figsize'] = (7.5,2.5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n- 왼쪽에는 \\((x_i,y_i)\\) and \\((x_i,\\hat{y}_i)\\) 을 그리고 오른쪽에는 \\(loss(w_0,w_1)\\) 을 그릴것임\n\nfig = plt.figure()\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n\n\n\n- 왼쪽그림!\n\nax1.plot(x,y,'o')\nline, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요해요..\n\n\nfig\n\n\n\n\n- 오른쪽 그림1: \\(loss(w_0,w_1)\\)\n\n_w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n_w1 = np.arange(-6, 11, 0.5)\nw1,w0 = np.meshgrid(_w1,_w0)\nlss=w0*0\nfor i in range(len(_w0)):\n    for j in range(len(_w1)):\n        lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\nax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \nax2.azim = 40  ## 3d plot의 view 조절 \nax2.dist = 8   ## 3d plot의 view 조절 \nax2.elev = 5   ## 3d plot의 view 조절 \n\n\nfig\n\n\n\n\n- 오른쪽 그림2: \\((w_0,w_1)=(2.5,4)\\) 와 \\(loss(2.5,4)\\) 값 <- loss 함수가 최소가 되는 값 (이거 진짜야? ㅋㅋ)\n\nax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f65aca1f040>\n\n\n\nfig\n\n\n\n\n- 오른쪽 그림3: \\((w_0,w_1)=(-3.66, 8.81)\\) 와 \\(loss(-3.66,8.81)\\) 값\n\nWhat_history[0]\n\n[-3.657747745513916, 8.81106948852539]\n\n\n\nax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='grey') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f65ac8728b0>\n\n\n\nfig\n\n\n\n\n- 애니메이션\n\ndef animate(epoc):\n    line.set_ydata(yhat_history[epoc])\n    ax2.scatter(What_history[epoc][0],What_history[epoc][1],loss_history[epoc],color='grey')\n    return line\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 함수로 만들자..\n\ndef show_lrpr(data,history):\n    x,y = data \n    loss_history,yhat_history,What_history = history \n    \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,y,'o')\n    line, = ax1.plot(x,yhat_history[0]) \n    ## ax2: 오른쪽그림 \n    _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n    _w1 = np.arange(-6, 11, 0.5)\n    w1,w0 = np.meshgrid(_w1,_w0)\n    lss=w0*0\n    for i in range(len(_w0)):\n        for j in range(len(_w1)):\n            lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\n    ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \n    ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n    ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='b') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n    ax2.azim = 40  ## 3d plot의 view 조절 \n    ax2.dist = 8   ## 3d plot의 view 조절 \n    ax2.elev = 5   ## 3d plot의 view 조절 \n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(서연필기) 알파의 정도에 따라 학습 속도가 달라져.."
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#alpha에-대하여-alpha는-학습률",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#alpha에-대하여-alpha는-학습률",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)",
    "text": "\\(\\alpha\\)에 대하여 (\\(\\alpha\\)는 학습률)\n\n(1) \\(\\alpha=0.0001\\): \\(\\alpha\\) 가 너무 작다면? \\(\\to\\) 비효율적이다.\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(2) \\(\\alpha=0.0083\\): \\(\\alpha\\)가 너무 크다면? \\(\\to\\) 다른의미에서 비효율적이다 + 위험하다..\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0083\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(3) \\(\\alpha=0.0085\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0085\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad.data; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(서연필기) 최솟값보다 오히려 커지는 경향이 나와버림\n\n\n(4) \\(\\alpha=0.01\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.01\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-09-21-ml_3주차_9월21일.html#숙제",
    "href": "posts/ml/2022-09-21-ml_3주차_9월21일.html#숙제",
    "title": "기계학습 특강 (3주차) 9월21일",
    "section": "숙제",
    "text": "숙제\n- 학습률(\\(\\alpha\\))를 조정하며 실습해보고 스크린샷 제출\n\n(1) \\(\\alpha=0.0015\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.015\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n(2) \\(\\alpha=0.0038\\)\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0038\nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html",
    "title": "기계학습 특강 (8주차) 10월26일–(1)",
    "section": "",
    "text": "(8주차) 10월26일–(1) [이미지자료분석 - CNN 다중클래스 분류, fastai metric 사용]"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#imports",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#imports",
    "title": "기계학습 특강 (8주차) 10월26일–(1)",
    "section": "imports",
    "text": "imports\n\nimport torch \nimport torchvision\nimport numpy as np\nfrom fastai.vision.all import * \n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');\n\n\n#hide\ngraphviz.set_jupyter_format('png')\n\n'svg'"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#cnn-다중클래스-분류",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#cnn-다중클래스-분류",
    "title": "기계학습 특강 (8주차) 10월26일–(1)",
    "section": "CNN 다중클래스 분류",
    "text": "CNN 다중클래스 분류\n\n결론 (그냥 외우세요)\n- 2개의 class를 구분하는 문제가 아니라 \\(k\\)개의 class를 구분해야 한다면?\n일반적인 개념\n\n손실함수: BCE loss \\(\\to\\) Cross Entropy loss\n마지막층의 선형변환: torch.nn.Linear(?,1) \\(\\to\\) torch.nn.Linear(?,k)\n마지막층의 활성화: sig \\(\\to\\) softmax\n\n파이토치 한정 - y의형태: (n,) vector + int형 // (n,k) one-hot encoded vector + float형 - 손실함수: torch.nn.BCEWithLogitsLoss, \\(\\to\\) torch.nn.CrossEntropyLoss - 마지막층의 선형변환: torch.nn.Linear(?,1) \\(\\to\\) torch.nn.Linear(?,k) - 마지막층의 활성화: None \\(\\to\\) None (손실함수에 이미 마지막층의 활성화가 포함)\n\n\n실습: 3개의 클래스를 구분\n\npath = untar_data(URLs.MNIST)\n\ntraining set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n다중일때 int가 아닌float으로서 y를 정의해준 모습\ntest set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n\ndls\n\n\nlen(X)\n\n18623\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr\n\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\n\nnet1(X).shape\n\ntorch.Size([18623, 2304])\n\n\n\nnet = torch.nn.Sequential(\n    net1,\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\nloss_fn = torch.nn.CrossEntropyLoss() \n\n\nlrnr = Learner(dls,net,loss_fn) \n\nadam기본인 learner\n\n학습\n\n지금은 epoch당 11번 도는 설정, 18623/1862 = 11.xx\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.532752\n      1.059955\n      00:00\n    \n    \n      1\n      1.190896\n      0.830852\n      00:00\n    \n    \n      2\n      1.008513\n      0.646931\n      00:00\n    \n    \n      3\n      0.865353\n      0.427843\n      00:00\n    \n    \n      4\n      0.728408\n      0.264087\n      00:00\n    \n    \n      5\n      0.602026\n      0.179980\n      00:00\n    \n    \n      6\n      0.497519\n      0.137681\n      00:00\n    \n    \n      7\n      0.415113\n      0.112264\n      00:00\n    \n    \n      8\n      0.349265\n      0.096033\n      00:00\n    \n    \n      9\n      0.296159\n      0.084770\n      00:00\n    \n  \n\n\n\n\n예측\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n  (1): Linear(in_features=2304, out_features=3, bias=True)\n)\n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      2.838031\n      -14.031689\n      -1.230620\n      0\n    \n    \n      1\n      -0.732540\n      -6.829875\n      -0.657546\n      0\n    \n    \n      2\n      2.525343\n      -7.813309\n      -2.658828\n      0\n    \n    \n      3\n      1.173236\n      -5.229916\n      -2.532024\n      0\n    \n    \n      4\n      0.102843\n      -3.444337\n      -1.044323\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3142\n      -2.697058\n      -3.533814\n      -0.154926\n      2\n    \n    \n      3143\n      -5.334007\n      -6.445426\n      2.196163\n      2\n    \n    \n      3144\n      -3.041989\n      -5.655945\n      1.335649\n      2\n    \n    \n      3145\n      -4.720510\n      -5.899189\n      1.208340\n      2\n    \n    \n      3146\n      -2.413806\n      -3.101650\n      0.852677\n      2\n    \n  \n\n3147 rows × 4 columns\n\n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      2.838031\n      -14.031689\n      -1.230620\n      0\n    \n    \n      1\n      -0.732540\n      -6.829875\n      -0.657546\n      0\n    \n    \n      2\n      2.525343\n      -7.813309\n      -2.658828\n      0\n    \n    \n      3\n      1.173236\n      -5.229916\n      -2.532024\n      0\n    \n    \n      4\n      0.102843\n      -3.444337\n      -1.044323\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      975\n      1.330218\n      -6.934738\n      -0.893682\n      0\n    \n    \n      976\n      3.073657\n      -11.082842\n      -3.012246\n      0\n    \n    \n      977\n      3.607128\n      -7.156256\n      -5.264734\n      0\n    \n    \n      978\n      1.993969\n      -7.487792\n      -2.306112\n      0\n    \n    \n      979\n      1.534865\n      -7.852367\n      -1.404178\n      0\n    \n  \n\n980 rows × 4 columns\n\n\n\n\n대체적으로 첫번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==1')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      980\n      -4.239265\n      2.068619\n      -1.274470\n      1\n    \n    \n      981\n      -4.559580\n      2.755761\n      -1.822832\n      1\n    \n    \n      982\n      -4.617976\n      1.838857\n      -0.515022\n      1\n    \n    \n      983\n      -4.119075\n      2.247138\n      -0.991911\n      1\n    \n    \n      984\n      -3.344346\n      1.100410\n      -1.496944\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2110\n      -4.141958\n      2.405002\n      -1.260467\n      1\n    \n    \n      2111\n      -4.405143\n      2.479209\n      -1.356262\n      1\n    \n    \n      2112\n      -3.695343\n      1.773260\n      -1.218412\n      1\n    \n    \n      2113\n      -3.986775\n      2.423826\n      -1.349702\n      1\n    \n    \n      2114\n      -4.925949\n      2.532830\n      -1.160674\n      1\n    \n  \n\n1135 rows × 4 columns\n\n\n\n\n대체적으로 두번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==2')\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      2115\n      -4.723238\n      -3.105680\n      1.052694\n      2\n    \n    \n      2116\n      -2.576618\n      -7.337523\n      2.118495\n      2\n    \n    \n      2117\n      -3.796456\n      -6.393374\n      2.169248\n      2\n    \n    \n      2118\n      -3.276625\n      -2.622900\n      0.176427\n      2\n    \n    \n      2119\n      -4.627345\n      -5.335648\n      1.157538\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3142\n      -2.697058\n      -3.533814\n      -0.154926\n      2\n    \n    \n      3143\n      -5.334007\n      -6.445426\n      2.196163\n      2\n    \n    \n      3144\n      -3.041989\n      -5.655945\n      1.335649\n      2\n    \n    \n      3145\n      -4.720510\n      -5.899189\n      1.208340\n      2\n    \n    \n      3146\n      -2.413806\n      -3.101650\n      0.852677\n      2\n    \n  \n\n1032 rows × 4 columns\n\n\n\n\n대체적으로 세번째 칼럼의 숫자들이 다른칼럼보다 크다.\n\n- 예측하는방법? - 칼럼0의 숫자가 크다 -> y=0일 확률이 큼 - 칼럼1의 숫자가 크다 -> y=1일 확률이 큼 - 칼럼2의 숫자가 크다 -> y=2일 확률이 큼\n\n\n공부: Softmax\n- 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음.\n- torch.nn.Softmax() 손계산\n(예시1) – 잘못계산\n\ntorch.nn.Softmax?\n\n\nInit signature: torch.nn.Softmax(dim: Union[int, NoneType] = None) -> None\nDocstring:     \nApplies the Softmax function to an n-dimensional input Tensor\nrescaling them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\nSoftmax is defined as:\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\nWhen the input Tensor is a sparse tensor then the unspecifed\nvalues are treated as ``-inf``.\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\nArgs:\n    dim (int): A dimension along which Softmax will be computed (so every slice\n        along dim will sum to 1).\n.. note::\n    This module doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use `LogSoftmax` instead (it's faster and has better numerical properties).\nExamples::\n    >>> m = nn.Softmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\nFile:           ~/anaconda3/envs/csy/lib/python3.8/site-packages/torch/nn/modules/activation.py\nType:           type\nSubclasses:     \n\n\n\n\n\nsftmax = torch.nn.Softmax(dim=0) # columns\n\n\n_netout = torch.tensor([[-2.0,-2.0,0.0],\n                        [3.14,3.14,3.14],\n                        [0.0,0.0,2.0],\n                        [2.0,2.0,4.0],\n                        [0.0,0.0,0.0]])\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout) \n\ntensor([[0.0041, 0.0041, 0.0115],\n        [0.7081, 0.7081, 0.2653],\n        [0.0306, 0.0306, 0.0848],\n        [0.2265, 0.2265, 0.6269],\n        [0.0306, 0.0306, 0.0115]])\n\n\n(예시2) – 이게 맞게 계산되는 것임\n\nsftmax = torch.nn.Softmax(dim=1) # rows\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout)\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시3) – 차원을 명시안하면 맞게 계산해주고 경고 줌\n\nsftmax = torch.nn.Softmax()\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\nsftmax(_netout)\n\n/tmp/ipykernel_2380807/3715462293.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  sftmax(_netout)\n\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시4) – 진짜 손계산\n\n_netout \n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\ntorch.exp(_netout)\n\ntensor([[ 0.1353,  0.1353,  1.0000],\n        [23.1039, 23.1039, 23.1039],\n        [ 1.0000,  1.0000,  7.3891],\n        [ 7.3891,  7.3891, 54.5981],\n        [ 1.0000,  1.0000,  1.0000]])\n\n\n\n0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs\n\n(0.10648512513773022, 0.10648512513773022, 0.7870297497245397)\n\n\n\nnp.exp(_netout[1])/np.exp(_netout[1]).sum() # 두번째 obs \n\ntensor([0.3333, 0.3333, 0.3333])\n\n\n\nnp.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(),1,_netout)\n\narray([[0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334],\n       [0.10650699, 0.10650699, 0.78698605],\n       [0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334]], dtype=float32)\n\n\n위에서 1은 축방향을 의미\n\n\n공부: CrossEntropyLoss\n\n# torch.nn.CrossEntropyLoss() 손계산: one-hot version\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y_onehot = torch.tensor([[0,0,1],\n                          [0,1,0],\n                          [0,0,1],\n                          [0,0,1],\n                          [1,0,0]])*1.0\n_y_onehot\n\ntensor([[0., 0., 1.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 1.],\n        [1., 0., 0.]])\n\n\n위에서 꼭 1.0 곱해줌으로써 int가 아닌 float으로 만들어주기\n\nsftmax = torch.nn.Softmax(dim=1) \nsftmax(_netout), _y_onehot\n\n(tensor([[0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333],\n         [0.1065, 0.1065, 0.7870],\n         [0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333]]),\n tensor([[0., 0., 1.],\n         [0., 1., 0.],\n         [0., 0., 1.],\n         [0., 0., 1.],\n         [1., 0., 0.]]))\n\n\n- 계산결과\n\nloss_fn(_netout,_y_onehot)\n\ntensor(0.5832)\n\n\n\n- torch.sum(torch.log(sftmax(_netout)) * _y_onehot)/5 \n\ntensor(0.5832)\n\n\n- 계산하는 방법도 중요한데 torch.nn.CrossEntropyLoss() 에는 softmax 활성화함수가 이미 포함되어 있다는 것을 확인하는 것이 더 중요함.\n- 따라서 torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다.\n\n\n# torch.nn.CrossEntropyLoss() 손계산: lenght \\(n\\) vertor version\n\n_netout \n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y = torch.tensor([2,1,2,2,0])\n\n원핫인코딩 안하면 int로 만든 다음에 넣기, float은 또 계산되지 않음!\n\nloss_fn(_netout,_y)\n\ntensor(0.5832)\n\n\n\n\n\n실습: \\(k=2\\)로 두면 이진분류도 가능\n- download data\n\npath = untar_data(URLs.MNIST) \n\ntraining\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\ny_onehot = torch.nn.functional.one_hot(y).float()\n#y_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],y))).float()\n\nfloat만들어주기 원핫인코딩이기\ntest\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\nyy_onehot = torch.nn.functional.one_hot(yy).float()\n#yy_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],yy))).float()\n\n\ndls\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot) \nds2 = torch.utils.data.TensorDataset(XX,yy_onehot) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n\n학습\n\n\nlrnr.fit(10) \n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.233556\n      0.787265\n      00:00\n    \n    \n      1\n      0.829398\n      0.433228\n      00:00\n    \n    \n      2\n      0.650216\n      0.319202\n      00:00\n    \n    \n      3\n      0.540207\n      0.183107\n      00:00\n    \n    \n      4\n      0.444210\n      0.113277\n      00:00\n    \n    \n      5\n      0.365939\n      0.074700\n      00:00\n    \n    \n      6\n      0.303410\n      0.049914\n      00:00\n    \n    \n      7\n      0.253710\n      0.035714\n      00:00\n    \n    \n      8\n      0.214157\n      0.027470\n      00:00\n    \n    \n      9\n      0.182333\n      0.022121\n      00:00\n    \n  \n\n\n\n\n예측 및 시각화\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=2, bias=True)\n)\n\n\n\nsftmax = torch.nn.Softmax(dim=1) \nsig = torch.nn.Sigmoid()\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).diff(axis=1).data,',',color=\"C1\") # u2-u1\nax[1].plot(y)\nax[1].plot(sftmax(net(X))[:,1].data,',')\n#ax[1].plot(sig(net(X).diff(axis=1)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).diff(axis=1).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(sftmax(net(XX))[:,1].data,',')\n#ax[1].plot(sig(net(XX).diff(axis=1)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')\n\n\n\n\n\n- note: softmax(u1,u2)=[sig(u1-u2), sig(u2-u1)]=[1-sig(u2-u1),sig(u2-u1)]\n\\(\\frac{1}{e^{u_1}+e^{u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+e^{u_2-u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+1} \\to sig(u_2-u_1)\\)\n\n\n공부: 이진분류에서 소프트맥스 vs 시그모이드\n- 이진분류문제 = “y=0 or y=1” 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제\n- softmax, sigmoid - softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. - sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다.\n- 그런데 “실패확률=1-성공확률” 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까)\n- 아래는 사실상 같은 모형이다.\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y1\"\n    \"node2\" -> \"y1\"\n    \"...\" -> \"y1\"\n    \"node2304\" -> \"y1\"\n    \n    \"node1\" -> \"y2\"\n    \"node2\" -> \"y2\"\n    \"...\" -> \"y2\"\n    \"node2304\" -> \"y2\"    \n    label = \"Layer: Softmax\"\n}\n''')\n\n\n\n\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node2304\" -> \"y\"\n    label = \"Layer: Sigmoid\"\n}\n''')\n\n\n\n\n- 둘은 사실상 같은 효과를 주는 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. \\(\\to\\) sigmoid를 사용하는 모형이 비용은 싸고(학습할 파라메터가 적음) 효과는 동일하다는 말 \\(\\to\\) 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. - softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음\n\n\n소프트맥스 vs 시그모이드 정리\n- 결론 1. 소프트맥스는 시그모이드의 확장이다. 2. 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다.\n- 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (흑백이미지를 칼라잉크로 출력하는 느낌)\n참고\n\n\n\n\\(y\\)\n분포가정\n마지막층의 활성화함수\n손실함수\n\n\n\n\n3.45, 4.43, … (연속형)\n정규분포\nNone (or Identity)\nMSE\n\n\n0 or 1\n이항분포 with \\(n=1\\) (=베르누이)\nSigmoid\nBCE\n\n\n[0,0,1], [0,1,0], [1,0,0]\n다항분포 with \\(n=1\\)\nSoftmax\nCross Entropy"
  },
  {
    "objectID": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#fastai-metric-사용",
    "href": "posts/ml/2022-10-26-ml_(8주차)_10월26일(1).html#fastai-metric-사용",
    "title": "기계학습 특강 (8주차) 10월26일–(1)",
    "section": "fastai metric 사용",
    "text": "fastai metric 사용\n\n데이터준비\n- download data\n\npath = untar_data(URLs.MNIST)\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))\n\n\n\n\n사용자정의 메트릭이용\n\ndls 만들기\n\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \n\n\ndef acc(yhat,y) : \n    return ((yhat>0.5)==y).float().mean()\n\n\ndef err(yhat,y):\n    return 1-((yhat>0.5)==y).float().mean()\n\n\nlrnr = Learner(dls,net,loss_fn,metrics=[acc,err])\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      acc\n      err\n      time\n    \n  \n  \n    \n      0\n      1.012566\n      0.676096\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.738655\n      0.477148\n      0.994799\n      0.005201\n      00:00\n    \n    \n      2\n      0.603908\n      0.335415\n      0.985816\n      0.014184\n      00:00\n    \n    \n      3\n      0.497049\n      0.183633\n      0.995745\n      0.004255\n      00:00\n    \n    \n      4\n      0.394664\n      0.097668\n      0.995745\n      0.004255\n      00:00\n    \n    \n      5\n      0.309929\n      0.056333\n      0.995745\n      0.004255\n      00:00\n    \n    \n      6\n      0.244836\n      0.037147\n      0.995745\n      0.004255\n      00:00\n    \n    \n      7\n      0.195441\n      0.027278\n      0.995745\n      0.004255\n      00:00\n    \n    \n      8\n      0.157570\n      0.021531\n      0.995745\n      0.004255\n      00:00\n    \n    \n      9\n      0.128163\n      0.017795\n      0.997163\n      0.002837\n      00:00\n    \n  \n\n\n\n\n예측\n\n\n생략\n\n\n\nfastai지원 메트릭이용– 잘못된사용\n\ndls 만들기\n\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\naccuracy??\n\n\nSignature: accuracy(inp, targ, axis=-1)\nSource:   \ndef accuracy(inp, targ, axis=-1):\n    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n    pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n    return (pred == targ).float().mean()\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\n\nerror_rate??\n\n\nSignature: error_rate(inp, targ, axis=-1)\nSource:   \ndef error_rate(inp, targ, axis=-1):\n    \"1 - `accuracy`\"\n    return 1 - accuracy(inp, targ, axis=axis)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/metrics.py\nType:      function\n\n\n\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.971997\n      0.616424\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.671642\n      0.380434\n      0.463357\n      0.536643\n      00:00\n    \n    \n      2\n      0.525948\n      0.232161\n      0.463357\n      0.536643\n      00:00\n    \n    \n      3\n      0.414203\n      0.123899\n      0.463357\n      0.536643\n      00:00\n    \n    \n      4\n      0.322394\n      0.071857\n      0.463357\n      0.536643\n      00:00\n    \n    \n      5\n      0.252299\n      0.045784\n      0.463357\n      0.536643\n      00:00\n    \n    \n      6\n      0.199783\n      0.032276\n      0.463357\n      0.536643\n      00:00\n    \n    \n      7\n      0.160118\n      0.024500\n      0.463357\n      0.536643\n      00:00\n    \n    \n      8\n      0.129659\n      0.019576\n      0.463357\n      0.536643\n      00:00\n    \n    \n      9\n      0.105914\n      0.016207\n      0.463357\n      0.536643\n      00:00\n    \n  \n\n\n\n\n이상하다..?\n\n\n예측\n\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nplt.plot(yy)\nplt.plot(lrnr.model(XX).data,'.')\n\n\n\n\n\n맞추는건 잘 맞추는데?\n\n\n\nfastai지원 메트릭이용– 올바른 사용(1)\n- 가정 - X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. - y의 형태는 (n,) 벡터이다. 즉 \\(n\\times 1\\) 이 아니라 그냥 길이가 \\(n\\)인 벡터로 가정한다. - y의 각 원소는 0,1,2,3,… 와 같이 카테고리를 의미하는 숫자이어야 하며 이 숫자는 int형으로 저장되어야 한다. - loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\ndls 만들기\n\n지원하는 함수로 바꿔주기\n\ny.to(torch.int64).reshape(-1),yy.to(torch.int64).reshape(-1)\n\n(tensor([0, 0, 0,  ..., 1, 1, 1]), tensor([0, 0, 0,  ..., 1, 1, 1]))\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.038122\n      0.539247\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.621439\n      0.261176\n      0.977778\n      0.022222\n      00:00\n    \n    \n      2\n      0.451623\n      0.118811\n      0.989125\n      0.010875\n      00:00\n    \n    \n      3\n      0.333172\n      0.059299\n      0.995272\n      0.004728\n      00:00\n    \n    \n      4\n      0.250918\n      0.037678\n      0.996217\n      0.003783\n      00:00\n    \n    \n      5\n      0.193416\n      0.026810\n      0.996217\n      0.003783\n      00:00\n    \n    \n      6\n      0.152078\n      0.020631\n      0.996217\n      0.003783\n      00:00\n    \n    \n      7\n      0.121511\n      0.016605\n      0.996690\n      0.003310\n      00:00\n    \n    \n      8\n      0.098301\n      0.013718\n      0.997636\n      0.002364\n      00:00\n    \n    \n      9\n      0.080287\n      0.011546\n      0.998109\n      0.001891\n      00:00\n    \n  \n\n\n\n\n\nfastai지원 메트릭이용– 올바른 사용(2)\n- 가정 - X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. - y의 형태는 (n,클래스의수)로 가정한다. 즉 y가 one_hot 인코딩된 형태로 가정한다. - y의 각 원소는 0 혹은 1이다. - loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\ndls 만들기\n\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n# y_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\n# yy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n\nlrnr 생성\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\naccuracy_multi\n\n학습\n\n\nlrnr.fit(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      1.038750\n      0.569555\n      0.463357\n      00:00\n    \n    \n      1\n      0.640057\n      0.285553\n      0.977778\n      00:00\n    \n    \n      2\n      0.469265\n      0.137582\n      0.987943\n      00:00\n    \n    \n      3\n      0.348698\n      0.064898\n      0.995035\n      00:00\n    \n    \n      4\n      0.262547\n      0.038338\n      0.996217\n      00:00\n    \n    \n      5\n      0.201805\n      0.025988\n      0.996690\n      00:00\n    \n    \n      6\n      0.158089\n      0.019443\n      0.996927\n      00:00\n    \n    \n      7\n      0.125811\n      0.015470\n      0.997163\n      00:00\n    \n    \n      8\n      0.101381\n      0.012772\n      0.998109\n      00:00\n    \n    \n      9\n      0.082515\n      0.010802\n      0.998582\n      00:00"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "",
    "text": "(6주차) 10월5일 [딥러닝의 기초 - 깊은신경망(2)– 시벤코정리, 신경망의표현, CPU vs GPU, 확률적경사하강법, 오버피팅]"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#imports",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#imports",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "imports",
    "text": "imports\n\nimport torch\nimport torchvision\nfrom fastai.data.all import *\nimport matplotlib.pyplot as plt\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');\n\n\n#hide\ngraphviz.set_jupyter_format('png')\n\n'png'"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "시벤코정리",
    "text": "시벤코정리\n\n지난시간 논리전개\n- 아이디어: linear -> relu -> linear (-> sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다. - 아이디어의 실용성: 실제자료에서 꺾은선으로 표현되는 underlying은 몇개 없을 것 같음. 그건 맞는데 꺾이는 점을 많이 설정하면 얼추 비슷하게는 “근사” 시킬 수 있음. - 아이디어의 확장성: 이러한 논리전개는 X:(n,2)인 경우도 가능했음. (이 경우 꺾인선은 꺾인평면이 된다) - 아이디어에 해당하는 용어정리: 이 구조가 x->y 로 바로 가는 것이 아니라 x->(u1->v1)->(u2->v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. (이 용어는 이따가..)\n\n\n시벤코정리\nuniversal approximation thm: (범용근사정리,보편근사정리,시벤코정리), 1989\n\n하나의 은닉층을 가지는 “linear -> sigmoid -> linear” 꼴의 네트워크를 이용하여 세상에 존재하는 모든 (다차원) 연속함수를 원하는 정확도로 근사시킬 수 있다. (계수를 잘 추정한다면)\n\n- 사실 엄청 이해안되는 정리임. 왜냐햐면, - 그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아니야? - 요즘은 “linear -> sigmoid -> linear” 가 아니라 “linear -> relu -> linear” 조합으로 많이 쓰던데? - 요즘은 하나의 은닉층을 포함하는 네트워크는 잘 안쓰지 않나? 은닉층이 여러개일수록 좋다고 어디서 본 것 같은데?\n- 약간의 의구심이 있지만 아무튼 universal approximation thm에 따르면 우리는 아래와 같은 무기를 가진 꼴이 된다. - 우리의 무기: \\({\\bf X}: (n,p)\\) 꼴의 입력에서 \\({\\bf y}:(n,1)\\) 꼴의 출력으로 향하는 맵핑을 “linear -> relu -> linear”와 같은 네트워크를 이용해서 “근사”시킬 수 있다.\n(서연 필기) 한 층만 있어도 노드가 충분히 크면 은닉층 한 층으로 충분히 맞출 수 있다."
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리-proof",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리-proof",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "시벤코정리 proof",
    "text": "시벤코정리 proof\n\n그림으로 보는 증명과정\n- 데이터\n\nx = torch.linspace(-10,10,200).reshape(-1,1)\n\n- 아래와 같은 네트워크를 고려하자.\n\nl1 = torch.nn.Linear(in_features=1,out_features=2)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=2,out_features=1)\n\n- 직관1: \\(l_1\\),\\(l_2\\)의 가중치를 잘 결합하다보면 우연히 아래와 같이 만들 수 있다.\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+10.00,+10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,color='C2'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$')\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ \\\\l_1)(x)$')\n\n\n\n\n\n- 직관2: 아래들도 가능할듯?\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data,'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\n\n\n\n\n(서연 필기) 밑에 fig 다시 정의 안 해줬잖아. 그러니까 덮어쓴 거라 생각하면 돼\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+0.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nax[0].plot(x,l1(x).data,'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nfig\n\n\n\n\n- 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 \\((l_2\\circ a_1 \\circ l_1)(x)\\)의 결과로 주황색선 + 파란색선도 가능할 것 같다. \\(\\to\\) 실제로 가능함\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\n\nplt.plot(l2(a1(l1(x))).data)\n\n\n\n\n- 2개의 시그모이드를 우연히 잘 결합하면 아래와 같은 함수 \\(h\\)를 만들 수 있다.\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n- 위와 같은 함수 \\(h\\)를 활성화함수로 하고 \\(m\\)개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- \\(h(x)\\)를 활성화함수로 가지는 네트워크를 설계하여 보자.\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) # activation 의 출력 \n\nforward 순전파\nbackward 역전파\n\na1=MyActivation()\n# a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation()\n\n\nplt.plot(x,a1(x)) \n\n\n\n\n히든레이어가 1개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1),\n            MyActivation(),\n            torch.nn.Linear(1,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 2개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2),\n            MyActivation(),\n            torch.nn.Linear(2,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 3개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n히든레이어가 1024개의 노드를 가지는 경우\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n\ntorch.manual_seed(43052)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2048),\n            MyActivation(),\n            torch.nn.Linear(2048,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리-활용",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#시벤코정리-활용",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "시벤코정리 활용",
    "text": "시벤코정리 활용\n- 아래와 같이 하나의 은닉층을 가지고 있더라도 많은 노드수만 보장되면 매우 충분한 표현력을 가짐\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n예제1 (sin, exp)\n\ntorch.manual_seed(43052)\nx = torch.linspace(-10,10,200).reshape(-1,1)\nunderlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x)\neps = torch.randn(200).reshape(-1,1)*0.1\ny = underlying + eps \nplt.plot(x,y,'o',alpha=0.5)\nplt.plot(x,underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\nmseloss쓴 거 확인\n\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(x,underlying,lw=3)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제2 (스펙높아도 취업X)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/master/_notebooks/2022-10-04-dnnex0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n\n\n\nx = torch.tensor(df.x).reshape(-1,1).float()\ny = torch.tensor(df.y).reshape(-1,1).float()\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(df.x,df.underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\ntorch.manual_seed(43052)\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\nBCEloss쓴 거 확인\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(df.x,df.underlying,lw=3)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\n예제3 (MNIST data with DNN)\n\n# 예비학습\n(예비학습1) Path\n\npath = untar_data(URLs.MNIST) \npath\n\nPath('/home/csy/.fastai/data/mnist_png')\n\n\n\npath 도 오브젝트임\npath 도 정보+기능이 있음\n\n- path의 정보\n\npath._str # 숨겨놓았네?\n\n'/home/csy/.fastai/data/mnist_png'\n\n\n- 기능1\npath는 객체,\n\npath.ls()\n\n(#2) [Path('/home/csy/.fastai/data/mnist_png/training'),Path('/home/csy/.fastai/data/mnist_png/testing')]\n\n\npath object의 list 보여주는 역할\n- 기능2\n\npath/'training'\n\nPath('/home/csy/.fastai/data/mnist_png/training')\n\n\n\npath/'testing'\n\nPath('/home/csy/.fastai/data/mnist_png/testing')\n\n\n- 기능1과 기능2의 결합\n\n(path/'training/3').ls()\n\n(#6131) [Path('/home/csy/.fastai/data/mnist_png/training/3/35407.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/26671.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/16171.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/15346.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/34710.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/48873.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/28796.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/15651.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/6894.png'),Path('/home/csy/.fastai/data/mnist_png/training/3/37927.png')...]\n\n\n\n! ls /home/csy/.fastai/data/mnist_png\n\ntesting  training\n\n\n\n! ls /home/csy/.fastai/data/mnist_png/training\n\n0  1  2  3  4  5  6  7  8  9\n\n\n\n! ls /home/csy/.fastai/data/mnist_png/testing\n\n0  1  2  3  4  5  6  7  8  9\n\n\n\n‘/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’ 이 파일을 더블클릭하면 이미지가 보인단 말임\n\n(예비학습2) plt.imshow\n\nimgtsr = torch.tensor([[1.0,2],[2.0,4.0]])\nimgtsr\n\ntensor([[1., 2.],\n        [2., 4.]])\n\n\n\nplt.imshow(imgtsr,cmap='gray')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fa03ee40f10>\n\n\n\n\n\n\nimgtsr = torch.tensor([0.1,0.2,0.3,0.4]).reshape(2,2)\nimgtsr\n\ntensor([[0.1000, 0.2000],\n        [0.3000, 0.4000]])\n\n\n\nplt.imshow(imgtsr,cmap='gray')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fa03ebb70a0>\n\n\n\n\n\n(예비학습3) torchvision\n- ’/home/cgb4/.fastai/data/mnist_png/training/3/37912.png’의 이미지파일을 torchvision.io.read_image 를 이용하여 텐서로 만듬\n!ls '/home/csy/.fastai/data/mnist_png/training/3'\n\nimgtsr = torchvision.io.read_image('/home/csy/.fastai/data/mnist_png/training/3/37912.png')\nimgtsr\n\ntensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66, 138,\n          149, 180, 138, 138,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  22, 162, 161, 228, 252, 252,\n          253, 252, 252, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 253, 252, 252, 252, 189,\n          184, 110, 119, 252, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,  74, 161, 160,  77,  45,   4,\n            0,   0,  70, 252, 210,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,  22, 205, 252,  32,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0, 162, 253, 245,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           36, 219, 252, 139,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          222, 252, 202,  13,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,\n          253, 252,  89,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 240,\n          253, 157,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 160, 253,\n          231,  42,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 142, 252, 252,\n           42,  30,  78, 161,  36,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          185, 228, 252, 252, 168,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 184, 252, 252,\n          253, 252, 252, 252, 116,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 179, 252,\n          253, 252, 252, 210,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22,\n          255, 253, 215,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  34,  89, 244,\n          253, 223,  98,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 116, 123, 142, 234, 252, 252,\n          184,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 230, 253, 252, 252, 252, 168,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0, 126, 253, 252, 168,  43,   2,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n       dtype=torch.uint8)\n\n\n- 이 텐서는 (1,28,28)의 shape을 가짐\n\nimgtsr.shape\n\ntorch.Size([1, 28, 28])\n\n\n- imgtsr를 plt.imshow 로 시각화\n\nplt.imshow(imgtsr.reshape(28,28),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fa03e223370>\n\n\n\n\n\n\n진짜 숫자3이 있음\n\n\n\n# 데이터정리\n- 데이터정리\n\nthrees_fnames = (path/'training/3').ls()\nsevens_fnames = (path/'training/7').ls()\nlen(threes_fnames),len(sevens_fnames)\n\n(6131, 6265)\n\n\n\n6131, 1, 28, 28\n6265, 1, 28, 28\n\n\ntorch.stack([torchvision.io.read_image(str(threes_fnames[i])) for i in [0,1]]).shape\n\ntorch.Size([2, 1, 28, 28])\n\n\n\ntorch.stack([torchvision.io.read_image(str(fn)) for fn in threes_fnames]).shape\n\ntorch.Size([6131, 1, 28, 28])\n\n\n\ntorch.stack([torchvision.io.read_image(str(fn)) for fn in sevens_fnames]).shape\n\ntorch.Size([6265, 1, 28, 28])\n\n\n\nX3 = torch.stack([torchvision.io.read_image(str(threes_fnames[i])) for i in range(6131)])\nX7 = torch.stack([torchvision.io.read_image(str(sevens_fnames[i])) for i in range(6265)])\n\n\nX3.shape,X7.shape\n\n(torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28]))\n\n\n\nlen(threes_fnames) + len(sevens_fnames)\n\n12396\n\n\n\nX=torch.concat([X3,X7])\nX.shape\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nXnp = X.reshape(-1,1*28*28).float() # Xnp = X.reshape(-1,784).float()\nXnp.shape\n\ntorch.Size([12396, 784])\n\n\n\\(\\star\\) float형으로 바꿔주기\n\ny = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1) \ny.shape\n\ntorch.Size([12396, 1])\n\n\n\ny = torch.tensor([0.0]*len(threes_fnames) + [1.0]*len(sevens_fnames)).reshape(-1,1) \ny.shape\n\ntorch.Size([12396, 1])\n\n\n\nplt.plot(y,'o')\n\n\n\n\n\n“y=0”은 숫자3을 의미, “y=1”은 숫자7을 의미\n숫자3은 6131개, 숫자7은 6265개 있음\n\n\n\n# 학습\n- 네트워크의 설계\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1*28*28,out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\n\\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\n\n\n\n\n\nfor epoc in range(200):\n    ## 1\n    yhat = net(Xnp) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\n\n\n\n\n\n대부분 잘 적합되었음"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#신경망의-표현-boldsymbol-x-to-hatboldsymbol-y-로-가는-과정을-그림으로-표현",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#신경망의-표현-boldsymbol-x-to-hatboldsymbol-y-로-가는-과정을-그림으로-표현",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "신경망의 표현 (\\({\\boldsymbol x} \\to \\hat{\\boldsymbol y}\\) 로 가는 과정을 그림으로 표현)",
    "text": "신경망의 표현 (\\({\\boldsymbol x} \\to \\hat{\\boldsymbol y}\\) 로 가는 과정을 그림으로 표현)\n\n예제1: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(1)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n- 모든 observation과 가중치를 명시한 버전\n(표현1)\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xₙ\" -> \"ŵ₀ + xₙ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xₙ*ŵ₁,    bias=False\" -> \"ŷₙ\"[label=\"sigmoid\"]\n\n    \".\" -> \"....................................\"[label=\"* ŵ₀\"]\n    \"..\" -> \"....................................\"[label=\"* ŵ₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₂\" -> \"ŵ₀ + x₂*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₂*ŵ₁,    bias=False\" -> \"ŷ₂\"[label=\"sigmoid\"]\n    \n    \"1  \" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x₁\" -> \"ŵ₀ + x₁*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x₁*ŵ₁,    bias=False\" -> \"ŷ₁\"[label=\"sigmoid\"]\n''')\n\n\n\n\n\n단점: 똑같은 그림의 반복이 너무 많음\n\n- observation 반복을 생략한 버전들\n(표현2) 모든 \\(i\\)에 대하여 아래의 그림을 반복한다고 하면 (표현1)과 같다.\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"xᵢ\" -> \"ŵ₀ + xᵢ*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + xᵢ*ŵ₁,    bias=False\" -> \"ŷᵢ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n(표현3) 그런데 (표현2)에서 아래와 같이 \\(x_i\\), \\(y_i\\) 대신에 간단히 \\(x\\), \\(y\\)로 쓰는 경우도 많음\n\n#collapse\ngv(''' \n    \"1\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₀\"]\n    \"x\" -> \"ŵ₀ + x*ŵ₁,    bias=False\"[label=\"* ŵ₁\"]\n    \"ŵ₀ + x*ŵ₁,    bias=False\" -> \"ŷ\"[label=\"sigmoid\"]\n\n''')\n\n\n\n\n- 1을 생략한 버전들\n(표현4) bais=False 대신에 bias=True를 주면 1을 생략할 수 있음\n\n#collapse\ngv('''\n\"x\" -> \"x*ŵ₁,    bias=True\"[label=\"*ŵ₁\"] ;\n\"x*ŵ₁,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n(표현4의 수정) \\(\\hat{w}_1\\)대신에 \\(\\hat{w}\\)를 쓰는 것이 더 자연스러움\n\n#collapse\ngv('''\n\"x\" -> \"x*ŵ,    bias=True\"[label=\"*ŵ\"] ;\n\"x*ŵ,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n(표현5) 선형변환의 결과는 아래와 같이 \\(u\\)로 표현하기도 한다.\n\n#collapse\ngv('''\n\"x\" -> \"u\";\n\"u\" -> \"y\"[label=\"sigmoid\"] ''')\n\n\n\n\n\n다이어그램은 그리는 사람의 취향에 따라 그리는 방법이 조금씩 다릅니다. 즉 교재마다 달라요.\n\n\n\n예제2: \\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}} =\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n참고: 코드로 표현\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n- 이해를 위해서 10월4일 강의노트에서 다루었던 아래의 상황을 고려하자.\n\n(강의노트의 표현)\n\n#collapse\ngv('''\n\"x\" -> \" -x\"[label=\"*(-1)\"];\n\"x\" -> \" x\"[label=\"*1\"]\n\" x\" -> \"rlu(x)\"[label=\"relu\"] \n\" -x\" -> \"rlu(-x)\"[label=\"relu\"] \n\"rlu(x)\" -> \"u\"[label=\"*(-4.5)\"] \n\"rlu(-x)\" -> \"u\"[label=\"*(-9.0)\"] \n\"u\" -> \"sig(u)=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n(좀 더 일반화된 표현) 10월4일 강의노트 상황을 일반화하면 아래와 같다.\n\n#collapse\ngv('''\n\"x\" -> \"u1[:,0]\"[label=\"*(-1)\"];\n\"x\" -> \"u1[:,1]\"[label=\"*1\"]\n\"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"] \n\"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"] \n\"v1[:,0]\" -> \"u2\"[label=\"*(-9.0)\"] \n\"v1[:,1]\" -> \"u2\"[label=\"*(-4.5)\"] \n\"u2\" -> \"v2=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n* Layer의 개념: \\({\\bf X}\\)에서 \\(\\hat{\\boldsymbol y}\\)로 가는 과정은 “선형변환+비선형변환”이 반복되는 구조이다. “선형변환+비선형변환”을 하나의 세트로 보면 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\left( \\underset{(n,2)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,2)}{\\boldsymbol v^{(1)}} \\right) \\overset{l_2}{\\to} \\left(\\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}\\right), \\quad \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{net({\\bf X})}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n이것을 다이어그램으로 표현한다면 아래와 같다.\n(선형+비선형을 하나의 Layer로 묶은 표현)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"u1[:,0]\"\n    \"X\" -> \"u1[:,1]\"\n    \"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -> \"u2\"\n    \"v1[:,1]\" -> \"u2\"\n    \"u2\" -> \"v2=yhat\"[label=\"sigmoid\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\nLayer를 세는 방법 - 정석: 학습가능한 파라메터가 몇층으로 있는지… - 일부 교재 설명: 입력층은 계산하지 않음, activation layer는 계산하지 않음. - 위의 예제의 경우 number of layer = 2 이다.\n\n사실 input layer, activation layer 등의 표현을 자주 사용해서 layer를 세는 방법이 처음에는 헷갈립니다..\n\nHidden Layer의 수를 세는 방법 - Layer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1 - 위의 예제의 경우 number of hidden layer = 1 이다.\n* node의 개념: \\(u\\to v\\)로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음.\n(노드의 개념이 포함된 그림)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat \"\n    \"node2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n여기에서 node의 숫자 = feature의 숫자와 같이 이해할 수 있다. 즉 아래와 같이 이해할 수 있다.\n(“number of nodes = number of features”로 이해한 그림)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"feature1\"\n    \"X\" -> \"feature2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"feature1\" -> \"yhat \"\n    \"feature2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n\n다이어그램의 표현방식은 교재마다 달라서 모든 예시를 달달 외울 필요는 없습니다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 pytorch로 구현하는 능력은 매우 중요합니다.\n\n\n\n예제3: \\(\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,32)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,32)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n(다이어그램표현)\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Input Layer\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node32\"\n    \"x2\" -> \"node32\"\n    \"..\" -> \"node32\"\n    \"x784\" -> \"node32\"\n\n\n    label = \"Hidden Layer: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    \"...\" -> \"yhat\"\n    \"node32\" -> \"yhat\"\n    \n    label = \"Outplut Layer: sigmoid\"\n}\n''')\n\n\n\n\n\nLayer0,1,2 대신에 Input Layer, Hidden Layer, Output Layer로 표현함\n\n- 위의 다이어그램에 대응하는 코드\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#cpu-vs-gpu",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#cpu-vs-gpu",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "CPU vs GPU",
    "text": "CPU vs GPU\n- 파이토치에서 GPU를 쓰는 방법을 알아보자. (사실 지금까지 우리는 CPU만 쓰고 있었음)\n\nGPU 사용방법\n- cpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_cpu = torch.tensor([0.0,0.1,0.2]).reshape(-1,1) \ny_cpu = torch.tensor([0.0,0.2,0.4]).reshape(-1,1) \nnet_cpu = torch.nn.Linear(1,1) \n\n연산되게끔 reshape으로 shape 변경해주세요\n\nx_cpu, y_cpu\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]))\n\n\n\nnet_cpu.weight, net_cpu.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n- gpu 연산이 가능한 메모리에 데이터 저장\n\ntorch.manual_seed(43052)\nx_gpu = x_cpu.to(\"cuda:0\")\ny_gpu = y_cpu.to(\"cuda:0\")\nnet_gpu = torch.nn.Linear(1,1).to(\"cuda:0\") \n\ncpu있는 자체는 못 넣고\nnet_gpu = net_cpu.to(\"cuda:0\")\ncpu에 있는 net을 가져와서 정의해줘야 한다.\n\n\n_a = torch.nn.Linear(1,1)\n\n\n_a.weight, _a.bias\n\n(Parameter containing:\n tensor([[0.4074]], requires_grad=True),\n Parameter containing:\n tensor([-0.8885], requires_grad=True))\n\n\n\n_a.to(\"cuda:0\") \n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\n_a.weight, _a.bias\n\n(Parameter containing:\n tensor([[0.4074]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8885], device='cuda:0', requires_grad=True))\n\n\n\n\nx_gpu, y_gpu\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'))\n\n\n\nnet_gpu.weight, net_gpu.bias\n\n(Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- cpu 혹은 gpu 연산이 가능한 메모리에 저장된 값들을 확인\n\nx_cpu, y_cpu, net_cpu.weight, net_cpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]]),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]]),\n Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n\nx_gpu, y_gpu, net_gpu.weight, net_gpu.bias\n\n(tensor([[0.0000],\n         [0.1000],\n         [0.2000]], device='cuda:0'),\n tensor([[0.0000],\n         [0.2000],\n         [0.4000]], device='cuda:0'),\n Parameter containing:\n tensor([[-0.3467]], device='cuda:0', requires_grad=True),\n Parameter containing:\n tensor([-0.8470], device='cuda:0', requires_grad=True))\n\n\n- gpu는 gpu끼리 연산가능하고 cpu는 cpu끼리 연산가능함\n(예시1)\n\nnet_cpu(x_cpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9164]], grad_fn=<AddmmBackward0>)\n\n\n(예시2)\n\nnet_gpu(x_gpu) \n\ntensor([[-0.8470],\n        [-0.8817],\n        [-0.9163]], device='cuda:0', grad_fn=<AddmmBackward0>)\n\n\n(예시3)\n\nnet_cpu(x_gpu) \n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시4)\n\nnet_gpu(x_cpu)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n(예시5)\n\ntorch.mean((y_cpu-net_cpu(x_cpu))**2)\n\ntensor(1.2068, grad_fn=<MeanBackward0>)\n\n\n(예시6)\n\ntorch.mean((y_gpu-net_gpu(x_gpu))**2)\n\ntensor(1.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n\n\n(예시7)\n\ntorch.mean((y_gpu-net_cpu(x_cpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n(예시8)\n\ntorch.mean((y_cpu-net_gpu(x_gpu))**2)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n\n둘다 cpu에 있던가 둘 다 gpu에 있던가\n\n\n시간측정 (예비학습)\n\nimport time \n\n\nt1 = time.time()\n\n\nt2 = time.time()\n\n\nt2-t1\n\n0.42019009590148926\n\n\n\n\nCPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\n- for문 준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n0.5373966693878174\n\n\n\n\nGPU (512)\n- 데이터준비\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\n- for문돌릴준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문 + 학습시간측정\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.4511945247650146\n\n\n\n!! CPU가 더 빠르다?\n\n\n\nCPU vs GPU (20480)\n- CPU (20480)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n3.806452512741089\n\n\n- GPU (20480)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,20480),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20480,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.308497667312622\n\n\n- 왜 이런 차이가 나는가? 연산을 하는 주체는 코어인데 CPU는 수는 적지만 일을 잘하는 코어들을 가지고 있고 GPU는 일은 못하지만 다수의 코어를 가지고 있기 때문\n\n\nCPU vs GPU (204800)\n- CPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1)\ny=torch.randn(100).reshape(-1,1)*0.01\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n65.33969926834106\n\n\n- GPU (204800)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny=(torch.randn(100).reshape(-1,1)*0.01).to(\"cuda:0\")\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n).to(\"cuda:0\")\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n2.077875852584839\n\n\n\n!nvidia-smi\n\nWed Oct 12 21:15:05 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n|  0%   41C    P8    28W / 420W |  12812MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A    482816      C   ...onda3/envs/csy/bin/python     4261MiB |\n|    0   N/A  N/A    580526      C   ...onda3/envs/csy/bin/python     3249MiB |\n|    0   N/A  N/A    605977      C   ...onda3/envs/csy/bin/python     2979MiB |\n|    0   N/A  N/A   1035719      C   ...onda3/envs/csy/bin/python     2321MiB |\n+-----------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#확률적경사하강법-배치-에폭",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#확률적경사하강법-배치-에폭",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "확률적경사하강법, 배치, 에폭",
    "text": "확률적경사하강법, 배치, 에폭\n\n좀 이상하지 않아요?\n- 우리가 쓰는 GPU: 다나와 PC견적 - GPU 메모리 끽해봐야 24GB\n- 우리가 분석하는 데이터: 빅데이터..?\n- 데이터의 크기가 커지는순간 X.to(\"cuda:0\"), y.to(\"cuda:0\") 쓰면 난리나겠는걸?\n\nx = torch.linspace(-10,10,100000).reshape(-1,1)\neps = torch.randn(100000).reshape(-1,1)\ny = x*2 + eps \n\n\nplt.plot(x,y,'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n- 데이터를 100개중에 1개만 꼴로만 쓰면 어떨까?\n\nplt.plot(x[::100],y[::100],'o',alpha=0.05)\nplt.plot(x,2*x)\n\n\n\n\n\n대충 이거만 가지고 적합해도 충분히 정확할것 같은데\n\n\n\nX,y 데이터를 굳이 모두 GPU에 넘겨야 하는가?\n- 데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나?\n- 아래의 알고리즘을 생각해보자. 1. 데이터를 반으로 나눈다. 2. 짝수obs의 x,y 그리고 net의 모든 파라메터를 GPU에 올린다. 3. yhat, loss, grad, update 수행 4. 짝수obs의 x,y를 GPU메모리에서 내린다. 그리고 홀수obs의 x,y를 GPU메모리에 올린다. 5. yhat, loss, grad, update 수행 6. 홀수obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수obs의 x,y를 GPU메모리에 올린다. 7. 반복\n(서연 필기) 전체 다 올리면 경사하강법 부분적으로 올리면 확률적 경사하강법\n\n\n경사하강법, 확률적경사하강법, 미니배치 경사하강법\n10개의 샘플이 있다고 가정. \\(\\{(x_i,y_i)\\}_{i=1}^{10}\\)\n- ver1: 모든 샘플을 이용하여 slope 계산\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\)\n…\n- ver2: 하나의 샘플만을 이용하여 slope 계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\to slope \\to update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\to slope \\to update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n(서연 필기) 불안해 - for 문도 많이 돌아야 해.\n- ver3: \\(m (\\leq n)\\) 개의 샘플을 이용하여 slope 계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\to slope \\to update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\to slope \\to update\\)\n…\n(서연 필기) 미니배치하고 남은 것도 계산된다.\n\n\n용어의 정리\n옛날\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n\nds, dl\n- ds\n\nx=torch.tensor(range(10)).float()#.reshape(-1,1)\ny=torch.tensor([1.0]*5+[0.0]*5)#.reshape(-1,1)\n\n\nds=torch.utils.data.TensorDataset(x,y)\nds\n\n<torch.utils.data.dataset.TensorDataset at 0x7fa03d24f940>\n\n\n\nds.tensors # 그냥 (x,y)의 튜플\n\n(tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]))\n\n\n- dl\n\ndl=torch.utils.data.DataLoader(ds,batch_size=3)\n#set(dir(dl)) & {'__iter__'}\n\n\nset(dir(dl)) & {'__iter__'}\n\n{'__iter__'}\n\n\ndir에 __iter__있으면 for문 쓰기 가능\n\nfor xx,yy in dl:\n    print(xx,yy)\n\ntensor([0., 1., 2.]) tensor([1., 1., 1.])\ntensor([3., 4., 5.]) tensor([1., 1., 0.])\ntensor([6., 7., 8.]) tensor([0., 0., 0.])\ntensor([9.]) tensor([0.])\n\n\n\n\nds, dl을 이용한 MNIST 구현\n- 데이터정리\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,y.shape\n\n(torch.Size([12665, 784]), torch.Size([12665, 1]))\n\n\n- ds \\(\\to\\) dl\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048) \n\n\n12665/2048\n\n6.18408203125\n\n\n\ni = 0 \nfor xx,yy in dl: # 총 7번 돌아가는 for문 \n    print(i)\n    i=i+1\n\n0\n1\n2\n3\n4\n5\n6\n\n\n- 미니배치 안쓰는 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(70): \n    ## 1 \n    yhat = net(X) \n    ## 2 \n    loss= loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad() \n\n\ntorch.sum((yhat>0.5) == y) / len(y) \n\ntensor(0.9981)\n\n\n\ntorch.mean(((yhat>0.5) == y)*1.0)\n\ntensor(0.9981)\n\n\n\nlen(y) / 2048\n\n6.18408203125\n\n\n- 미니배치 쓰는 학습 (GPU 올리고 내리는 과정은 생략)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(10):\n    for xx,yy in dl: ## 7번\n        ## 1\n        #yhat = net(xx)\n        ## 2 \n        loss = loss_fn(net(xx),yy) \n        ## 3 \n        loss.backward() \n        ## 4 \n        optimizr.step()\n        optimizr.zero_grad()\n\n(서연 필기)xx넣어서 학습 시키고 전체 X넣어서 확인\n\nlen(X)\n\n12665\n\n\n\ntorch.mean(((net(X)>0.5) == y)*1.0)\n\ntensor(0.9949)"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#오버피팅",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#오버피팅",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "오버피팅",
    "text": "오버피팅\n- 오버피팅이란? - 위키: In mathematical modeling, overfitting is “the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably”. - 제 개념: 데이터를 “데이터 = 언더라잉 + 오차”라고 생각할때 우리가 데이터로부터 적합할 것은 언더라잉인데 오차항을 적합하고 있는 현상.\n\n오버피팅 예시\n- \\(m\\)이 매우 클때 아래의 네트워크 거의 무엇이든 맞출수 있다고 보면 된다.\n\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{sig}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\\(\\underset{(n,1)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,m)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,m)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\hat{\\boldsymbol y}}\\)\n\n- 그런데 종종 맞추지 말아야 할 것들도 맞춘다.\nmodel: \\(y_i = (0\\times x_i) + \\epsilon_i\\), where \\(\\epsilon_i \\sim N(0,0.01^2)\\)\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)\n\n\n\n\n\ny는 그냥 정규분포에서 생성한 오차이므로 \\(X \\to y\\) 로 항햐는 규칙따위는 없음\n\n\ntorch.manual_seed(1) \nnet=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)) \noptimizer= torch.optim.Adam(net.parameters())\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    yhat=net(x) \n    ## 2 \n    loss=loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizer.step()\n    net.zero_grad() \n\n\nplt.plot(x,y)\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n우리는 데이터를 랜덤에서 뽑았는데, 데이터의 추세를 따라간다 \\(\\to\\) 오버피팅 (underlying이 아니라 오차항을 따라가고 있음)\n\n\n\n오버피팅이라는 뚜렷한 증거! (train / test)\n- 데이터의 분리하여 보자.\n\ntorch.manual_seed(5) \nx=torch.linspace(0,1,100).reshape(100,1)\ny=torch.randn(100).reshape(100,1)*0.01\nxtr = x[:80] \nytr = y[:80]\nxtest = x[80:]\nytest = y[80:]\nplt.plot(xtr,ytr)\nplt.plot(xtest,ytest)\nplt.title('train: blue / test: orange');\n\n\n\n\n- train만 학습\n\ntorch.manual_seed(1) \nnet1=torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512), \n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=512,out_features=1)) \noptimizr1= torch.optim.Adam(net1.parameters())\nloss_fn= torch.nn.MSELoss()\n\nfor epoc in range(1000): \n    ## 1 \n    # net(xtr) \n    ## 2 \n    loss=loss_fn(net1(xtr),ytr) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr1.step()\n    optimizr1.zero_grad() \n\n- training data로 학습한 net를 training data 에 적용\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net1(xtr).data,'--') # prediction (train) \n\n\n\n\n\ntrain에서는 잘 맞추는듯이 보인다.\n\n- training data로 학습한 net를 test data 에 적용\n\nplt.plot(x,y,alpha=0.5)\nplt.plot(xtr,net1(xtr).data,'--') # prediction (train) \nplt.plot(xtest,net1(xtest).data,'--') # prediction with unseen data (test) \n\n\n\n\n\ntrain은 그럭저럭 따라가지만 test에서는 엉망이다. \\(\\to\\) overfit"
  },
  {
    "objectID": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#숙제-해설-및-풀이는-여기참고",
    "href": "posts/ml/2022-10-12-ml-(6주차)_10월12일.html#숙제-해설-및-풀이는-여기참고",
    "title": "기계학습 특강 (6주차) 10월5일",
    "section": "숙제 (해설 및 풀이는 여기참고)",
    "text": "숙제 (해설 및 풀이는 여기참고)\n\n숫자0과 숫자1을 구분하는 네트워크를 아래와 같은 구조로 설계하라\n\n\\[\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,64)}{\\boldsymbol u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,64)}{\\boldsymbol v^{(1)}} \\overset{l_1}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}\\]\n위에서 \\(a_1\\)은 relu를, \\(a_2\\)는 sigmoid를 의미한다.\n\n“y=0”은 숫자0을 의미하도록 하고 “y=1”은 숫자1을 의미하도록 설정하라.\n\n\npath = untar_data(URLs.MNIST)\n\n\nzero_fnames = (path/'training/0').ls()\n\n\none_fnames = (path/'training/1').ls()\n\n\nX0 = torch.stack([torchvision.io.read_image(str(zf)) for zf in zero_fnames])\n\n\nX1 = torch.stack([torchvision.io.read_image(str(of)) for of in one_fnames])\n\n\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28).float()\n\n\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라.\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss() 를 이용할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr = torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\nyhat.data\n\ntensor([[nan],\n        [nan],\n        [nan],\n        ...,\n        [nan],\n        [nan],\n        [nan]])\n\n\n학습이 잘 되지 않았다.\n\n아래의 지침에 따라 200 epoch 학습을 진행하라. 학습이 잘 되는가?\n\n\n이미지의 값을 0과 1사이로 규격화 하라. (Xnp = Xnp/255 를 이용하세요!)\n손실함수는 BECLoss를 이용할 것. torch.nn.BCELoss()를 사용하지 않고 수식을 직접 입력할 것.\n옵티마이저는 아담으로 설정할 것. 학습률은 lr=0.002로 설정할 것.\n\n\nX = X/255\n\n\ntorch.manual_seed(12345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,1),\n    torch.nn.Sigmoid()\n)\n\n\noptimizr=torch.optim.Adam(net.parameters(),lr=0.002)\n\n\nfor epoc in range(200):\n    yhat = net(X)\n    loss = -torch.mean(y*torch.log(yhat) + (1-y)*torch.log(1-yhat))\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.',alpha=0.4)\n\n\n\n\n\n아래와 같은 수식을 이용하여 accuracy를 계산하라.\n\n\\(\\text{accuracy}=\\frac{1}{n}\\sum_{i=1}^n I(\\tilde{y}_i=y_i)\\) - \\(\\tilde{y}_i = \\begin{cases}  1 & \\hat{y}_i > 0.5 \\\\  0 & \\hat{y}_i \\leq 0.5 \\end{cases}\\) - \\(I(\\tilde{y}_i=y_i) = \\begin{cases} 1 & \\tilde{y}_i=y_i \\\\ 0 & \\tilde{y}_i \\neq y_i \\end{cases}\\)\n단, \\(n\\)은 0과 1을 의미하는 이미지의 수\n\nytilde = (yhat > 0.5) * 1\n\n\nytilde\n\ntensor([[0],\n        [0],\n        [0],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\n(ytilde == y) * 1\n\ntensor([[1],\n        [1],\n        [1],\n        ...,\n        [1],\n        [1],\n        [1]])\n\n\n\ntorch.sum((ytilde == y) * 1)\n\ntensor(12661)\n\n\n\ntorch.sum((ytilde == y) * 1)/len(y)\n\ntensor(0.9997)\n\n\n\nprint(\"accuraccy: \",torch.sum((ytilde == y) * 1)/len(y))\n\naccuraccy:  tensor(0.9997)\n\n\n\nprint(\"accuracy: \",((yhat>0.5) == y).sum() / len(y))\n\naccuracy:  tensor(0.9997)"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html",
    "href": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html",
    "title": "기계학습 특강 (9주차) 11월02일",
    "section": "",
    "text": "(9주차) 11월02일 [순환신경망– ab예제, embedding layer]"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#import",
    "href": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#import",
    "title": "기계학습 특강 (9주차) 11월02일",
    "section": "import",
    "text": "import\n\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#define-some-funtions",
    "href": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#define-some-funtions",
    "title": "기계학습 특강 (9주차) 11월02일",
    "section": "Define some funtions",
    "text": "Define some funtions\n- 활성화함수들\n\nsig = torch.nn.Sigmoid()\nsoft = torch.nn.Softmax(dim=1)\ntanh = torch.nn.Tanh()\n\n\n_x = torch.linspace(-5,5,100)\nplt.plot(_x,tanh(_x))\nplt.title(\"tanh(x)\", size=15)\n\nText(0.5, 1.0, 'tanh(x)')\n\n\n\n\n\nhyperblic tangent(https://en.wikipedia.org/wiki/Hyperbolic_functions) - sigmoid(범위가0 ~ 1)와 차이점(범위가 -1 ~ 1)\n- 문자열 -> 숫자로 바꾸는 함수\n\ndef f(txt,mapping):\n    return [mapping[key] for key in txt] \n\n(사용예시1)\n\ntxt = ['a','b','a']\nmapping = {'a':33,'b':-22}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [33, -22, 33]\n\n\n(사용예시2)\n\ntxt = ['a','b','a']\nmapping = {'a':[1,0],'b':[0,1]}\nprint('변환전: %s'% txt)\nprint('변환후: %s'% f(txt,mapping))\n\n변환전: ['a', 'b', 'a']\n변환후: [[1, 0], [0, 1], [1, 0]]"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#exam1-ab",
    "href": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#exam1-ab",
    "title": "기계학습 특강 (9주차) 11월02일",
    "section": "Exam1: ab",
    "text": "Exam1: ab\n\ndata\n\ntxt = list('ab')*100\ntxt[:10]\n\n['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])\n\n\n\n\n선형모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터정리\n\nx = torch.tensor(f(txt_x,{'a':0,'b':1})).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,{'a':0,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습 및 결과 시각화\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n\n잘 학습이 안되었다.\n\n- 학습이 잘 안된 이유\n\npd.DataFrame({'x':x[:5].reshape(-1),'y':y[:5].reshape(-1)})\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      1.0\n    \n    \n      1\n      1.0\n      0.0\n    \n    \n      2\n      0.0\n      1.0\n    \n    \n      3\n      1.0\n      0.0\n    \n    \n      4\n      0.0\n      1.0\n    \n  \n\n\n\n\n현재 \\(\\hat{y}_i = \\hat{w}x_i\\) 꼴의 아키텍처이고 \\(y_i \\approx \\hat{w}x_i\\) 가 되는 적당한 \\(\\hat{w}\\)를 찾아야 하는 상황 - \\((x_i,y_i)=(0,1)\\) 이면 어떠한 \\(\\hat{w}\\)를 선택해도 \\(y_i \\approx \\hat{w}x_i\\)를 만드는 것이 불가능\n- \\((x_i,y_i)=(1,0)\\) 이면 \\(\\hat{w}=0\\)일 경우 \\(y_i \\approx \\hat{w}x_i\\)로 만드는 것이 가능\n상황을 종합해보니 \\(\\hat{w}=0\\)으로 학습되는 것이 그나마 최선\n0에 무엇을 곱하든 0이 되어서 학습이 안 돼\n\n\n(풀이2) 1개의 파라메터 – 성공, but 확장성이 없는 풀이\n- 0이라는 값이 문제가 되므로 인코딩방식의 변경\n\nx = torch.tensor(f(txt_x,{'a':-1,'b':1})).float().reshape(-1,1) \ny = torch.tensor(f(txt_y,{'a':-1,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[-1.],\n         [ 1.],\n         [-1.],\n         [ 1.],\n         [-1.]]),\n tensor([[ 1.],\n         [-1.],\n         [ 1.],\n         [-1.],\n         [ 1.]]))\n\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(2000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과는 성공\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n\n딱봐도 클래스가 3개일 경우 확장이 어려워 보인다.\n\n원핫인코딩해줘야 좋은데 그러면 마지막 무조건 softmax 그러면 loss는 BCELoss\n\n\n\n로지스틱 모형을 이용한 풀이\n\n(풀이1) 1개의 파라메터 – 실패\n- 데이터를 다시 a=0, b=1로 정리\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 결과해석: 예상되었던 실패임 - 아키텍처는 \\(\\hat{y}_i = \\text{sig}(\\hat{w}x_i)\\) 꼴이다. - \\((x_i,y_i)=(0,1)\\) 이라면 어떠한 \\(\\hat{w}\\)을 선택해도 \\(\\hat{w}x_i=0\\) 이다. 이경우 \\(\\hat{y}_i = \\text{sig}(0) = 0.5\\) 가 된다. - \\((x_i,y_i)=(1,0)\\) 이라면 \\(\\hat{w}=-5\\)와 같은 값으로 선택하면 \\(\\text{sig}(-5) \\approx 0 = y_i\\) 와 같이 만들 수 있다. - 상황을 종합하면 net의 weight는 \\(\\text{sig}(\\hat{w}x_i) \\approx 0\\) 이 되도록 적당한 음수로 학습되는 것이 최선임을 알 수 있다.\n\nnet.weight # 적당한 음수값으로 학습되어있음을 확인\n\nParameter containing:\ntensor([[-2.8288]], requires_grad=True)\n\n\n\n\n(풀이2) 2개의 파라메터 + 좋은 초기값 – 성공\n- 동일하게 a=0, b=1로 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 네트워크에서 bias를 넣기로 결정함\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- net의 초기값을 설정 (이것은 좋은 초기값임)\n\nnet.weight.data = torch.tensor([[-5.00]])\nnet.bias.data = torch.tensor([+2.500])\n\n\nnet(x)[:10]\n\ntensor([[ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\nbias 쓰게 되면서 한쪽을 뭉개주는 효과?\n- 학습후결과\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n\n(풀이3) 2개의 파라메터 + 나쁜초기값 – 성공\n- a=0, b=1\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 이전과 동일하게 바이어스가 포함된 네트워크 설정\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 초기값설정 (이 초기값은 나쁜 초기값임)\n\nnet.weight.data = torch.tensor([[+5.00]])\nnet.bias.data = torch.tensor([-2.500])\n\n\nnet(x)[:10]\n\ntensor([[-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000],\n        [-2.5000],\n        [ 2.5000]], grad_fn=<SliceBackward0>)\n\n\n- 학습전상태: 반대모양으로 되어있다.\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n결국 수렴하긴 할듯\n\n\n\n(풀이4) 3개의 파라메터를 쓴다면?\n- a=0, b=1로 코딩\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 3개의 파라메터를 사용하기 위해서 아래와 같은 구조를 생각하자.\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n위와 같은 네트워크를 설정하면 3개의 파라메터를 사용할 수 있다. 적절한 ACTIVATION_FUNCTION을 골라야 하는데 실험적으로 tanh가 적절하다고 알려져있다. (\\(\\to\\) 그래서 우리도 실험적으로 이해해보자)\n\n(예비학습1) net(x)와 사실 net.forwardx(x)는 같다.\n\nnet(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n\nnet.forward(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n그래서 net.forward를 재정의하면 net(x)의 기능을 재정의 할 수 있다.\n\nnet.forward = lambda x: 1 \n\n\n“lambda x: 1” 은 입력이 x 출력이 1인 함수를 의미 (즉 입력값에 상관없이 항상 1을 출력하는 함수)\n“net.forward = lambda x:1” 이라고 새롭게 선언하였므로 앞으론 net.forward(x), net(x) 도 입력값에 상관없이 항상 1을 출력하게 될 것임\n\n\nnet(x)\n\n1\n\n\n(예비학습2) torch.nn.Module을 상속받아서 네트워크를 만들면 (= “class XXX(torch.nn.Module):” 와 같은 방식으로 클래스를 선언하면) 약속된 아키텍처를 가진 네트워크를 찍어내는 함수를 만들 수 있다.\n(예시1)\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet1()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시2)\n\nclass Mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.ReLU()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet2()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.RuLU(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n(예시3)\n\nclass Mynet3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이제\nnet = Mynet3()\n는 아래와 같은 효과를 가진다.\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n클래스에 대한 이해가 부족한 학생을 위한 암기방법\nstep1: 아래와 코드를 복사하여 틀을 만든다. (이건 무조건 고정임, XXXX 자리는 원하는 이름을 넣는다)\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        \n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\n\nnet(x)에 사용하는 x임, yhat은 net.forward(x) 함수의 리턴값임\n사실, x/yhat은 다른 변수로 써도 무방하나 (예를들면 input/output 이라든지) 설명의 편의상 x와 yhat을 고정한다.\n\nstep2: def __init__(self):에 사용할 레이어를 정의하고 이름을 붙인다. 이름은 항상 self.xxx 와 같은 식으로 정의한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\nstep3: def forward:에 “x –> yhat” 으로 가는 과정을 묘사한 코드를 작성하고 yhat을 리턴하도록 한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        u = self.xxx1(x) \n        v = self.xxx2(u)\n        yhat = self.xxx3(v) \n        ## 정의 끝\n        return yhat\n예비학습 끝\n\n- 우리가 하려고 했던 것: 아래의 아키텍처에서\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\nACTIVATION의 자리에 tanh가 왜 적절한지 직관을 얻어보자.\n- 실험결과1(Sig): Sigmoid activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet1()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_1(x):=Sigmoid(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n큰 폭 -> 학습 속도가 빠르다\n- 실험결과2(ReLU): RuLU activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet2()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=ReLU(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 실험결과3(Tanh): Tanh activation을 포함한 아키텍처로 학습시킨 25개의 적합결과\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet3()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=Tanh(x)$\",size=20)        \nfig.tight_layout()\n\n\n\n\n- 실험해석 - sig: 주황색선의 변동폭이 작음 + 항상 0.5근처로 머무는 적합값이 존재 - relu: 주황색선의 변동폭이 큼 + 항상 0.5근처로 머무는 적합값이 존재 - tanh: 주황색선의 변동폭이 큼 + 0.5근처로 머무는 적합값이 존재X\n- 실험해보니까 tanh가 우수한것 같다. \\(\\to\\) 앞으로는 tanh를 쓰자.\n\\(x \\to wx \\to \\tanh \\to wx \\to sig \\to y\\) - x가 양이면 wx 양수 이런 식으로 y로 가게끔 설정하면 설명의 여지가 존재(?)\n(서연 필기)sigmoid하면 0에 머무르는 값 존재해서 0.5에 머무르는 경향, 조금 사용하면 학습 능력이 떨어지기도\n\n\n\n소프트맥스로 확장\n\n(풀이1) 로지스틱모형에서 3개의 파라메터 버전을 그대로 확장\n\nmapping = {'a':[1,0],'b':[0,1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2)\nx[:5],y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]]),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]]))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nsoft(net(x))[:2]\n\ntensor([[0.0048, 0.9952],\n        [0.9953, 0.0047]], grad_fn=<SliceBackward0>)\n\n\n\ny[:5][:,1]\n\ntensor([1., 0., 1., 0., 1.])\n\n\n\nplt.plot(y[:5][:,1],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\nb,a,b,a,,,…\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5])\nax[1].imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7fe521441490>\n\n\n\n\n\n비슷하게 나왔다, 학습이 잘 되었다(중간 대체과제 참고)"
  },
  {
    "objectID": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#embedding-layer",
    "href": "posts/ml/2022-11-02-ml_(9주차)_10월31일.html#embedding-layer",
    "title": "기계학습 특강 (9주차) 11월02일",
    "section": "Embedding Layer",
    "text": "Embedding Layer\n\nmotive\n- 결국 최종적으로는 아래와 같은 맵핑방식이 확장성이 있어보인다.\n\nmapping = {'a':[1,0,0],'b':[0,1,0],'c':[0,0,1]} # 원핫인코딩 방식 \n\n- 그런데 매번 \\(X\\)를 원핫인코딩하고 Linear 변환하는것이 번거로운데 이를 한번에 구현하는 함수가 있으면 좋겠다. \\(\\to\\) torch.nn.Embedding Layer가 그 역할을 한다.\nx dimension은 3(원핫인코딩)\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping))\ny = torch.tensor(f(list('bca')*100,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1)\n\n\nebdd(x)[:5]\n\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843],\n        [-0.8178],\n        [-0.7052]], grad_fn=<SliceBackward0>)\n\n\n- 그런데 사실 언뜻보면 아래의 linr 함수와 역할의 차이가 없어보인다.\n\ntorch.manual_seed(43052)\nlinr = torch.nn.Linear(in_features=1,out_features=1)\n\n\nlinr(x.float().reshape(-1,1))[:5]\n\ntensor([[-0.8470],\n        [-1.1937],\n        [-1.5404],\n        [-0.8470],\n        [-1.1937]], grad_fn=<SliceBackward0>)\n\n\n- 차이점: 파라메터수에 차이가 있다.\n파라메터 적게 쓰는게 비용측면에서 좋으니까\n\nebdd.weight\n\nParameter containing:\ntensor([[-0.8178],\n        [-0.7052],\n        [-0.5843]], requires_grad=True)\n\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.3467]], requires_grad=True),\n Parameter containing:\n tensor([-0.8470], requires_grad=True))\n\n\n결국 ebdd는 아래의 구조에 해당하는 파라메터들이고\n\n$=\n\\[\\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nnet(x)=\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} -0.8178 \\\\ -0.7052 \\\\ -0.5843 \\\\ -0.8178 \\\\ -0.7052  \\end{bmatrix}\\]\n$\n\nlinr는 아래의 구조에 해당하는 파라메터이다.\n\n\\(\\text{x[:5]}= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\quad net(x)= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\times (-0.3467) + (-0.8470)=\\begin{bmatrix} -0.8470 \\\\ -1.1937 \\\\ -1.5404 \\\\ -0.8470 \\\\ -1.1937 \\end{bmatrix}\\)\n\n\n\n연습 (ab문제 소프트맥스로 확장한 것 다시 풀이)\n- 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 1, 0]), tensor([1, 0, 1, 0, 1]))\n\n\n- torch.nn.Embedding 을 넣은 네트워크\nnum_embedding이 2인 이유 a,b만 있어서\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\n\nplt.imshow(soft(net(x[:5])).data)\n\n<matplotlib.image.AxesImage at 0x7fe52160a2b0>"
  },
  {
    "objectID": "posts/ml/2022-09-07-ml_1주차_9월7일.html",
    "href": "posts/ml/2022-09-07-ml_1주차_9월7일.html",
    "title": "기계학습 특강 (1주차) 9월7일",
    "section": "",
    "text": "(1주차) 9월7일 [pytorch]\n\n우리의 1차 목표: 이미지 -> 개/고양이 판단하는 모형을 채용하고, 그 모형에 데이터를 넣어서 학습하고, 그 모형의 결과를 판단하고 싶다. (즉 클래시파이어를 만든다는 소리)\n\n\n우리의 2차 목표: 그 모형에 “새로운” 자료를 전달하여 이미지를 분류할 것이다. (즉 클래시파이어를 쓴다는 소리)\n\n\nimport\n\nfrom fastai.vision.all import *\n\n\n#!nvidia-smi\n\n\nURLs.PETS\n\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n\n\npath = untar_data(URLs.PETS)/'images'\n\n\npath\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images')\n\n\n\nPILImage.create('/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\n_lst = ['/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg','/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg']\n_lst\n\n['/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg',\n '/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_10.jpg']\n\n\n\n_lst[0]\n\n'/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg'\n\n\n\nPILImage.create(_lst[1])\n\n\n\n\n\nfilenames = get_image_files(path)\nfilenames\n\n(#7390) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]\n\n\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nprint(filenames[0])\nPILImage.create(filenames[0])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg\n\n\n\n\n\n\nprint(filenames[1])\nPILImage.create(filenames[1])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg\n\n\n\n\n\n\nprint(filenames[2])\nPILImage.create(filenames[2])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg\n\n\n\n\n\n\nprint(filenames[3])\nPILImage.create(filenames[3])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg\n\n\n\n\n\n\nprint(filenames[4])\nPILImage.create(filenames[4])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg\n\n\n\n\n\n\nprint(filenames[5])\nPILImage.create(filenames[5])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg\n\n\n\n\n\n\nprint(filenames[6])\nPILImage.create(filenames[6])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg\n\n\n\n\n\n\nprint(filenames[7])\nPILImage.create(filenames[7])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg\n\n\n\n\n\n\nprint(filenames[8])\nPILImage.create(filenames[8])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg\n\n\n\n\n\n\nprint(filenames[9])\nPILImage.create(filenames[9])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg\n\n\n\n\n\n\nprint(filenames[20])\nPILImage.create(filenames[20])\n\n/home/csy/.fastai/data/oxford-iiit-pet/images/Maine_Coon_266.jpg\n\n\n\n\n\nvector로 되어 있는 tensor\n\n\n'A'.isupper()\n\nTrue\n\n\n\n\ndef f(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\nf('dddd')\n\n'dog'\n\n\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nImageDataLoaders.from_name_func??\n\n\nSignature:\nImageDataLoaders.from_name_func(\n    path,\n    fnames,\n    label_func,\n    valid_pct=0.2,\n    seed=None,\n    item_tfms=None,\n    batch_tfms=None,\n    bs=64,\n    val_bs=None,\n    shuffle=True,\n    device=None,\n)\nSource:   \n    @classmethod\n    def from_name_func(cls, path, fnames, label_func, **kwargs):\n        \"Create from the name attrs of `fnames` in `path`s with `label_func`\"\n        if sys.platform == 'win32' and isinstance(label_func, types.LambdaType) and label_func.__name__ == '<lambda>':\n            # https://medium.com/@jwnx/multiprocessing-serialization-in-python-with-pickle-9844f6fa1812\n            raise ValueError(\"label_func couldn't be lambda function on Windows\")\n        f = using_attr(label_func, 'name')\n        return cls.from_path_func(path, fnames, f, **kwargs)\nFile:      ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/data.py\nType:      method\n\n\n\n\ndls는 object - 동사 - 명사(method)\nsize가 다르기 때문에 dls 적용이 되지 않아 resize로 조정을 해주었다.\n\npath\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images')\n\n\n\ndls = ImageDataLoaders.from_name_func(path,filenames,f,item_tfms=Resize(224))\n#dls\n\n\ndls.show_batch(max_n=16)\n\n\n\n\n\n\n학습\n\nobject\n\nnoun\n\n\ndata\n채용할 모델의 이름\n평가기준 metric\n\n\nverb\n\n\n학습\n판단\n\n\nysj = cnn_learner(dls,resnet34,metrics=error_rate)\n\n\nysj.fine_tune(1)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.138703\n      0.014957\n      0.004060\n      00:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.061359\n      0.010080\n      0.002706\n      00:11\n    \n  \n\n\n\n\n\n\n기존 데이터를 잘 맞추는지 확인\n\nfilenames[0]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg')\n\n\n\nysj.predict(PILImage.create(filenames[0]))\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.5260e-07]))\n\n\n\nysj.predict(filenames[0])\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.5260e-07]))\n\n\n\nfilenames[1]\n\nPath('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg')\n\n\n\nysj.predict(filenames[1])\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.0373e-04, 9.9980e-01]))\n\n\n\nysj.show_results()\n\n\n\n\n\n\n\n\n\n오답분석\n\nchecker = Interpretation.from_learner(ysj)\n\n\n\n\n\nchecker.plot_top_losses(k=16)\n\n\n\n\n\n\n좋은 모델인가?\n\nPILImage.create('2022-01-13-cat.jpg')\n\n\n\n\n\nysj.predict(PILImage.create('2022-01-13-cat.jpg'))\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 3.8330e-16]))\n\n\n\nPILImage.create(requests.get('https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg').content)\n\n\n\n\n\nimg=PILImage.create(requests.get('https://dimg.donga.com/ugc/CDB/SHINDONGA/Article/5e/0d/9f/01/5e0d9f011a9ad2738de6.jpg').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.1535e-06, 1.0000e+00]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-cat1.png?raw=true').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9982e-01, 1.8307e-04]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-cat2.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.0889e-07]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani01.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([9.5189e-06, 9.9999e-01]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani02.jpeg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.0720e-05, 9.9998e-01]))\n\n\n\nimg=PILImage.create(requests.get('https://github.com/guebin/STML2022/blob/master/_notebooks/2022-09-06-hani03.jpg?raw=true').content)\nysj.predict(img)\n\n\n\n\n('dog', TensorBase(1), TensorBase([0.0513, 0.9487]))\n\n\n\n\n\nhomework\n\n임의의 사진으로 잘 맞추는지 확인\n\n\nPILImage.create('2022-09-07-dogs.jpeg')\n\n\n\n\n\nysj.predict(PILImage.create('2022-09-07-dogs.jpeg'))\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.7947e-04, 9.9972e-01]))\n\n\n\nimg2=PILImage.create('2022-09-07-dogs.jpeg')\nysj.predict(img2)\n\n\n\n\n('dog', TensorBase(1), TensorBase([2.7947e-04, 9.9972e-01]))\n\n\n\nPILImage.create(requests.get('https://media.npr.org/assets/img/2021/08/11/gettyimages-1279899488_wide-f3860ceb0ef19643c335cb34df3fa1de166e2761-s900-c85.webp').content)\n\n\n\n\n\nimg=PILImage.create(requests.get('https://media.npr.org/assets/img/2021/08/11/gettyimages-1279899488_wide-f3860ceb0ef19643c335cb34df3fa1de166e2761-s900-c85.webp').content)\nysj.predict(img)\n\n\n\n\n('cat', TensorBase(0), TensorBase([1.0000e+00, 2.5169e-10]))"
  },
  {
    "objectID": "posts/rl/2022-11-28-rl-CH13.html",
    "href": "posts/rl/2022-11-28-rl-CH13.html",
    "title": "고급회귀분석 실습 CH13",
    "section": "",
    "text": "chapter 13"
  },
  {
    "objectID": "posts/rl/2022-11-28-rl-CH13.html#example",
    "href": "posts/rl/2022-11-28-rl-CH13.html#example",
    "title": "고급회귀분석 실습 CH13",
    "section": "Example",
    "text": "Example\n\ndt <- data.frame(\n  y = c(17,26,21,30,22,1,12,19,4,16,\n        28,15,11,38,31,21,20,13,30,14),\n  x1 = c(151,92,175,31,104,277,210,120,290,238,\n         164,272,295,68,85,224,166,305,124,246),\n  x2 = factor(rep(c(0,1), each=10))\n)\n\n할 수 있는 경우의 수 1.\nx2 = factor(rep(c('M','F'), each=10))\n\ncharacter로 넣어도 factor로 인식한다.\n조심할 점 알파벳 순으로 숫자가 부여된다.\n따라서 F는 0, M은 1로 부여되었다.\n\n참고: 강의록은 F는 1, M은 0으로 부여되어 결과가 다름\n해석의 결과가 다르진 않지만 어떻게 해석하느냐가 달라짐\n\n\n\n\n\nx2 = factor(rep(c(0,1), each=10))\n\nM은 0, F는 1-> 강의록과 같음\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    yx1x2\n    <dbl><dbl><fct>\n\n\n    1171510\n    226 920\n    3211750\n    430 310\n    5221040\n    6 12770\n\n\n\n\n\ncontrasts(factor(dt$x2))\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1\n\n\n    00\n    11\n\n\n\n\n잘 부여되었나 확인 필요 - chracter일때는 이 함수를 사용해도 의미가 없어서 factor로 바꿔서 해줘야 한다.\n\nm <- lm(y~x1+x2, dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.834912   1.758659  19.239 5.64e-13 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx21          7.933953   1.414702   5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n1.\nx2 = factor(rep(c('M','F'), each=10))\n로 입력한 경우\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\\(x_2 = 0\\), F\n\\(x_2 = 1\\), M\n\\(x_2\\)는 0 아니면 1 인 지수함수\n\\(E(y|M) : \\beta_0 + \\beta_1x_1 + \\beta_2 = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1\\)\n2.\nx2 = factor(rep(c(0,1), each=10))\n로 입력한 경우\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\\(x_2 = 0\\), M\n\\(x_2 = 1\\), F\n\\(E(y|M) : \\beta_0 + \\beta_1x_1\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1+ \\beta_2 = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() +\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), \n                     values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\nm <- lm(y~x1+x2, dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.834912   1.758659  19.239 5.64e-13 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx21          7.933953   1.414702   5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\\(\\beta_2\\)가 유의함을 확인함(3.13e-05) - 남성과 여성이 차이가 있다.\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(m)[2], intercept = coef(m)[1], col= 'darkorange')+\n  geom_abline(slope = coef(m)[2], intercept = coef(m)[1]+coef(m)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n기울기 차이 고려 안해서 기울기는 같을 것\n\n교호작용\n\nm1 <- lm(y~x1*x2, dt)\n\n곱하기로 교호작용 표현\n\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1 * x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0463 -1.7591 -0.6232  1.9311  6.1102 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.656104   2.365392  14.229 1.68e-10 ***\nx1          -0.099858   0.012650  -7.894 6.59e-07 ***\nx21          8.313516   3.541379   2.348   0.0321 *  \nx1:x21      -0.002089   0.017766  -0.118   0.9078    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.218 on 16 degrees of freedom\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8803 \nF-statistic: 47.56 on 3 and 16 DF,  p-value: 3.405e-08\n\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\) - 성별, 성적, 성별*성적\n\\(M : x_2=0 \\to E(y|M) = \\beta_0+\\beta_1x_1\\)\n\\(F : x_2=1 \\to E(y|F) = \\beta_0 + \\beta_1x_1 + \\beta_2 + \\beta_3x_1 = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)x_1\\)\n\n식으로 봤을 때절편, 기울기 모두 차이가 난다.\n절편 차이는 유의하다.\n기울기 차이는 거의 없다.\n교호작용은 없는 것으로 확인.\n\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(m1)[2], intercept = coef(m1)[1], col= 'darkorange')+\n  geom_abline(slope = coef(m1)[2]+coef(m1)[4], intercept = coef(m1)[1]+coef(m1)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"남자\", \"여자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\n책 예제\n\nlibrary(ISLR)\n\n\nhead(Carseats)\n\n\n\nA data.frame: 6 × 11\n\n    SalesCompPriceIncomeAdvertisingPopulationPriceShelveLocAgeEducationUrbanUS\n    <dbl><dbl><dbl><dbl><dbl><dbl><fct><dbl><dbl><fct><fct>\n\n\n    1 9.50138 7311276120Bad   4217YesYes\n    211.22111 4816260 83Good  6510YesYes\n    310.06113 3510269 80Medium5912YesYes\n    4 7.40117100 4466 97Medium5514YesYes\n    5 4.15141 64 3340128Bad   3813YesNo \n    610.8112411313501 72Bad   7816No Yes\n\n\n\n\n\ndim(Carseats)\n\n\n40011\n\n\n• Sales : 판매량 (단위: 1,000)\n• Price : 각 지점에서의 카시트 가격\n• ShelveLoc : 진열대의 등급 (Bad, Medium, Good)\n• Urban :도시 여부 (Yes, No)\n• US: 미국 여부 (Yes, No)\n\\(\\to\\) 세 개의 범주형 자료, 가변수 4개(3-2,2-1,2-1)\n\nfit <- lm(fit<-lm(Sales~Price+ShelveLoc+US, \n                  data=Carseats))\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = fit <- lm(Sales ~ Price + ShelveLoc + US, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1720 -1.2587 -0.0056  1.2815  4.7462 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     11.476347   0.498083  23.041  < 2e-16 ***\nPrice           -0.057825   0.003938 -14.683  < 2e-16 ***\nShelveLocGood    4.827167   0.277294  17.408  < 2e-16 ***\nShelveLocMedium  1.893360   0.227486   8.323 1.42e-15 ***\nUSYes            1.013071   0.195034   5.194 3.30e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.857 on 395 degrees of freedom\nMultiple R-squared:  0.5718,    Adjusted R-squared:  0.5675 \nF-statistic: 131.9 on 4 and 395 DF,  p-value: < 2.2e-16\n\n\n\ncontrasts(Carseats$ShelveLoc)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    GoodMedium\n\n\n    Bad00\n    Good10\n    Medium01\n\n\n\n\n알파벳 순으로 부여된 것 확인\n\\(y= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 + \\beta_5x_5\\)\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\n\\(H_0 : \\beta_2 = \\beta_3 = 0\\)\n\\(b_2 = 0, b_3 = 0\\)\n\nC<-rbind(c(0,0,1,0,0,0),\n         c(0,0,0,1,0,0))\n\n\nlinearHypothesis(fit, C)\n\nERROR: Error in L %*% b: non-conformable arguments\n\n\n유의미한 결과 확인\n\n\n구간별 회귀분석\n\ndt <- data.frame(\n  y = c(377,249,355,475,139,452,440,257),\n  x1 = c(480,720,570,300,800,400,340,650)\n)\n\n\ndt$x2 = sapply(dt$x1, function(x) max(0, x-500))\n\n\nm <- lm(y ~ x1+x2, dt)\n\n\nsummary(m)\n\n\ndt2 <- rbind(dt[,2:3], c(500,0))\n\n\ndt2$y <- predict(m, newdata = dt2)\n\n** this is the predicted line of multiple linear regression**\n\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') +\n  geom_line(color='darkorange',data = dt2, aes(x=x1, y=y))+\n  geom_vline(xintercept = 500, lty=2, col='red')+\n  theme_bw()"
  },
  {
    "objectID": "posts/rl/2022-10-23-rl-HW2.html",
    "href": "posts/rl/2022-10-23-rl-HW2.html",
    "title": "Regression HW 2",
    "section": "",
    "text": "고급회귀분석 과제, CH03,04,05\n\n고급회귀분석 두번째 과제입니다.\n제출 기한 : 10월 23일\n(마지막 문제는 R을 이용해서 풀이해도 됨)\n제출 방법\n\n직접 제출(607호) 도 가능하지만,\n문서 작성 후 pdf로 변환(★★★)하여 lms에 제출을 추천\n\n(pdf 아닌 문서는 미제출로 간주)\n주의사항\n\npdf로 꼭 변환하여 제출\n풀이가 꼭 있어야 함 (답만 적혀 있는 경우 ’0’점 처리)\n부정행위 시 ’F’학점\n계산은 R로 해도 되지만 계산 풀이 과정이 꼭 있어야 함!!\n\n예) R에서 lm으로 beta의 추정량을 구하면 안 됨. 수업 시간에 배운 식으로 풀이를 적어야 함.\n************ R을 이용해서 푸는 문제는, R 코드도 같이 업로드.\n\n\n. 단순회귀에서 회귀제곱합, \\[SSE = \\sum^{n}_{i=1} (y_i - \\hat{y}_i)^2\\] 을 이차형식 \\(y^\\top B y\\) 로 표현하시오. 이 이차형식의 분포를 구하고, 또한 기대치를 ⟨정리 5.1⟩에 의하여 구하시오\n\nAnswer\n$^{n}_{i=1} (y_i - _i)^2 $\n$= ^{n}_{i=1} (y_i - _i)^(y_i - _i) $\n$= (\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_4\n\n\n\nA data.frame: 10 × 7\n\n    xyx_barxy_baryx_barx2y_bary2xy\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     08.5-6 0.89360.7921-5.34\n     08.4-6 0.79360.6241-4.74\n     37.9-3 0.29 90.0841-0.87\n     38.1-3 0.49 90.2401-1.47\n     67.8 0 0.19 00.0361 0.00\n     67.6 0-0.01 00.0001 0.00\n     97.3 3-0.31 90.0961-0.93\n     97.0 3-0.61 90.3721-1.83\n    126.8 6-0.81360.6561-4.86\n    126.7 6-0.91360.8281-5.46\n\n\n\n\n\nround(colSums(df_4),3)\n\nx60y76.1x_barx0y_bary0x_barx2180y_bary23.729xy-25.5\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_4 <- as.numeric(colSums(df_4)[7]/colSums(df_4)[5])\nbeta0_4 <- mean(df_4$y) - beta1_4 *  mean(df_4$x)\n\n\ncat(\"hat beta0 = \", beta0_4)\ncat(\"\\nhat beta1 = \", beta1_4)\n\nhat beta0 =  8.46\nhat beta1 =  -0.1416667\n\n\n\\(\\hat{y} = 8.46 - 0.1417x\\)\n\nSST_4 = sum((df_4$y - mean(df_4$y))^2)\n\n\nSSR_4 = sum( ( ( beta0_4 + beta1_4 *df_4$x)-mean(df_4$y) )^2 )\nMSR_4 = SSR_4/1\n\n\nSSE_4 = sum( ( df_4$y-( beta0_4 + beta1_4 *df_4$x))^2 )\nMSE_4 = SSE_4/8\n\n\ncat(\"SST = \", SST_4,\", df = 9\")\ncat(\"\\nSSR = \", SSR_4,\", df = 1\")\ncat(\"\\nSSE = \", SSE_4, \", df = 8\")\n\nSST =  3.729 , df = 9\nSSR =  3.6125 , df = 1\nSSE =  0.1165 , df = 8\n\n\n\nF_4 = MSR_4 / MSE_4\nF_4\n\n248.068669527898\n\n\n\nqf(0.95,1,8)\n\n5.31765507157871\n\n\n\\(H_0 : \\beta_1 = 0\\)\n\\(H_1 : \\beta_1 \\neq 0\\)\nF값이 유의수준 0.05에서 기준 F보다 크기 때문에 \\(H_0\\) 기각하고, \\(\\beta_1\\) 은 유의미하다.\n\ndf_4_ex <- cbind(df_4[c(1,3,5,7,9),c(1,2)],df_4[c(2,4,6,8,10),c(2)])\n\n\ncolnames(df_4_ex) <- c('x','y1','y2')\n\n\ndf_4_ex$ymean <- (df_4_ex$y1+df_4_ex$y2)/2\ndf_4_ex$y1_ymean2 <- (df_4_ex$y1 - df_4_ex$ymean)^2\ndf_4_ex$y2_ymean2 <- (df_4_ex$y2 - df_4_ex$ymean)^2\ndf_4_ex$yhat <- 8.46 - 0.146667 * df_4_ex$x\ndf_4_ex\n\n\n\nA data.frame: 5 × 7\n\n    xy1y2ymeany1_ymean2y2_ymean2yhat\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1 08.58.48.450.00250.00258.460000\n    3 37.98.18.000.01000.01008.019999\n    5 67.87.67.700.01000.01007.579998\n    7 97.37.07.150.02250.02257.139997\n    9126.86.76.750.00250.00256.699996\n\n\n\n\n\\(\\hat{y} = 8.46 - 01417x\\)\n\nSSPE_4 = sum(df_4_ex$y1_ymean2) + sum(df_4_ex$y2_ymean2)\nSSPE_4\n\n0.0949999999999998\n\n\n\nSSLF_4 = SSE_4 - SSPE_4\nSSLF_4\n\n0.0214999999999996\n\n\n\nF_4_0 = SSLF_4/3 / (SSPE_4 / 5)\nF_4_0\n\n0.377192982456135\n\n\n\ncat(\"유의수준 5%에서 \",qf(0.95,3,5), \"보다 \",F_4_0,\"값이 작기 때문에 귀무가설을 기각하지 못한다. 따라서 선형회귀모형은 타당하다.\")\n\n유의수준 5%에서  5.409451 보다  0.377193 값이 작기 때문에 귀무가설을 기각하지 못한다. 따라서 선형회귀모형은 타당하다.\n\n\n\\(H_0 : E(Y|X = x) = \\beta_0 + \\beta_1 x\\) 기각못함\n\n선형회귀모형이 타당한 경우, 신선도의 점수가 시간당 얼마만큼이나 떨어지는가를 95% 신뢰계수를 가지고 구간추정하라(즉, \\(\\beta_1\\)의 구간추정).\n\nAnswer\n\\(\\hat{\\beta}_1\\)의 \\(100(1-\\alpha)\\)%의 신뢰구간\n\\(\\hat{\\beta}_1 \\pm t_{\\alpha/2}(n-2)\\frac{\\sqrt{MSE}}{\\sqrt{S_{xx}}}\\)\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\ncat(\"beta1 is \",beta1_4)\ncat(\"\\nMSE is \",MSE_4)\ncat(\"\\nSxx is \",sum(df_4$x_barx2))\n\nbeta1 is  -0.1416667\nMSE is  0.0145625\nSxx is  180\n\n\n\ncat(\"95% 신뢰계수는 (\",beta1_4-qt(0.975,8)*sqrt(MSE_4/sum(df_4$x_barx2)),\"-\",beta1_4+qt(0.975,8)*sqrt(MSE_4/sum(df_4$x_barx2)),\") 이다.\")\n\n95% 신뢰계수는 ( -0.1624082 - -0.1209251 ) 이다.\n\n\n신뢰계수가 0을 포함하지 않는다. 신뢰구간에서 \\(\\beta_1\\)이 유의미함을 알 수 있다.\n\n\n두 타이어회사 A, B에서 생산되는 타이어를 비교하기 위하여 고속도로에서 트럭이 달리는 상황을 모의실험(simulated experiment)하여 다음의 데이터를 얻었다. \\(x\\)는 트럭이 달리는 속도이고 \\(y\\)는 타이어가 마모되기까지의 총 주행거리이다.\n\n\n\n\n\\(x_{1j}\\)\n10\n20\n30\n40\n50\n60\n70\n\n\n\n\n\\(y_{1j}(A)\\)\n9.8\n12.5\n14.9\n16.5\n22.4\n24.1\n25.8\n\n\n\\(y_{2j}(B)\\)\n15.0\n14.5\n16.5\n19.1\n22.3\n20.8\n22.4\n\n\n\n\n산점도를 그리시오.\n\nAnswer\n\ndf_5 = data.frame(x = c(10,20,30,40,50,60,70),\n                  yA = c(9.8,12.5,14.9,16.5,22.4,24.1,25.8),\n                  yB = c(15.0,14.5,16.5,19.1,22.3,20.8,22.4))\ndf_5\n\n\n\nA data.frame: 7 × 3\n\n    xyAyB\n    <dbl><dbl><dbl>\n\n\n    10 9.815.0\n    2012.514.5\n    3014.916.5\n    4016.519.1\n    5022.422.3\n    6024.120.8\n    7025.822.4\n\n\n\n\n\nplot(df_5$yA~df_5$x,\n     xlab = \"x\",\n     ylab = \"yA(orange),yB(blue)\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\npar(new=TRUE)\nplot(df_5$yB~df_5$x,\n     xlab='',\n     ylab='',\n     pch  = 16,\n     cex  = 1,\n     col  = \"blue\")\n\n\n\n\n\n각 회사별로 속도와 총주행거리 간의 회귀모형을 구한다면, 두 개의 직선이 동일하다고 볼수 있는가? 유의수준 \\(\\alpha = 0.05\\)로 가설검정하시오.\n\nAnswer\n\\(H_0: \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1: \\beta_{01} \\ne \\beta_{02} \\text{ or } \\beta_{11} \\ne \\beta_{12}\\)\n\ndf_5_A <- df_5\n\n\ndf_5_A$x_barx = df_5_A$x - mean(df_5_A$x)\ndf_5_A$yA_baryA = df_5_A$yA - mean(df_5_A$yA) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_A$x_barx2 <- df_5_A$x_barx^2\ndf_5_A$yA_baryA2 <- df_5_A$yA_baryA^2\ndf_5_A$xyA <-df_5_A$x_barx * df_5_A$yA_baryA\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_A\n\n\n\nA data.frame: 7 × 8\n\n    xyAyBx_barxyA_baryAx_barx2yA_baryA2xyA\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.815.0-30-8.290067.24246\n    2012.514.5-20-5.540030.25110\n    3014.916.5-10-3.1100 9.61 31\n    4016.519.1  0-1.5  0 2.25  0\n    5022.422.3 10 4.410019.36 44\n    6024.120.8 20 6.140037.21122\n    7025.822.4 30 7.890060.84234\n\n\n\n\n\nround(colSums(df_5_A),3)\n\nx280yA126yB130.6x_barx0yA_baryA0x_barx22800yA_baryA2226.76xyA787\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xyA <- as.numeric(colSums(df_5_A)[8]/colSums(df_5_A)[6])\nbeta0_xyA <- mean(df_5_A$yA) - beta1_xyA *  mean(df_5_A$x)\n\n\ncat(\"xyA hat beta0 = \", beta0_xyA)\ncat(\"\\nxyA hat beta1 = \", beta1_xyA)\n\nxyA hat beta0 =  6.757143\nxyA hat beta1 =  0.2810714\n\n\n\\(yA = 6.757143 + 0.2810714x\\)\n\nSST_A = sum((df_5_A$yA - mean(df_5_A$yA))^2)\n\n\nSSR_A = sum( ( ( 6.757143 + 0.2810714 *df_5_A$x)-mean(df_5_A$yA) )^2 )\n\n\nSSE_A = sum( ( df_5_A$yA-( 6.757143 + 0.2810714 *df_5_A$x))^2 )\n\n\ncat(\"yA SST = \", SST_A,\", df = 6\")\ncat(\"\\nyA SSR = \", SSR_A,\", df = 1\")\ncat(\"\\nyA SSE = \", SSE_A, \", df = 5\")\n\nyA SST =  226.76 , df = 6\nyA SSR =  221.2032 , df = 1\nyA SSE =  5.556786 , df = 5\n\n\n\ndf_5_B <- df_5\n\n\ndf_5_B$x_barx = df_5_B$x - mean(df_5_B$x)\ndf_5_B$yB_baryB = df_5_B$yB - mean(df_5_B$yB) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_B$x_barx2 <- df_5_B$x_barx^2\ndf_5_B$yB_baryB2 <- df_5_B$yB_baryB^2\ndf_5_B$xyB <-df_5_B$x_barx * df_5_B$yB_baryB\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_B\n\n\n\nA data.frame: 7 × 8\n\n    xyAyBx_barxyB_baryBx_barx2yB_baryB2xyB\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.815.0-30-3.657142990013.3746939109.71429\n    2012.514.5-20-4.157142940017.2818367 83.14286\n    3014.916.5-10-2.1571429100 4.6532653 21.57143\n    4016.519.1  0 0.4428571  0 0.1961224  0.00000\n    5022.422.3 10 3.642857110013.2704082 36.42857\n    6024.120.8 20 2.1428571400 4.5918367 42.85714\n    7025.822.4 30 3.742857190014.0089796112.28571\n\n\n\n\n\nround(colSums(df_5_B),3)\n\nx280yA126yB130.6x_barx0yB_baryB0x_barx22800yB_baryB267.377xyB406\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xyB <- as.numeric(colSums(df_5_B)[8]/colSums(df_5_B)[6])\nbeta0_xyB <- mean(df_5_B$yB) - beta1_xyB *  mean(df_5_B$x)\n\n\ncat(\"xyB hat beta0 = \", beta0_xyB)\ncat(\"\\nxyB hat beta1 = \", beta1_xyB)\n\nxyB hat beta0 =  12.85714\nxyB hat beta1 =  0.145\n\n\n\\(yB = 12.85714 + 0.145x\\)\n\nSST_B = sum((df_5_B$yB - mean(df_5_B$yB))^2)\n\n\nSSR_B = sum( ( ( 12.85714 + 0.145*df_5_B$x)-mean(df_5_B$yB) )^2 )\n\n\nSSE_B = sum( ( df_5_B$yB-( 12.85712 + 0.145 *df_5_B$x))^2 )\n\n\ncat(\"yB SST = \", SST_B,\", df = 6\")\ncat(\"\\nyB SSR = \", SSR_B,\", df = 1\")\ncat(\"\\nyB SSE = \", SSE_B, \", df = 5\")\n\nyB SST =  67.37714 , df = 6\nyB SSR =  58.87 , df = 1\nyB SSE =  8.507143 , df = 5\n\n\n\na <- df_5[,c(1,2)]\n\n\ncolnames(a) <- c('x','y')\n\n\nb <- df_5[,c(1,3)]\n\n\ncolnames(b) <- c('x','y')\n\n\ndf_5_AB <- rbind(a,b)\ndf_5_AB\n\n\n\nA data.frame: 14 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    10 9.8\n    2012.5\n    3014.9\n    4016.5\n    5022.4\n    6024.1\n    7025.8\n    1015.0\n    2014.5\n    3016.5\n    4019.1\n    5022.3\n    6020.8\n    7022.4\n\n\n\n\n\ndf_5_AB$x_barx = df_5_AB$x - mean(df_5_AB$x)\ndf_5_AB$y_bary = df_5_AB$y - mean(df_5_AB$y) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_5_AB$x_barx2 <- df_5_AB$x_barx^2\ndf_5_AB$y_bary2 <- df_5_AB$y_bary^2\ndf_5_AB$xy <-df_5_AB$x_barx * df_5_AB$y_bary\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_5_AB\n\n\n\nA data.frame: 14 × 7\n\n    xyx_barxy_baryx_barx2y_bary2xy\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10 9.8-30-8.528571490072.736531255.85714\n    2012.5-20-5.828571440033.972245116.57143\n    3014.9-10-3.428571410011.755102 34.28571\n    4016.5  0-1.8285714  0 3.343673  0.00000\n    5022.4 10 4.071428610016.576531 40.71429\n    6024.1 20 5.771428640033.309388115.42857\n    7025.8 30 7.471428690055.822245224.14286\n    1015.0-30-3.328571490011.079388 99.85714\n    2014.5-20-3.828571440014.657959 76.57143\n    3016.5-10-1.8285714100 3.343673 18.28571\n    4019.1  0 0.7714286  0 0.595102  0.00000\n    5022.3 10 3.971428610015.772245 39.71429\n    6020.8 20 2.4714286400 6.107959 49.42857\n    7022.4 30 4.071428690016.576531122.14286\n\n\n\n\n\nround(colSums(df_5_AB),3)\n\nx560y256.6x_barx0y_bary0x_barx25600y_bary2295.649xy1193\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_xy <- as.numeric(colSums(df_5_AB)[7]/colSums(df_5_AB)[5])\nbeta0_xy <- mean(df_5_AB$y) - beta1_xy *  mean(df_5_AB$x)\n\n\ncat(\"xy hat beta0 = \", beta0_xy)\ncat(\"\\nxy hat beta1 = \", beta1_xy)\n\nxy hat beta0 =  9.807143\nxy hat beta1 =  0.2130357\n\n\n\\(\\text{y} = 9.807143 +0.2130357\\text{x}\\)\n\nSST_5 = sum((df_5_AB$y - mean(df_5_AB$y))^2)\n\n\nSSR_5 = sum( ( (9.807143 + 0.2130357*df_5_AB$x)-mean(df_5_AB$y) )^2 )\n\n\nSSE_5 = sum( ( df_5_AB$y-(9.807143  +0.2130357 *df_5_AB$x))^2 )\n\n\ncat(\"y = \",round(beta0_xy,4), \"+ \",round(beta1_xy,4) ,\"x\")\n\ny =  9.8071 +  0.213 x\n\n\n\ncat(\"SST = \", SST_5,\", df = 13\")\ncat(\"\\nSSR = \", SSR_5,\", df = 1\")\ncat(\"\\nSSE = \", SSE_5, \", df = 12\")\n\nSST =  295.6486 , df = 13\nSSR =  254.1516 , df = 1\nSSE =  41.49696 , df = 12\n\n\n\ncat(\"yA = \",round(beta0_xyA,4), \"+ \",round(beta1_xyA,4) ,\"x\")\n\nyA =  6.7571 +  0.2811 x\n\n\n\ncat(\"yA SST = \", SST_A,\", df = 6\")\ncat(\"\\nyA SSR = \", SSR_A,\", df = 1\")\ncat(\"\\nyA SSE = \", SSE_A, \", df = 5\")\n\nyA SST =  226.76 , df = 6\nyA SSR =  221.2032 , df = 1\nyA SSE =  5.556786 , df = 5\n\n\n\ncat(\"yB = \",round(beta0_xyB,4), \"+ \",round(beta1_xyB,4) ,\"x\")\n\nyB =  12.8571 +  0.145 x\n\n\n\ncat(\"yB SST = \", SST_B,\", df = 6\")\ncat(\"\\nyB SSR = \", SSR_B,\", df = 1\")\ncat(\"\\nyB SSE = \", SSE_B, \", df = 5\")\n\nyB SST =  67.37714 , df = 6\nyB SSR =  58.87 , df = 1\nyB SSE =  8.507143 , df = 5\n\n\n가설\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1 : \\beta_{01} \\neq \\beta_{02} \\text{ or } \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(F_0 = \\frac{SSE(R) - SSE(F)}{df_R-df_F} \\times \\frac{df_F}{SSE(F)}\\)\n\nSSE_5_F = SSE_A + SSE_B\ndf_5_F = 5 -2 + 5 -2\n\n\nSSE_5_R = SSE_5\ndf_5_R = 5 -1 + 5 -1\n\n\nF_5_0 = (SSE_5_R - SSE_5_F)/(df_5_R - df_5_F) / (SSE_5_F/df_5_F)\nF_5_0\n\n5.8517864828756\n\n\n\ndf_5_R - df_5_F\n\n2\n\n\n\ndf_5_F\n\n6\n\n\n\nF_5_stan = qf(0.95,2,10)\nF_5_stan\n\n4.1028210151304\n\n\n\ncat(F_5_0, \" 는 유의수준 0.05에서 F값 \", F_5_stan , \" 보다 크다.\")\n\ncat(\"\\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\")\n\ncat(\"\\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\")\n\n5.851786  는 유의수준 0.05에서 F값  4.102821  보다 크다.\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\n\n\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\) 기각\n\n관심의 대상이 \\(x\\)가 증가함에 따라 \\(y\\) 가 얼마나 증가하는가에 있다. 두 회사의 타이어에 대하여 각각 회귀모형을 적합했을 때, 기울기가 같은지 유의수준 5%로 검정하시오.\n\nAnswer\n기울기 비교에 대한 가설\n\\(H_0 : \\beta_{11} = \\beta_{12} \\text{ vs. } H_1 : \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(t_0 = \\frac{ \\hat{\\beta}_{11} - \\hat{\\beta}_{12} }{ \\sqrt{ \\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) } }\\)\n\\(\\text{Degree of Freedom} = t((n_1 - 1) + (n_2 - 1))\\)\n\\(\\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) = MSE(F) [\\frac{1}{\\sum(x_{1j} - \\bar{x}_1)^2} + \\frac{1}{\\sum(x_{2j} - \\bar{x}_2)^2}]\\)\n\nround(beta1_xyA,4)\n\n0.2811\n\n\n\nround(beta1_xyB,4)\n\n0.145\n\n\n\nSSE_5_F\n\n14.063928575095\n\n\n\nMSE_5_F = SSE_5_F / df_5_F\nMSE_5_F\n\n2.34398809584917\n\n\n\nsum(df_5_A$x_barx2)\n\n2800\n\n\n\nsum(df_5_B$x_barx2)\n\n2800\n\n\n\nvar_5_diff = MSE_5_F * (1/sum(df_5_A$x_barx2) + 1/sum(df_5_B$x_barx2))\nvar_5_diff\n\n0.00167427721132084\n\n\n\nt_5_0 = (beta1_xyA - beta1_xyB)/sqrt(var_5_diff)\nt_5_0\n\n3.32547173820247\n\n\n\nqt(0.95,df_5_F)\n\n1.9431802805153\n\n\n\ncat(t_5_0,\"는 \",qt(0.95,df_5_F),\"보다 크다. 따라서 유의수준 5%에서 귀무가설을 기각하여 두 회귀모형의 기울기가 다르다고 할 수 있다.\")\n\n3.325472 는  1.94318 보다 크다. 따라서 유의수준 5%에서 귀무가설을 기각하여 두 회귀모형의 기울기가 다르다고 할 수 있다.\n\n\n\\(H_0 : \\beta_{11} = \\beta_{12}\\) 기각\n\n\nR 실습. 아마존 강 수위 문제 아마존 강 유역은 지구상의 가장 큰 열대림 지역이지만 대부분의 다른 자연자원과 마찬가지로 개발의 손길이 미치면서 열대림이 급속히 파괴됐다. 1970년대 이후 아마존 상류지역에 도로가 건설되면서 인구가 빠르게 증가되었고 대규모의 삼림파괴가 이뤄졌다. 강수량과 유수량이 모두 영향을 받을 수 있기 때문에 이것은 결국 아마존 강 전체에 영향을 미치는 심각한 기후학적 및 수문학적 변화를 가져왔다. 다음의 표는 페루 이키토스(Iquitos)에서 1962년부터 1978년까지 기록한 아마존 강 최고수위 (High)와, 최저수위 (Low)를 기록한 것이다(단위: 미터).1962년부터 1969년까지의 데이터는 개발 이전에 수집된 데이터이고, 1970년부터 1978년까지의 데이터는 개발이후에 관측된 데이터를 나타낸다. 이 데이터는 아마존 상류지역의 삼림파괴가 아마존 유역의 강 수위에 변화를 일으켰는지 분석하고자 한다. 우리의 관심은 시간에 따른 아마존 강 수위 변화여부이다. 예를 들어, 우리가 다음을 적합한다면\n\n\\[\\text{High} = \\beta_0 + \\beta_1 \\times \\text{Year} + \\epsilon\\]\n\n\\(\\beta_1 = 0\\)은 시간에 따른 아마존 강의 최고수위에 아무런 (선형)변화가 없다는 것을 의미하고,\n\\(\\beta_1 > 0\\)은 아마존 강의 최고수위가 증가된 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 늘어난 것을 나타낼 수 있다.\n\\(\\beta_1 < 0\\)은 시간에 따라 아마존 강의 최고수위가 낮아진 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 줄어든 것을 의미한다. 다음의 물음에 답하시오.\n\n\\[\\text{Table 1: 아마존 강 데이터 (Amazon River data)}\\]\n\n\n\nYear\nHigh(m)\nLow(m)\n\n\n\n\n1962\n25.82\n18.24\n\n\n1963\n25.35\n16.50\n\n\n1964\n24.29\n20.26\n\n\n1965\n24.05\n20.97\n\n\n1966\n24.89\n19.43\n\n\n1967\n25.35\n19.31\n\n\n1968\n25.23\n20.85\n\n\n1969\n25.06\n19.54\n\n\n1970\n27.13\n20.49\n\n\n1971\n27.36\n21.91\n\n\n1972\n26.65\n22.51\n\n\n1973\n27.13\n18.81\n\n\n1974\n27.49\n19.42\n\n\n1975\n27.08\n19.10\n\n\n1976\n27.51\n18.80\n\n\n1977\n27.54\n18.80\n\n\n1978\n26.21\n17.57\n\n\n\n\nHigh와 Year, Low와 Year, 그리고 High와 Low에 대해 산점도를 그리시오.\n\nAnswer\n\ndf_6 = data.frame(Year = c(1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978),\n           High = c(25.82, 25.35, 24.29, 24.05, 24.89, 25.35, 25.23, 25.06, 27.13, 27.36, 26.65, 27.13, 27.49, 27.08, 27.51, 27.54, 26.21),\n           Low = c(18.24, 16.50, 20.26, 20.97, 19.43, 19.31, 20.85, 19.54, 20.49, 21.91, 22.51, 18.81, 19.42, 19.10, 18.80, 18.80, 17.57))\ndf_6\n\n\n\nA data.frame: 17 × 3\n\n    YearHighLow\n    <dbl><dbl><dbl>\n\n\n    196225.8218.24\n    196325.3516.50\n    196424.2920.26\n    196524.0520.97\n    196624.8919.43\n    196725.3519.31\n    196825.2320.85\n    196925.0619.54\n    197027.1320.49\n    197127.3621.91\n    197226.6522.51\n    197327.1318.81\n    197427.4919.42\n    197527.0819.10\n    197627.5118.80\n    197727.5418.80\n    197826.2117.57\n\n\n\n\n\nplot(df_6$High,df_6$Year,\n     xlab = \"Year\",\n     ylab = \"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n양의 기울기로 선형 관계를 갖는 모습이다.\n\nplot(df_6$Low~df_6$Year,\n     xlab = \"Year\",\n     ylab = \"Low\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n이차함수 모양의 연관이 있는 모양새이다.\n\nplot(df_6$Low~df_6$High,\n    xlab = \"Low\",\n     ylab = \"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n관련이 있는지 모르겠다.\n\nYear에 대한 High, Year에 대한 Low, 그리고 Low에 대한 High의 회귀모형을 구하시오. 3개 회귀모형의 결과를 요약하고, 각 모형별로 회귀계수의 의미를 설명하시오.\n\nAnswer\nYear에 대한 High의 회귀모형\n\ndf_6_YearHigh <- df_6\n\n\ndf_6_YearHigh$Year_barYear = df_6_YearHigh$Year - mean(df_6_YearHigh$Year)\ndf_6_YearHigh$High_barHigh = df_6_YearHigh$High - mean(df_6_YearHigh$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_YearHigh$Year_barYear2 <- df_6_YearHigh$Year_barYear^2\ndf_6_YearHigh$High_barHigh2 <- df_6_YearHigh$High_barHigh^2\ndf_6_YearHigh$YearHigh <-df_6_YearHigh$Year_barYear * df_6_YearHigh$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_YearHigh\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowYear_barYearHigh_barHighYear_barYear2High_barHigh2YearHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-8-0.30588235640.093564014 2.4470588\n    196325.3516.50-7-0.77588235490.601993426 5.4311765\n    196424.2920.26-6-1.83588235363.37046401411.0152941\n    196524.0520.97-5-2.07588235254.30928754310.3794118\n    196624.8919.43-4-1.23588235161.527405190 4.9435294\n    196725.3519.31-3-0.77588235 90.601993426 2.3276471\n    196825.2320.85-2-0.89588235 40.802605190 1.7917647\n    196925.0619.54-1-1.06588235 11.136105190 1.0658824\n    197027.1320.49 0 1.00411765 01.008252249 0.0000000\n    197127.3621.91 1 1.23411765 11.523046367 1.2341176\n    197226.6522.51 2 0.52411765 40.274699308 1.0482353\n    197327.1318.81 3 1.00411765 91.008252249 3.0123529\n    197427.4919.42 4 1.36411765161.860816955 5.4564706\n    197527.0819.10 5 0.95411765250.910340484 4.7705882\n    197627.5118.80 6 1.38411765361.915781661 8.3047059\n    197727.5418.80 7 1.41411765491.999728720 9.8988235\n    197826.2117.57 8 0.08411765640.007075779 0.6729412\n\n\n\n\n\nround(colSums(df_6_YearHigh),3)\n\nYear33490High444.14Low332.51Year_barYear0High_barHigh0Year_barYear2408High_barHigh222.951YearHigh73.8\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_YearHigh <- as.numeric(colSums(df_6_YearHigh)[8]/colSums(df_6_YearHigh)[6])\nbeta0_YearHigh <- mean(df_6_YearHigh$High) - beta1_YearHigh *  mean(df_6_YearHigh$Year)\n\n\ncat(\"YearHigh hat beta0 = \", beta0_YearHigh)\ncat(\"\\nYearHigh hat beta1 = \", beta1_YearHigh)\n\nYearHigh hat beta0 =  -330.2124\nYearHigh hat beta1 =  0.1808824\n\n\n\\(\\text{High} = -330.21235 + 0.18088\\text{Year}\\)\nR결과와 비교\n\nsummary(lm(df_6$High~df_6$Year))\n\n\nCall:\nlm(formula = df_6$High ~ df_6$Year)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3629 -0.5341  0.1479  0.4903  1.1412 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -330.21235   78.03319  -4.232 0.000725 ***\ndf_6$Year      0.18088    0.03961   4.567 0.000371 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8001 on 15 degrees of freedom\nMultiple R-squared:  0.5816,    Adjusted R-squared:  0.5537 \nF-statistic: 20.85 on 1 and 15 DF,  p-value: 0.0003708\n\n\nR결과 해석 - beta0과 beta1이 5%보다 유의확률이 작아 유의미하다. - 모형의 설명력은 50%정도에 머문다. - 모형의 p값이 0.05보다 작아 유의미하다고 볼 수 있다.\nYear에 대한 Low의 회귀모형\n\ndf_6_YearLow <- df_6\n\n\ndf_6_YearLow$Year_barYear = df_6_YearLow$Year - mean(df_6_YearLow$Year)\ndf_6_YearLow$Low_barLow = df_6_YearLow$Low - mean(df_6_YearLow$Low) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_YearLow$Year_barYear2 <- df_6_YearLow$Year_barYear^2\ndf_6_YearLow$Low_barLow2 <- df_6_YearLow$Low_barLow^2\ndf_6_YearLow$YearLow <-df_6_YearLow$Year_barYear * df_6_YearLow$Low_barLow\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_YearLow\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowYear_barYearLow_barLowYear_barYear2Low_barLow2YearLow\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-8-1.31941176641.7408474048 10.55529412\n    196325.3516.50-7-3.05941176499.3600003460 21.41588235\n    196424.2920.26-6 0.70058824360.4908238754 -4.20352941\n    196524.0520.97-5 1.41058824251.9897591696 -7.05294118\n    196624.8919.43-4-0.12941176160.0167474048  0.51764706\n    196725.3519.31-3-0.24941176 90.0622062284  0.74823529\n    196825.2320.85-2 1.29058824 41.6656179931 -2.58117647\n    196925.0619.54-1-0.01941176 10.0003768166  0.01941176\n    197027.1320.49 0 0.93058824 00.8659944637  0.00000000\n    197127.3621.91 1 2.35058824 15.5252650519  2.35058824\n    197226.6522.51 2 2.95058824 48.7059709343  5.90117647\n    197327.1318.81 3-0.74941176 90.5616179931 -2.24823529\n    197427.4919.42 4-0.13941176160.0194356401 -0.55764706\n    197527.0819.10 5-0.45941176250.2110591696 -2.29705882\n    197627.5118.80 6-0.75941176360.5767062284 -4.55647059\n    197727.5418.80 7-0.75941176490.5767062284 -5.31588235\n    197826.2117.57 8-1.98941176643.9577591696-15.91529412\n\n\n\n\n\nround(colSums(df_6_YearLow),3)\n\nYear33490High444.14Low332.51Year_barYear0Low_barLow0Year_barYear2408Low_barLow236.327YearLow-3.22\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_YearLow <- as.numeric(colSums(df_6_YearLow)[8]/colSums(df_6_YearLow)[6])\nbeta0_YearLow <- mean(df_6_YearLow$Low) - beta1_YearLow *  mean(df_6_YearLow$Year)\n\n\ncat(\"YearLow hat beta0 = \", beta0_YearLow)\ncat(\"\\nYearLow hat beta1 = \", beta1_YearLow)\n\nYearLow hat beta0 =  35.10696\nYearLow hat beta1 =  -0.007892157\n\n\n\\(\\text{Low} = 35.106961 -0.007892\\text{Year}\\)\nR결과와 비교\n\nsummary(lm(df_6$Low~df_6$Year))\n\n\nCall:\nlm(formula = df_6$Low ~ df_6$Year)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1147 -0.7121 -0.1610  0.9306  2.9664 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  35.106961 151.723912   0.231     0.82\ndf_6$Year    -0.007892   0.077017  -0.102     0.92\n\nResidual standard error: 1.556 on 15 degrees of freedom\nMultiple R-squared:  0.0006996, Adjusted R-squared:  -0.06592 \nF-statistic: 0.0105 on 1 and 15 DF,  p-value: 0.9197\n\n\nR결과 해석 - beta0과 beta1이 5%보다 유의확률이 커 유의미하지 않다. - 모형의 설명력은 굉장히 낮았다. - 모형의 p값이 0.05보다 커 유의미하지 않다.\nLow에 대한 High의 회귀모형\n\ndf_6_LowHigh <- df_6\n\n\ndf_6_LowHigh$Low_barLow = df_6_LowHigh$Low - mean(df_6_LowHigh$Low)\ndf_6_LowHigh$High_barHigh = df_6_LowHigh$High - mean(df_6_LowHigh$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_LowHigh$Low_barLow2 <- df_6_LowHigh$Low_barLow^2\ndf_6_LowHigh$High_barHigh2 <- df_6_LowHigh$High_barHigh^2\ndf_6_LowHigh$LowHigh <-df_6_LowHigh$Low_barLow * df_6_LowHigh$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndf_6_LowHigh\n\n\n\nA data.frame: 17 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    196225.8218.24-1.31941176-0.305882351.74084740480.093564014 0.40358478\n    196325.3516.50-3.05941176-0.775882359.36000034600.601993426 2.37374360\n    196424.2920.26 0.70058824-1.835882350.49082387543.370464014-1.28619758\n    196524.0520.97 1.41058824-2.075882351.98975916964.309287543-2.92821522\n    196624.8919.43-0.12941176-1.235882350.01674740481.527405190 0.15993772\n    196725.3519.31-0.24941176-0.775882350.06220622840.601993426 0.19351419\n    196825.2320.85 1.29058824-0.895882351.66561799310.802605190-1.15621522\n    196925.0619.54-0.01941176-1.065882350.00037681661.136105190 0.02069066\n    197027.1320.49 0.93058824 1.004117650.86599446371.008252249 0.93442007\n    197127.3621.91 2.35058824 1.234117655.52526505191.523046367 2.90090242\n    197226.6522.51 2.95058824 0.524117658.70597093430.274699308 1.54645536\n    197327.1318.81-0.74941176 1.004117650.56161799311.008252249-0.75249758\n    197427.4919.42-0.13941176 1.364117650.01943564011.860816955-0.19017405\n    197527.0819.10-0.45941176 0.954117650.21105916960.910340484-0.43833287\n    197627.5118.80-0.75941176 1.384117650.57670622841.915781661-1.05111522\n    197727.5418.80-0.75941176 1.414117650.57670622841.999728720-1.07389758\n    197826.2117.57-1.98941176 0.084117653.95775916960.007075779-0.16734464\n\n\n\n\n\nround(colSums(df_6_LowHigh),3)\n\nYear33490High444.14Low332.51Low_barLow0High_barHigh0Low_barLow236.327High_barHigh222.951LowHigh-0.511\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1_LowHigh <- as.numeric(colSums(df_6_LowHigh)[8]/colSums(df_6_LowHigh)[6])\nbeta0_LowHigh <- mean(df_6_LowHigh$High) - beta1_LowHigh *  mean(df_6_LowHigh$Low)\n\n\ncat(\"LowHigh hat beta0 = \", beta0_LowHigh)\ncat(\"\\nLowHigh hat beta1 = \", beta1_LowHigh)\n\nLowHigh hat beta0 =  26.40088\nLowHigh hat beta1 =  -0.01405959\n\n\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\)\nR결과와 비교\n\nsummary(lm(df_6$Low~df_6$High))\n\n\nCall:\nlm(formula = df_6$Low ~ df_6$High)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0767 -0.7279 -0.1569  0.9529  2.9623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 20.14079    8.49368   2.371   0.0315 *\ndf_6$High   -0.02225    0.32478  -0.069   0.9463  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.556 on 15 degrees of freedom\nMultiple R-squared:  0.0003129, Adjusted R-squared:  -0.06633 \nF-statistic: 0.004695 on 1 and 15 DF,  p-value: 0.9463\n\n\nR결과 해석 - beta0은 유의미 하지만 beta1이 5%보다 유의확률이 커 유의미하지 않다. - 모형의 설명력은 굉장히 낮았다. - 모형의 p값이 0.05보다 커 유의미하지 않다.\n모형 정리\n\\(\\text{Low} = 35.106961 -0.007892\\text{Year}\\) 모형의 의미 - \\(\\beta_0 = 35.106961\\): 해당 모형에서 시간에 영향을 받지 않았을때의 아마존 강의 최저 수위가 35.106961m이다. - \\(\\beta_1 = -0.007892\\): 아마존 강의 최저수위가 0.007892m만큼 감소한 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 감소하는 것을 의미하지만, 0에 가까운 값으로서 영향이 미세해 보인다.\n\\(\\text{High} = -330.21235 + 0.18088\\text{Year}\\) 모형의 의미 - \\(\\beta_0 = -330.21235\\): 해당 모형에서 시간에 영향을 받지 않았을때의 아마존 강의 최고 수위가 -330.21235m이다. - \\(\\beta_1 = 0.18088\\): 아마존 강의 최고수위가 0.18088m만큼 증가된 것을 의미하는데, 이것은 해마다 아마존 강의 흐르는 물이 0.18088m 늘어난 것을 나타낼 수 있다. 이 모형에서도 영향이 크게 끼치지 않는 것 같다.\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\) 모형의 의미 - \\(\\beta_0 = -26.40088\\): 해당 모형에서 최저수위의 영향을 받지 않았을 때의 아마존 강의 최고수위가 026.40088m 라는 것을 의미한다. - \\(\\beta_1 = -0.014061\\): 아마존 강의 최고수위가 최저수위가 1m 증가함에 따라 -0.0014061m 만큼 감소함을 의미한다. 아마존 강의 최저수위는 최고수위에 미치는 영향이 미미해 보인다.\n\n이 자료를 근거로 우리는 삼림파괴가 아마존 강 수위의 변화를 일으킨다고 할 수 있는가?\n\nAnswer\n\n1970년대 이후 삼림파괴가 이루어졌다. 구간을 나누어 보지 않는 이상 삼림파괴가 아마존 강 수위의 변화를 일으켰다는 판단은 섣불러 보인다.\n모형만 봐도 시간과 아마존 강의 최고수위 및 최저수위의 영향과 아마존 강의 최저수위 및 최고수위 간의 영향이 거의 없어보인다. 모두 1도 넘지 않았기도 하다.\n따라서 삼림파괴가 아마존 강 수위의 변화를 일으켰다고 2번의 근거로는 할 수 없다.\n\n\n아마존강의 최저수위와 최고수위와의 산점도를 1960년대, 1970년대 자료별로 다르게 그리고, 각각의 회귀선을 적합하시오.\n\nAnswer\n1960년대 아마존강의 최저수위와 최고수위와의 산점도\n\nplot(df_6$High[df_6$Year<1970]~df_6$Low[df_6$Year<1970],\n     xlab = \"Low\",\n     ylab =\"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n1960년대 아마존강의 최저수위와 최고수위와의 회귀선\n\ndf_6_1960 <- df_6[df_6$Year<1970,]\n\n\ndf_6_1960$Low_barLow = df_6_1960$Low - mean(df_6_1960$Low)\ndf_6_1960$High_barHigh = df_6_1960$High - mean(df_6_1960$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_1960$Low_barLow2 <- df_6_1960$Low_barLow^2\ndf_6_1960$High_barHigh2 <- df_6_1960$High_barHigh^2\ndf_6_1960$LowHigh <-df_6_1960$Low_barLow * df_6_1960$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\nround(colSums(df_6_1960),3)\n\nYear15724High200.04Low155.1Low_barLow0High_barHigh0Low_barLow215.09High_barHigh22.392LowHigh-3.761\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\ndf_6_1960\n\n\n\nA data.frame: 8 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1196225.8218.24-1.1475 0.8151.316756250.664225-0.9352125\n    2196325.3516.50-2.8875 0.3458.337656250.119025-0.9961875\n    3196424.2920.26 0.8725-0.7150.761256250.511225-0.6238375\n    4196524.0520.97 1.5825-0.9552.504306250.912025-1.5112875\n    5196624.8919.43 0.0425-0.1150.001806250.013225-0.0048875\n    6196725.3519.31-0.0775 0.3450.006006250.119025-0.0267375\n    7196825.2320.85 1.4625 0.2252.138906250.050625 0.3290625\n    8196925.0619.54 0.1525 0.0550.023256250.003025 0.0083875\n\n\n\n\n\nbeta1_1960 <- as.numeric(colSums(df_6_1960)[8]/colSums(df_6_1960)[6])\nbeta0_1960 <- mean(df_6_1960$High) - beta1_1960 *  mean(df_6_1960$Low)\n\n\ncat(\"hat beta0 1960 = \", round(beta0_1960,4))\ncat(\"\\nhat beta1 1960 = \", round(beta1_1960,4))\n\nhat beta0 1960 =  29.8367\nhat beta1 1960 =  -0.2492\n\n\n\ncat(\"회귀선은 다음과 같았다. 1960 High = \",round(beta0_1960,4),\" + \", round(beta1_1960,4),\"Low\")\n\n회귀선은 다음과 같았다. 1960 High =  29.8367  +  -0.2492 Low\n\n\nR결과 비교\n\nsummary(lm(df_6_1960$High~df_6_1960$Low))\n\n\nCall:\nlm(formula = df_6_1960$High ~ df_6_1960$Low)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5606 -0.4053 -0.0057  0.3765  0.5895 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    29.8367     2.4640  12.109 1.93e-05 ***\ndf_6_1960$Low  -0.2492     0.1268  -1.966   0.0969 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4925 on 6 degrees of freedom\nMultiple R-squared:  0.3918,    Adjusted R-squared:  0.2904 \nF-statistic: 3.864 on 1 and 6 DF,  p-value: 0.09691\n\n\n\nSST_1960 = sum((df_6_1960$High - mean(df_6_1960$High))^2)\n\n\nSSR_1960 = sum( ( (29.8367  +  -0.2492*df_6_1960$Low)-mean(df_6_1960$High) )^2 )\n\n\nSSE_1960 = sum( ( df_6_1960$High-(29.8367  +  -0.2492*df_6_1960$Low))^2 )\n\n\ncat(\"1960 SST = \", SST_1960,\", df = 7\")\ncat(\"\\n1960 SSR = \", SSR_1960,\", df = 1\")\ncat(\"\\n1960 SSE = \", SSE_1960, \", df = 6\")\n\n1960 SST =  2.3924 , df = 7\n1960 SSR =  0.9370965 , df = 1\n1960 SSE =  1.455164 , df = 6\n\n\nR결과 비교\n\nanova(lm(df_6_1960$High~df_6_1960$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6_1960$Low10.93723730.93723733.8644640.09690958\n    Residuals61.45516270.2425271      NA        NA\n\n\n\n\n1970년대 아마존강의 최저수위와 최고수위와의 산점도\n\nplot(df_6$High[df_6$Year>=1970]~df_6$Low[df_6$Year>=1970],\n     xlab = \"Low\",\n     ylab =\"High\",\n     pch  = 16,\n     cex  = 1)\n\n\n\n\n1970년대 아마존강의 최저수위와 최고수위와의 회귀선\n\ndf_6_1970 <- df_6[df_6$Year>=1970,]\n\n\ndf_6_1970$Low_barLow = df_6_1970$Low - mean(df_6_1970$Low)\ndf_6_1970$High_barHigh = df_6_1970$High - mean(df_6_1970$High) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndf_6_1970$Low_barLow2 <- df_6_1970$Low_barLow^2\ndf_6_1970$High_barHigh2 <- df_6_1970$High_barHigh^2\ndf_6_1970$LowHigh <-df_6_1970$Low_barLow * df_6_1970$High_barHigh\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\nround(colSums(df_6_1970),3)\n\nYear17766High244.1Low177.41Low_barLow0High_barHigh0Low_barLow220.79High_barHigh21.574LowHigh0.338\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\ndf_6_1970\n\n\n\nA data.frame: 9 × 8\n\n    YearHighLowLow_barLowHigh_barHighLow_barLow2High_barHigh2LowHigh\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    9197027.1320.49 0.7777778 0.0077777780.604938276.049383e-05 0.006049383\n    10197127.3621.91 2.1977778 0.2377777784.830227165.653827e-02 0.522582716\n    11197226.6522.51 2.7977778-0.4722222227.827560492.229938e-01-1.321172840\n    12197327.1318.81-0.9022222 0.0077777780.814004946.049383e-05-0.007017284\n    13197427.4919.42-0.2922222 0.3677777780.085393831.352605e-01-0.107472840\n    14197527.0819.10-0.6122222-0.0422222220.374816051.782716e-03 0.025849383\n    15197627.5118.80-0.9122222 0.3877777780.832149381.503716e-01-0.353739506\n    16197727.5418.80-0.9122222 0.4177777780.832149381.745383e-01-0.381106173\n    17197826.2117.57-2.1422222-0.9122222224.589116058.321494e-01 1.954182716\n\n\n\n\n\nbeta1_1970 <- as.numeric(colSums(df_6_1970)[8]/colSums(df_6_1970)[6])\nbeta0_1970 <- mean(df_6_1970$High) - beta1_1970 *  mean(df_6_1970$Low)\n\n\ncat(\"hat beta0 1970 = \", round(beta0_1970,4))\ncat(\"\\nhat beta1 1970 = \", round(beta1_1970,4))\n\nhat beta0 1970 =  26.8016\nhat beta1 1970 =  0.0163\n\n\nR결과 비교\n\nsummary(lm(df_6_1970$High~df_6_1970$Low))\n\n\nCall:\nlm(formula = df_6_1970$High ~ df_6_1970$Low)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.87738 -0.03226  0.02245  0.37253  0.43262 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   26.80160    2.05235  13.059  3.6e-06 ***\ndf_6_1970$Low  0.01627    0.10381   0.157     0.88    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4733 on 7 degrees of freedom\nMultiple R-squared:  0.003495,  Adjusted R-squared:  -0.1389 \nF-statistic: 0.02455 on 1 and 7 DF,  p-value: 0.8799\n\n\n\ncat(\"회귀선은 다음과 같았다. 1970 High = \",round(beta0_1970,4),\" + \", round(beta1_1970,4),\"Low\")\n\n회귀선은 다음과 같았다. 1970 High =  26.8016  +  0.0163 Low\n\n\n\nSST_1970 = sum((df_6_1970$High - mean(df_6_1970$High))^2)\n\n\nSSR_1970 = sum( ( (26.8016  +  0.0163 *df_6_1970$Low)-mean(df_6_1970$High) )^2 )\n\n\nSSE_1970 = sum( ( df_6_1970$High-(26.8016  +  0.0163 *df_6_1970$Low))^2 )\n\n\ncat(\"1970 SST = \", SST_1970,\", df = 8\")\ncat(\"\\n1970 SSR = \", SSR_1970,\", df = 1\")\ncat(\"\\n1970 SSE = \", SSE_1970, \", df = 7\")\n\n1970 SST =  1.573756 , df = 8\n1970 SSR =  0.005528037 , df = 1\n1970 SSE =  1.56826 , df = 7\n\n\nR결과 비교\n\nanova(lm(df_6_1970$High~df_6_1970$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6_1970$Low10.0055001070.0055001070.024550050.8799168\n    Residuals71.5682554490.224036493        NA       NA\n\n\n\n\n\n아마존강의 최저수위와 최고수위와의 관계가 1960년대와 1970년대에 따라 차이가 있는가? 두 회귀모형의 동일성 여부를 유의수준 \\(\\alpha = 0.01\\)에서 검정하시오.\n\nAnswer\n가설\n\\(H_0 : \\beta_{01} = \\beta_{02} \\text{ and } \\beta_{11} = \\beta_{12}\\)\n\\(H_1 : \\beta_{01} \\neq \\beta_{02} \\text{ pr } \\beta_{11} \\neq \\beta_{12}\\)\n(2)에서 구했던 것\n\\(\\text{High} = 26.40088 -0.01406\\text{Low}\\)\n\nSST = sum((df_6$High - mean(df_6$High))^2)\n\n\nSSR = sum( ( (26.40088 - 0.01406 *df_6$Low)-mean(df_6$High) )^2 )\n\n\nSSE = sum( ( df_6$High-(26.40088 - 0.01406 *df_6$Low))^2 )\n\n\ncat(\"SST = \", SST,\", df = 16\")\ncat(\"\\nSSR = \", SSR,\", df = 1\")\ncat(\"\\nSSE = \", SSE, \", df = 15\")\n\nSST =  22.95141 , df = 16\nSSR =  0.007181232 , df = 1\nSSE =  22.94423 , df = 15\n\n\nR결과와 비교\n\nanova(lm(df_6$High~df_6$Low))\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    df_6$Low 1 0.0071808110.0071808110.004694520.9462794\n    Residuals1522.9442309541.529615397        NA       NA\n\n\n\n\n\\(\\text{1960 High} = 29.8367 + -0.2492\\text{Low}\\)\n\ncat(\"1960 SST = \", SST_1960,\", df = 7\")\ncat(\"\\n1960 SSR = \", SSR_1960,\", df = 1\")\ncat(\"\\n1960 SSE = \", SSE_1960, \", df = 6\")\n\n1960 SST =  2.3924 , df = 7\n1960 SSR =  0.9370965 , df = 1\n1960 SSE =  1.455164 , df = 6\n\n\n\\(\\text{1970 High} = 26.8016 + 0.0163\\text{Low}\\)\n\ncat(\"1970 SST = \", SST_1970,\", df = 8\")\ncat(\"\\n1970 SSR = \", SSR_1970,\", df = 1\")\ncat(\"\\n1970 SSE = \", SSE_1970, \", df = 7\")\n\n1970 SST =  1.573756 , df = 8\n1970 SSR =  0.005528037 , df = 1\n1970 SSE =  1.56826 , df = 7\n\n\n검정통계량\n\\(F_0 = \\frac{SSE(R) - SSE(F)}{df_R} - \\times \\frac{df_F}{SSE(F)}\\)\n\nSSE_F = SSE_1960 + SSE_1970\ndf_F = 6 + 7\n\n\nSSE_R = SSE\ndf_R = 15\n\n\nF_0 = (SSE_R - SSE_F)/(df_R - df_F) / (SSE_F/df_F)\nF_0\n\n42.8273639841797\n\n\n\ndf_R - df_F\n\n2\n\n\n\ndf_F\n\n13\n\n\n\nF_stan = qf(0.95,2,13)\n\n\ncat(F_0, \" 는 유의수준 0.05에서 F값 \", F_stan , \" 보다 크다.\")\n\ncat(\"\\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\")\n\ncat(\"\\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\")\n\n42.82736  는 유의수준 0.05에서 F값  3.805565  보다 크다.\n따라서 귀무가설을 기각하였고, 두 회귀모형은 beta0가 다르거나\n혹은 beta1이 다르거나 혹은 beta0,beta1 모두가 다르다.\n\n\n\n(4)에서 구한 두 회귀모형의 기울기가 같은지 유의수준 \\(\\alpha = 0.01\\)에서 검정하시오.\n\nAnswer\n기울기 비교에 대한 가설\n\\(H_0 : \\beta_{11} = \\beta_{12} \\text{ vs. } H_1 : \\beta_{11} \\neq \\beta_{12}\\)\n검정통계량\n\\(t_0 = \\frac{ \\hat{\\beta}_{11} - \\hat{\\beta}_{12} }{ \\sqrt{ \\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) } }\\)\n\\(\\text{Degree of Freedom} = t((n_1 - 1) + (n_2 - 1))\\)\n\\(\\hat{Var}( \\hat{\\beta}_{11} - \\hat{\\beta}_{12} ) = MSE(F) [\\frac{1}{\\sum(x_{1j} - \\bar{x}_1)^2} + \\frac{1}{\\sum(x_{2j} - \\bar{x}_2)^2}]\\)\n\nround(beta1_1960,4)\n\n-0.2492\n\n\n\nround(beta1_1970,4)\n\n0.0163\n\n\n\nSSE_F\n\n3.02342329210101\n\n\n\nMSE_F = SSE_F / df_F\nMSE_F\n\n0.232571022469308\n\n\n\nsum(df_6_1960$Low_barLow2)\n\n15.08995\n\n\n\nsum(df_6_1970$Low_barLow2)\n\n20.7903555555556\n\n\n\nvar_diff = MSE_F * (1/sum(df_6_1960$Low_barLow2) + 1/sum(df_6_1970$Low_barLow2))\nvar_diff\n\n0.026598798385161\n\n\n\nt_0 = (beta1_1960 - beta1_1970)/sqrt(var_diff)\nt_0\n\n-1.62782282241728\n\n\n\nqt(0.995,df_F)\n\n3.01227583871658\n\n\n\ncat(t_0,\"는 \",qt(0.995,df_F),\"보다 작다.\")\n\ncat(\"\\n따라서 유의수준 1%에서 귀무가설을 기각하지 못하여 두 회귀모형의 기울기가 같다고 할 수 있다.\")\n\n-1.627823 는  3.012276 보다 작다.\n따라서 유의수준 1%에서 귀무가설을 기각하지 못하여 두 회귀모형의 기울기가 같다고 할 수 있다.\n\n\n\\(H_0 : \\beta_{11} = \\beta_{12}\\) 채택"
  },
  {
    "objectID": "posts/rl/index.html",
    "href": "posts/rl/index.html",
    "title": "Advanced Regression Analysis",
    "section": "",
    "text": "This is posts of Advanced Regression Analysis."
  },
  {
    "objectID": "posts/rl/2022-11-21-rl-HW3.html",
    "href": "posts/rl/2022-11-21-rl-HW3.html",
    "title": "Regression HW 3",
    "section": "",
    "text": "고급회귀분석 과제, CH03,07\n\n고급회귀분석 세번째 과제입니다.\n제출 기한 : 11월 21일\n제출 방법\n\n직접 제출(607호) 도 가능하지만,\n문서 작성 후 pdf로 변환(★★★)하여 lms에 제출을 추천\n\n(pdf 아닌 문서는 미제출로 간주)\n주의사항\n\npdf로 꼭 변환하여 제출\n풀이가 꼭 있어야 함 (답만 적혀 있는 경우 ’0’점 처리)\n부정행위 시 ’F’학점\n계산은 R로 해도 되지만 계산 풀이 과정이 꼭 있어야 함!!\n\n예) R에서 lm으로 beta의 추정량을 구하면 안 됨. 수업 시간에 배운 식으로 풀이를 적어야 함.\n\n행렬식 계산은 R로 해도 됩니다.\n\n************ R을 이용해서 푸는 문제는, R 코드도 같이 업로드.\n\n\n1. 어떤 큰 공장에서 동일한 기계들의 정비기록에 관한 표본자료를 취하였다. 이는 기계의 사용연도(age of machines)와 정비비용(maintenance cost) 간에 어떤 관계가 있는가를 밝혀내기 위한 것이다. 그 자료는 다음과 같다 (표본의 크기 \\(n = 14\\)).\n\n\n\n사용연도\\(X\\)(단위:년)\n정비비용\\(Y\\)(단위:1,000원)\n\n\n\n\n3\n39\n\n\n1\n24\n\n\n5\n115\n\n\n8\n105\n\n\n1\n50\n\n\n4\n86\n\n\n2\n67\n\n\n6\n90\n\n\n9\n140\n\n\n3\n112\n\n\n5\n70\n\n\n7\n186\n\n\n2\n43\n\n\n6\n126\n\n\n\n\ndt <- data.frame(y=c(39,24,115,105,50,86,67,90,140,112,70,186,43,126), \n                 x=c(3,1,5,8,1,4,2,6,9,3,5,7,2,6))\n\n\nx <- matrix(c(rep(1,14), dt$x), ncol=2)\n\n\ny <- matrix(c(dt$y), ncol=1)\n\n\n(1) \\(X^\\top X, X^\\top y, y^\\top y\\)와 \\((X^\\top X)^{-1}\\)을 구하시오.\n\nxx <- t(x) %*% x\n\n\nxx\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    14 62\n    62360\n\n\n\n\n\nxy <- t(x) %*% y\n\n\nxy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1253\n    6714\n\n\n\n\n\nyy <- t(y) %*% y\n\n\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    138197\n\n\n\n\n\nxx_i <- solve(xx)\n\n\nxx_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     0.30100334-0.05183946\n    -0.05183946 0.01170569\n\n\n\n\n\n\n(2) \\(\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y\\)를 구하고, 적합된 회귀선형을 써 보아라.\n\nbetahat <- xx_i %*%  xy\n\n\nbetahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    29.10702\n    13.63712\n\n\n\n\n\\(y = 29.1 + 13.6x\\)\n\ncoef(lm(y~x, dt))\n\n(Intercept)29.1070234113712x13.6371237458194\n\n\n\n\n(3) \\(\\sigma^2\\)을 MSE 로 추정할 경우, \\(\\hat{\\beta}\\)의 분산-공분산행렬의 추정은 \\(\\hat{Var}(\\hat{\\beta}) = (X^\\top X)^{-1}\\)(MSE)이다. 먼저 분산분석하여 MSE 를 구하고 \\(\\hat{Var}(\\hat{\\beta})\\)을 구하시오\n\nn <- 14\n\n\np <- 1\n\n\nIn = diag(rep(1,n))\n\n\nH = x %*% xx_i %*% t(x)\n\n\\(SSE = y^\\top(I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (In-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    10166.25\n\n\n\n\n\nsst = t(y) %*% y - n * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    26053.5\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15887.25\n\n\n\n\n\nmsr = ssr/p\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15887.25\n\n\n\n\n\nmse = sse/(n-p-1)\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    847.1876\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    18.75293\n\n\n\n\n\nqf(0.95,p,n-p-1)\n\n4.74722534672251\n\n\n분산분석표\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n15887.25\n1\n15887.25\n18.75293\n4.747225\n\n\n잔차\n10166.25\n12\n847.19\n\n\n\n\n계\n26053.5\n13\n\n\n\n\n\n\n\nhat_var_betahat <- mse[,] * xx_i\n\n\nvcov(lm(y~x, dt))\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    (Intercept)x\n\n\n    (Intercept)255.00629-43.917750\n    x-43.91775  9.916911\n\n\n\n\n\n\n\n2.어떤 공정에서 나오는 제품의 강도\\((kg/cm^2)\\)가 그 공정의 온도와 압력에 어떠한 영향을 받는가를 조사하기 위하여 다음의 데이터를 얻었다.\n\n\n\n\n\n\n\n\n공정온도\\(x_1\\)(단위:°C)\n공정압력\\(x_2\\)(단위:\\(psi\\))\n강도\\(y\\)(단위:\\(kg/cm^2\\))\n\n\n\n\n195\n57\n81.4\n\n\n179\n61\n122.2\n\n\n205\n60\n101.7\n\n\n204\n62\n175.6\n\n\n201\n61\n150.3\n\n\n184\n54\n64.8\n\n\n210\n58\n92.1\n\n\n209\n61\n113.8\n\n\n\n\ndt <- data.frame(y=c(81.4,122.2,101.7,175.6,150.3,64.8,92.1,113.8),\n                 x1 = c(195,179,205,204,201,184,210,209),\n                 x2 = c(57,61,60,62,61,54,58,61))\n\n\nm2 <- lm(y~., dt)\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n -5.259 -14.775 -18.742  31.259  17.279  11.744  -3.723 -17.783 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -554.3112   196.9775  -2.814   0.0374 *\nx1            -0.1797     0.7626  -0.236   0.8230  \nx2            11.8600     3.2301   3.672   0.0144 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 21.64 on 5 degrees of freedom\nMultiple R-squared:  0.7478,    Adjusted R-squared:  0.6469 \nF-statistic: 7.412 on 2 and 5 DF,  p-value: 0.03195\n\n\n\nanova(m2)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11 628.467 628.4670 1.3423710.29894191\n    x216311.7286311.727813.4815050.01441752\n    Residuals52340.884 468.1768       NA        NA\n\n\n\n\n\n(1) 선형회귀모형, \\(y_j = \\beta_0 + \\beta_1 x_{1j} + \\beta_2 x_{2j} + \\epsilon_j\\) 가 성립된다고 가정하고 데이터로부터 회귀모형을 추정하시오.\n\nx = matrix(c(rep(1,8),dt$x1, dt$x2), ncol=3)\n\n\ny = dt$y\n\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 3 × 3 of type dbl\n\n       8  1587  474\n    158731574594108\n     474 9410828136\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    82.8749894-0.134598917-0.945973490\n    -0.1345989 0.001242266-0.001887521\n    -0.9459735-0.001887521 0.022285408\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 3 × 1 of type dbl\n\n       901.9\n    179676.4\n     54034.3\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    110959\n\n\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    -554.3112232\n      -0.1797396\n      11.8599927\n\n\n\n\n\\(y = -554.31 - 0.1797 x_1 + 11.86 x_2\\)\n\n\n(2) 오차분산이 \\(\\sigma^2 = 3\\)이라 하면, \\(Var(\\hat{\\beta}_0), Var(\\hat{\\beta}_1), Var(\\hat{\\beta}_2)\\)와 \\(Cov(\\hat{\\beta}_1, \\hat{\\beta}_2)\\)는 무엇인가?\n\\(var(\\hat{\\beta}) = (x^\\top x)^{-1} \\sigma^2\\)\n\nxx_i * 3\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    248.6249683-0.403796751-2.837920471\n     -0.4037968 0.003726798-0.005662562\n     -2.8379205-0.005662562 0.066856223\n\n\n\n\n\\(var(\\hat{\\beta}_0) = 248.6250\\)\n\\(var(\\hat{\\beta}_1) = 0.0037\\)\n\\(var(\\hat{\\beta}_2) = 0.0669\\)\n\\(cov(\\hat{\\beta}_1, \\hat{\\beta}_2) = -0.0057\\)\n\n\n(3) \\(x_1 = 200\\)°C이고 \\(x_2 = 59\\text{psi}\\)에서 평균 제품의 강도의 추정값 \\(\\hat{y}\\)는 얼마인가? 이의 분산을 \\(\\sigma^2 = 3\\)이라 가정하고 구하시오.\n\nnew_dt <- data.frame(x1=200, x2=59)\nnew_dt\n\n\n\nA data.frame: 1 × 2\n\n    x1x2\n    <dbl><dbl>\n\n\n    20059\n\n\n\n\n\npredict(m2, newdata = new_dt)\n\n1: 109.480424963516\n\n\n\nx0 <- c(1,200,59)\nx0\n\n\n120059\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    109.4804\n\n\n\n\n\\(\\hat{\\mu}_0 = 109.4804\\)\n\n\n(4) 추정된 회귀계수 \\(\\hat{\\beta}_1, \\hat{\\beta}_2\\)의 의미는 무엇인가?\n\\(\\hat{\\beta}_1\\) : 공정압력이 일정하게 유지되었을 때, 공정온도가 1도씨 증가하면 강도는 0.1797만큼 감소한다.\n\\(\\hat{\\beta}_2\\) : 공정온도가 일정하게 유지되었을 때, 공정압력이 1psi 증가하면 강도는 11.8600만큼 증가한다.\n\n\n(5) 분산분석표를 작성하고 \\(\\alpha = 0.05\\)로 \\(F\\)-검정을 행하시오.\n\nanova(m2)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11 628.467 628.4670 1.3423710.29894191\n    x216311.7286311.727813.4815050.01441752\n    Residuals52340.884 468.1768       NA        NA\n\n\n\n\n\nI8 = diag(rep(1,8))\nI8\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    10000000\n    01000000\n    00100000\n    00010000\n    00001000\n    00000100\n    00000010\n    00000001\n\n\n\n\n\nH = x %*% xx_i %*% t(x)\nH\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    0.223303342 0.04734782240.0925307250 0.004932881 0.04854185 0.354021685 0.18034566 0.04897604\n    0.047347822 0.78758156140.0003377034 0.178850120 0.18539614 0.121730006-0.28766297-0.03358038\n    0.092530725 0.00033770340.1733021360 0.174906227 0.15025388 0.004944942 0.19895553 0.20476885\n    0.004932881 0.17885012000.1749062270 0.274444297 0.21838554-0.166837528 0.08255641 0.23276205\n    0.048541845 0.18539613810.1502538806 0.218385537 0.18446745-0.053127978 0.08195337 0.18412975\n    0.354021685 0.12173000620.0049449423-0.166837528-0.05312798 0.711046519 0.14493505-0.11671270\n    0.180345664-0.28766297200.1989555317 0.082556415 0.08195337 0.144935052 0.38255762 0.21635932\n    0.048976035-0.03358037940.2047688541 0.232762052 0.18412975-0.116712699 0.21635932 0.26329707\n\n\n\n\n\\(SSE = y^\\top (I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (I8-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2340.884\n\n\n\n\n\nsst = t(y) %*% y - 8 * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    9281.079\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    6940.195\n\n\n\n\n\nmsr = ssr/2\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3470.097\n\n\n\n\n\nmse = sse/5\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    468.1768\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.411938\n\n\n\n\n\nqf(0.95,2,5)\n\n5.78613504334997\n\n\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n6940.195\n2\n3470.097\n7.4119\n5.786135\n\n\n잔차\n2340.884\n5\n468.177\n\n\n\n\n계\n9281.079\n7\n\n\n\n\n\n\n결론 : 검정통계량의 관측값이 기각역에 속하므로 귀무가설 기각. 즉 모형이 유의수준 0.05하에서 유의하다고 할 수 있다.\n\n\n(6) 결정계수 \\(R^2\\)을 구하시오.\n\nsummary(m2)$r.squared\n\n0.747778895028607\n\n\n\\(R^2 = \\frac{SSR}{SST}\\)\n\nssr/sst\n# 0.7478\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.7477789\n\n\n\n\n\n\n(7) 수정된 결정계수 \\(R^2_{adj}\\) 을 구하시오.\n\nsummary(m2)$adj\n\n0.64689045304005\n\n\n\\(R^2_{adj} = 1-\\frac{SSE(n-p-1)}{SST/(n-1)}\\)\n\n1-(sse/5)/(sst/7)\n# 0.6469\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.6468905\n\n\n\n\n\n\n(8) \\(\\sigma^2\\)의 추정값 MSE 를 구하시오.\n\\(\\sigma^2 = MSE\\)\n\nmse\n##468.17687\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    468.1768\n\n\n\n\n\n\n(9) MSR의 기대값을 \\(\\sigma^2\\)과 \\(\\beta_0, \\beta_1, \\beta_2\\)의 함수로 표시하여라.\n\nI8 <- diag(rep(1,8))\nI8\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    10000000\n    01000000\n    00100000\n    00010000\n    00001000\n    00000100\n    00000010\n    00000001\n\n\n\n\n\none1 <- rep(1,8)\none1\n\n\n11111111\n\n\n\nJn <- one1 %*% t(one1)\nJn\n\n\n\nA matrix: 8 × 8 of type dbl\n\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n    11111111\n\n\n\n\n\nt(x) %*% (I8 - Jn/8) %*% x\n\n\n\nA matrix: 3 × 3 of type dbl\n\n    0  0.000 0.00\n    0923.87578.25\n    0 78.25051.50\n\n\n\n\n\n\n\n3. 어떤 공장에서 물의 소비량을 조사하기 위하여 매달의 물소비량(\\(y\\)), 평균온도(\\(x_1\\)), 작업일수(\\(x_2\\))와 작업량(\\(x_3\\))에 관한 데이터를 얻었다.\n\n\n\n\n\n\n\n\n\n물소비량\\(y\\)(단위:1,000톤)\n평균온도\\(x_1\\)(단위:°C)\n작업일수\\(x_2\\)(단위:일)\n작업량\\(x_3\\)(단위:1,000톤)\n\n\n\n\n2.8\n10\n27\n64\n\n\n3.9\n24\n26\n72\n\n\n3.9\n25\n28\n80\n\n\n4.4\n28\n26\n88\n\n\n3.1\n15\n30\n81\n\n\n3.1\n18\n24\n45\n\n\n3.5\n22\n27\n46\n\n\n3.6\n22\n25\n69\n\n\n3.0\n12\n27\n54\n\n\n3.3\n15\n25\n39\n\n\n\n\ndt <- data.frame(y=c(2.8,3.9,3.9,4.4,3.1,3.1,3.5,3.6,3.0,3.3),\n                 x1 = c(10,24,25,28,15,18,22,22,12,15),\n                 x2 = c(27,26,28,26,30,24,27,25,27,25),\n                 x3 = c(64,72,80,88,81,45,46,69,54,39))\n\n\nm3 <- lm(y~., dt)\n\n\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23490 -0.07744 -0.02166  0.08840  0.23442 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  2.409213   1.125954   2.140  0.07618 . \nx1           0.069788   0.012640   5.521  0.00149 **\nx2          -0.024767   0.044830  -0.552  0.60060   \nx3           0.005864   0.005052   1.161  0.28978   \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.172 on 6 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.8803 \nF-statistic: 23.05 on 3 and 6 DF,  p-value: 0.001079\n\n\n\nanova(m3)\n\n\n\nA anova: 4 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x112.0043158872.00431588767.738585990.0001737553\n    x210.0022730710.002273071 0.076821530.7909535228\n    x310.0398771420.039877142 1.347702340.2897755827\n    Residuals60.1775339000.029588983         NA          NA\n\n\n\n\n\n(1) 데이터로부터 회귀모형, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3\\) 을 구하여라. 그리고 어째서 이 모형이 선택되었는가에 대하여 토의하시오.\n\nx = matrix(c(rep(1,10),dt$x1, dt$x2, dt$x3), ncol=4)\n\n\ny = dt$y\n\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 4 × 4 of type dbl\n\n     10  191  265  638\n    191 3971 504712620\n    265 5047 704917038\n    638126201703843324\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    42.8460778-0.274517258-1.666784957 0.1044984818\n    -0.2745173 0.005399764 0.009802163-0.0013851965\n    -1.6667850 0.009802163 0.067921641-0.0050213139\n     0.1044985-0.001385196-0.005021314 0.0008624387\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 4 × 1 of type dbl\n\n      34.6\n     686.3\n     916.0\n    2249.9\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    121.94\n\n\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 4 × 1 of type dbl\n\n     2.409213010\n     0.069788005\n    -0.024766597\n     0.005864434\n\n\n\n\n\\(\\hat{y} = 2.409 + 0.0698x_1 - 0.0248x_2 + 0.0059x_3\\)\n\n\n(2) \\(\\hat{\\beta}_1, \\hat{\\beta}_2\\)와 \\(\\hat{\\beta}_3\\)의 의미는 무엇인가?\n\\(\\hat{\\beta}_1\\) : 작업일수, 작업량이 변하지 않을 때, 평균온도가 1도씨 증가하면 물소비량은 69.8톤 증가한다.\n\\(\\hat{\\beta}_2\\) : 평균온도, 작업량이 변하지 않을 때, 작업일수가 1일 증가하면 물소비량은 24.8톤 감소한다.\n\\(\\hat{\\beta}_3\\) : 평균온도, 작업일수이 변하지 않을 때, 작업량이 1000톤 증가하면 물소비량은 5.9톤 증가한다.\n\n\n(3) \\(Var(\\hat{\\beta}_3)\\)을 구하시오. \\(\\sigma^2\\)을 MSE 로 추정하면 \\(\\hat{Var}(\\hat{\\beta}_3)\\)은 무엇인가?\n\nsummary(m3)$coefficients\n\n\n\nA matrix: 4 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept) 2.4092130101.125953765 2.13970870.076181015\n    x1 0.0697880050.012640155 5.52113530.001485354\n    x2-0.0247665970.044830038-0.55245540.600595338\n    x3 0.0058644340.005051602 1.16090580.289775583\n\n\n\n\n\\(var(\\beta_3) = (x^\\top x)^{-1}_{(4,4)} \\sigma^2\\)\n\nxx_i[4,4]\n## 0.0009*sigma^2\n\n0.000862438702127396\n\n\n\nhat_var_b3 <- xx_i[4,4] * mse\nhat_var_b3\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.4037738\n\n\n\n\n\\(var(\\hat{\\beta}_3) = 0.0000255\\)\n\n\n(4) 분산분석표를 작성하고, 결정계수 \\(R^2\\)을 구하시오.\n\nanova(m3)\n\n\n\nA anova: 4 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x112.0043158872.00431588767.738585990.0001737553\n    x210.0022730710.002273071 0.076821530.7909535228\n    x310.0398771420.039877142 1.347702340.2897755827\n    Residuals60.1775339000.029588983         NA          NA\n\n\n\n\n\nI10 = diag(rep(1,10))\nI10\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1000000000\n    0100000000\n    0010000000\n    0001000000\n    0000100000\n    0000010000\n    0000001000\n    0000000100\n    0000000010\n    0000000001\n\n\n\n\n\nH = x %*% xx_i %*% t(x)\nH\n\n\n\nA matrix: 10 × 10 of type dbl\n\n     0.479007503-0.005805719-0.08066882-0.01958705 0.26225012 0.09655116-0.23996631 0.10866366 0.28730870 0.11224675\n    -0.005805719 0.186448902 0.16479959 0.27196857 0.01139329 0.10399383 0.06627062 0.18477003-0.01335934 0.02952023\n    -0.080668821 0.164799589 0.33179614 0.23355629 0.24558622-0.08987596 0.23998739 0.04338569-0.01509426-0.07347228\n    -0.019587049 0.271968566 0.23355629 0.48736361 0.01178437 0.05219047-0.10292798 0.28777554-0.11085118-0.11127263\n     0.262250120 0.011393292 0.24558622 0.01178437 0.58743384-0.22034983 0.06947229-0.08527926 0.20954991-0.09184095\n     0.096551163 0.103993827-0.08987596 0.05219047-0.22034983 0.36048096 0.08217297 0.20539705 0.10911272 0.30032662\n    -0.239966310 0.066270622 0.23998739-0.10292798 0.06947229 0.08217297 0.69646084-0.11029224 0.06925883 0.22956358\n     0.108663660 0.184770026 0.04338569 0.28777554-0.08527926 0.20539705-0.11029224 0.27283220 0.01617393 0.07657339\n     0.287308704-0.013359337-0.01509426-0.11085118 0.20954991 0.10911272 0.06925883 0.01617393 0.25886069 0.18903998\n     0.112246749 0.029520231-0.07347228-0.11127263-0.09184095 0.30032662 0.22956358 0.07657339 0.18903998 0.33931531\n\n\n\n\n\\(SSE = y^\\top (I-H)y\\)\n\\(SST = y^\\top y - n(\\bar{y})^2\\)\n\nsse = t(y) %*% (I10-H) %*% y\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1775339\n\n\n\n\n\nsst = t(y) %*% y - 10 * mean(y)^2\nsst\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.224\n\n\n\n\n\nssr = sst- sse\nssr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.046466\n\n\n\n\n\nmsr = ssr/3\nmsr\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.6821554\n\n\n\n\n\nmse = sse/6\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.02958898\n\n\n\n\n\nf0 = msr/mse\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    23.05437\n\n\n\n\n\nqf(0.95,3,6)\n\n4.75706266308941\n\n\n\n\n\n요인\n제곱합\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n2.0465\n3\n0.6822\n23.0544\n4.7571\n\n\n잔차\n0.1775\n6\n0.0296\n\n\n\n\n계\n2.2240\n9\n\n\n\n\n\n\n\\(R^2\\)\n\nssr/sst\n#0.9202\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.9201736\n\n\n\n\n\n\n(5) \\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 평균 물소비량을 추정하시오. 이 추정된 소비량의 표준편차의 추정값을 구하시오.\n\nnew_dt <- data.frame(x1=20, x2=27, x3 = 60)\nnew_dt\n\n\n\nA data.frame: 1 × 3\n\n    x1x2x3\n    <dbl><dbl><dbl>\n\n\n    202760\n\n\n\n\n\npredict(m3, newdata = new_dt)\n\n1: 3.48814105556645\n\n\n\nx0 <- c(1,20,27,60)\nx0\n\n\n1202760\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.488141\n\n\n\n\n\\(\\hat{\\mu}_0 = 3.4881\\)\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.005065205\n\n\n\n\n\nsqrt(var_mu0)\n# se(hat mu0) = 0.0712\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.07117026\n\n\n\n\n\n\n(6) \\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 어느 한 달의 물소비량 \\(y_s\\) 를 예측하여라. 이 예측된 물소비량의 표준편차의 추정값을 구하시오.\n\nx0 <- c(1,20,27,60)\nx0\n\n\n1202760\n\n\n\nmu0 <- x0 %*% b\nmu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.488141\n\n\n\n\n\\(\\hat{y}_s = 3.4881\\)\n\nvar_ys <- (1+t(x0) %*% xx_i %*% x0) * mse\nvar_ys\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.03465419\n\n\n\n\n\nsqrt(var_ys)\n# se(hat ys) = 0.1862\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1861564\n\n\n\n\n\n\n(7) \\(y_j = \\beta_0 + \\beta_1 x_{1j} + \\beta_2 x_{2j} + \\epsilon_j\\) 라 가정하고 회귀제곱합 SSR의 기대값을 \\(\\sigma^2\\)과 \\(\\beta_i, i = 0, 1, 2, 3\\)의 함수로 표시하여라.\n\nI10 <- diag(rep(1,10))\nI10\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1000000000\n    0100000000\n    0010000000\n    0001000000\n    0000100000\n    0000010000\n    0000001000\n    0000000100\n    0000000010\n    0000000001\n\n\n\n\n\none1 <- rep(1,10)\none1\n\n\n1111111111\n\n\n\nJn <- one1 %*% t(one1)\nJn\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n    1111111111\n\n\n\n\n\nt(x) %*% (I10 - Jn/10) %*% x\n\n\n\nA matrix: 4 × 4 of type dbl\n\n     6.661338e-16 1.160183e-14 1.751377e-144.096723e-14\n    -1.065814e-14 3.229000e+02-1.450000e+014.342000e+02\n    -1.332268e-14-1.450000e+01 2.650000e+011.310000e+02\n    -1.421085e-14 4.342000e+02 1.310000e+022.619600e+03\n\n\n\n\n\n\n(8) \\(\\beta_3\\)의 95% 신뢰구간을 구하시오. 이 신뢰구간의 의미를 해석하시오.\n\nconfint(m3)\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-0.3458966015.16432262\n    x1 0.0388586610.10071735\n    x2-0.1344617480.08492855\n    x3-0.0064963910.01822526\n\n\n\n\n\\(\\beta_3 \\pm t_{0.025}(6) se(\\hat{\\beta}_3)\\)\n\nb[4] - qt(0.975,6) * sqrt(hat_var_b3)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -1.548982\n\n\n\n\n\nb[4] + qt(0.975,6) * sqrt(hat_var_b3)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.56071\n\n\n\n\n\n#(-0.0065, 0.0182)\n\n이 신뢰구간이 \\(\\beta_3\\)을 포함하고 있을 것이라고 95% 확신한다.\n\n\n(9) \\(\\beta_1\\)의 99% 신뢰구간을 구하시오. 이 신뢰구간의 의미를 해석하시오.\n\nconfint(m3, level=0.99)\n\n\n\nA matrix: 4 × 2 of type dbl\n\n    0.5 %99.5 %\n\n\n    (Intercept)-1.765179536.58360555\n    x1 0.022925540.11665047\n    x2-0.190970740.14143754\n    x3-0.012864020.02459289\n\n\n\n\n\\(\\beta_1 \\pm t_{0.005}(6) se(\\hat{\\beta}_1)\\)\n\nhat_var_b1 = xx_i[2,2] * mse\nhat_var_b1\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.0001597735\n\n\n\n\n\nb[2] - qt(0.995,6) * sqrt(hat_var_b1)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.02292554\n\n\n\n\n\nb[2] + qt(0.995,6) * sqrt(hat_var_b1)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.1166505\n\n\n\n\n\n#(0.0229, 0.1167)\n\n이 신뢰구간이 \\(\\beta_1\\)을 포함하고 있을 것이라고 99% 확신한다.\n\n\n(10) \\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 평균 물소비량, \\(E(y)\\)의 95% 신뢰구간을 구하시오.\n\npredict(m3, newdata = new_dt, interval = c(\"confidence\"))\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    13.4881413.3139943.662288\n\n\n\n\n\nmu0 - qt(0.975,6) * sqrt(var_mu0)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.313994\n\n\n\n\n\nmu0 + qt(0.975,6) * sqrt(var_mu0)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.662288\n\n\n\n\n\n#(3.3140, 3.6623)\n\n\n\n(11) 가설 \\(H_0 : \\beta_1 = 0, H_1 : \\beta_1 > 0\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.23490 -0.07744 -0.02166  0.08840  0.23442 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  2.409213   1.125954   2.140  0.07618 . \nx1           0.069788   0.012640   5.521  0.00149 **\nx2          -0.024767   0.044830  -0.552  0.60060   \nx3           0.005864   0.005052   1.161  0.28978   \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.172 on 6 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.8803 \nF-statistic: 23.05 on 3 and 6 DF,  p-value: 0.001079\n\n\n검정통계량 : \\(t_0 = \\frac{\\hat{\\beta}_1}{se(\\hat{\\beta}_1)}\\)\n\nt0 <- b[2]/sqrt(hat_var_b1)\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    5.521135\n\n\n\n\n\\(t_0 = 5.5211\\)\n기각역 : \\(t_{0.05}(6) = 1.9432\\)\n\nqt(0.95,6)\n\n1.9431802805153\n\n\n결론 : \\(H_0\\) 기각 가능. 즉 유의수준 5%에서 \\(\\beta_2\\)는 0보다 크다고 할 수 있다.\n\\(T = \\frac{\\hat{\\beta}_i - \\beta^0_i}{\\hat{\\sigma} \\sqrt{c_{ii}}}\\)\n\n\n(12) 가설 \\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\nlibrary(car) \n\nLoading required package: carData\n\n\n\n\nlinearHypothesis(m3, matrix(c(0,1,-1,0,0,0,1,-1), byrow = T, nrow=2),c(0,0))\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    181.1224764NA       NA      NA         NA\n    260.1775339 20.944942515.967810.003956509\n\n\n\n\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)\n\\(T= (0,1,-1,0) (0,0,1,-1)\\)\n$y= _0 + _1 x_1 + _1 x_2 + _1 x_3 + = _0 + _1(x_1+x_2+x_3)\n\nz <- matrix(c(rep(1,10), dt$x1+dt$x2+dt$x3), ncol=2)\nz\n\n\n\nA matrix: 10 × 2 of type dbl\n\n    1101\n    1122\n    1133\n    1142\n    1126\n    1 87\n    1 95\n    1116\n    1 93\n    1 79\n\n\n\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n      10  1094\n    1094123754\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     3.04034002-0.0268769654\n    -0.02687697 0.0002456761\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n      34.6\n    3852.2\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    121.94\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 10 × 10 of type dbl\n\n    0.117334910.073997642 0.051297170 0.03272406 0.065742925 0.146226415 0.129716980.08637972 0.133844340 0.162735849\n    0.073997640.139003538 0.173054245 0.20091392 0.151385613 0.030660377 0.055424530.12043042 0.049233491 0.005896226\n    0.051297170.173054245 0.236831761 0.28901336 0.196246069-0.029874214 0.016509430.13826651 0.004913522-0.076257862\n    0.032724060.200913915 0.289013365 0.36109473 0.232950079-0.079402516-0.015330190.15285967-0.031348270-0.143474843\n    0.065742920.151385613 0.196246069 0.23295008 0.167698506 0.008647799 0.041273580.12691627 0.033117138-0.023977987\n    0.146226420.030660377-0.029874214-0.07940252 0.008647799 0.223270440 0.179245280.06367925 0.190251572 0.267295597\n    0.129716980.055424528 0.016509434-0.01533019 0.041273585 0.179245283 0.150943400.07665094 0.158018868 0.207547170\n    0.086379720.120430425 0.138266509 0.15285967 0.126916274 0.063679245 0.076650940.11070165 0.073408019 0.050707547\n    0.133844340.049233491 0.004913522-0.03134827 0.033117138 0.190251572 0.158018870.07340802 0.166077044 0.222484277\n    0.162735850.005896226-0.076257862-0.14347484-0.023977987 0.267295597 0.207547170.05070755 0.222484277 0.327044025\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1.66031840\n    0.01645047\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 10 × 1 of type dbl\n\n    -0.521816038\n     0.232724057\n     0.051768868\n     0.403714623\n    -0.633077830\n     0.008490566\n     0.276886792\n     0.031426887\n    -0.190212264\n     0.340094340\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.122476\n\n\n\n\n\nf0 = ((sse_rm - sse)/(2))/(sse/6)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    15.96781\n\n\n\n\n\nqf(0.95,2,6)\n\n5.14325284978472\n\n\n검정통계량 : \\(\\frac{(SSE_{RM} - SSE_{FM})/r}{SSE_{FM}/(n-p-1)}\\)\n관측값 \\(f_0 = 15.968\\)\n기각역 : \\(f_0 > 5.1433\\)\n결론 : 기각할 수 있다.\n\n\n(13) 가설 \\(H_0 : \\beta_1 = \\beta_2 + 3\\)을 \\(\\alpha = 0.05\\)로 검정하시오.\n\\(H_0 : \\beta_1 = \\beta_2 = \\beta_3\\)\n\\(T= (0,1,-1,0)\\)\n\\(y = \\beta_0 + (\\beta_2+3)x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\) \\(y - 3x_1 = \\beta_0 + \\beta_2(x_1 + x_2) + \\beta_3 x_3 + \\epsilon\\)\n\nyy <- y-3*dt$x1\nyy\n\n\n-27.2-68.1-71.1-79.6-41.9-50.9-62.5-62.4-33-41.7\n\n\n\nz <- matrix(c(rep(1,10), dt$x1+dt$x2,dt$x3), ncol=3)\nz\n\n\n\nA matrix: 10 × 3 of type dbl\n\n    13764\n    15072\n    15380\n    15488\n    14581\n    14245\n    14946\n    14769\n    13954\n    14039\n\n\n\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     10  456  638\n    4562111429658\n    6382965843324\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     6.76054651-0.160413550 0.0102556645\n    -0.16041355 0.005038964-0.0010871974\n     0.01025566-0.001087197 0.0006163093\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% yy\ngammahat\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    77.7140851\n    -3.1683286\n     0.2025345\n\n\n\n\n\ne <- yy - z %*% gammahat\ne\n\n\n\nA matrix: 10 × 1 of type dbl\n\n    -0.6481331\n    -1.9801368\n     2.9045732\n    -4.0473741\n     6.5554097\n    -4.6583347\n     5.7174312\n    -5.1775193\n     1.9138690\n    -0.5797851\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    157.327\n\n\n\n\n\nf0 = ((sse_rm - sse)/(1))/(sse/6)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    5311.082\n\n\n\n\n\nqf(0.95,1,6)\n\n5.9873776072737\n\n\n검정통계량 : \\(\\frac{(SSE_RM - SSE_FM)/r)}{(SSE_FM/(n-p-1))}\\)\n관측값 \\(f_0 = 5311.082\\)\n기각역 : \\(f_0 > 5.9874\\)\n결론 : 기각할 수 있다.\n\n\n(14) \\(x_1 = 20, x_2 = 27, x_3 = 60\\)에서 \\(E(y)\\)에 관한 가설 \\(H_0 : E(y) = 3.5, H_1 : E(y) ̸\\neq 3.5\\)를 \\(\\alpha = 0.05\\)로 검정하시오.\nsy 풀이\n\\(T_0 = \\frac{\\hat{y} - E(y)}{\\sqrt{Var{\\hat{y}}}} = \\frac{\\hat{y} - E(Y)}{\\sqrt{x^\\top (X^\\top X)^{-1} x MSE}} \\sim t(n-1)\\)\n\nx_indi <- matrix(c(1,20,27,60),nrow=1,ncol=4)\n\n\nyhat <- x_indi %*% b\nround(yhat,5)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.48814\n\n\n\n\n\\(Var{y} = x_i (X^\\top X)^{-1} x_i^\\top \\sigma^2\\)\n\nvar_y <- ((x_indi %*%solve(t(x) %*% x) %*% t(x_indi) ) * mse)[,]\nround(var_y,5)\n\n0.00507\n\n\n\nt_0 <- (yhat - 3.5)/sqrt(var_y)\nround(t_0[,],5)\n\n-0.16663\n\n\n\nround(-pt(0.025,9),5)\n\n-0.5097\n\n\n\\(F_0=-0.16663\\) > \\(t_{0.025}(9)=-0.5097\\) 로 귀무가설을 기각하지 못하여 \\(E(y) = 3.5\\)라는 결론이 나온다.\n\n\n4. 중회귀모형, \\(y = X\\beta + \\epsilon, \\epsilon \\sim N(0_n, I\\sigma^2)\\)에서 \\(X\\) 가 \\(n \\times (p + 1)\\)행렬이고 rank가 \\(p + 1\\)이라면 적합된 모형 \\(\\hat{y} = X\\hat{\\beta}\\)에 대하여\n\\[\\sum^{n}_{j=1} Var(\\hat{y}_j ) = (p + 1)\\sigma^2\\] 이 됨을 증명하여라.\n\\(\\hat{\\bf{y}} X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y = Hy\\)\n\\(Var(X\\hat{\\beta}) = XVar(\\hat{\\beta})X^\\top\\)\n\\(Var(\\hat{\\beta}) = Var((X^\\top X)^{-1} X^\\top y) = (X^\\top X)^{-1} X^\\top Var(y)X(X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top X(X^\\top X)^{-1} \\sigma^2 = (X^\\top X )^{-1} \\sigma^2\\)\n\\(Var(\\hat{y}) = Var(X\\hat{\\beta}) = X(X^\\top X)^{-1} X^\\top \\sigma^2\\)\n\\(\\to Var(\\hat{y}) = H(Var(y) H^\\top = HH\\sigma^2 = H\\sigma^2\\)\n\\(\\star H^\\top = H, H^2 = H, Var(y) = I_n \\sigma^2\\)\n$^{n}_{j=1} Var(j) = tr(Var()) = tr(H^2) = ^2 tr(H) = tr((XX){-1} XX)2 = tr(I{p+1})^2 = (p+1)^2 $\n\\(\\star tr(X (X^\\top X)^{-1} X^\\top)\\)\n\n\n5. 중회귀모형에서 다음을 증명하시오.\n\n(1) \\(Cov(e, y) = \\sigma^2[I_n − X(X^\\top X)^{-1}X^\\top]\\)\n\\(\\bf{e} = y - \\hat{y} = y - Hy = (I-H)y\\)\n\\(Cov(\\bf{e},y) = Cov((I-H)y,y) = (I-H)Var(y) = (I-H) \\sigma^2\\)\n\\(\\star Cov(Ax,y) = A Cov(X,Y)\\)\n\\(\\star Var(y) = \\sigma^2\\)\n\n\n(2) \\(Cov(e, \\hat{y}) = \\mathbb{O}_n\\)\n\\(Cov(e(I - H)\\bf{y},Hy) = (I-H) Cov(y,y)H^\\top = (I-H)H\\sigma^2 = \\mathbb{O}_n \\sigma^2\\)\n\\(\\star Cov(Ax,By) = ACov(X,Y)B^\\top\\)\n\\(\\star H^\\top= H\\)\n\\(\\star H^2 = H\\)\n\n\n(3) \\(Cov(e, \\hat{\\beta}) = \\mathbb{O}_{n\\times(k+1)}\\)\n\\(Cov((I-H)\\bf{y},(X^\\top X)^{-1} y) = (I-H) Cov(y,y) X (X^\\top X)^{-1} = (I - X(X^\\top X)^{-1} X^\\top ) X (X^\\top X)^{-1} \\sigma^2 = \\{ X(X^\\top X)^{-1} - X(X^\\top X)^{-1} \\} \\sigma^2 = \\mathbb{O}_{n \\times ([+1)}\\)\n\n\n(4) \\(Cov(\\epsilon, \\hat{\\beta}) = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\bf{\\hat{\\beta}} = (X^\\top X)^{-1} X^\\top y = (X^\\top X)^{-1} X^\\top (X\\beta + \\epsilon) = \\beta + (X^\\top X)^{-1} \\epsilon\\)\n\\(Cov(\\bf{\\epsilon} , \\beta + (X^\\top X)^{-1} X^\\top \\epsilon ) = Cov(\\epsilon, \\beta) + Cov(\\epsilon, \\epsilon)X(X^\\top X)^{-1} = \\sigma^2 X(X^\\top X)^{-1}\\)\n\\(\\star Cov(\\epsilon, \\beta)\\)\n\\(\\star Cov(\\epsilon, \\epsilon) = I_n \\sigma^2\\)\n\n\n(5) \\(\\sum^{n}_{j=1} e_j y_j = SSE\\)\n\\(\\sum^{n}_{j=1} e_j y_j = \\bf{e^\\top y} = \\{ (I - H)y\\}^\\top y = y^\\top (I-H)y\\)\n\\(\\star \\bf{e}^\\top (e_1, \\dots , e_n) = (y-\\hat{y})^\\top)\\)\n\\(\\star \\bf{y}^\\top = (y_1 , \\dots , y_n)\\)\n\\(SSE = \\sum^n_{j=1} (y_i - \\hat{y}_j)^2 = (\\bf{y} - \\hat{y} ) ^\\top ( y - \\hat{y} ) = e^\\top e = \\{ (I - H)y \\}^\\top \\{ (I - H)y \\} = y^\\top (I - H) (I-H)y = y^\\top (I - H)y\\)\n\\(\\star I - H_H+H^2 = I-H, H^2 = H\\)\n\\(\\therefore \\sum^{n}_{j=1} e_j y_j = SSE\\)\n\n\n(6) \\(\\sum^{n}_{j=1} e_j \\hat{y}_j = 0\\)\n\\(\\sum^{n}_{j=1} \\bf{e_j \\hat{y}_j} = e^\\top \\hat{y} = y^\\top (I-H) Hy = y^\\top_{1\\times n} \\mathbb{O}_{n\\times n} y_{n \\times 1} = 0\\)\n\\(\\star H - H^2 = H - H = \\mathbb{O}_{n \\times n}\\)\n\n\n\n1 plus. 다음의 데이터에 대하여 모형 \\(y_i = \\beta_0 + \\beta_1 x_{1j} + \\beta_2 x_{2j} + \\epsilon_j, \\epsilon \\sim N(0,\\sigma^2)\\)이 옳다고 가정하고 질문에 답하여라.\n\n\n\ny\n1\n5\n0\n4\n4\n-1\n\n\n\n\nx_1\n1\n2\n1\n3\n3\n3\n\n\nx_2\n1\n1\n2\n1\n2\n3\n\n\n\n\nlibrary(car)\n\n\ny <- c(1,5,0,4,4,-1)\n\n\nx <- matrix(c(1,1,1,1,1,1,1,2,1,3,3,3,1,1,2,1,2,3), ncol=3)\n\n\ndt <- data.frame(y=y,x1=x[,2],x2 = x[,3])\n\n\nm <- lm(y~., dt)\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n      1       2       3       4       5       6 \n-1.1395  1.3488  0.4651 -1.1628  1.4419 -0.9535 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   3.2326     1.9740   1.638   0.2000  \nx1            1.5116     0.7713   1.960   0.1449  \nx2           -2.6047     0.9288  -2.804   0.0676 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.599 on 3 degrees of freedom\nMultiple R-squared:  0.7511,    Adjusted R-squared:  0.5852 \nF-statistic: 4.527 on 2 and 3 DF,  p-value: 0.1242\n\n\n\nanova(m)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11 3.040230 3.040231.1884540.35537433\n    x2120.11868520.118687.8645770.06760655\n    Residuals3 7.674419 2.55814      NA        NA\n\n\n\n\n\n(1) \\(X^\\top X, X^\\top y y^\\top y\\)와 \\((X^\\top X)^{-1}\\) 을 구하시오.\n\nxx <- t(x) %*% x\nxx\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     61310\n    133323\n    102320\n\n\n\n\n\nxx_i <- solve(xx)\nxx_i\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     1.5232558-0.34883721-0.36046512\n    -0.3488372 0.23255814-0.09302326\n    -0.3604651-0.09302326 0.33720930\n\n\n\n\n\nxx_i2 <- round(xx_i,2)\nxx_i2\n\n\n\nA matrix: 3 × 3 of type dbl\n\n     1.52-0.35-0.36\n    -0.35 0.23-0.09\n    -0.36-0.09 0.34\n\n\n\n\n\nxy <- t(x) %*% y\nxy\n\n\n\nA matrix: 3 × 1 of type dbl\n\n    13\n    32\n    15\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59\n\n\n\n\n\n\n(2) \\(\\beta_0, \\beta_1, \\beta_2, \\sigma^2\\)를 추정하시오.\n\ncoef(m)\n\n(Intercept)3.23255813953489x11.51162790697674x2-2.6046511627907\n\n\n\nanova(m)$Mean[3]\n\n2.55813953488372\n\n\n\nb <- xx_i %*% xy\nb\n\n\n\nA matrix: 3 × 1 of type dbl\n\n     3.232558\n     1.511628\n    -2.604651\n\n\n\n\n\nb1 <- round(round(xx_i,2)%*% xy,2)\nb1\n\n\n\nA matrix: 3 × 1 of type dbl\n\n     3.16\n     1.46\n    -2.46\n\n\n\n\n\nhaty <- x %*% b\nhaty\n\n\n\nA matrix: 6 × 1 of type dbl\n\n     2.13953488\n     3.65116279\n    -0.46511628\n     5.16279070\n     2.55813953\n    -0.04651163\n\n\n\n\n\nhaty_2 <- round(x %*% b1,2)\nhaty_2\n\n\n\nA matrix: 6 × 1 of type dbl\n\n     2.16\n     3.62\n    -0.30\n     5.08\n     2.62\n     0.16\n\n\n\n\n\ne <- y - haty\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.1395349\n     1.3488372\n     0.4651163\n    -1.1627907\n     1.4418605\n    -0.9534884\n\n\n\n\n\ne_2 <- y - haty_2\ne_2\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.16\n     1.38\n     0.30\n    -1.08\n     1.38\n    -1.16\n\n\n\n\n\nsse <- t(e) %*% e\nsse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.674419\n\n\n\n\n\nsse_2 <- t(e_2) %*% e_2\nsse_2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    7.7564\n\n\n\n\n\nmse <- sse / 3\nmse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.55814\n\n\n\n\n\nmse_2 <- sse_2 / 3\nmse_2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.585467\n\n\n\n\n\n\n(3) \\(x_1 = x_2 = 2\\)에서 \\(E(y)\\)의 95% 신뢰구간을 구하시오.\n\nnew_dt <- data.frame(x1=2, x2=2)\nnew_dt\n\n\n\nA data.frame: 1 × 2\n\n    x1x2\n    <dbl><dbl>\n\n\n    22\n\n\n\n\n\npredict(m, newdata = new_dt,interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    11.046512-1.3459823.439005\n\n\n\n\n\nx0 <- c(1,2,2)\n\n\nmu0 <- x0 %*% b\n\n\nt(x0) %*% xx_i %*% x0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.2209302\n\n\n\n\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.5651704\n\n\n\n\n\ntse <- qt(0.975,3) * sqrt(var_mu0)\ntse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.392494\n\n\n\n\n\nmu0 - (tse)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -1.345982\n\n\n\n\n\nmu0 + (tse)\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.439005\n\n\n\n\n\n\n(4) \\(\\beta_1\\)의 90% 신뢰구간을 구하시오.\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n      1       2       3       4       5       6 \n-1.1395  1.3488  0.4651 -1.1628  1.4419 -0.9535 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   3.2326     1.9740   1.638   0.2000  \nx1            1.5116     0.7713   1.960   0.1449  \nx2           -2.6047     0.9288  -2.804   0.0676 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.599 on 3 degrees of freedom\nMultiple R-squared:  0.7511,    Adjusted R-squared:  0.5852 \nF-statistic: 4.527 on 2 and 3 DF,  p-value: 0.1242\n\n\n\nconfint(m, level = 0.9)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    5 %95 %\n\n\n    (Intercept)-1.4129961 7.8781124\n    x1-0.3035404 3.3267962\n    x2-4.7904032-0.4188991\n\n\n\n\n\nse_b1 <- sqrt(xx_i[2,2] * mse)\nse_b1\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.7713081\n\n\n\n\n\ntse <- qt(0.95,3) *se_b1\ntse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.815168\n\n\n\n\n\nb[2] - tse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -0.3035404\n\n\n\n\n\nb[2] + tse\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    3.326796\n\n\n\n\n\n\n(5) 가설 \\(H_0: \\beta_1 = \\beta_2, H_1: \\beta_1 \\neq \\beta_2\\)을 유의수준 \\(\\alpha = 0.05\\)\n\nlibrary(car)\n\n\nlinearHypothesis(m, c(0,1,-1), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1430.092308NA      NA      NA        NA\n    23 7.674419 122.417898.7633570.05952979\n\n\n\n\n\nz <- matrix(c(1,1,1,1,1,1,x[,2]+x[,3]), ncol=2)\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     623\n    2399\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     1.5230769-0.35384615\n    -0.3538462 0.09230769\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    13\n    47\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 6 × 6 of type dbl\n\n     0.476923083.076923e-013.076923e-010.1384615-0.03076923-2.000000e-01\n     0.307692312.307692e-012.307692e-010.1538462 0.07692308 1.110223e-16\n     0.307692312.307692e-012.307692e-010.1538462 0.07692308 1.110223e-16\n     0.138461541.538462e-011.538462e-010.1692308 0.18461538 2.000000e-01\n    -0.030769237.692308e-027.692308e-020.1846154 0.29230769 4.000000e-01\n    -0.200000001.110223e-161.110223e-160.2000000 0.40000000 6.000000e-01\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n     3.1692308\n    -0.2615385\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.646154\n     2.615385\n    -2.384615\n     1.876923\n     2.138462\n    -2.600000\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    30.09231\n\n\n\n\n\nf0 = (sse_rm - sse)/(sse/3)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    8.763357\n\n\n\n\n\nqf(0.95,1,3)\n\n10.1279644860139\n\n\nT검정 방법\n\nq = c(0,1,-1)\n\n\nvar_qb = t(q) %*% xx_i %*% q*  mse\nvar_qb\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.933478\n\n\n\n\n\nse_qb = sqrt(var_qb)\nse_qb\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    1.390495\n\n\n\n\n\nt0 = (t(q) %*% b )/se_qb\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.960297\n\n\n\n\n\nt0^2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    8.763357\n\n\n\n\n\n\n(6) 가설 \\(H_0 : \\beta_1 = 0, H_1 : \\beta_1 \\neq 0\\)을 유의수준 \\(\\alpha = 0.1\\)로 검정하시오. 위 (3)의 질문의 결과와 어떠한 연관을 지을 수 있는가?\n\nt0 <- b[2] / se_b1\n\n\nqt(0.95,3)\n\n2.35336343480182\n\n\n\n\n(7) \\(x_1 = 2, x_2 = 3\\)에서 가설 \\(H_0 : E(y) = 4,H_1 : E(y) \\neq 4\\)를 \\(\\alpha = 0.05\\)로 검정하시오\n\nnew_dt <- data.frame(x1=2, x2=2)\n\n\npredict(m, newdata = new_dt,interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    11.046512-1.3459823.439005\n\n\n\n\n\nx0 <- c(1,2,3)\n\n\nmu0 <- x0 %*% b\n\n\nt(x0) %*% xx_i %*% x0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.8139535\n\n\n\n\n\nvar_mu0 <- (t(x0) %*% xx_i %*% x0) * mse\nvar_mu0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    2.082207\n\n\n\n\n\nt0 <- (mu0-4)/ sqrt(var_mu0)\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -3.851834\n\n\n\n\n\nqt(0.975,3)\n\n3.18244630528371\n\n\n\n\n(8) 가설 \\(H_0 : \\beta_2 = 0, H_1: \\beta_2 <0\\)을 \\(\\alpha = 0.05\\) 로 검정하시오.\n\nse_b2 <- sqrt(xx_i[3,3] * mse)\nse_b2\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    0.9287779\n\n\n\n\n\nt0<- b[3] / se_b2\nt0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    -2.804385\n\n\n\n\n\n\n(9) 가설 $H_0: _1 = 2_2, H_1 : _1 _2 $를 \\(\\alpha=0..05\\)로 검정하시오.\n\nlinearHypothesis(m, c(0,1,-2), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1430.797619NA     NA      NA        NA\n    23 7.674419 123.12329.0390690.05737101\n\n\n\n\n\nz <- matrix(c(1,1,1,1,1,1,2*x[,2]+x[,3]), ncol=2)\n\n\nzz <- t(z) %*% z\nzz\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     6 36\n    36244\n\n\n\n\n\nzz_i <- solve(zz)\nzz_i\n\n\n\nA matrix: 2 × 2 of type dbl\n\n     1.4523810-0.21428571\n    -0.2142857 0.03571429\n\n\n\n\n\nzy <- t(z) %*% y\nzy\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    13\n    79\n\n\n\n\n\nyy <- t(y) %*% y\nyy\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    59\n\n\n\n\n\nz %*% zz_i %*% t(z)\n\n\n\nA matrix: 6 × 6 of type dbl\n\n     0.488095240.27380952 0.380952380.05952381-0.04761905-0.15476190\n     0.273809520.20238095 0.238095240.13095238 0.09523810 0.05952381\n     0.380952380.23809524 0.309523810.09523810 0.02380952-0.04761905\n     0.059523810.13095238 0.095238100.20238095 0.23809524 0.27380952\n    -0.047619050.09523810 0.023809520.23809524 0.30952381 0.38095238\n    -0.154761900.05952381-0.047619050.27380952 0.38095238 0.48809524\n\n\n\n\n\ngammahat <- zz_i %*% t(z) %*% y\ngammahat\n\n\n\nA matrix: 2 × 1 of type dbl\n\n    1.95238095\n    0.03571429\n\n\n\n\n\ne <- y - z %*% gammahat\ne\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    -1.059524\n     2.869048\n    -2.095238\n     1.797619\n     1.761905\n    -3.273810\n\n\n\n\n\nsse_rm <- t(e) %*% e\nsse_rm\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    30.79762\n\n\n\n\n\nf0 = (sse_rm - sse)/(sse/3)\nf0\n\n\n\nA matrix: 1 × 1 of type dbl\n\n    9.039069\n\n\n\n\n\nqf(0.95,1,3)\n\n10.1279644860139"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_CH03, CH04.html",
    "href": "posts/rl/2022-09-21-rl_CH03, CH04.html",
    "title": "고급회귀분석 실습 CH03, CH04",
    "section": "",
    "text": "chapter 3, chapter 4"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_CH03, CH04.html#data",
    "href": "posts/rl/2022-09-21-rl_CH03, CH04.html#data",
    "title": "고급회귀분석 실습 CH03, CH04",
    "section": "Data",
    "text": "Data\n\ndt <- data.frame(x = c(4,8,9,8,8,12,6,10,6,9),\n                 y = c(9,20,22,15,17,30,18,25,10,20))\ndt\n\n\n\nA data.frame: 10 × 2\n\n    xy\n    <dbl><dbl>\n\n\n     4 9\n     820\n     922\n     815\n     817\n    1230\n     618\n    1025\n     610\n     920\n\n\n\n\ncorrelation check\n\ncor(dt$x, dt$y)\n\n0.921812343945765\n\n\n산점도 확인\n\nplot(y~x, \n     data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 16,\n     cex  = 2,\n     col  = \"darkorange\")\n\n\n\n\n\npch 점 모양\ncex 점 크기\n양의상관관계 강하네,\n우상향이네, 단순상관선형 적용해보면 되겠다.\n\n\nggplot(dt, aes(x, y)) +\n  geom_point(col='steelblue', lwd=2) +\n  # geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1.2) +\n  xlab(\"광고료\")+ylab(\"총판매액\")+\n  # scale_x_continuous(breaks = seq(1,10))+\n  theme_bw() +\n  theme(axis.title = element_text(size = 14))\n\n\n\n\n\n적합\n\\(\\hat{ y} = \\widehat{(E(y|X=x))} = \\hat{\\beta_0} + \\hat{\\beta_1} * x\\)\n\\(H_0\\) : \\(\\beta_0\\) =0 vs \\(H_1\\) : \\(\\beta_0 \\ne 0\\)\n\\(H_0\\) : \\(\\beta_1 =0\\) vs \\(H_1\\) : \\(\\beta_1 \\ne 0\\)\n모형 적합을 한다 yhat을 찾는다. - 회귀분석을 한다. 평균 반응을 추정한다.\nlm linear model 사용\n\n## y = beta0 + beta1*x + epsilon\nmodel1 <- lm(y ~ x, dt)\n# lm(y ~ 0 + x, dt) beta0 없이 분석하고 싶을때\nmodel1\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nCoefficients:\n(Intercept)            x  \n     -2.270        2.609  \n\n\n설명변수 x 하나일때\n\nbeta0hat = -2.270\nbeta1hat = 2.609\n\n\nsummary(model1) \n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.600 -1.502  0.813  1.128  4.617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.2696     3.2123  -0.707 0.499926    \nx             2.6087     0.3878   6.726 0.000149 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.631 on 8 degrees of freedom\nMultiple R-squared:  0.8497,    Adjusted R-squared:  0.831 \nF-statistic: 45.24 on 1 and 8 DF,  p-value: 0.0001487\n\n\n\n모형의 유의성 검정 자체(f검정)\n개별 회귀계수에 대한 유의성검정(t검정)\nbeta1은 유의하지 않다\nbeta0는 유의하다\nf통계량은 45.24(msr/mse) p값 충분히 작아서 모형은 유의하다.\ny의 총 변동 중에 85%정도를 설명하고 있다.\nroot mse(RMSE) = 2.631\n\n\n6.726**2\n\n45.239076\n\n\n단순선형에서만 해당\n0.000149도 같음\n\nnames(model1)\n\n\n'coefficients''residuals''effects''rank''fitted.values''assign''qr''df.residual''xlevels''call''terms''model'\n\n\n\nmodel1$residuals # 보고 싶은 변수 입력해봐~\n\n10.83478260869565621.430.7913043478260874-3.65-1.660.9652173913043574.6173913043478281.182608695652179-3.3826086956521810-1.20869565217391\n\n\n\nmodel1$fitted.values  ##hat y\nmodel1$coefficients\n\n18.16521739130434218.6321.2086956521739418.6518.6629.0347826086957713.3826086956522823.8173913043478913.38260869565221021.2086956521739\n\n\n(Intercept)-2.2695652173913x2.60869565217391\n\n\n\nanova(model1)  ## 회귀모형의 유의성 검정\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1313.04348313.04347845.240340.0001486582\n    Residuals8 55.35652  6.919565      NA          NA\n\n\n\n\n\n설명변수의 개수가 x 자유도\n잔차의 자유도는 n-2\n\n\na <- summary(model1)\nls(a)\n\n\n'adj.r.squared''aliased''call''coefficients''cov.unscaled''df''fstatistic''r.squared''residuals''sigma''terms'\n\n\n\nsummary(model1)$coef   ## 회귀계수의 유의성 검정\n\n\n\nA matrix: 2 × 4 of type dbl\n\n    EstimateStd. Errort valuePr(>|t|)\n\n\n    (Intercept)-2.2695653.212348-0.70651290.4999255886\n    x 2.6086960.387847 6.72609390.0001486582\n\n\n\n\n\nconfint(model1, level = 0.95)  ##회귀계수의 신뢰구간\n## beta +- t_alpha/2 (n-2) * se(beta)\nqt(0.025, 8)\nqt(0.975, 8)\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-9.6772525.138122\n    x 1.7143193.503073\n\n\n\n\n-2.30600413520417\n\n\n2.30600413520417\n\n\n\nqt _ tquantile\n\n\n## y = beta1*x + epsilon\nmodel2 <- lm(y ~ 0 + x, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ 0 + x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0641 -1.5882  0.2638  1.4818  3.9359 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   2.3440     0.0976   24.02  1.8e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.556 on 9 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9829 \nF-statistic: 576.8 on 1 and 9 DF,  p-value: 1.798e-09\n\n\n\nintercept 없는 모습\nr squre가 두 번째가 높고,\np값도 훨씬 유의하게 나옴\n\n\nanova(model1)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x1313.04348313.04347845.240340.0001486582\n    Residuals8 55.35652  6.919565      NA          NA\n\n\n\n\n\nanova(model2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x13769.18953769.1895576.81381.79763e-09\n    Residuals9  58.8105   6.5345      NA         NA\n\n\n\n\n\n###########\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"darkorange\")\nabline(model1, col='steelblue', lwd=2)\nabline(model2, col='green', lwd=2)\n\n\n\n\nmodel들이 기울기가 살짝씩 다르다\n\nco <- coef(model1)\n\n\nggplot(dt, aes(x, y)) +\n  geom_point(col='steelblue', lwd=1) +\n  geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1) +\n  xlab(\"광고료\")+ylab(\"총판매액\")+\n  theme_bw()+\n  theme(axis.title = element_text(size = 16))\n\n\n\n\n\n######## LSE 구하기\n# lm을 사용하지 않고 구할때\n\ndt1 <- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x), # x - x평균\n  y_bary = dt$y - mean(dt$y))  # y - y평균\n\n\ndt1$x_barx2 <- dt1$x_barx^2 # x 편차의 제곱\ndt1$y_bary2 <- dt1$y_bary^2 # y편차의 제곱\ndt1$xy <-dt1$x_barx * dt1$y_bary\n\n\ndt1\n\n\n\nA data.frame: 10 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2xy\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n     1 4 9-4-9.616 92.1638.4\n     2 820 0 1.4 0  1.96 0.0\n     3 922 1 3.4 1 11.56 3.4\n     4 815 0-3.6 0 12.96 0.0\n     5 817 0-1.6 0  2.56 0.0\n     61230 411.416129.9645.6\n     7 618-2-0.6 4  0.36 1.2\n     81025 2 6.4 4 40.9612.8\n     9 610-2-8.6 4 73.9617.2\n    10 920 1 1.4 1  1.96 1.4\n\n\n\n\n\nround(colSums(dt1),3)\n\ni55x80y186x_barx0y_bary0x_barx246y_bary2368.4xy120\n\n\n\n### hat beta1 = S_xy / S_xx\n##hat beta0 = bar y - hat beta_1 * bar x\nbeta1 <- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\nbeta0 <- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\ncat(\"hat beta1 = \", beta1)\n\nhat beta0 =  -2.269565hat beta1 =  2.608696\n\n\n\n\n평균반응, 개별 y 추정\n구분할 수 있어야 한다\n신뢰구간 달라진다.\n\n## E(Y|x0) 평균반응\n## y = E(Y|x0) + epsilon 개별 y 추정\n# x0 = 4.5\nnew_dt <- data.frame(x = 4.5)\n\n\n# hat y0 = hat beta0 + hat beta1 * 4.5\n\npredict(model1, \n        newdata = new_dt,\n        interval = c(\"confidence\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695655.7982613.14087\n\n\n\n\nnew_data=new_df 정의 안 하면 fitted value가 나온다.\nconfidence는 평균반응\n\npredict(model1, newdata = new_dt, \n        interval = c(\"prediction\"), level = 0.95)\n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    19.4695652.37912516.56001\n\n\n\n\nprediction은 개별 y 추정\n신뢰구간이 커진다. \\(\\to\\) 표준오차가 달라지기 때문\n\ndt_pred <- data.frame(\n  x = 1:12,\n  predict(model1, \n          newdata=data.frame(x=1:12), \n          interval=\"confidence\", level = 0.95))\ndt_pred\n\n\n\nA data.frame: 12 × 4\n\n    xfitlwrupr\n    <int><dbl><dbl><dbl>\n\n\n    1 1 0.3391304-6.2087835 6.887044\n    2 2 2.9478261-2.7509762 8.646628\n    3 3 5.5565217 0.690585410.422458\n    4 4 8.1652174 4.105889112.224546\n    5 510.7739130 7.475614014.072212\n    6 613.382608710.759780816.005437\n    7 715.991304313.874822318.107786\n    8 818.600000016.681775320.518225\n    9 921.208695719.092213623.325178\n    101023.817391321.194563426.440219\n    111126.426087023.127788029.724386\n    121229.034782624.975454333.094111\n\n\n\n\n\ndt_pred2 <- as.data.frame(predict(model1, \n                                  newdata=data.frame(x=1:12), \n                                  interval=\"prediction\", level = 0.95))\ndt_pred2\n\n\n\nA data.frame: 12 × 3\n\n    fitlwrupr\n    <dbl><dbl><dbl>\n\n\n    1 0.3391304-8.5867330 9.264994\n    2 2.9478261-5.375166611.270819\n    3 5.5565217-2.219929713.332973\n    4 8.1652174 0.866312815.464122\n    510.7739130 3.869230817.678595\n    613.3826087 6.773895719.991322\n    715.9913043 9.566714322.415894\n    818.600000012.237968324.962032\n    921.208695714.784105627.633286\n    1023.817391317.208678330.426104\n    1126.426087019.521404733.330769\n    1229.034782621.735878136.333687\n\n\n\n\n\nnames(dt_pred2)[2:3] <- c('plwr', 'pupr')\n\nplot 같이 그리게 데이터 합치기\n\ndt_pred3 <- cbind.data.frame(dt_pred, dt_pred2[,2:3])\n\n\nbarx <- mean(dt$x)\nbary <- mean(dt$y)\n\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"grey\",\n     ylim = c(min(dt_pred3$plwr), max(dt_pred3$pupr)))\nabline(model1, lwd = 5, col = \"darkorange\")\nlines(dt_pred3$x, dt_pred3$lwr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred3$x, dt_pred3$upr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred3$x, dt_pred3$plwr, col = \"dodgerblue\", lwd = 3, lty = 3)\nlines(dt_pred3$x, dt_pred3$pupr, col = \"dodgerblue\", lwd = 3, lty = 3)\n\nabline(h=bary,v=barx, lty=2, lwd=0.2, col='dark grey')\n\n\n\n\n\nggplot(dt_pred3, aes(x, fit)) +\n  geom_line(col='steelblue', lwd=2) +\n  xlab(\"\")+ylab(\"\")+\n  scale_x_continuous(breaks = seq(1,10))+\n  geom_line(aes(x, lwr), lty=2, lwd=1.5, col='darkorange') +\n  geom_line(aes(x, upr), lty=2, lwd=1.5, col='darkorange') +\n  geom_line(aes(x, plwr), lty=2, lwd=1.5, col='dodgerblue') +\n  geom_line(aes(x, pupr), lty=2, lwd=1.5, col='dodgerblue') +\n  geom_vline(xintercept = barx, lty=2, lwd=0.2, col='dark grey')+\n  geom_hline(yintercept = bary, lty=2, lwd=0.2, col='dark grey')+\n  theme_bw()\n\n\n\n\n\nbb <- summary(model1)$sigma * ( 1 + 1/10 +(dt$x - 8)^2/46)\ndt$ma95y <- model1$fitted + 2.306*bb\ndt$mi95y <- model1$fitted - 2.306*bb\n\n\nggplot(dt, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  geom_line(aes(x, mi95y), col = 'darkgrey', lty=2) +\n  geom_line(aes(x, ma95y), col = 'darkgrey', lty=2) +\n  theme_bw() +\n  theme(axis.title = element_blank())\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n잔차분석\n\n### epsilon : 선형성, 등분산성, 정규성, 독립성\n\n\ndt\ndt$yhat <- model1$fitted\n# fitted.values(model1) # y에 대한 추정값 구하기\ndt$resid <- model1$residuals\n# resid(model1)\n\n\n\nA data.frame: 10 × 4\n\n    xyma95ymi95y\n    <dbl><dbl><dbl><dbl>\n\n\n     4 916.94766-0.6172208\n     82025.2725411.9274568\n     92228.0131114.4042841\n     81525.2725411.9274568\n     81725.2725411.9274568\n    123037.8172220.2523444\n     61820.58263 6.1825918\n    102531.0174116.6173744\n     61020.58263 6.1825918\n     92028.0131114.4042841\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(resid ~ x, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\npar(mfrow=c(1,1))\n\n\n\n\n단순선형에서는 두 plot의 차이가 없다.\n\n선형성 만족\n등분산성 나름 만족\n정규성 아웃라이어 있는 거 같은데..\n독립성?\n\n\n# 독립성검정 : DW test\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\n## \ndwtest(model1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n# dwtest(model1, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho > 0\n# dwtest(model1, alternative = \"less\")  #H0 : uncorrelated vs H1 : rho < 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.3916\nalternative hypothesis: true autocorrelation is not 0\n\n\np 값 커서 기각할 수 없다.\n첫 번째꺼 주로 보기\n\n## 정규분포 (QQ plot)\nqqnorm(dt$resid, pch=16)\nqqline(dt$resid, col = 2)\n\n\n\n\n분위수분위수 그림 - 정규분포의 실제 - 어떤 분포의 이론적 분위수와 내가 가진 sample의 분위수 비교\n주로 꼬리쪽을 많이 본다. - 이 데이터의 경우 꼬리부분이 차이가 커 보임\n\nggplot(dt, aes(sample = resid)) + \n  stat_qq() + stat_qq_line() +\n  theme_bw()\n\n\n\n\n\n## 정규분포 검정 \nshapiro.test(dt$resid)  ##shapiro-wilk test\n#H0 : normal distributed vs H1 : not\n\n\n    Shapiro-Wilk normality test\n\ndata:  dt$resid\nW = 0.92426, p-value = 0.3939\n\n\np값 작게 나오면 정규분포라고 하기 어렵다.\n\n정규성은 잔차를 넣어줬는데\nbptest는 model을 넣었다.\n\n\n## 등분산성 검정 \nbptest(model1) #Breusch–Pagan test\n# H0 : 등분산 vs H1 : 이분산 \n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 1.6727, df = 1, p-value = 0.1959\n\n\n\n\n책 예제\n\n# install.packages('UsingR')\nlibrary(UsingR)\n\nLoading required package: MASS\n\nLoading required package: HistData\n\nLoading required package: Hmisc\n\nLoading required package: lattice\n\nLoading required package: survival\n\nLoading required package: Formula\n\n\nAttaching package: ‘Hmisc’\n\n\nThe following objects are masked from ‘package:base’:\n\n    format.pval, units\n\n\n\nAttaching package: ‘UsingR’\n\n\nThe following object is masked from ‘package:survival’:\n\n    cancer\n\n\n\n\n\ndata(father.son)\n\n\nnames(father.son)\n\n\n'fheight''sheight'\n\n\n\nlm.fit<-lm(sheight~fheight, data=father.son)\n\n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = sheight ~ fheight, data = father.son)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8772 -1.5144 -0.0079  1.6285  8.9685 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.88660    1.83235   18.49   <2e-16 ***\nfheight      0.51409    0.02705   19.01   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.437 on 1076 degrees of freedom\nMultiple R-squared:  0.2513,    Adjusted R-squared:  0.2506 \nF-statistic: 361.2 on 1 and 1076 DF,  p-value: < 2.2e-16\n\n\n아버지의 키가 아들의 키의 25%만 설명\n\nplot(sheight~fheight, \n     data=father.son, \n     pch=16, cex=0.5,\n     xlab=\"father’s height (inches)\", \n     ylab=\"son’s height (inches)\")\nabline(lm.fit)\n\n\n\n\n\n\namazon<-read.csv(\"amazon.csv\")\nplot(High   ~Year  , amazon, pch=16)\n\nlm.fit<-lm(High~Year, data=amazon)\nsummary(lm.fit)\n\nconfint(lm.fit)\n\npar(mfrow=c(1,2))\nscatter.smooth(x=1:length(amazon$Year), y=residuals(lm.fit), xlab=\"Year\")\nscatter.smooth(x=predict(lm.fit), y=residuals(lm.fit), xlab=expression(hat(y)))\n\nlibrary(lmtest)\ndwtest(lm.fit)\n\n\nCall:\nlm(formula = High ~ Year, data = amazon)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3629 -0.5341  0.1479  0.4903  1.1412 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -330.21235   78.03319  -4.232 0.000725 ***\nYear           0.18088    0.03961   4.567 0.000371 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8001 on 15 degrees of freedom\nMultiple R-squared:  0.5816,    Adjusted R-squared:  0.5537 \nF-statistic: 20.85 on 1 and 15 DF,  p-value: 0.0003708\n\n\n\n\nA matrix: 2 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-496.53615985-163.8885460\n    Year   0.09645429   0.2653104\n\n\n\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  lm.fit\nDW = 1.0487, p-value = 0.006864\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\n양의 상관관계가 있다. - 시간순 index - 최근 관측 데이터에 영향 많이 받는 편"
  },
  {
    "objectID": "posts/rl/2022-09-21-rl_HW1.html",
    "href": "posts/rl/2022-09-21-rl_HW1.html",
    "title": "Regression HW 1",
    "section": "",
    "text": "고급회귀분석 과제, CH03, CH04\n\n과제1\n풀이 :\nR 이용하지 않고 직접 계산. (R로 단순 계산은 해도 됨)\n모든 문제에는 풀이가 있어야 함.\n풀이 없이 답만 있는 경우 ’0’점 처리.\n\n제출 기한 : 10월 03일\n\n제출 방법 :\n직접 제출 (607호) 또는 스캔, 사진, tex 작업, 문서 작업 등 후 pdf로 변환 후 제출\npdf 아닌 경우 미제출 처리\n\n\n원점을 지나는 회귀모형은 다음과 같이 정의할 수 있다. \\[y_i =β_1x_i +ε_i, ε_i ∼_{i.i.d.} N(0,σ^2), i=1,...,n\\] 오차제곱합을 정의하고 \\(β_1\\) 의 최소제곱추정량 (\\(\\hat{β}_1\\))을 구하여라.\n\nAnswer\n\\(SSE = \\sum(y_i - \\hat{y_i})^2 = \\sum(y_i - \\hat{\\beta_1}x_i)^2\\)\n\\(SSE' = 0 = \\sum(-x_i)2(y_i - \\hat{\\beta_1}x_i )\\)\n\\(\\sum(-x_iy_i) + \\sum(x_i^2\\hat{\\beta_1}) = 0\\)\n\\(\\hat{\\beta_1} = \\frac{\\sum(x_iy_i)}{\\sum(x_i^2)}\\)\n\n자동차의 무게가 무거우면 이를 움직이는 데 더 많은 연료가 소모된다는 것은 알려진 사실이다. 자동차의 무게와 자동차를 1km 움직이는 데 필요한 에너지량과의 함수관계를 정확히 판단하기 위하여 A 자동차회사는 다음의 자료를 실험을 통하여 얻었다. 실험 비용이 많이 드는 관계로 9번만 실험하였다.\n\n\n\n\n무게 X(단위: 1,000kg)\n에너지소모량 Y(단위: 1,000btu)\n\n\n\n\n0.9\n2.0\n\n\n1.3\n2.6\n\n\n2.1\n4.3\n\n\n2.5\n5.8\n\n\n2.4\n5.1\n\n\n1.7\n3.2\n\n\n0.7\n1.8\n\n\n1.2\n2.3\n\n\n1.6\n3.0\n\n\n\n\n이 데이터의 산점도를 그리시오.\n\n\ndt <- data.frame(x = c(0.9,1.3,2.1,2.5,2.4,1.7,0.7,1.2,1.6),\n                 y = c(2.0,2.6,4.3,5.8,5.1,3.2,1.8,2.3,3.0))\ndt\n\n\n\nA data.frame: 9 × 2\n\n    xy\n    <dbl><dbl>\n\n\n    0.92.0\n    1.32.6\n    2.14.3\n    2.55.8\n    2.45.1\n    1.73.2\n    0.71.8\n    1.22.3\n    1.63.0\n\n\n\n\nAnswer\n\nplot(y~x, \n     data = dt,\n     xlab = \"무게\",\n     ylab = \"에너지소모량\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\n\n\n\n\n\n양의 상관관계가 있어보인다.\n무게가 커질수록 에너지소모량도 큰 경향이 보이기 때문이다.\n우상향의 모양이라, 단순상관선형 적용해보면 되겠다.\n\n\n최소제곱법의 의한 회귀직선을 적합시키시오.\n\nAnswer\n\ndt1 <- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x),\n  y_bary = dt$y - mean(dt$y)) \n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주기 위해 \\(x_i - \\bar{x}, y_i - \\bar{y}\\)를 구했다.\n\n\ndt1$x_barx2 <- dt1$x_barx^2\ndt1$y_bary2 <- dt1$y_bary^2\ndt1$xy <-dt1$x_barx * dt1$y_bary\n\n\n\\(S_{xx}, S_{yy},S_{xy}\\)를 구해주었다.\n\n\ndt1\n\n\n\nA data.frame: 9 × 8\n\n    ixyx_barxy_baryx_barx2y_bary2xy\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.92.0-0.7-1.34444440.491.8075309 0.94111111\n    21.32.6-0.3-0.74444440.090.5541975 0.22333333\n    32.14.3 0.5 0.95555560.250.9130864 0.47777778\n    42.55.8 0.9 2.45555560.816.0297531 2.21000000\n    52.45.1 0.8 1.75555560.643.0819753 1.40444444\n    61.73.2 0.1-0.14444440.010.0208642-0.01444444\n    70.71.8-0.9-1.54444440.812.3853086 1.39000000\n    81.22.3-0.4-1.04444440.161.0908642 0.41777778\n    91.63.0 0.0-0.34444440.000.1186420 0.00000000\n\n\n\n\n\n반올림 해주었다.\n\n\nround(colSums(dt1),3)\n\ni45x14.4y30.1x_barx0y_bary0x_barx23.26y_bary216.002xy7.05\n\n\n\\(\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\)\n\\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\\)\n\nbeta1 <- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\nbeta0 <- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\ncat(\"hat beta1 = \", beta1)\n\nhat beta0 =  -0.1156783hat beta1 =  2.162577\n\n\n\n\\(\\hat{y} = -0.1156783 + 2.162577x\\)의 모형으로 적합되었다.\n\n\n데이터의 산점도를 그리고 추정한 회귀직선을 (1)에서 그린 산점도 위에 그리시오.\n\nAnswer\n\nplot(y~x, \n     data = dt,\n     xlab = \"무게\",\n     ylab = \"에너지소모량\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"darkorange\")\npar(new=TRUE)\nplot(-0.1156783 +  2.162577*x~x,\n     data = dt,\n     xlab = \"\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"blue\")\n\n\n\n\n\n추정한 회귀직선을 그려보니 오차가 클 것처럼 y와 \\(\\hat{y}\\)가 떨어진 값이 많아보인다.\n\n\n결정계수와 상관계수를 구하시오.\n\n\\(r_{xy} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\)\n\nrxy = colSums(dt1)[8]/sqrt(colSums(dt1)[6]*colSums(dt1)[7])\nrxy\n\nxy: 0.976090685311348\n\n\n\n상관계수는 약 98% 로, \\(x,y\\)간 높은 양의 상관관계가 있었다.\n\n\\(R^2 = \\frac{SSR}{SST} = r^2\\)\n\nSST = sum((dt$y - mean(dt$y))^2)\n\n\nSSR = sum( ( (-0.1156783 +  2.162577*dt$x)-mean(dt$y) )^2 )\n\n\nSSE = sum( ( dt$y-(-0.1156783 +  2.162577*dt$x))^2 )\n\n\ncat(\"SST = \", SST)\ncat(\"\\nSSR = \", SSR)\ncat(\"\\nSSE = \", SSE)\n\nSST =  16.00222\nSSR =  15.24617\nSSE =  0.7560566\n\n\n\nSSR/SST\n\n0.95275330164195\n\n\n\nrxy**2\n\nxy: 0.952753025951577\n\n\n\n결정계수는 약 95%로, 설명력도 높은 편이라고 말할 수 있지만, 결정계수는 다른 모델과 비교할때 언급되는 것이 적절하다.\n\n\n분산분석표를 작성하고 회귀직선의 유의 여부를 검정하시오 (유의수준 \\(α = 0.05\\) 사용).\n\n\nMSR = SSR/1\nMSE = SSE/7\n\n\ncat(\"MSR = \", MSR)\ncat(\"\\nMSE = \", MSE)\n\nMSR =  15.24617\nMSE =  0.1080081\n\n\n\nFvalue = MSR/MSE\n\n\ncat(\"F value = \",Fvalue)\n\nF value =  141.1577\n\n\n\ncat(\"p value = \",df(Fvalue,1,7))\n\np value =  1.614709e-07\n\n\n\n\n\n\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n15.24617\n15.24617\n141.1584\n1.614672e-07\n\n\nResiduals\n7\n0.7560522\n0.1080075\n\n\n\n\n\n\nF값은 141.1584, p value는 1.614672e-07가 나왔다.\n유의수준 5%에서 모형이 유의하다는 것을 알 수 있었다.\n\n\n\\(β_0,β_1\\) 에 대한 90% 신뢰구간을 구하시오.\n\nAnswer\n\\[\\hat{\\beta_0} \\pm t_{(\\alpha/2,n-2)}\\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}}\\]\n\ncat(\"Beta0 confidence level\",beta0 + qt(0.025, 7) * sqrt((MSE)*(1/9 + mean(dt$x)^2/sum((dt$x - mean(dt$x))^2))),\"~\",beta0 + qt(0.975, 7) * sqrt((MSE)*(1/9 + mean(dt$x)^2/sum((dt$x - mean(dt$x))^2))))\n\nBeta0 confidence level -0.8514415 ~ 0.620085\n\n\n\\[\\hat{\\beta_1} \\pm t_{(\\alpha/2,n-2)} \\sqrt{\\frac{MSE}{S_{xx}}}\\]\n\ncat(\"Beta1 confidence level\",beta1 + qt(0.025, 7) * sqrt((MSE)/sum((dt$x - mean(dt$x))^2)),'~',beta1 + qt(0.975, 7) * sqrt((MSE)/sum((dt$x - mean(dt$x))^2)))\n\nBeta1 confidence level 1.732168 ~ 2.592986\n\n\n\n\\(\\beta_0\\)의 신뢰구간은 0을 포함하였다.(\\(H_0 : \\beta_0=0\\) 채택)\n\\(\\beta_1\\)의 신뢰구간은 0을 포함하지 않았다.(\\(H_0 : \\beta_1=0\\) 기각)\n신뢰구간으로 신뢰구간에 0이 포함된 \\(\\beta_1\\) 계수만 유의미하다는 것을 알 수 있다.\n\n\n\\(H_0 :β_1 =1\\) vs. \\(H_1 :β_1 \\ne 1\\)의 가설검정을 유의수준 \\(α=0.1\\)에서 수행하시오.\n\n\\[\\text{t value} = \\frac{\\hat{\\beta_1} - 1}{s.e(\\hat{\\beta_1})}\\]\n\nTvalue = (beta1 - 1)/(sqrt((MSE/sum((dt$x - mean(dt$x))^2))))\n\n\nTvalue\n\n6.3870789106442\n\n\n\n-Tvalue\n\n-6.3870789106442\n\n\n\nqt(0.95,7)\n\n1.89457860509001\n\n\n\nqt(0.05,7)\n\n-1.89457860509001\n\n\n\n구힌 t value = 6.39가 유의수준 \\(\\alpha = 0.1\\) 에서의 t value = 1.89보다 크기 때문에 유의하다는 결과가 나와 귀무가설을 기각한다.\n따라서 \\(\\beta_1\\)은 1이 아니다.\n\nfigure로 표현\n\npar(mfrow=c(1,1))\nbasic <- seq(-3,3,by=0.01)\nplot(basic,dt(basic,df=7),type=\"l\",xlim=c(-8,8))\nabline(v=Tvalue,col=\"red\",lty=2)\nabline(v=qt(0.95,7),col=\"blue\",lty=2)\ntext(x=Tvalue, y=c(0.2), labels=c(\"tvalue\\n6.39\"), pos=4, col=\"black\")\ntext(x=qt(0.95,7), y=c(0.2), labels=c(\"t(0.05,7)\\n1.89\"), pos=4, col=\"black\")\nabline(v=-Tvalue,col=\"red\",lty=2)\nabline(v=qt(0.05,7),col=\"blue\",lty=2)\n\n\n\n\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측하시오. 이것은 무게가 1,000kg이 되는 차량의 에너지 소모량의 몇 배인가?\n\n\\[\\hat{\\mu}_0 = \\hat{\\beta_0} + \\hat{\\beta_1} x_0\\]\n\ncat(\"평균 에너지 소모량 = \",beta0 + beta1 * 3)\n\n평균 에너지 소모량 =  6.372052\n\n\n\ncat(\"무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측해보니 무게가 1,000kg이 되는 차량의 에너지 소모량의\",(beta0 + beta1 * 3)/(beta0 + beta1 * 1),\"배 였다.\")\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량을 예측해보니 무게가 1,000kg이 되는 차량의 에너지 소모량의 3.113028 배 였다.\n\n\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량과 하나의 개별 \\(y\\) 값의 90% 신뢰구간을 각각 구하시오.\n\nAnswer\n\ncat(\"무게가 3,000kg 이 되는 차량의 평균 에너지 소모량은\",beta0 + beta1 * 3,\"이다.\")\n\n무게가 3,000kg 이 되는 차량의 평균 에너지 소모량은 6.372052 이다.\n\n\n\\[\\hat{Var(\\hat{\\mu}_0}) = \\sigma (\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}})\\]\n\nsigma = MSE\n\n\nmean(dt$x)\n\n1.6\n\n\n\\(S_{xx}\\)\n\nsum((dt$x - mean(dt$x))^2)\n\n3.26\n\n\n\ncolSums(dt1)[6]\n\nx_barx2: 3.26\n\n\n\\(\\hat{Var(\\hat{\\mu_0})}\\)\n\nsqrt(sigma)*(1/9 + (3-1.6)^2/3.26)\n\n0.234106948837031\n\n\n\ncat(\"hat var(hat mu zero) = \",sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\nhat var(hat mu zero) =  0.2341069\n\n\n\\(\\hat{\\sigma}_{\\hat{\\mu_0}}\\)\n\nsqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n0.483845997024912\n\n\n\ncat(\"hat sigma(hat mu zero) = \", sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26)))\n\nhat sigma(hat mu zero) =  0.483846\n\n\n\\(\\hat{\\mu_0} \\pm t_{(\\alpha/2,(n-2))}\\hat{\\sigma_{\\hat{\\mu_0}}}\\)\n\n6.372052 + qt(0.95,7)*sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n7.28873627412184\n\n\n\n6.372052 - qt(0.95,7)*sqrt(sqrt(sigma)*(1/9 + (3-1.6)^2/3.26))\n\n5.45536772587816\n\n\n\ncat(\"개별 y값의 90% 신뢰구간은 (\",5.4553690631157,\"~\",7.2887349368843,\") 이다.\")\n\n개별 y값의 90% 신뢰구간은 ( 5.455369 ~ 7.288735 ) 이다.\n\n\n\n잔차 \\(e_i = y_i − \\hat{y}_i\\) 를 구하고 잔차의 합이 0 임을 확인하시오.\n\nAnswer\n\nepsilon = dt$y - (beta0 + beta1*dt$x)\n\n\nepsilon\n\n\n0.169359236537151-0.0956714383094752-0.1257327880027260.5092365371506480.0254942058623033-0.36070211315610.401874573960464-0.179413769597819-0.344444444444445\n\n\n\ncat(\"잔차의 합 = \",sum(epsilon))\n\n잔차의 합 =  8.881784e-16\n\n\n합이 0인 것을 확인했다.\n\n잔차들의 \\(x_i\\) 에 대한 가중합, \\(\\sum x_ie_i\\) 를 구하시오.\n\n\nsum(dt$x * epsilon)\n\n9.99200722162641e-16\n\n\n잔차들의 \\(x_i\\) 에 대한 가중합, \\(\\sum x_ie_i\\)이 0인 것을 확인했다.\n\n잔차들의 \\(\\hat{y}\\)에 대한 가중합 \\(\\sum \\hat{y}_ie_i\\), 를 구하시오.\n\n\nsum((beta0 + beta1*dt$x)*epsilon)\n\n1.74860126378462e-15\n\n\n잔차들의 \\(\\hat{y}\\)에 대한 가중합 \\(\\sum \\hat{y}_ie_i\\)이 0인 것을 확인했다.\n\n원점을 지나는 회귀직선을 구하시오.\n\nAnswer\n\\(\\hat{\\beta_1} = \\frac{\\sum(x_iy_i)}{\\sum(x_i^2)}\\)\n\nsum(dt$x * dt$y)/sum((dt$x^2))\n\n2.09923954372624\n\n\n\nbeta1_0 <- sum(dt$x * dt$y)/sum((dt$x^2))\n\n\ncat(\"hat beta1_0 = \", beta1_0)\n\nhat beta1_0 =  2.09924\n\n\n\n원점을 지나는 회귀직선은 \\(\\hat{y} = 2.09924x\\)의 모형으로 적합되었다.\n\n\n위 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간을 구하시오.\n\nAnswer\n\\[\\hat{\\beta_1} \\pm t_{(\\alpha/2,n-1)}\\frac{\\hat{\\sigma}}{\\sqrt{S_{xx}}}\\]\n\ncolSums(dt1)[6]\n\nx_barx2: 3.26\n\n\n\nSSR_0 = sum( ( (2.09924*dt$x) )^2 )\n\n\nSSE_0 = sum((dt$y - 2.09924*dt$x)^2)\n\n\nSST_0 = sum(dt$y^2)\n\n\nMSE_0 = SSE_0/8\n\n\nMSR_0 = SSR_0/1\n\n\nsigma_0 = sqrt(MSE_0)\n\n\nbeta1_0 + qt(0.95,8) * sigma_0/sqrt(3.26)\n\n2.41896448320474\n\n\n\nbeta1_0 - qt(0.95,8) * sigma_0/sqrt(3.26)\n\n1.77951460424773\n\n\n\ncat(\"원점을 지나는 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간은 (\",beta1_0 - qt(0.95,8) * sigma_0/sqrt(3.26),\"~\",beta1_0 + qt(0.95,8) * sigma_0/sqrt(3.26),\")이다.\")\n\n원점을 지나는 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간은 ( 1.779515 ~ 2.418964 )이다.\n\n\n\n\\(\\beta_1\\)의 신뢰구간은 0을 포함하지 않았다.(\\(H_0 : \\beta_1=0\\) 기각)\n신뢰구간으로 신뢰구간에 0이 포함된 \\(\\beta_1\\) 계수가 유의미하다는 것을 알 수 있다.\n\n\n원점을 지나는 회귀직선의 결정계수를 구하시오.\n\nAnswer\n\\(R^2 = \\frac{SSR}{SST} = r^2\\)\n\nSSR_0/SST_0\n\n0.993392179573841\n\n\n\ncat(\"원점을 지나는 회귀직선의 결정계수는 \", SSR_0/SST_0,\"로 약\",round(SSR_0/SST_0,2),\"% 였다.\")\n\n원점을 지나는 회귀직선의 결정계수는  0.9933922 로 약 0.99 % 였다.\n\n\n\n원점을 포함한 회귀직선과 포함하지 않은 회귀직선의 결과를 비교하여라.\n\nAnswer\n\ncat(\"원점을 포함하지 않는 회귀직선의 결정계수는 \",SSR/SST,\"로, 원점을 포함하는 회귀직선의 결정계수인 \",SSR_0/SST_0,\"보다 작다.\")\n\n원점을 포함하지 않는 회귀직선의 결정계수는  0.9527533 로, 원점을 포함하는 회귀직선의 결정계수인  0.9933922 보다 작다.\n\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 평균재곱오차는 \",MSE,\"이며, 원점을 포함하는 회귀직선의 평균제곱오차는 \",MSE_0,\"이다. 원점을 포함하는 모형의 오차가 조금 더 작았다.\")\n\n원점을 포함하지 않는 회귀직선의 평균재곱오차는  0.1080081 이며, 원점을 포함하는 회귀직선의 평균제곱오차는  0.0963731 이다. 원점을 포함하는 모형의 오차가 조금 더 작았다.\n\n\n\nFvalue = MSR/MSE\n\n\nFvalue_0 = MSR_0 / MSE_0\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 F 값은 \",Fvalue,\"로, 원점을 포함하는 회귀직선의 F 값인 \",Fvalue_0,\"보다 작다. 따라서 원점을 포함한 모델이 회귀모형애 의해 설명되는 부분이 더 크며, 오차항에 기인된 부분이 더 작다.\")\n\n원점을 포함하지 않는 회귀직선의 F 값은  141.1577 로, 원점을 포함하는 회귀직선의 F 값인  1202.608 보다 작다. 따라서 원점을 포함한 모델이 회귀모형애 의해 설명되는 부분이 더 크며, 오차항에 기인된 부분이 더 작다.\n\n\n\ncat(\"원점을 포함하지 않는 회귀직선의 p value 는 \",df(Fvalue,1,7),\"로, 원점을 포함하는 회귀직선의 p value인\",df(Fvalue_0,1,8),\"과 같이 p value이 충분히 작아 두 모형이 모두 유의함을 알 수 있었다.\")\n\n원점을 포함하지 않는 회귀직선의 p value 는  1.614709e-07 로, 원점을 포함하는 회귀직선의 p value인 1.728622e-12 과 같이 p value이 충분히 작아 두 모형이 모두 유의함을 알 수 있었다.\n\n\n\nplot(beta0 + beta1*x~x,\n     data = dt,\n     xlab = \"원점을 지나지 않는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"blue\")\nplot(beta1_0*x~x,\n     data = dt,\n     xlab = \"원점을 지나는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,type='l',\n     col  = \"red\")\n\n\n\n\n\n\n\n\n선형성 만족\n\n\nepsilon = dt$y - beta0 + beta1*dt$x\nepsilon_0 = dt$y - beta1_0*dt$x\n\n\nplot(epsilon,\n     xlab = \"원점을 지나지 않는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"blue\")\nplot(epsilon_0,\n     xlab = \"원점을 지나는 모형\",\n     ylab = \"\",\n     pch  = 16,\n     cex  = 1,\n     col  = \"red\")\n\n\n\n\n\n\n\n\n원점을 지나지 않는 모형이 한쪽 부호에 머무는 경향이 있어보인다.(독립성을 만족하지 않을 수 있다.)\n원점을 지나는 모형은 그러한 경향이 없고, 등분산성을 만족하는 것처럼 보인다.\n\n\nshapiro.test(beta0 + beta1*dt$x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  beta0 + beta1 * dt$x\nW = 0.95279, p-value = 0.7207\n\n\n\nshapiro.test(beta1_0*dt$x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  beta1_0 * dt$x\nW = 0.95279, p-value = 0.7207\n\n\n\n두 모형 모두 정규성 가정을 만족했다.\n\nANOVA table 비교\n\n\n\ny=beta0+beta1x\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n15.24617\n15.24617\n141.1584\n1.614672e-07\n\n\nResiduals\n7\n0.7560522\n0.1080075\n\n\n\n\n\n\n\n\ny=beta1x\ndf\nsum of square\nmean of square\nF value\np value\n\n\n\n\nx\n1\n115.89906559088\n115.89906559088\n1202.60806139735\n1.728622e-12\n\n\nResiduals\n8\n0.77098479088\n0.09637309886\n\n\n\n\n\n\n(강의노트 CH04, p9) 다음이 성립함을 증명하시오.\n\n\\[\\hat{beta}_0 ~ N\\big( \\beta_0 , \\sigma^2\\big( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{(xx)}} \\big) \\big)\\]\nAnswer\n\\(\\hat{\\beta_0}\\)\n\\(= \\bar{y} - \\hat{\\beta_1} \\bar{x} = \\bar{y} - \\frac{S_{xy}}{S_{xx}}\\bar{x}\\)\n\\(= \\bar{y} - \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{S_{xx}}\\bar{x}\\)\n\\(= \\bar{y} - \\sum\\frac{(x_i-\\bar{x})y_i\\bar{x}-(x_i - \\bar{x})\\bar{y}\\bar{x}}{S_{xx}}\\)\n\\(= \\bar{y} - \\sum\\frac{(x_i - \\bar{x})}{S_{xx}}y_i\\bar{x} - \\frac{\\bar{x}\\bar{y}}{S_{xx}}\\sum(s_i - \\bar{x})\\)\n\\(= \\bar{y} - \\sum\\frac{(s_i - \\bar{x})}{S_{xx}}y_i\\bar{x}\\)\n\\(\\approx \\bar{y} - \\sum a_iy_i\\bar{x}\\)\n\\(E(\\hat{\\beta_0})\\)\n\\(=\\bar{y} - \\bar{x}\\sum a_i E(y_i)\\)\n\\(=\\bar{y} - \\bar{x}\\sum a_i (\\beta_0 + \\beta_1 x_i)\\)\n\\(=\\bar{y} - (\\beta_0\\bar{x}\\sum a_i + \\beta_1 \\bar{x} \\sum a_i x_i)\\)\n\\(=\\bar{y} - \\beta_1\\bar{x} = \\beta_0\\)\n\\(\\star \\sum a_i = \\sum\\frac{x_i - \\bar{x}}{S_{xx}} = \\frac{1}{S_{xx}}\\sum(x_i-\\bar{x} )= 0\\)\n\\(\\star \\sum a_i x_i = \\sum\\frac{(x_i - \\bar{x})x_i}{S_{xx}}= \\frac{1}{S_{xx}} \\sum(x_i - \\bar{x}(x_i - \\bar{x} + \\bar{x} = \\frac{1}{S_{xx}} \\sum(x_i - \\bar{x})^2 = 1\\)\n\\(Var(\\hat{\\beta_0})\\)\n\\(=Var(\\bar{y} - \\bar{x}\\sum a_i x_i)\\)\n\\(=var(\\frac{y_i}{n} - \\bar{x}\\sum a_i y_i)\\)\n\\(=\\frac{\\sigma^2}{n} - \\bar{x}^2\\sigma^2\\sum a_i^2\\)\n$ = - $\n\\(= \\sigma^2(\\frac{1}{n} - \\frac{\\bar{x}^2}{S_{xx}})\\)\n\\(\\star \\sum a_i^2 = \\sum\\frac{(x_i - \\bar{x})^2}{S_{xx}} = \\frac{1}{S_{xx}^2}\\sum(x_i - \\bar{x})^2 = \\frac{1}{S_{xx}}\\)"
  },
  {
    "objectID": "posts/rl/2022-11-14-rl_CH06, CH07.html",
    "href": "posts/rl/2022-11-14-rl_CH06, CH07.html",
    "title": "고급회귀분석 실습 CH06, CH07",
    "section": "",
    "text": "chapter 6, chapter 7\n\nlibrary(MASS)\n\n\ndata(Boston)\n\n\nhead(Boston)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00632182.3100.5386.57565.24.0900129615.3396.904.9824.0\n    20.02731 07.0700.4696.42178.94.9671224217.8396.909.1421.6\n    30.02729 07.0700.4697.18561.14.9671224217.8392.834.0334.7\n    40.03237 02.1800.4586.99845.86.0622322218.7394.632.9433.4\n    50.06905 02.1800.4587.14754.26.0622322218.7396.905.3336.2\n    60.02985 02.1800.4586.43058.76.0622322218.7394.125.2128.7\n\n\n\n\n\nnrow(Boston)\n\n506\n\n\n보스턴 집값 데이터 이 데이터는 보스턴 근교 지역의 집값 및 다른 정보를 포함한다.\nMASS 패키지를 설치하면 데이터를 로딩할 수 있다.\nB보스턴 근교 506개 지역에 대한 범죄율 (crim)등 14개의 변수로 구성\n• crim : 범죄율\n• zn: 25,000평방비트 기준 거지주 비율\n• indus: 비소매업종 점유 구역 비율\n• chas: 찰스강 인접 여부 (1=인접, 0=비인접)\n• nox: 일산화질소 농도 (천만개 당)\n• rm: 거주지의 평균 방 갯수 ***\n• age: 1940년 이전에 건축된 주택의 비율\n• dis: 보스턴 5대 사업지구와의 거리\n• rad: 고속도로 진입용이성 정도\n• tax: 재산세율 (10,000달러 당)\n• ptratio: 학생 대 교사 비율\n• black: 1000(B − 0.63)2, B: 아프리카계 미국인 비율\n• lstat : 저소득층 비율 ****\n• medv: 주택가격의 중앙값 (단위:1,000달러 당)\n\n데이터 세 변수만 가지고 산점도 그려보기\n음의 상관관계가 있는 것 같기도 하고~\n\n\npairs(Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))], pch=16, col='darkorange')\n\n\n\n\n\nlm(y ~ x_1 + x_2, data)\n\n\nfit_Boston<-lm(medv~rm+lstat, data=Boston)\n\n\nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\nR-squared:  0.6386\n\n집값 y의 총변동 중에 회귀모형이 63.86%정도 설명하고 있다.\n\nAdjusted R-squared:  0.6371\n\n설명변수가 증가하면 R-squared값이 증가하는 현상때문에 수정된 R-squared값이 필요\n모형 비교시 Adjusted R-squared가 나을 수도 있음\n\nrm,lstat의 p-value <2e-16\n\n\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\n\n원하는 변수만 추출해내서 회귀분석 진행가능\n\n\ndt <- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    rmlstatmedv\n    <dbl><dbl><dbl>\n\n\n    16.5754.9824.0\n    26.4219.1421.6\n    37.1854.0334.7\n    46.9982.9433.4\n    57.1475.3336.2\n    66.4305.2128.7\n\n\n\n\n\n. 쓰면 설명변수 다 쓰겠다는 의미\n\n\nfit_Boston<-lm(medv~., data=dt)\n\n\\(\\hat{y} = -1.3583 + 5.0948*rm - 0.6424*lstat\\)\n\n저소득층이 변하지 않고 방이 하나 더 증가한다면 집값의 중앙값은 5.0948 정도 증가한다.\n방의 수는 변하지 않고 저소득층이 증가한다면 집값의 중앙값은 0.6424 정도 감소한다.\nSSR = 41308.84 = 20654.42 + 20654.42\n밑의 F 통계량이 아니라 summary 의 F 통계량을 봐야 한다.\n\n\nanova(fit_Boston)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    rm  120654.4220654.41622672.90398.266887e-95\n    lstat  1 6622.57 6622.56999215.75796.669365e-41\n    Residuals50315439.31   30.69445      NA          NA\n\n\n\n\n\\(var(\\hat{\\beta}) = (X^TX)^{-1} \\sigma^2\\)\n\nvcov(fit_Boston)  \n\n\n\nA matrix: 3 × 3 of type dbl\n\n    (Intercept)rmlstat\n\n\n    (Intercept)10.06683612-1.39248641-0.099178133\n    rm-1.39248641 0.19754958 0.011930670\n    lstat-0.09917813 0.01193067 0.001912441\n\n\n\n\n\nconfint(fit_Boston, level = 0.95)\n\n\n\nA matrix: 3 × 2 of type dbl\n\n    2.5 %97.5 %\n\n\n    (Intercept)-7.5919003 4.8753547\n    rm 4.2215504 5.9680255\n    lstat-0.7282772-0.5564395\n\n\n\n\n\ncoef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n(Intercept)4.87535465808389rm5.96802553290791lstat-0.556439501179164\n\n\n\ncoef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n(Intercept)-7.59190028183298rm4.22155043576519lstat-0.728277167309095\n\n\n평균반응, 개별 y 추정\n\\(E(Y|x_0), y = E(Y|x_0) + \\epsilon\\)\n\nnew_dt <- data.frame(rm=7, lstat=10)\n\n\\(\\hat{y}_0 = -1.3583 + 5.0948*7 - 0.6424*10\\)\n\npredict(fit_Boston, newdata = new_dt)\n\n1: 27.88165973604\n\n\n평균반응\n\npredict(fit_Boston, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  \n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816627.1734728.58985\n\n\n\n\n개별 y\n\npredict(fit_Boston, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  \n\n\n\nA matrix: 1 × 3 of type dbl\n\n    fitlwrupr\n\n\n    127.8816616.9737538.78957\n\n\n\n\n\nfit은 같고 lwr,upr만 달라진다.\n\n잔차분석\n\\(\\epsilon\\) : 선형성, 등분산성, 정규성, 독립성\nyhat <- fitted_values(fit_Boston)\n\nyhat <- fitted(fit_Boston)\n\n\nres <- resid(fit_Boston)\n\n잔차그림 - 대칭 아니다 - u 턴 커브 모형이다. - 애초에 산점도에서도 커브가 보였다. - 나중에 제곱텀 넣어볼 예정\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n제곱텀 필요해보인다.\n\n\nplot(res ~ dt$rm,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n제곱텀 필요해보인다.\n\n\nplot(res ~ dt$lstat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n독립성검정 : DW test\n\nlibrary(lmtest)\n\n\\(H_0 : \\text{ uncorrelated vs } H_1 : \\rho \\neq 0\\)\n\ndwtest(fit_Boston, alternative = \"two.sided\")  \n\n\n    Durbin-Watson test\n\ndata:  fit_Boston\nDW = 0.83421, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\np-value < 2.2e-16로 귀무가설 기각한다. 잔차는 독립이 아니다.\n\n잔차의 QQ plot\n정규성 확인 결과도 좋지 않음\n\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\n\n\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 3\n\n    rmlstatmedv\n    <dbl><dbl><dbl>\n\n\n    16.5754.9824.0\n    26.4219.1421.6\n    37.1854.0334.7\n    46.9982.9433.4\n    57.1475.3336.2\n    66.4305.2128.7\n\n\n\n\n\ndt$lstat2 <- (dt$lstat)^2\n\n\nhead(dt)\n\n\n\nA data.frame: 6 × 4\n\n    rmlstatmedvlstat2\n    <dbl><dbl><dbl><dbl>\n\n\n    16.5754.9824.024.8004\n    26.4219.1421.683.5396\n    37.1854.0334.716.2409\n    46.9982.9433.4 8.6436\n    57.1475.3336.228.4089\n    66.4305.2128.727.1441\n\n\n\n\n1번 방법\n\nfit_Boston2<-lm(medv~., data=dt)\n\n# 이렇게 쓰면 위에와 결과 같음\nfit_Boston2<-lm(medv~rm+lstat+lstat^2, data=Boston)\n2번 방법 변수 생성 없이 수행가능\n\nfit_Boston2<-lm(medv~rm+lstat+I(lstat^2), data=Boston)\n\n\nAdjusted R-squared:  0.7013이 63%에서 증가한 모습\n\n\nsummary(fit_Boston2)\n\n\nCall:\nlm(formula = medv ~ rm + lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.1427  -3.2429  -0.4829   2.3607  27.0256 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.68964    3.13810   3.725 0.000217 ***\nrm           4.22727    0.41172  10.267  < 2e-16 ***\nlstat       -1.84863    0.12213 -15.136  < 2e-16 ***\nI(lstat^2)   0.03634    0.00348  10.443  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.027 on 502 degrees of freedom\nMultiple R-squared:  0.7031,    Adjusted R-squared:  0.7013 \nF-statistic: 396.2 on 3 and 502 DF,  p-value: < 2.2e-16\n\n\n\nyhat2 <- fitted.values(fit_Boston2)\n\n\nres2 <- resid(fit_Boston2)\n\n\n곡선 느낌은 사라지고 outlier는 보이고 있다.\n\n\nplot(res2 ~ yhat2,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n꼬리가 죽은 모습\n\n\nqqnorm(res2, pch=16)\nqqline(res2, col = 2)\n\n\n\n\n\\(H_0 : \\text{ uncorrelated vs } H_1 : \\rho \\neq 0\\)\n\ndwtest(fit_Boston2, alternative = \"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit_Boston2\nDW = 0.84831, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n여전히 잔차가 독립이 아니고 기각한다.\n\n\nfit_Boston3 <- lm(medv~rm, data=Boston)\n\n\nfit_Boston4 <- lm(medv~lstat, data=Boston)\n\n\nsummary(fit_Boston3)\n\n\nCall:\nlm(formula = medv ~ rm, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -34.671      2.650  -13.08   <2e-16 ***\nrm             9.102      0.419   21.72   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\nsummary(fit_Boston4)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\nx1<-c(4,8,9,8,8,12,6,10,6,9)\nx2<-c(4,10,8,5,10,15,8,13,5,12)\ny<-c(9,20,22,15,17,30,18,25,10,20)\n\nFM\n\nfit<-lm(y~x1+x2)\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4575 -1.9100  0.3314  0.6388  3.2628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.6507     2.9075  -0.224   0.8293  \nx1            1.5515     0.6462   2.401   0.0474 *\nx2            0.7599     0.3968   1.915   0.0970 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.278 on 7 degrees of freedom\nMultiple R-squared:  0.9014,    Adjusted R-squared:  0.8732 \nF-statistic:    32 on 2 and 7 DF,  p-value: 0.0003011\n\n\n\nanova(fit)\n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11313.04348313.04347860.3231030.0001100467\n    x21 19.03040 19.030400 3.6671350.0970444465\n    Residuals7 36.32612  5.189446       NA          NA\n\n\n\n\ninstall.packages(\"car\")\n\nlibrary(car)\n\n\\(H_0 : T\\times\\beta = c\\)\n$b_1-b_2=0 (0,1,-1) $\n\\(H_0 : \\beta_1 = \\beta_2\\)\n\nlinearHypothesis(fit, c(0,1,-1), 0)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1839.53514NA      NA       NA       NA\n    2736.32612 13.2090140.61837310.4574425\n\n\n\n\n\n제약조건, T matrix c(0,1,-1)\nc = 0\n\n\n3.209014 / (36.32612/7)\n\n0.618373170600108\n\n\n같다~\n\\(H_0 : \\beta_1 = 1\\)\n\nlinearHypothesis(fit, c(0,1,0), 1)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1840.10492NA      NA       NA       NA\n    2736.32612 13.7787970.72816960.4217136\n\n\n\n\n\n기각하지 못함\n\n\\(H_0 : \\beta_1 = \\beta_2 + 1\\)\n\nlinearHypothesis(fit, c(0,1,-1), 1)\n\n\n\nA anova: 2 × 6\n\n    Res.DfRSSDfSum of SqFPr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1836.54865NA       NA        NA      NA\n    2736.32612 10.22252730.042880740.841845\n\n\n\n\n\nF0\n\n0.0428807391894599\n\n\n\\(H_0 : \\beta_1 = \\beta_2 + 1\\)\n\\(y = b_0 + b_1x_1 + b_2x_2 + \\epsilon = b_0+x_1 + b_2(x_1+x_2)+\\epsilon\\)\n\\(y-x_1 = b_0+b_2(x_1+x_2)+\\epsilon\\) : RM\n\ny1 <- y-x1\nz1 <- x1 + x2\n\n\nfit2 <- lm(y1~z1)\n\n\nsummary(fit2)\n\n\nCall:\nlm(formula = y1 ~ z1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5054 -1.9294  0.4236  0.6821  3.4473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.0014     2.2175  -0.452 0.663574    \nz1            0.6824     0.1242   5.493 0.000578 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.137 on 8 degrees of freedom\nMultiple R-squared:  0.7904,    Adjusted R-squared:  0.7642 \nF-statistic: 30.17 on 1 and 8 DF,  p-value: 0.0005785\n\n\n\nanova(fit2)\n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\nFM\n\nanova(fit)  \n\n\n\nA anova: 3 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    x11313.04348313.04347860.3231030.0001100467\n    x21 19.03040 19.030400 3.6671350.0970444465\n    Residuals7 36.32612  5.189446       NA          NA\n\n\n\n\nRM\n\nanova(fit2)  \n\n\n\nA anova: 2 × 5\n\n    DfSum SqMean SqF valuePr(>F)\n    <int><dbl><dbl><dbl><dbl>\n\n\n    z11137.85135137.85135130.173780.0005784583\n    Residuals8 36.54865  4.568581      NA          NA\n\n\n\n\n\\(F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}\\)\nSSE_FM\n\nSSE_FM <- anova(fit)$Sum[3]  \n\nSSE_RM\n\nSSE_RM <- anova(fit2)$Sum[2]  \n\n\nF0 <- (SSE_RM-SSE_FM)/(SSE_FM/7)\nF0\n\n0.0428807391894599\n\n\n기각역 \\(F_{0.05}(1,7)\\)\n\nqf(0.95, 1, 7)\n\n5.59144785122073\n\n\n기각할 수 없다.\\(\\beta_1\\)은 \\(\\beta_2\\)에 1 더한 값과 같다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Master’s degree",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 28, 2022\n\n\n고급회귀분석 실습 CH13\n\n\nSEOYEON CHOI\n\n\n\n\nNov 21, 2022\n\n\n기계학습 특강 (11주차) 11월16일\n\n\nSEOYEON CHOI\n\n\n\n\nNov 21, 2022\n\n\nRegression HW 3\n\n\nSEOYEON CHOI\n\n\n\n\nNov 14, 2022\n\n\n고급회귀분석 실습 CH06, CH07\n\n\nSEOYEON CHOI\n\n\n\n\nNov 9, 2022\n\n\n기계학습 특강 (10주차) 11월9일\n\n\nSEOYEON CHOI\n\n\n\n\nNov 2, 2022\n\n\n기계학습 특강\n\n\nSEOYEON CHOI\n\n\n\n\nNov 2, 2022\n\n\n기계학습 특강 (9주차) 11월02일\n\n\nSEOYEON CHOI\n\n\n\n\nOct 26, 2022\n\n\n기계학습 특강 (8주차) 10월26일–(2)\n\n\nSEOYEON CHOI\n\n\n\n\nOct 26, 2022\n\n\n기계학습 특강 (8주차) 10월26일–(1)\n\n\nSEOYEON CHOI\n\n\n\n\nOct 23, 2022\n\n\nRegression HW 2\n\n\nSEOYEON CHOI\n\n\n\n\nOct 19, 2022\n\n\n기계학습 특강 (7주차) 10월19일\n\n\nSEOYEON CHOI\n\n\n\n\nOct 12, 2022\n\n\n기계학습 특강 (6주차) 10월5일\n\n\nSEOYEON CHOI\n\n\n\n\nOct 5, 2022\n\n\n기계학습 특강 (5주차) 10월5일\n\n\nSEOYEON CHOI\n\n\n\n\nOct 5, 2022\n\n\n기계학습 특강 (6주차) 10월5일 Homework\n\n\nSEOYEON CHOI\n\n\n\n\nSep 29, 2022\n\n\n기계학습 특강 (4주차) 9월28일\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\n기계학습 특강 (3주차) 9월21일\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\n고급회귀분석 실습 CH03, CH04\n\n\nSEOYEON CHOI\n\n\n\n\nSep 21, 2022\n\n\nRegression HW 1\n\n\nSEOYEON CHOI\n\n\n\n\nSep 19, 2022\n\n\nAssignment 1 (22.09.19) – 풀이O\n\n\nSEOYEON CHOI\n\n\n\n\nSep 14, 2022\n\n\n기계학습 특강 (2주차) 9월14일\n\n\nSEOYEON CHOI\n\n\n\n\nSep 7, 2022\n\n\n기계학습 특강 (1주차) 9월7일 Intro\n\n\nSEOYEON CHOI\n\n\n\n\nSep 7, 2022\n\n\n기계학습 특강 (1주차) 9월7일\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nSpecial Topics in Machine Learning\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nAdvanced Regression Analysis\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, there :)"
  }
]