{"title":"A3: 강화학습 (3) – LunarLander","markdown":{"yaml":{"title":"A3: 강화학습 (3) – LunarLander"},"headingText":"강의영상","containsRefs":false,"markdown":"\n\n\n\n\n최규빈  \n2023-09-01\n\n\n<https://youtu.be/playlist?list=PLQqh36zP38-zBEizLbjgRE8qMfsJML6Ua&si=HALKE6fjiWB12AGW>\n\n# imports\n\n\n\n# 예비학습\n\n`-` `collections.deque` 의 기능\n\n`-` 단점? numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\n`-` 그렇지만 필요하다면 np.array 화 시킬 수 있음.\n\n`-` collection.deque 는 리플레이 버퍼를 구현할때 유용한 자료구조이다.\n\n-   (우리가 했던) 기존방식: 모든 데이터를 저장하며 하나의 경험씩 학습함\n-   리플레이버퍼: 최근 $N$개의 데이터를 저장하여 여러경험을 샘플링하여\n    학습하는 방식\n-   리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을\n    저장하고 무작위로 재사용하여 학습이 안정적으로 된다, “저장 -\\> 학습\n    -\\> 저장” 순으로 반드시 실시간으로 학습할 필요가 없어서 병렬처리에\n    용이하다, 강화학습에서 연속된 경험은 상관관계가 있을 수 있는데\n    무작위 샘플로 이러한 상관관계를 제거할 수 있음\n\n# Game3: LunarLander\n\n`-` 환경생성\n\n`-` state_space\n\n`-` action_space\n\n`-` env.reset()\n\n`-` env.render()\n\n`-` env.step\n\n`-` play\n\n-   0 : 아무행동도 하지 않음\n-   1 : 왼쪽\n-   2 : 위\n-   3 : 오른쪽\n\n# 시각화\n\n# `q_net`\n\n`-` 원래는 `agent.q` 에 해당하는 것인데, 이전에서는 `agent.q`를 (4,4,4)\nshape의 numpy array 를 사용했는데 여기서는 불가능\n\n-   4x4 grid: 상태공간의 차원은 2차원이며 가질수 있는 값은 16개, 각\n    상태공간에서 할수 있는 행동이 4개 -\\> 총 16\\*4의 경우의 수에 대한\n    reward만 조사하면 되었음\n-   LunarLander: 상태공간의 차원은 8차원이지만 가질수 있는 값의 범위는\n    무한대 -\\> 무수히 많은 경우에 대한 reward 값을 조사하는건 현실적으로\n    불가능\n\n`-` 데이터를 모아보자.\n\n`-` 이전코드에서 아래에 대응하는 부분을 구현하면 된다.\n\n``` python\n## 1. q[x,y,a]를 초기화: q(s)를 넣으면 action에 대한 q값을 알려주는 기능 \nagent.q = np.zeros([4,4,4]) \n\n## 2. q_estimated 를 계산 \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = agent.reward + 0.99 * q_future 를 수행하는 과정 \nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff \n```\n\n`1`. agent.q 에 대응하는 과정\n\n-   q_net은 8개의 숫자가 입력으로 오면 4개의 숫자가 리턴되는 함수이다.\n-   해석을 하면 8개의 숫자는 state를 나타내는 숫자로 이해할 수 있고\n    4개의 숫자는 각 action에 대한 q값으로 해석할 수 있다.\n-   하지만 이 숫자가 합리적인건 아님 (아무숫자임)\n-   q_net의 특징: 고정된 함수가 아니고 데이터를 이용하여 점점 더\n    그럴듯한 숫자를 뱉어내도록 학습할 수 있는 함수이다. (뉴럴네트워크)\n\n`1`. agent.q 에 대응하는 과정 (배치버전)\n\n– get batch –\n\n– q_net –\n\n`2`. q_estimated\n\n`3`. q_realistic = agent.reward + 0.99 \\* q_future\n\n– q_future –\n\n`4`. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정\n\n``` python\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야합니다.. \noptimizer = torch.optim.Adam(q_net.parameters(),lr=0.0001) \nfor _ in range(2000):\n    ~~~\n    ~~~\n    q_estimated = ~~~ \n    q_realistic = ~~~ \n    loss = torch.nn.functional.mse_loss(q_estimated,q_realistic)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n# policy\n\n# Agent 클래스 + run\n\n`-` 시각화를 위한코드\n","srcMarkdownNoYaml":"\n\n\n\n\n최규빈  \n2023-09-01\n\n# 강의영상\n\n<https://youtu.be/playlist?list=PLQqh36zP38-zBEizLbjgRE8qMfsJML6Ua&si=HALKE6fjiWB12AGW>\n\n# imports\n\n\n\n# 예비학습\n\n`-` `collections.deque` 의 기능\n\n`-` 단점? numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\n`-` 그렇지만 필요하다면 np.array 화 시킬 수 있음.\n\n`-` collection.deque 는 리플레이 버퍼를 구현할때 유용한 자료구조이다.\n\n-   (우리가 했던) 기존방식: 모든 데이터를 저장하며 하나의 경험씩 학습함\n-   리플레이버퍼: 최근 $N$개의 데이터를 저장하여 여러경험을 샘플링하여\n    학습하는 방식\n-   리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을\n    저장하고 무작위로 재사용하여 학습이 안정적으로 된다, “저장 -\\> 학습\n    -\\> 저장” 순으로 반드시 실시간으로 학습할 필요가 없어서 병렬처리에\n    용이하다, 강화학습에서 연속된 경험은 상관관계가 있을 수 있는데\n    무작위 샘플로 이러한 상관관계를 제거할 수 있음\n\n# Game3: LunarLander\n\n`-` 환경생성\n\n`-` state_space\n\n`-` action_space\n\n`-` env.reset()\n\n`-` env.render()\n\n`-` env.step\n\n`-` play\n\n-   0 : 아무행동도 하지 않음\n-   1 : 왼쪽\n-   2 : 위\n-   3 : 오른쪽\n\n# 시각화\n\n# `q_net`\n\n`-` 원래는 `agent.q` 에 해당하는 것인데, 이전에서는 `agent.q`를 (4,4,4)\nshape의 numpy array 를 사용했는데 여기서는 불가능\n\n-   4x4 grid: 상태공간의 차원은 2차원이며 가질수 있는 값은 16개, 각\n    상태공간에서 할수 있는 행동이 4개 -\\> 총 16\\*4의 경우의 수에 대한\n    reward만 조사하면 되었음\n-   LunarLander: 상태공간의 차원은 8차원이지만 가질수 있는 값의 범위는\n    무한대 -\\> 무수히 많은 경우에 대한 reward 값을 조사하는건 현실적으로\n    불가능\n\n`-` 데이터를 모아보자.\n\n`-` 이전코드에서 아래에 대응하는 부분을 구현하면 된다.\n\n``` python\n## 1. q[x,y,a]를 초기화: q(s)를 넣으면 action에 대한 q값을 알려주는 기능 \nagent.q = np.zeros([4,4,4]) \n\n## 2. q_estimated 를 계산 \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = agent.reward + 0.99 * q_future 를 수행하는 과정 \nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff \n```\n\n`1`. agent.q 에 대응하는 과정\n\n-   q_net은 8개의 숫자가 입력으로 오면 4개의 숫자가 리턴되는 함수이다.\n-   해석을 하면 8개의 숫자는 state를 나타내는 숫자로 이해할 수 있고\n    4개의 숫자는 각 action에 대한 q값으로 해석할 수 있다.\n-   하지만 이 숫자가 합리적인건 아님 (아무숫자임)\n-   q_net의 특징: 고정된 함수가 아니고 데이터를 이용하여 점점 더\n    그럴듯한 숫자를 뱉어내도록 학습할 수 있는 함수이다. (뉴럴네트워크)\n\n`1`. agent.q 에 대응하는 과정 (배치버전)\n\n– get batch –\n\n– q_net –\n\n`2`. q_estimated\n\n`3`. q_realistic = agent.reward + 0.99 \\* q_future\n\n– q_future –\n\n`4`. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정\n\n``` python\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야합니다.. \noptimizer = torch.optim.Adam(q_net.parameters(),lr=0.0001) \nfor _ in range(2000):\n    ~~~\n    ~~~\n    q_estimated = ~~~ \n    q_realistic = ~~~ \n    loss = torch.nn.functional.mse_loss(q_estimated,q_realistic)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n# policy\n\n# Agent 클래스 + run\n\n`-` 시각화를 위한코드\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"A3.out.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"pulse","code-copy":false,"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"A3: 강화학습 (3) – LunarLander"},"extensions":{"book":{"multiFile":true}}},"ipynb":{"identifier":{"display-name":"Jupyter","target-format":"ipynb","base-format":"ipynb"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"A3.out.ipynb"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"A3: 강화학습 (3) – LunarLander"}}},"projectFormats":["html"]}