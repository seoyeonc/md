{"title":"Transformers","markdown":{"yaml":{"title":"Transformers","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Transformers"]},"headingText":"seq2seq 의 한계","containsRefs":false,"markdown":"\n\n> Attention is all you need\n\nref: [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379), [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\n$\\star$ seq2seq 구조인 인코더-디코더를 따르면서 어텐션만으로 구현한 모델, RNN을 사용하지 않고 인코더-디코더 구조로 설계하였지만 RNN보다 우수한 성능을 보임\n\n\n- 인코더, 디코더로 구성되어 있는 seq2seq\n- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축\n    - $\\to$ 입력 시퀀스의 정보가 일부 손실된다는 단점 존재\n    - $\\to$ 이를 위해 어텐션 메카니즘 등장\n- 디코더는 이 벡터 표현을 통해 출력 시퀀스를 만듦\n\n\n- $\\star$ 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만든다면??\n\n# 트랜스포머의 주요 하이퍼파라미터\n\n각 값은 논문의 설정으로서, 바뀔 수 있음\n\n$d_{model} = 512$\n\n- 인코더와 디코더에서의 정해진 입력 및 출력의 크기\n- 임베딩 벡터의 차원도 이와 같음\n- 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때도 유지\n\n$num_layers = 6$\n\n- 하나의 인코더와 디코더를 층으로 생각하였을때, 모델에서 인코더와 디코더가 몇 층으로 구성되어 있는지를 의미\n\n$num_heads = 8$\n\n- 어텐션을 병렬로 수행하고 결과값을 다시 합치는 방식을 수행하기 위함, 즉 병렬의 개수\n\n$d_{ff} = 2048$\n\n- 트랜스포머 내부에 피드 포워드 신경망이 존재, 그 신경망의 은닉층의 크기를 의미\n- 단, 피드 포워드 신경망의 입력층과 출력층의 크기는 $d_{model}$\n\n# Transformer\n\n```{mermaid}\nflowchart LR\n  subgraph _\n    direction LR\n    subgraph Transformer\n        direction LR\n        Encoders -->Decoders\n    end\n  end\n  Text1(\"'I am a student`\") --> _ --> Text2(\"'je suis étudiant`\")\n```\n\n- 인코더-디코더 구조를 가진 트랜스포머\n- 인코더와 디코더라는 단위가 N개로 구성되는 구조 $\\to$ Encoders, Decoders로 표현\n    - seq2seq에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점time step을 가지는 구조\n\n```{mermaid}\nflowchart LR\n    _-->__\n    __-->out\n    subgraph _\n    direction BT\n    Embedding1-->Encoders\n        subgraph Encoders\n        Encoder\n        end\n        subgraph Embedding1\n        Text1(\"'I|am|a|student'\")\n        end\n    end\n    subgraph __\n    direction BT\n    Embedding2-->Decoders\n        subgraph Decoders\n        Decoder\n        end\n        subgraph Embedding2\n        Text2(\"'<\\br>sos|je|suis|étudiant'\")\n        end\n    end\n    out(\"'je|suis|étudiant|eos'\")\n```\n\n- symbol인 sos를 입력받아 eos symbol 나올때까지 연산을 진행하는, RNN을 사용하지 않지만 인코더 디코더 구조 유지되는 모습을 보임\n\n## Positional Encoding\n\n::: {.callout-note}\n\nRNN이 자연어 처리에서 유용했던 이유\n\n- 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보 position information을 가질 수 있어서\n\n:::\n\n\n**포지셔널 인코딩**\n\n- 트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용\n\n```{mermaid}\nflowchart LR\n    _-->__\n    __-->out\n    subgraph _\n    direction BT\n    Embedding_en-->Positional\\nEncoding_en\n    Positional\\nEncoding_en-->Encoders\n        subgraph Encoders\n        Encoder\n        end\n        subgraph Embedding_en\n        Text1(\"'I|am|a|student'\")\n        end\n    end\n    subgraph __\n    direction BT\n    Embedding_de-->Positional\\nEncoding_de\n    Positional\\nEncoding_de-->Decoders\n        subgraph Decoders\n        Decoder\n        end\n        subgraph Embedding_de\n        Text2(\"'<\\br>sos|je|suis|étudiant'\")\n        end\n    end\n    out(\"'je|suis|étudiant|eos'\")\n```\n\n- 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩 값이 더해지는 과정\n\n$$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n\n$$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n\n- 트랜스포머가 위치 정보를 가진 값을 만들기 위해 사용하는 두 개의 함수\n\n- pos는 입력 문장에서 임베딩 벡터의 위치\n- i는 임베딩 벡터 내의 차원의 인덱스를 의미\n- 위 식에 따르면\n    - **짝수**면 *sin*함수 사용 $\\to$ (pos,2i)\n    - **홀수**면 *cos*함수 사용 $\\to$ (pos,2i+1)\n\n- 여기서 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 하이퍼파라미터\n- 임베딩 벡터도 같은 차원임\n- 이와 깉은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨.\n    - **각 임베딩 멕터에 포지셔널 인코딩의 값을 더하면(차원 같음!) 같은 단어라도 문장 내의 위치에 따라 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐**\n\n포지셔널 인코딩 행렬 시각화\n\n# Attention\n\n**Encoder Self-Attention**\n\n- 인코더에서 이루어짐\n- Query = Key = Value(값이 같다는 말이 아님)\n\n```{mermaid}\nflowchart LR\n    Encoder1 --> Encoder2 \n    Encoder2 --> Encoder1\n    Encoder2 --> Encoder3\n    Encoder3 --> Encoder2\n    Encoder1 --> Encoder3\n    Encoder3 --> Encoder1\n```\n\n**Masked Decoder Self-Attention**\n\n- 디코더에서 이루어짐\n- Query = Key = Value(값이 같다는 말이 아님)\n\n```{mermaid}\nflowchart RL\n    Decoder1\n    Decoder2 --> Decoder1\n    Decoder3 --> Decoder1\n    Decoder3 --> Decoder2\n```\n\n**Encoder-Decoder Attention**\n\n- 디코더에서 이루어짐\n- Query = 디코더 벡터, Key = Value = 인코더 벡터\n\n```{mermaid}\nflowchart RL\n    Encoder1\n    Encoder2\n    Encoder3\n    Decoder --> Encoder1\n    Decoder --> Encoder2\n    Decoder --> Encoder3\n```\n\n::: {.callout-important}\n\n셀프 어텐션은 Query, Key, Value가 동일한 경우를 말함\n\nEncoder-Decoder Attention의 경우 Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않음\n\n**여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아닌 벡터의 출처가 같다는 의미**\n\n:::\n\n\n```{mermaid}\nflowchart BT\n  subgraph Encoders\n    direction BT\n        direction BT\n        Multi-head\\nSelf-Attention -->Position-wise\\nFFNN_en\n    end\n    subgraph Decoders\n    direction BT\n        direction BT\n        Masked\\nMulti-head\\nSelf-Attention -->Multi-head\\nAttention --> Position-wise\\nFFNN_de\n    end\n  Position-wise\\nFFNN_en --> Multi-head\\nAttention\n  embedding_en-->Positional\\nencoding_en-->Multi-head\\nSelf-Attention\n  embedding_de-->Positional\\nencoding_de-->Masked\\nMulti-head\\nSelf-Attention \n```\n\n위(각각 하나의 층으로 봄)의 인코더, 디코더가 num_layer지정한 수만큼 있음\n\n# Encoder\n\n- 하나의 인코더 층은 크게 두 개의 서브층sublayer로 나뉨\n    - 셀프 어텐션 Self Attention\n    - 피드 포워드 신경망 Feed Forward Neural Network\n\n::: {.callout-tip}\n\n**Multi-head Self Attention**\n\n- 셀프 어텐션을 병렬적으로 사용하였다는 의미\n\n**Position-wise FFNN**\n\n- 순방향신경망\n\n:::\n\n# Self Attention of Encoder\n\n::: {.callout-note}\n\nRemind\n\n- 어텐션 함수는 주어진 Query에 대해 모든 Key와의 유사도를 구함\n- 유사도를 가중치로 하여 Key와 맴핑되어 있는 각각의 Value에 반영해 좀\n- 이 유사도가 반영된 Value를 모두 가중합하여 Return\n\n:::\n\n- seq2seq에서의 Q,K,V의 정의\n    - Q = Query, t 시점의 디코더 셀에서의 은닉상태\n    - K = Key, 모든 시점의 인코더 셀의 은닉상태들\n    - V = Value, 모든 시점의 인코더 셀의 은닉상태들\n    \n$\\to$ t 시점의~의 의미는 변하면서 반복적으로 쿼리 수행하니까 결국은 전체 시점에 대해서 일반화 가능\n\n- seq2seq에서의 Q,K,V의 정의\n    - Q = Query, 모든 시점의 디코더 셀에서의 은닉상태들\n    - K = Key, 모든 시점의 인코더 셀의 은닉상태들\n    - V = Value, 모든 시점의 인코더 셀의 은닉상태들\n\n## 1. Self Attention\n\n- *셀프 어텐션*\n    - Q = 입력 문장의 모든 단어 벡터들\n    - K = 입력 문장의 모든 단어 벡터들\n    - V = 입력 문장의 모든 단어 벡터들\n    \n- **연속된 문장들에 대하여 지칭하는 단어가 다르지만 의미는 같을 수 있는데, 셀프 어텐션은 이 유사도를 구하여서 연관 가능성을 찾아낸다.**\n\n## 2. Q, K, V\n\n- 셀프 어텐션은 일단 문장의 각 단어 벡터로부터 Q벡터, K벡터, V벡터를 얻음.\n    - Q벡터, K벡터, V벡터는 $d_{model}$ 차원을 가지는 단어 벡터들보다 더 작은 차원을 가짐\n    - 논문을 예로 들면 $d_{model}$의 차원은 512, Q벡터, K벡터, V벡터의 차원은 각각 64\n    - 이 64는 또 다른 하이퍼파라미터인 num_heads로 결정되는데, 트랜스포머는 $d_{model}$을 num_heads로 나눈 값을 Q벡터, K벡터, V벡터의 차원으로 결정.\n    - 논문의 num_heads = 8이었으니까 $512/8 = 64$로 결정된 것\n    - 이 Q벡터, K벡터, V벡터는 단어마다, 벡터마다 서로 다른 가중치 행렬을 곱하여 얻음\n    - 각 단어마다 Q벡터, K벡터, V벡터 각각의 가중치, Q벡터, K벡터, V벡터 각각이 존재하는 것\n\n## 3. Scaled dot-product Attention\n\n각 단어 별로 Q벡터, K벡터, V벡터를 구한 후 각 Q벡터는 모든 K벡터에 대해서 어텐션 스코어를 구하고, 어텐션 분포를 구한 뒤 이를 사용하여 모든 V벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함-> 모든 Q벡터에 대해 반복\n\n- 내적한 후 특정값을 나눔으로써 값을 조정하는 과정 추가한 스케일드 갓-프로덕트 어텐션\n\n$$score(q,k) = \\frac{q k}{\\sqrt{n}}$$\n\n$\\sqrt{n}$이 결정되는 과정\n\n- 논문을 예로 들면 $d_{model}$의 차원이 512, num_heads가 8, Q,K,V의 차원 $d_k$가 64(512/8) 이었음, 여기서 64에 root 취한 8으로 결정되는 것\n\n## 4. 행렬 연산으로 일괄 처리\n\nQ 벡터마다 3을 연산하는 것을 피하기 위함\n\n1. 문장 행렬에 가중치 행렬을 곱하여 Q행렬, K행렬, V행렬을 구한다(단지 벡터를 행렬화한 것 뿐).\n2. 각 단어의 Q벡터와 전치한 K벡터의 내적이 각 행렬의 원소가 되는 행렬을 결과로 추출.\n3. 2번의 결과에 $\\sqrt{d_k}$를 나누어 softmax취한 후 V행렬을 곱하여 각 행과 열이 어텐션 스코어 값을 가지는 행렬을 구함.\n\n\n$$Attention(Q,K,V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$$\n\n입력 문장의 길이가 seq_len이라면, 문장 행렬의 크기는 (seq_len,$d_{model}$)\n\n*차원 정리*\n\n- $Q$ = (seq_len, $d_k)$\n    - $W^Q = (d_{model},d_k)$\n- $K^\\top$ = ($d_k$, seq_len)\n    - $W^K = (d_{model},d_k)$\n- $V$ = (seq_len, $d_v)$\n    - $W^V = (d_{model},d_v)$\n- 논문에서는 $d_k,d_v$의 차원이 $d_{model}$/num_heads로 같게 설정함\n- attention score matrix = (seq_len, $d_v$)\n\n## 5. Scaled dot-product attention 구현\n\nscaled_dot_product_attention 실행\n\nquery에 해당하는 [0,10,0]은 key에 해당하는 두 번째 값과 일치해야 함.\n\n두 번째 값과 일치했어서 두 번째가 1인 값을 반환, 결과적으로 [10,0]의 어텐션 값 반환\n\n세 번째 값과 네 번째 값이 같이 일치하다면? 합이 1이되게 나눠서 0.5,0.5씩 나눠짐\n\n- [100,5] * 0.5 + [1000,6] * 0.5 = [550,5.5]\n\n## 6. Multi-head Attention\n\n왜 스케일링하여 어텐션 스코어를 구했을까?\n\n- 논문에서는 한 번의 어텐션보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단\n- 그래서 $d_{model}$의 차원을 num_heads로 나누어 $d_{model}$/num_heads의 차원을 가지는 Q,K,V에 대해서 num_heads 개의 병렬 어텐션 수행\n- num_heads만큼 병렬이 이뤄지는데, 이 때 나오는 각각의 **어텐션 값 행렬을 어텐션 헤드**라고 함.\n    - 이 때 가중치 행렬의 값$W^Q,W^K,W^V$은 num_heads의 어텐션 해드마다 전부 다름\n    \n병렬로 수행한 효과?\n\n- 어텐션을 병렬로 수행하여 다른 시각으로 정보를 수집할 수 있음\n\n## 7. Multi-head Attention 구현\n\n가중치 행렬\n\n- Q, K, V 행렬을 만들기 위한 가중치 행렬 $W^Q,W^K,W^V$\n- 어텐션 헤드들을 연결concatenation 후에 곱해주는 행렬 $W^O$\n\n가중치 행렬을 곱하는 것은 Dense layer 지나게 하여 구현\n\n1. $W^Q,W^K,W^V$에 해당하는 $d_{model}$의 크기의 밀집층(Dense layer)을 지나게 한다.\n2. 지정된 헤드수 num_heads 만큼 나눈다(split).\n3. scaled dot-product attention\n4. 나눠졌던 헤드들을 연결concatenatetion한다.\n5. $W^O$에 해당하는 밀집층을 지나게 한다.\n\n## 8. Padding Mask\n\n어텐션에서 제외하기 위해 값을 가리는 역할\n\n- 방법: 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주기\n    - 소프트맥스 함수를 지나면 값이 0이 되어 유사도 구할때 반영되지 않름.\n\n# Position-wise Feed Forward Neural Network\n\n인코더와 디코더에서 공통적으로 가지고 있는 서브층\n\n= FFNN(Fully Connected FFNN)\n\n$$FFNN(x) = MAX(0,xW-1 + b_1)W_2 + b_2$$\n\n$x$ -> $F_1 = xW_1 + b_1$ -> 활성화 함수:ReLU $F_2 = max(0,F_1)$ -> $F_3 = F_2W_2 + b_2$\n\n- 여기서 $x$는 멀티 헤드 어텐션의 결과로 나온 (seq_len, $d_{model}$)의 차원을 가지는 행렬\n- 가중치 행렬 $W_1$ = ($d_{model}, d_{ff}$)\n- 가중치 행렬 $W_2$ = ($d_{ff},d_{model}$)\n- 논문은 $d_{ff}$를 2048로 정의\n- 매개변수 $W_1,W_2,b_1,b_2$는 각 인코더 층마다 동일하게 계산되지만 값은 층마다 다 다르다.\n\n```python\n# 다음의 코드는 인코더와 디코더 내부에서 사용할 예정입니다.\noutputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\noutputs = tf.keras.layers.Dense(units=d_model)(outputs)\n```\n\n# Residual connection and Layer Normalization\n\n## 1. 잔차 연결\n\n$$H(x) = x + F(x)$$\n\n- $F(x)$는 트랜스포머에서 서브층에 해당\n- 즉, 장차 연결은 서브층의 입력과 출력을 더하는 것\n- 서브층의 입력과 출력은 동일한 차원을 갖고 있어서 가능\n- 그래서 재귀하는 것처럼 다이어그램 그려보면 화살표가 출력층에서 나와 입력층으로 들어가는 모습\n- *잔차 연결은 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법*\n\n식으로 표현 -> $x + Sublayer(x)$\n\n서브층이 멀티 헤드 어텐션이었다면 $H(x) - x + Multi - head Attention(x)\n\n참고 : [잔차연결 관련 논문](https://arxiv.org/pdf/1512.03385.pdf)\n\n## 2. 층 정규화\n\n잔차연결과 층 정규화 모두 수행한 함수\n\n$$LN = LayerNorm(x+Sublayer(x))$$\n\n텐서의 마지막 차원에 대하여 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움\n\n- 텐서의 마지막 차원 = 트랜스포머에서는 $d_{model}$ 차원을 의미\n\n1. 평균과 분산을 통한 벡터 $x_i$ 정규화\n\n    - 스칼라인 평균과 분산 도출\n    - $\\epsilon$은 분모가 0이 되는 것을 방지\n    \n$$\\hat{x}_{i,k} = \\frac{x_{i,k} - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}}$$\n    \n2. 감마와 베타 도입\n\n- LayerNormalization(케라스에 내장되어 있음)\n\n$$ln_i = \\gamma \\hat{x}_i + \\beta = LayerNorm(x_i)$$\n\n참고: [층 정규화 관련 논문](https://arxiv.org/pdf/1607.06450.pdf)\n\n# Encoder 구현\n\n인코더 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로 어텐션 시 패딩 토큰을 제외하도록 패딩 마스크를 사용\n\n- multiheadattention 함수의 mask 인자값으로 padding_mask가 사용되는 이유\n- 인코더는 두 개의 서브층으로 이루어짐\n    - 멀티 헤드 어텐션\n    - 피드 포워드 신경망\n- 서브층 이후 드롭 아웃, 잔차 연결, 층 정규화 수행\n\n# Encoder 쌓기\n\n인코더 층을 num_layers 만큼 쌓는 클래스\n\n# Encoder에서 Decoder로\n\n인코더에서 num_layers만큼 총 연산을 순차적으로 한 후 마지막 층의 인코더의 출력을 디코더로 전달\n\n# Decoder: Self-Attention and Look-ahead Mask\n\n트랜스포머는 문장 행렬로 입력을 한 번에 받기 때문에 현재 시점의 단어를 예측하고자 할 때 입력 문장 행렬로부터 미래 시점의 단어까지 참고하느 현상이 발생\n\n이를 위해 디코더에서 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크 도입\n\n**디코더의 첫번때 서브층에서 이루어짐**\n\n**디코더의 셀프 어텐션은 인코더의 멀티 헤드 셀프 어텐션과 동일한 연산을 수행하나, 어텐션 스코어 행렬에서 마스킹을 적용하는 점이 다름**\n\n-> 미리보기 방지를 위함\n\n트랜스포머 마스킹의 종류\n\n1. 인코더의 셀프 어텐션 = 패딩 마스크를 전달\n2. 디코더의 첫번째 서브층인 마스크드 셀프 어텐션 = 룩-어헤드 마스크를 전달\n3. 디코더의 두번째 서브층인 인코더-디코더 어텐션 = 패딩 마스크를 전달\n\n마스킹을 하고자 하는 위치에 1, 마스킹을 하지 않고자 하는 위이에 0을 리턴\n\n## 2nd Decoder sublayer : Encoder-Decoder Attention\n\n디코더 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이건 **셀프 어텐션이 아님!**\n\n- **셀프 어텐션은 Query, Key, Value가 출처가 같은 경우를 말하는데, 인코더-디코더 어텐션은 Query가 디코더인 행렬인 반면, Key, Value가 인코더 행렬이기 때문**\n\n- 인코더의 첫번째 서브층 = Query = Key = Value\n- 디코더의 첫번째 서브층 = Query = Key = Value\n- 디코더의 두번째 서브층 = Query = 디코더 행렬(의 첫번째 서브층 결과), Key = Value = 인코더 행렬(의 마지막 층에서 얻은 값)\n\n# Decoder 구현\n\n첫번째 서브층은 mask 인자값으로 look_ahead_mask가 들어가고, 두 번째 서브층은 mask의 인자값으로 padding_mask가 들어가있음\n\n세 개의 서브층 모두 서브층 연산 후에는 드롭 아웃, 잔차 연결, 층 정규화가 수행\n\n# Decoder 쌓기\n\nnum_layers 개수만큼 쌓기\n\n# Transformer 구현\n\nvocab_size는 다중 클래스 분류 문제를 풀 수 있도록 추가\n\n# Transformer hyperparameter 정하기\n\n예제\n- num_layers = 4 # 인코더,디코더 층의 개수\n- $d_{ff}$ = 128 # 포지션 와이즈 피드 포워드 신경망의 은닉층\n- $d_{model}$ = 128 # 인코더와 디코더의 입, 출력의 차원\n- num_heads = 4 # 멀티-헤드 어텐션에서 병렬적으로 사용할 헤드의 수\n    - $d_v$ = 128 / 4 = 32\n\n# Loss Function 정의\n\n예제가 다중 클래스라 크로스 엔트로피 함수를 손실 함수로 정의함\n\n# 학습률\n\n$$lrate = d^{-0.5}_{model} \\times min(step_num^{-0.5} , step_num \\times warmup_steps^{-1/4})$$\n","srcMarkdownNoYaml":"\n\n> Attention is all you need\n\nref: [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379), [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\n$\\star$ seq2seq 구조인 인코더-디코더를 따르면서 어텐션만으로 구현한 모델, RNN을 사용하지 않고 인코더-디코더 구조로 설계하였지만 RNN보다 우수한 성능을 보임\n\n# seq2seq 의 한계\n\n- 인코더, 디코더로 구성되어 있는 seq2seq\n- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축\n    - $\\to$ 입력 시퀀스의 정보가 일부 손실된다는 단점 존재\n    - $\\to$ 이를 위해 어텐션 메카니즘 등장\n- 디코더는 이 벡터 표현을 통해 출력 시퀀스를 만듦\n\n\n- $\\star$ 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만든다면??\n\n# 트랜스포머의 주요 하이퍼파라미터\n\n각 값은 논문의 설정으로서, 바뀔 수 있음\n\n$d_{model} = 512$\n\n- 인코더와 디코더에서의 정해진 입력 및 출력의 크기\n- 임베딩 벡터의 차원도 이와 같음\n- 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때도 유지\n\n$num_layers = 6$\n\n- 하나의 인코더와 디코더를 층으로 생각하였을때, 모델에서 인코더와 디코더가 몇 층으로 구성되어 있는지를 의미\n\n$num_heads = 8$\n\n- 어텐션을 병렬로 수행하고 결과값을 다시 합치는 방식을 수행하기 위함, 즉 병렬의 개수\n\n$d_{ff} = 2048$\n\n- 트랜스포머 내부에 피드 포워드 신경망이 존재, 그 신경망의 은닉층의 크기를 의미\n- 단, 피드 포워드 신경망의 입력층과 출력층의 크기는 $d_{model}$\n\n# Transformer\n\n```{mermaid}\nflowchart LR\n  subgraph _\n    direction LR\n    subgraph Transformer\n        direction LR\n        Encoders -->Decoders\n    end\n  end\n  Text1(\"'I am a student`\") --> _ --> Text2(\"'je suis étudiant`\")\n```\n\n- 인코더-디코더 구조를 가진 트랜스포머\n- 인코더와 디코더라는 단위가 N개로 구성되는 구조 $\\to$ Encoders, Decoders로 표현\n    - seq2seq에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점time step을 가지는 구조\n\n```{mermaid}\nflowchart LR\n    _-->__\n    __-->out\n    subgraph _\n    direction BT\n    Embedding1-->Encoders\n        subgraph Encoders\n        Encoder\n        end\n        subgraph Embedding1\n        Text1(\"'I|am|a|student'\")\n        end\n    end\n    subgraph __\n    direction BT\n    Embedding2-->Decoders\n        subgraph Decoders\n        Decoder\n        end\n        subgraph Embedding2\n        Text2(\"'<\\br>sos|je|suis|étudiant'\")\n        end\n    end\n    out(\"'je|suis|étudiant|eos'\")\n```\n\n- symbol인 sos를 입력받아 eos symbol 나올때까지 연산을 진행하는, RNN을 사용하지 않지만 인코더 디코더 구조 유지되는 모습을 보임\n\n## Positional Encoding\n\n::: {.callout-note}\n\nRNN이 자연어 처리에서 유용했던 이유\n\n- 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보 position information을 가질 수 있어서\n\n:::\n\n\n**포지셔널 인코딩**\n\n- 트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용\n\n```{mermaid}\nflowchart LR\n    _-->__\n    __-->out\n    subgraph _\n    direction BT\n    Embedding_en-->Positional\\nEncoding_en\n    Positional\\nEncoding_en-->Encoders\n        subgraph Encoders\n        Encoder\n        end\n        subgraph Embedding_en\n        Text1(\"'I|am|a|student'\")\n        end\n    end\n    subgraph __\n    direction BT\n    Embedding_de-->Positional\\nEncoding_de\n    Positional\\nEncoding_de-->Decoders\n        subgraph Decoders\n        Decoder\n        end\n        subgraph Embedding_de\n        Text2(\"'<\\br>sos|je|suis|étudiant'\")\n        end\n    end\n    out(\"'je|suis|étudiant|eos'\")\n```\n\n- 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩 값이 더해지는 과정\n\n$$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n\n$$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n\n- 트랜스포머가 위치 정보를 가진 값을 만들기 위해 사용하는 두 개의 함수\n\n- pos는 입력 문장에서 임베딩 벡터의 위치\n- i는 임베딩 벡터 내의 차원의 인덱스를 의미\n- 위 식에 따르면\n    - **짝수**면 *sin*함수 사용 $\\to$ (pos,2i)\n    - **홀수**면 *cos*함수 사용 $\\to$ (pos,2i+1)\n\n- 여기서 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 하이퍼파라미터\n- 임베딩 벡터도 같은 차원임\n- 이와 깉은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨.\n    - **각 임베딩 멕터에 포지셔널 인코딩의 값을 더하면(차원 같음!) 같은 단어라도 문장 내의 위치에 따라 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐**\n\n포지셔널 인코딩 행렬 시각화\n\n# Attention\n\n**Encoder Self-Attention**\n\n- 인코더에서 이루어짐\n- Query = Key = Value(값이 같다는 말이 아님)\n\n```{mermaid}\nflowchart LR\n    Encoder1 --> Encoder2 \n    Encoder2 --> Encoder1\n    Encoder2 --> Encoder3\n    Encoder3 --> Encoder2\n    Encoder1 --> Encoder3\n    Encoder3 --> Encoder1\n```\n\n**Masked Decoder Self-Attention**\n\n- 디코더에서 이루어짐\n- Query = Key = Value(값이 같다는 말이 아님)\n\n```{mermaid}\nflowchart RL\n    Decoder1\n    Decoder2 --> Decoder1\n    Decoder3 --> Decoder1\n    Decoder3 --> Decoder2\n```\n\n**Encoder-Decoder Attention**\n\n- 디코더에서 이루어짐\n- Query = 디코더 벡터, Key = Value = 인코더 벡터\n\n```{mermaid}\nflowchart RL\n    Encoder1\n    Encoder2\n    Encoder3\n    Decoder --> Encoder1\n    Decoder --> Encoder2\n    Decoder --> Encoder3\n```\n\n::: {.callout-important}\n\n셀프 어텐션은 Query, Key, Value가 동일한 경우를 말함\n\nEncoder-Decoder Attention의 경우 Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않음\n\n**여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아닌 벡터의 출처가 같다는 의미**\n\n:::\n\n\n```{mermaid}\nflowchart BT\n  subgraph Encoders\n    direction BT\n        direction BT\n        Multi-head\\nSelf-Attention -->Position-wise\\nFFNN_en\n    end\n    subgraph Decoders\n    direction BT\n        direction BT\n        Masked\\nMulti-head\\nSelf-Attention -->Multi-head\\nAttention --> Position-wise\\nFFNN_de\n    end\n  Position-wise\\nFFNN_en --> Multi-head\\nAttention\n  embedding_en-->Positional\\nencoding_en-->Multi-head\\nSelf-Attention\n  embedding_de-->Positional\\nencoding_de-->Masked\\nMulti-head\\nSelf-Attention \n```\n\n위(각각 하나의 층으로 봄)의 인코더, 디코더가 num_layer지정한 수만큼 있음\n\n# Encoder\n\n- 하나의 인코더 층은 크게 두 개의 서브층sublayer로 나뉨\n    - 셀프 어텐션 Self Attention\n    - 피드 포워드 신경망 Feed Forward Neural Network\n\n::: {.callout-tip}\n\n**Multi-head Self Attention**\n\n- 셀프 어텐션을 병렬적으로 사용하였다는 의미\n\n**Position-wise FFNN**\n\n- 순방향신경망\n\n:::\n\n# Self Attention of Encoder\n\n::: {.callout-note}\n\nRemind\n\n- 어텐션 함수는 주어진 Query에 대해 모든 Key와의 유사도를 구함\n- 유사도를 가중치로 하여 Key와 맴핑되어 있는 각각의 Value에 반영해 좀\n- 이 유사도가 반영된 Value를 모두 가중합하여 Return\n\n:::\n\n- seq2seq에서의 Q,K,V의 정의\n    - Q = Query, t 시점의 디코더 셀에서의 은닉상태\n    - K = Key, 모든 시점의 인코더 셀의 은닉상태들\n    - V = Value, 모든 시점의 인코더 셀의 은닉상태들\n    \n$\\to$ t 시점의~의 의미는 변하면서 반복적으로 쿼리 수행하니까 결국은 전체 시점에 대해서 일반화 가능\n\n- seq2seq에서의 Q,K,V의 정의\n    - Q = Query, 모든 시점의 디코더 셀에서의 은닉상태들\n    - K = Key, 모든 시점의 인코더 셀의 은닉상태들\n    - V = Value, 모든 시점의 인코더 셀의 은닉상태들\n\n## 1. Self Attention\n\n- *셀프 어텐션*\n    - Q = 입력 문장의 모든 단어 벡터들\n    - K = 입력 문장의 모든 단어 벡터들\n    - V = 입력 문장의 모든 단어 벡터들\n    \n- **연속된 문장들에 대하여 지칭하는 단어가 다르지만 의미는 같을 수 있는데, 셀프 어텐션은 이 유사도를 구하여서 연관 가능성을 찾아낸다.**\n\n## 2. Q, K, V\n\n- 셀프 어텐션은 일단 문장의 각 단어 벡터로부터 Q벡터, K벡터, V벡터를 얻음.\n    - Q벡터, K벡터, V벡터는 $d_{model}$ 차원을 가지는 단어 벡터들보다 더 작은 차원을 가짐\n    - 논문을 예로 들면 $d_{model}$의 차원은 512, Q벡터, K벡터, V벡터의 차원은 각각 64\n    - 이 64는 또 다른 하이퍼파라미터인 num_heads로 결정되는데, 트랜스포머는 $d_{model}$을 num_heads로 나눈 값을 Q벡터, K벡터, V벡터의 차원으로 결정.\n    - 논문의 num_heads = 8이었으니까 $512/8 = 64$로 결정된 것\n    - 이 Q벡터, K벡터, V벡터는 단어마다, 벡터마다 서로 다른 가중치 행렬을 곱하여 얻음\n    - 각 단어마다 Q벡터, K벡터, V벡터 각각의 가중치, Q벡터, K벡터, V벡터 각각이 존재하는 것\n\n## 3. Scaled dot-product Attention\n\n각 단어 별로 Q벡터, K벡터, V벡터를 구한 후 각 Q벡터는 모든 K벡터에 대해서 어텐션 스코어를 구하고, 어텐션 분포를 구한 뒤 이를 사용하여 모든 V벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함-> 모든 Q벡터에 대해 반복\n\n- 내적한 후 특정값을 나눔으로써 값을 조정하는 과정 추가한 스케일드 갓-프로덕트 어텐션\n\n$$score(q,k) = \\frac{q k}{\\sqrt{n}}$$\n\n$\\sqrt{n}$이 결정되는 과정\n\n- 논문을 예로 들면 $d_{model}$의 차원이 512, num_heads가 8, Q,K,V의 차원 $d_k$가 64(512/8) 이었음, 여기서 64에 root 취한 8으로 결정되는 것\n\n## 4. 행렬 연산으로 일괄 처리\n\nQ 벡터마다 3을 연산하는 것을 피하기 위함\n\n1. 문장 행렬에 가중치 행렬을 곱하여 Q행렬, K행렬, V행렬을 구한다(단지 벡터를 행렬화한 것 뿐).\n2. 각 단어의 Q벡터와 전치한 K벡터의 내적이 각 행렬의 원소가 되는 행렬을 결과로 추출.\n3. 2번의 결과에 $\\sqrt{d_k}$를 나누어 softmax취한 후 V행렬을 곱하여 각 행과 열이 어텐션 스코어 값을 가지는 행렬을 구함.\n\n\n$$Attention(Q,K,V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$$\n\n입력 문장의 길이가 seq_len이라면, 문장 행렬의 크기는 (seq_len,$d_{model}$)\n\n*차원 정리*\n\n- $Q$ = (seq_len, $d_k)$\n    - $W^Q = (d_{model},d_k)$\n- $K^\\top$ = ($d_k$, seq_len)\n    - $W^K = (d_{model},d_k)$\n- $V$ = (seq_len, $d_v)$\n    - $W^V = (d_{model},d_v)$\n- 논문에서는 $d_k,d_v$의 차원이 $d_{model}$/num_heads로 같게 설정함\n- attention score matrix = (seq_len, $d_v$)\n\n## 5. Scaled dot-product attention 구현\n\nscaled_dot_product_attention 실행\n\nquery에 해당하는 [0,10,0]은 key에 해당하는 두 번째 값과 일치해야 함.\n\n두 번째 값과 일치했어서 두 번째가 1인 값을 반환, 결과적으로 [10,0]의 어텐션 값 반환\n\n세 번째 값과 네 번째 값이 같이 일치하다면? 합이 1이되게 나눠서 0.5,0.5씩 나눠짐\n\n- [100,5] * 0.5 + [1000,6] * 0.5 = [550,5.5]\n\n## 6. Multi-head Attention\n\n왜 스케일링하여 어텐션 스코어를 구했을까?\n\n- 논문에서는 한 번의 어텐션보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단\n- 그래서 $d_{model}$의 차원을 num_heads로 나누어 $d_{model}$/num_heads의 차원을 가지는 Q,K,V에 대해서 num_heads 개의 병렬 어텐션 수행\n- num_heads만큼 병렬이 이뤄지는데, 이 때 나오는 각각의 **어텐션 값 행렬을 어텐션 헤드**라고 함.\n    - 이 때 가중치 행렬의 값$W^Q,W^K,W^V$은 num_heads의 어텐션 해드마다 전부 다름\n    \n병렬로 수행한 효과?\n\n- 어텐션을 병렬로 수행하여 다른 시각으로 정보를 수집할 수 있음\n\n## 7. Multi-head Attention 구현\n\n가중치 행렬\n\n- Q, K, V 행렬을 만들기 위한 가중치 행렬 $W^Q,W^K,W^V$\n- 어텐션 헤드들을 연결concatenation 후에 곱해주는 행렬 $W^O$\n\n가중치 행렬을 곱하는 것은 Dense layer 지나게 하여 구현\n\n1. $W^Q,W^K,W^V$에 해당하는 $d_{model}$의 크기의 밀집층(Dense layer)을 지나게 한다.\n2. 지정된 헤드수 num_heads 만큼 나눈다(split).\n3. scaled dot-product attention\n4. 나눠졌던 헤드들을 연결concatenatetion한다.\n5. $W^O$에 해당하는 밀집층을 지나게 한다.\n\n## 8. Padding Mask\n\n어텐션에서 제외하기 위해 값을 가리는 역할\n\n- 방법: 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주기\n    - 소프트맥스 함수를 지나면 값이 0이 되어 유사도 구할때 반영되지 않름.\n\n# Position-wise Feed Forward Neural Network\n\n인코더와 디코더에서 공통적으로 가지고 있는 서브층\n\n= FFNN(Fully Connected FFNN)\n\n$$FFNN(x) = MAX(0,xW-1 + b_1)W_2 + b_2$$\n\n$x$ -> $F_1 = xW_1 + b_1$ -> 활성화 함수:ReLU $F_2 = max(0,F_1)$ -> $F_3 = F_2W_2 + b_2$\n\n- 여기서 $x$는 멀티 헤드 어텐션의 결과로 나온 (seq_len, $d_{model}$)의 차원을 가지는 행렬\n- 가중치 행렬 $W_1$ = ($d_{model}, d_{ff}$)\n- 가중치 행렬 $W_2$ = ($d_{ff},d_{model}$)\n- 논문은 $d_{ff}$를 2048로 정의\n- 매개변수 $W_1,W_2,b_1,b_2$는 각 인코더 층마다 동일하게 계산되지만 값은 층마다 다 다르다.\n\n```python\n# 다음의 코드는 인코더와 디코더 내부에서 사용할 예정입니다.\noutputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\noutputs = tf.keras.layers.Dense(units=d_model)(outputs)\n```\n\n# Residual connection and Layer Normalization\n\n## 1. 잔차 연결\n\n$$H(x) = x + F(x)$$\n\n- $F(x)$는 트랜스포머에서 서브층에 해당\n- 즉, 장차 연결은 서브층의 입력과 출력을 더하는 것\n- 서브층의 입력과 출력은 동일한 차원을 갖고 있어서 가능\n- 그래서 재귀하는 것처럼 다이어그램 그려보면 화살표가 출력층에서 나와 입력층으로 들어가는 모습\n- *잔차 연결은 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법*\n\n식으로 표현 -> $x + Sublayer(x)$\n\n서브층이 멀티 헤드 어텐션이었다면 $H(x) - x + Multi - head Attention(x)\n\n참고 : [잔차연결 관련 논문](https://arxiv.org/pdf/1512.03385.pdf)\n\n## 2. 층 정규화\n\n잔차연결과 층 정규화 모두 수행한 함수\n\n$$LN = LayerNorm(x+Sublayer(x))$$\n\n텐서의 마지막 차원에 대하여 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움\n\n- 텐서의 마지막 차원 = 트랜스포머에서는 $d_{model}$ 차원을 의미\n\n1. 평균과 분산을 통한 벡터 $x_i$ 정규화\n\n    - 스칼라인 평균과 분산 도출\n    - $\\epsilon$은 분모가 0이 되는 것을 방지\n    \n$$\\hat{x}_{i,k} = \\frac{x_{i,k} - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}}$$\n    \n2. 감마와 베타 도입\n\n- LayerNormalization(케라스에 내장되어 있음)\n\n$$ln_i = \\gamma \\hat{x}_i + \\beta = LayerNorm(x_i)$$\n\n참고: [층 정규화 관련 논문](https://arxiv.org/pdf/1607.06450.pdf)\n\n# Encoder 구현\n\n인코더 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로 어텐션 시 패딩 토큰을 제외하도록 패딩 마스크를 사용\n\n- multiheadattention 함수의 mask 인자값으로 padding_mask가 사용되는 이유\n- 인코더는 두 개의 서브층으로 이루어짐\n    - 멀티 헤드 어텐션\n    - 피드 포워드 신경망\n- 서브층 이후 드롭 아웃, 잔차 연결, 층 정규화 수행\n\n# Encoder 쌓기\n\n인코더 층을 num_layers 만큼 쌓는 클래스\n\n# Encoder에서 Decoder로\n\n인코더에서 num_layers만큼 총 연산을 순차적으로 한 후 마지막 층의 인코더의 출력을 디코더로 전달\n\n# Decoder: Self-Attention and Look-ahead Mask\n\n트랜스포머는 문장 행렬로 입력을 한 번에 받기 때문에 현재 시점의 단어를 예측하고자 할 때 입력 문장 행렬로부터 미래 시점의 단어까지 참고하느 현상이 발생\n\n이를 위해 디코더에서 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크 도입\n\n**디코더의 첫번때 서브층에서 이루어짐**\n\n**디코더의 셀프 어텐션은 인코더의 멀티 헤드 셀프 어텐션과 동일한 연산을 수행하나, 어텐션 스코어 행렬에서 마스킹을 적용하는 점이 다름**\n\n-> 미리보기 방지를 위함\n\n트랜스포머 마스킹의 종류\n\n1. 인코더의 셀프 어텐션 = 패딩 마스크를 전달\n2. 디코더의 첫번째 서브층인 마스크드 셀프 어텐션 = 룩-어헤드 마스크를 전달\n3. 디코더의 두번째 서브층인 인코더-디코더 어텐션 = 패딩 마스크를 전달\n\n마스킹을 하고자 하는 위치에 1, 마스킹을 하지 않고자 하는 위이에 0을 리턴\n\n## 2nd Decoder sublayer : Encoder-Decoder Attention\n\n디코더 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이건 **셀프 어텐션이 아님!**\n\n- **셀프 어텐션은 Query, Key, Value가 출처가 같은 경우를 말하는데, 인코더-디코더 어텐션은 Query가 디코더인 행렬인 반면, Key, Value가 인코더 행렬이기 때문**\n\n- 인코더의 첫번째 서브층 = Query = Key = Value\n- 디코더의 첫번째 서브층 = Query = Key = Value\n- 디코더의 두번째 서브층 = Query = 디코더 행렬(의 첫번째 서브층 결과), Key = Value = 인코더 행렬(의 마지막 층에서 얻은 값)\n\n# Decoder 구현\n\n첫번째 서브층은 mask 인자값으로 look_ahead_mask가 들어가고, 두 번째 서브층은 mask의 인자값으로 padding_mask가 들어가있음\n\n세 개의 서브층 모두 서브층 연산 후에는 드롭 아웃, 잔차 연결, 층 정규화가 수행\n\n# Decoder 쌓기\n\nnum_layers 개수만큼 쌓기\n\n# Transformer 구현\n\nvocab_size는 다중 클래스 분류 문제를 풀 수 있도록 추가\n\n# Transformer hyperparameter 정하기\n\n예제\n- num_layers = 4 # 인코더,디코더 층의 개수\n- $d_{ff}$ = 128 # 포지션 와이즈 피드 포워드 신경망의 은닉층\n- $d_{model}$ = 128 # 인코더와 디코더의 입, 출력의 차원\n- num_heads = 4 # 멀티-헤드 어텐션에서 병렬적으로 사용할 헤드의 수\n    - $d_v$ = 128 / 4 = 32\n\n# Loss Function 정의\n\n예제가 다중 클래스라 크로스 엔트로피 함수를 손실 함수로 정의함\n\n# 학습률\n\n$$lrate = d^{-0.5}_{model} \\times min(step_num^{-0.5} , step_num \\times warmup_steps^{-1/4})$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-05-28-Transformers.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"pulse","code-copy":false,"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Transformers","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Transformers"]},"extensions":{"book":{"multiFile":true}}},"ipynb":{"identifier":{"display-name":"Jupyter","target-format":"ipynb","base-format":"ipynb"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"2023-05-28-Transformers.ipynb"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Transformers","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Transformers"]}}},"projectFormats":["html"]}