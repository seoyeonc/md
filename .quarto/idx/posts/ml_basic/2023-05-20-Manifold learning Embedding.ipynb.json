{"title":"Manifold learning Embedding","markdown":{"yaml":{"title":"Manifold learning Embedding","author":"SEOYEON CHOI","date":"2023-05-20","categories":["LocallyLinearEmbedding","TSNE"]},"headingText":"LocallyLinearEmbedding","containsRefs":false,"markdown":"\n\n> Expectation Maximization\n\n참고: [LocallyLinearEmbedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE),  [머신러닝 도감](http://www.yes24.com/Product/Goods/84809164?pid=123487&cosemkid=go15760589536554250&gclid=CjwKCAjw36GjBhAkEiwAKwIWycfq1GuEe4xEi2QH73V1JvmypOej-HI910PEa5K8AJ7_5hsiTefiIBoCUAYQAvD_BwE), [sklean](https://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html#sphx-glr-auto-examples-manifold-plot-swissroll-py)\n\n:::{.callout-note}\n비선형 데이터의 차원 축소 방법을 매니폴드 학습이라고 한다.\n:::\n\n\n\n국소 선형 임베딩 Local Linearn Embedding = LLE\n\n; 높은 차원 공간에서 휘어지거나 뒤틀린 구조를 낮은 차원 공간에 단순한 구조로 나타내는 알고리즘\n\n- 주성분 분석은 변수 사이에 상관관계가 있는 데이터에 적용하는 알고리즘이므로 스위스 롤 같은 비선형 데이터를 차원 축소할 때는 국소 선형 임베딩이 더 적합하다.\n\n*국소 선형 임베딩은 근처에 있는 점 개수가 결과에 주는 영향이 크다.*\n\n## Algorithm\n\n1. 데이터 포인트 $x_i$ 근처에 있는 점($k$개)을 찾는다.\n2. 근처에 있는 $K$개 점의 선형 결합으로 $x_i$를 재구성하는 가중치 $W_{ij}$를 계산한다.\n3. 가중치 $W_{ij}$로 낮은 차원($d$차원)의 $y_i$를 계산한다.\n\n- 근처에 있는 점이 5개라면 숨어 있는 낮은 차원의 데이터 구조를 꺼낼 수 없으므로 차우너 축소 후 점들이 좁은 범위에 모여 있다. 따라서 넓은 범위의 데이터 정보를 알 수 없다.\n- 근처에 있는 점이 50개라면 색깔이 다른 점이 근처에 있다. 근처에 있는 점 개수가 너무 많아서 일부 범위의 데이터 구조를 파악할 수 없다.\n\n# t-SNE\n\nt-분포 확률적 임베딩(t-distributed stochastic neighbor embedding = t-SNE\n\n; 높은 차원의 복잡한 데이터를 2차원(또는 입체)에 차원 축소하는 방법, 낮은 차원 공간의 시각화에 사용\n\n- t-분포 확률적 임베딩으로 차원을 축소할 때는 자유도 1의 t분포를 이용한다는 특성\n    - t-분포를 이용하면 높은 차원 공간에서 비슷한 데이터 구조는 낮은 차원 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 대응한다.\n\n## Algorithm\n\n1. 모든 $i,j$ 쌍에 관한 $x_i, x_j$의 유사도를 가우스 분포를 이용한 유사도로 나타낸다.\n2. $x_i$와 같은 개수의 점 $y_i$를 낮은 차원 공간에 무작위로 배치하고, 모든 $i,j$쌍에 관한 $y_i,y_j$의 유사도를 t분포를 이용해 나타낸다.\n3. 1과 2에서 정의한 유사도 분포가 가능하면 같도록 데이터 포인트 $y_i$를 갱신한다.\n4. 수렴 조건까지 과정 3을 반복한다.\n\n- *t분포는 헤비테일분포^[heavy-tailed distribution, 일반적인 정규분포보다 끝단의 값이 두터운 분포]이므로 높은 차원 공간에서는 중심에서 먼 부분의 비중이 높다.*\n- *따라서 일부분의 정보를 유지하기 어려워 4차원 이상의 공간은 차원 축소가 제대로 되지 않을 수 있다.*\n\n# Comparison LLE vs TSNE vs PCA\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-05-20-Manifold learning Embedding.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"Manifold learning Embedding","author":"SEOYEON CHOI","date":"2023-05-20","categories":["LocallyLinearEmbedding","TSNE"]},"extensions":{"book":{"multiFile":true}}}}}