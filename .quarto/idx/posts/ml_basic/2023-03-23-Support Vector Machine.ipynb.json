{"title":"Support Vector Machine","markdown":{"yaml":{"title":"Support Vector Machine","author":"SEOYEON CHOI","date":"2023-03-25","categories":["Machine learning basic","지도학습"]},"headingText":"1. 선형 SVM 분류","containsRefs":false,"markdown":"\n\n> Support Vector Machine\n\n- Support Vector Machine은 모델이다.\n- hyperparameter는 C와 gamma\n- 주로 사용하는 손실함수는 힌지손실 Hinge Loss\n\nReference : [핸즈 온 머신러닝](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [머신러닝 도감](https://jpub.tistory.com/992), [사이킷런 홈페이지](https://scikit-learn.org/stable/modules/svm.html)\n\n[마진이 아래 식으로 구해지는 이유](https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw)\n\n- margin = $\\frac{2}{||w||}$\n\n[어딘가의 SVM lecture 노트임!](https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n\n![image](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/450px-SVM_margin.png)\n\n- 샘플들을 서포트벡터라고 보자\n- 마진은 $\\frac{2}{||w||}$\n- 점 하나가 랜드마크. 랜드마크를 어떤 기준으로 뽑을 것임.\n- 그 기준으로 슬랙 변수를 계산할건데\n\n![image.png](attachment:4605bb29-2b44-419f-a7fd-00ba904fc034.png)\n\n- 위의 경우는 소프트 마진에 한 함\n- 마진 경계선으로부터 반대쪽까지 거리를 슬랙 변수로 정의함\n- 경계에 걸치면 슬랙변수는 0이 됌.\n- 0과 1사이에 있으면 마진 오차 margin violation\n- 2보다 크다면 잘못 분류됨 misclassified\n- 소프트 마진에서는 1이나 -1이 아니라 $1- \\zeta_1$을 기준으로 경계선이 나눠져서!\n- 소프트 마진에서는 하이퍼파라미터 C를 거의 0으로 만들어서 하드 마진처럼 strick 하게 만들 수 있음\n\n**힌지 손실 Hinge Loss**\n\n- $max(0,1-t)$ function\n- $t=1$에서 미분가능하지 않지만 $t=1$에서 서브그레디언트로 경사 하강법 사용\n\n\n- 클래스가 선형으로 분류될 때\n- 클래스 분류의 경계에 있는 샘플 = 서포트 벡터\n\n\n## 1) 하드 마진 분류\n\n- 클래스 분류의 경계에 샘플이 없을 때 = 마진 오류가 하나도 없을때, 하드 마진 분류\n\n    - 선형적으로 구분되어야 함\n    - 이상치에 민감함\n\n이 때 생기는 이상치를 마진 오류로 본다.\n\n- 이상치 $\\to$ 클래스 구분했을때 그어지는 선에 걸쳐지는 값들 $\\to$ 마진 오류  margin violation\n\n적절한 균형 잡는 것이 필요\n\n$minimize_{w,b} ||w||^2_2$\n\nsubject to $y_i(w^\\top x_i - b) \\ge 1 \\forall_i \\in \\{1, \\dots , n \\}$\n\n- 거의 불가능..한 자료 구조..\n\n## 2) 소프트 마진 분류\n\n소프트 마진 분류 필요한 이유.\n\n- hyper parameter C가 너무 크면 과대적합 가능성 존재\n\n손실함수는 [힌지 손실 Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)\n\n- 주로 SVM과 함께 쓰임. $max(0,1-y), max(0,1+y)$ 로 $y$의 범위가 [-1,1]에 오도록 함\n\n$minimize_{w,b,\\zeta} ||w||^2_2 + C\\sum^n_{i=1} \\zeta_i$\n\nsubject to $y_i(w^\\top x_i - b) \\ge 1-\\zeta_i, \\zeta_i \\ge 0 \\forall_i \\in \\{ 1 ,\\dots, n\\}$\n\n- $1-\\zeta_i$로 잡음으로써 더 유연하게 이상치 정의 및 경계선 정의\n\n* 위처럼 선에 걸친게 마진 오류 margin violation\n\n# 2. 비선형 SVM 분류\n\n- 선형적으로 분류할 수 없는 데이터셋에 한해서, 2차, 3차,.. 등\n\n## 1) 다항식 커널\n\n- 다항식은 잘 작동하기는 한데 설명변수가 너무 많아지면 오히려 모델을 느리게 만들어서 안 좋아..\n\n## 2) 유사도 특성\n\n- 특정 랜드마크와 얼마나 닮았는지 유사도 함수 similarity function = similarity measure =  similarity metric 로 계산^[a real-valued function that quantifies the similarity between two objects]\n    - 특정 랜드마크 -> 데이터 분포가 있을떄 뽑아진 임의의 점?\n\n[유사도 함수의 종류](https://en.wikipedia.org/wiki/Similarity_measure)\n\n- 유클리디안 거리 Euclidean Distance ; 모든 속성 고려\n- [코사인 유사도 Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) ; 각으로 고려; -1~1사이\n- 마할라라비스 거리 ; 값들 사이의 공분산 이용\n- 민코스키 거리 Minkowski Distance ; 가장 큰 값만 고려\n\n## 3) 가우시안 RBF 커널^[커널svm 참고]\n\n- [Gaussian Radical Basis Function = Gaussian Kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n\n## 4) 계산 복잡도\n\n$O(m\\times n)$\n\ncomputational complexity theory\n\n# 3. SVM 회귀\n\nSVM은 선형, 비선형 분류 뿐만 아니라 선형, 비선형 회귀에도 사용\n\n- 마진을 크게하든지(오른쪽) 작게하든지(왼쪽) 영향을 받지 않는다면, 민감하지 않다고 표현\n\n![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcToqCwuryPQlMv-e3VZElBUqIHrJozxH0j19w&usqp=CAU)\n\n# 4. SVM 이론\n\n## 1) 결정 함수와 예측\n\n**선형 SVM을 훈련한다 = 마진 오류를 하나도 발생하지 않거나(하드 마진) 제한적인 마진 오류를 가지면서(소프트 마진) 가능한 한 마진을 크게 하는 $\\mathbb{w}$와 $b$를 찾는 것**\n\n단순히 선형 회귀식 계산해서 0인지 1인지를 나눈다.\n\n$\\hat{y} = \\begin{cases} 0 & \\mathbb{w}^\\top \\mathbb{x} + b < 0 \\text{ 일 때} \\\\ 1 & \\mathbb{w}^\\top \\mathbb{x} + b \\ge 0 \\text{ 일 때} \\end{cases}$\n\n## 2) 목적 함수\n\n결정 함수의 기울기는 가중치 벡터의 노름 $||w||$와 같다.\n\n- 마진 크게 = $||w||$ 최소화\n\n하드 마진 선형 SVM 분류기의 목적 함수\n\n$minimize_{w,b} \\frac{1}{2} \\mathbb{w^\\top w}$\n\n$\\star$ 왜 $\\frac{1}{2} \\mathbb{w^\\top w}$? $||w||$는 $w=0$에서 미분도 되지 않음\n\n소프트 마진 선형 SVM 분류기의 목적 함수\n\n- 슬랙 변수 slack variable $\\zeta^{(i)} \\ge 0$ 도입\n    - i번째 샘플이 얼마나 마진을 위반할지 정함\n    - $x > b$\n    - $x = b + slack$\n\n-> 슬랙변수는 경계를 나눌때 생기는 최소한의 오차로 생각하자\n\n마진 오류 최소화 방법\n\n1. 슬랙 변수 값을 작게 만들기\n2. 마진 크게 하기 위해 $\\frac{1}{2} \\mathbb{w^\\top w}$ 최소화\n\n$minimize_{w,b,\\zeta} \\frac{1}{2} \\mathbb{w^\\top w} + C \\sum^m_{i=1}\\zeta^{(i)}$\n\n$\\star$ hyper parameter $C$ $\\to$ 두 목표object 사이의 트레이드 오프 정의\n\n## 3) 콰드라틱 프로그래밍\n\n콰드라틱 프로그래밍 Quadratic Programming, QP = 하드 마진과 소프트 마진 문제는 모두 선형적인 제약 조건이 있는 볼록 함수의 이차 최적화 문제\n\n## 4) 쌍대 문제\n\n## 5) 커널 SVM\n\n원래 선형 분리할 수 없는 비선형 데이터를 커널을 통해서 선형화 시킴!\n\n$\\star$ 가우시안 커널이 복잡한 결정 경게를 학습한다고 데이터에 무조건 비선형 커널 기법을 적용하지 말고(분석에 의미가 없을 수 있음), 선형 커널을 이용한 분석으로 확린 후 커널 함수를 적용한 분석을 하는 것이 좋음\n\n![image](https://miro.medium.com/v2/resize:fit:838/1*gXvhD4IomaC9Jb37tzDUVg.png)\n\n커널의 종류\n\n- 선형 커널 linear kernel [예시, 선형, 비선형 포함](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py)\n    - $<x,x'>$\n- 시그모이드 커널 sigmoid kernel\n    - $\\tanh(\\gamma<x,x'>+r)$\n    - where $r$ is specified by `coef0`.\n- 다항 커널 polynomial kernel\n    - $(\\gamma<x,x'> + r)^d$\n    - where $d$ is specified by parameter `degree`, $r$ by `coef0`.\n- RBF 커널 = 가우시안 커널 rbf kernel\n    - $exp(-\\gamma||x - x'||^2)$\n    - where $r$ is specified by parameter `gamma`, must be grater than 0\n        - 복잡한 결정 경계 다룰때 좋을 듯\n\n![image.png](attachment:7892c7b2-ce01-4e1d-aac1-b57798724e72.png)\n\n쓰는 법\n```python\nlinear_svc = svm.SVC(kernel='linear')\nlinear_svc.kernel\n```\n선형 커널\n```python\nlinear_svc = svm.SVC(kernel='sigmoid')\nlinear_svc.kernel\n```\n시그모이드 커널\n```python\nlinear_svc = svm.SVC(kernel='poly')\nlinear_svc.kernel\n```\n다항 커널\n```python\nrbf_svc = svm.SVC(kernel='rbf')\nrbf_svc.kernel\n```\nrbf 커널\n\n## 6) 온라인 SVM\n\n\n\n# SVM과 로지스틱 회귀의 차이\n\n> 둘 다 지도학습이다!\n\n- SVM은 경계면으로 데이터 classification, 로지스틱은 시그모이드 함수로0~1 사이 확률값 추정해서 분류\n- 대용량 데이터에서는 SVM이 적당, 로지스틱 회귀는 느리다\n- multi-class를 위해 로지스틱 회귀 이용, SVM은 이진 분류만 가능\n- 슬랙변수 쓰는 SVM은 이상치에 덜 민감함, 하지만 로지스틱은 이런 역할 하는 변수 없어서 민감\n\n# 예제\n\nSVM\n\n커널 기법 이용\n","srcMarkdownNoYaml":"\n\n> Support Vector Machine\n\n- Support Vector Machine은 모델이다.\n- hyperparameter는 C와 gamma\n- 주로 사용하는 손실함수는 힌지손실 Hinge Loss\n\nReference : [핸즈 온 머신러닝](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [머신러닝 도감](https://jpub.tistory.com/992), [사이킷런 홈페이지](https://scikit-learn.org/stable/modules/svm.html)\n\n[마진이 아래 식으로 구해지는 이유](https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw)\n\n- margin = $\\frac{2}{||w||}$\n\n[어딘가의 SVM lecture 노트임!](https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n\n![image](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/450px-SVM_margin.png)\n\n- 샘플들을 서포트벡터라고 보자\n- 마진은 $\\frac{2}{||w||}$\n- 점 하나가 랜드마크. 랜드마크를 어떤 기준으로 뽑을 것임.\n- 그 기준으로 슬랙 변수를 계산할건데\n\n![image.png](attachment:4605bb29-2b44-419f-a7fd-00ba904fc034.png)\n\n- 위의 경우는 소프트 마진에 한 함\n- 마진 경계선으로부터 반대쪽까지 거리를 슬랙 변수로 정의함\n- 경계에 걸치면 슬랙변수는 0이 됌.\n- 0과 1사이에 있으면 마진 오차 margin violation\n- 2보다 크다면 잘못 분류됨 misclassified\n- 소프트 마진에서는 1이나 -1이 아니라 $1- \\zeta_1$을 기준으로 경계선이 나눠져서!\n- 소프트 마진에서는 하이퍼파라미터 C를 거의 0으로 만들어서 하드 마진처럼 strick 하게 만들 수 있음\n\n**힌지 손실 Hinge Loss**\n\n- $max(0,1-t)$ function\n- $t=1$에서 미분가능하지 않지만 $t=1$에서 서브그레디언트로 경사 하강법 사용\n\n# 1. 선형 SVM 분류\n\n- 클래스가 선형으로 분류될 때\n- 클래스 분류의 경계에 있는 샘플 = 서포트 벡터\n\n\n## 1) 하드 마진 분류\n\n- 클래스 분류의 경계에 샘플이 없을 때 = 마진 오류가 하나도 없을때, 하드 마진 분류\n\n    - 선형적으로 구분되어야 함\n    - 이상치에 민감함\n\n이 때 생기는 이상치를 마진 오류로 본다.\n\n- 이상치 $\\to$ 클래스 구분했을때 그어지는 선에 걸쳐지는 값들 $\\to$ 마진 오류  margin violation\n\n적절한 균형 잡는 것이 필요\n\n$minimize_{w,b} ||w||^2_2$\n\nsubject to $y_i(w^\\top x_i - b) \\ge 1 \\forall_i \\in \\{1, \\dots , n \\}$\n\n- 거의 불가능..한 자료 구조..\n\n## 2) 소프트 마진 분류\n\n소프트 마진 분류 필요한 이유.\n\n- hyper parameter C가 너무 크면 과대적합 가능성 존재\n\n손실함수는 [힌지 손실 Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)\n\n- 주로 SVM과 함께 쓰임. $max(0,1-y), max(0,1+y)$ 로 $y$의 범위가 [-1,1]에 오도록 함\n\n$minimize_{w,b,\\zeta} ||w||^2_2 + C\\sum^n_{i=1} \\zeta_i$\n\nsubject to $y_i(w^\\top x_i - b) \\ge 1-\\zeta_i, \\zeta_i \\ge 0 \\forall_i \\in \\{ 1 ,\\dots, n\\}$\n\n- $1-\\zeta_i$로 잡음으로써 더 유연하게 이상치 정의 및 경계선 정의\n\n* 위처럼 선에 걸친게 마진 오류 margin violation\n\n# 2. 비선형 SVM 분류\n\n- 선형적으로 분류할 수 없는 데이터셋에 한해서, 2차, 3차,.. 등\n\n## 1) 다항식 커널\n\n- 다항식은 잘 작동하기는 한데 설명변수가 너무 많아지면 오히려 모델을 느리게 만들어서 안 좋아..\n\n## 2) 유사도 특성\n\n- 특정 랜드마크와 얼마나 닮았는지 유사도 함수 similarity function = similarity measure =  similarity metric 로 계산^[a real-valued function that quantifies the similarity between two objects]\n    - 특정 랜드마크 -> 데이터 분포가 있을떄 뽑아진 임의의 점?\n\n[유사도 함수의 종류](https://en.wikipedia.org/wiki/Similarity_measure)\n\n- 유클리디안 거리 Euclidean Distance ; 모든 속성 고려\n- [코사인 유사도 Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) ; 각으로 고려; -1~1사이\n- 마할라라비스 거리 ; 값들 사이의 공분산 이용\n- 민코스키 거리 Minkowski Distance ; 가장 큰 값만 고려\n\n## 3) 가우시안 RBF 커널^[커널svm 참고]\n\n- [Gaussian Radical Basis Function = Gaussian Kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)\n\n## 4) 계산 복잡도\n\n$O(m\\times n)$\n\ncomputational complexity theory\n\n# 3. SVM 회귀\n\nSVM은 선형, 비선형 분류 뿐만 아니라 선형, 비선형 회귀에도 사용\n\n- 마진을 크게하든지(오른쪽) 작게하든지(왼쪽) 영향을 받지 않는다면, 민감하지 않다고 표현\n\n![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcToqCwuryPQlMv-e3VZElBUqIHrJozxH0j19w&usqp=CAU)\n\n# 4. SVM 이론\n\n## 1) 결정 함수와 예측\n\n**선형 SVM을 훈련한다 = 마진 오류를 하나도 발생하지 않거나(하드 마진) 제한적인 마진 오류를 가지면서(소프트 마진) 가능한 한 마진을 크게 하는 $\\mathbb{w}$와 $b$를 찾는 것**\n\n단순히 선형 회귀식 계산해서 0인지 1인지를 나눈다.\n\n$\\hat{y} = \\begin{cases} 0 & \\mathbb{w}^\\top \\mathbb{x} + b < 0 \\text{ 일 때} \\\\ 1 & \\mathbb{w}^\\top \\mathbb{x} + b \\ge 0 \\text{ 일 때} \\end{cases}$\n\n## 2) 목적 함수\n\n결정 함수의 기울기는 가중치 벡터의 노름 $||w||$와 같다.\n\n- 마진 크게 = $||w||$ 최소화\n\n하드 마진 선형 SVM 분류기의 목적 함수\n\n$minimize_{w,b} \\frac{1}{2} \\mathbb{w^\\top w}$\n\n$\\star$ 왜 $\\frac{1}{2} \\mathbb{w^\\top w}$? $||w||$는 $w=0$에서 미분도 되지 않음\n\n소프트 마진 선형 SVM 분류기의 목적 함수\n\n- 슬랙 변수 slack variable $\\zeta^{(i)} \\ge 0$ 도입\n    - i번째 샘플이 얼마나 마진을 위반할지 정함\n    - $x > b$\n    - $x = b + slack$\n\n-> 슬랙변수는 경계를 나눌때 생기는 최소한의 오차로 생각하자\n\n마진 오류 최소화 방법\n\n1. 슬랙 변수 값을 작게 만들기\n2. 마진 크게 하기 위해 $\\frac{1}{2} \\mathbb{w^\\top w}$ 최소화\n\n$minimize_{w,b,\\zeta} \\frac{1}{2} \\mathbb{w^\\top w} + C \\sum^m_{i=1}\\zeta^{(i)}$\n\n$\\star$ hyper parameter $C$ $\\to$ 두 목표object 사이의 트레이드 오프 정의\n\n## 3) 콰드라틱 프로그래밍\n\n콰드라틱 프로그래밍 Quadratic Programming, QP = 하드 마진과 소프트 마진 문제는 모두 선형적인 제약 조건이 있는 볼록 함수의 이차 최적화 문제\n\n## 4) 쌍대 문제\n\n## 5) 커널 SVM\n\n원래 선형 분리할 수 없는 비선형 데이터를 커널을 통해서 선형화 시킴!\n\n$\\star$ 가우시안 커널이 복잡한 결정 경게를 학습한다고 데이터에 무조건 비선형 커널 기법을 적용하지 말고(분석에 의미가 없을 수 있음), 선형 커널을 이용한 분석으로 확린 후 커널 함수를 적용한 분석을 하는 것이 좋음\n\n![image](https://miro.medium.com/v2/resize:fit:838/1*gXvhD4IomaC9Jb37tzDUVg.png)\n\n커널의 종류\n\n- 선형 커널 linear kernel [예시, 선형, 비선형 포함](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py)\n    - $<x,x'>$\n- 시그모이드 커널 sigmoid kernel\n    - $\\tanh(\\gamma<x,x'>+r)$\n    - where $r$ is specified by `coef0`.\n- 다항 커널 polynomial kernel\n    - $(\\gamma<x,x'> + r)^d$\n    - where $d$ is specified by parameter `degree`, $r$ by `coef0`.\n- RBF 커널 = 가우시안 커널 rbf kernel\n    - $exp(-\\gamma||x - x'||^2)$\n    - where $r$ is specified by parameter `gamma`, must be grater than 0\n        - 복잡한 결정 경계 다룰때 좋을 듯\n\n![image.png](attachment:7892c7b2-ce01-4e1d-aac1-b57798724e72.png)\n\n쓰는 법\n```python\nlinear_svc = svm.SVC(kernel='linear')\nlinear_svc.kernel\n```\n선형 커널\n```python\nlinear_svc = svm.SVC(kernel='sigmoid')\nlinear_svc.kernel\n```\n시그모이드 커널\n```python\nlinear_svc = svm.SVC(kernel='poly')\nlinear_svc.kernel\n```\n다항 커널\n```python\nrbf_svc = svm.SVC(kernel='rbf')\nrbf_svc.kernel\n```\nrbf 커널\n\n## 6) 온라인 SVM\n\n\n\n# SVM과 로지스틱 회귀의 차이\n\n> 둘 다 지도학습이다!\n\n- SVM은 경계면으로 데이터 classification, 로지스틱은 시그모이드 함수로0~1 사이 확률값 추정해서 분류\n- 대용량 데이터에서는 SVM이 적당, 로지스틱 회귀는 느리다\n- multi-class를 위해 로지스틱 회귀 이용, SVM은 이진 분류만 가능\n- 슬랙변수 쓰는 SVM은 이상치에 덜 민감함, 하지만 로지스틱은 이런 역할 하는 변수 없어서 민감\n\n# 예제\n\nSVM\n\n커널 기법 이용\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-03-23-Support Vector Machine.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"pulse","code-copy":false,"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Support Vector Machine","author":"SEOYEON CHOI","date":"2023-03-25","categories":["Machine learning basic","지도학습"]},"extensions":{"book":{"multiFile":true}}},"ipynb":{"identifier":{"display-name":"Jupyter","target-format":"ipynb","base-format":"ipynb"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"2023-03-23-Support Vector Machine.ipynb"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Support Vector Machine","author":"SEOYEON CHOI","date":"2023-03-25","categories":["Machine learning basic","지도학습"]}}},"projectFormats":["html"]}