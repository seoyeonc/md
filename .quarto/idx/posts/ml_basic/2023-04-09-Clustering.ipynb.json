{"title":"Clustering","markdown":{"yaml":{"title":"Clustering","author":"SEOYEON CHOI","date":"2023-04-09","categories":["Machine learning basic","비지도학습"]},"headingText":"k-평균 알고리즘","containsRefs":false,"markdown":"\n\n> Clustering\n\nReference: [Pattern](https://darmanto.akakom.ac.id/pengenalanpola/Pattern%20Recognition%204th%20Ed.%20(2009).pdf)\n\n경성 군집화 hard clustering = 한 셈플이 하나의 군집에 속하도록 강제하는 방식\n\n연성 군집화 soft clustering = 샘플마다 군집에 속하는 정도를 다르게 할 수 있음\n\n\n입력: 훈련집합 $X = \\{x_1,x_2,\\dots,x_n\\}$ 군집의 개수 k\n\n출력: 군집집합 $C = \\{c_1,c_2,\\dots,c_k\\}$\n\nk개의 군집 중심 $Z = \\{z_1,z_2,\\dots,z_n\\}$를 초기화한다.\n\n```python\nwhile (true)\n\n    for (i=1 to n)\n```    \n\n$x_i$를 가장 가까운 군집 중심에 배정한다.\n\n```python        \n    if (위에서 이루어진 배정이 이전 루프에서의 배정과 같으면) break\n   \n    for (j=1 to k)\n```            \n\n$z_j$에 배정된 샘플의 평균으로 $z_j$를 대치한다.\n\n```python        \nfor (j=1 to k)\n```\n\n\n\n$z_j$에 배정된 샘플을 $c_j$에 대입한다.\n\nor\n\n1. 데이터 포인트 중 적당한 점을 집단 개수만큼 선택해 중심으로 정함\n2. 데이터 포인트와 각 중심 사이의 거리를 계산해 가장 가까운 중심을 해당 데이터 포인트가 속한 집단으로 정함\n3. 집단마다 데이터 포인트의 평균을 계산하고 이를 새로운 중심으로 정함\n4. 데이터 포인트 모두가 속한 집단이 변하지 않거나 더는 계산할 수 없을 때까지 과정 2,3을 반복함\n\n## 최적화 문제로 해석\n\nk평균은 직관에 기초한 휴리스틱한 알고리즘으로 보이는데, 이면에는 이론적인 토대를 갖추고 있다.\n\nk평균의 목적함수\n\n$J(Z,A) = \\sum^n_{i=1} \\sum^k_{j=1} a_{ji} dist(x_i, z_j)$\n\nZ는 군집 중심으로 A는 샘플의 배정 정보를 나타내는 k*n 행렬이다. i번째 샘플이 j번째 군집에 배정되었다면 $a_{ji}$는 1이고, 그렇지 않으면 0이다.\n\nk-평균은 최적화 문제를 푸는 알고리즘으로 볼 수 있다.\n\n- 루프를 반복하면서 목적함수의 값이 작아지는 방향으로 해를 갱신한다. \n- 어떤 초기 군집 중심을 가지고 출발하더라도 수렴한다는 것은 증명되었으나\n- 초기 군집 중심이 달라지면 최종 결과가 달라지는 문제가 있다.\n\n## EM 기초\n\nZ는 입출력단계에서 보이지 않는 은닉변수 latent variable\n\nEM 알고리즘 Expectation Maximazation algorithm\n\nE단계\n\n- 은닉변수를 추정하는 단계\n\nM단계\n\n- 매개변수를 추정하는 단계\n\n## 친밀도 전파 알고리즘\n\n소셜네트워크 자료에서 사용?\n\n## 커널 클러스터링\n\n유클리드 거리로 표현되는 거리를 내적으로 표현해서 커널로 보내어 비선형으로 분리한 클러스터링 결과 얻기\n\n초기값의 영향을 많이 받아 결과가 바뀌기도\n\n## 스펙트럴 클러스터링\n\n차원축소를 통래 초기값에 대한 의존성을 줄이려는 시도\n\n## 파라미터의 자동 결정\n\n직접 초기값 입력하면 거기에 의존하여 결과가 바뀌기도 하니 객관적으로 결정되게끔 파라미터 선택되게 하는 기법\n\n제곱 손실 상호정보량mutual information 사용 상호정보량보다 이상값에 민감하게 반응하지 않는디.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-04-09-Clustering.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"Clustering","author":"SEOYEON CHOI","date":"2023-04-09","categories":["Machine learning basic","비지도학습"]},"extensions":{"book":{"multiFile":true}}}}}