{"title":"Lasso and Ridge","markdown":{"yaml":{"title":"Lasso and Ridge","author":"SEOYEON CHOI","date":"2023-03-29","categories":["Machine learning basic","벌점화모형","능형회귀"]},"headingText":"벌점화모형","containsRefs":false,"markdown":"\n\n> Lasso and Ridge\n\n릿지ridge 회귀 = 능형회귀 = 티호노프 tikhonov 규제 = L2 노름\n\n$\\star$ **숙지!**\n\n릿지 회귀는 L2 규제를 사용하여 모델의 가중치가 커지지 않도록 제한합니다. 이 규제는 일반적으로 극단적인 가중치 값을 갖는 것을 방지하고, 모델의 일반화 성능을 향상시킵니다.\n\n하지만 릿지 회귀는 가중치를 완전히 0으로 만들지는 않습니다. 대신, 가중치를 작게 만들어 모델이 더 일반적인 패턴을 학습하도록 유도합니다. 이는 희소성을 가진 데이터에서 모델의 성능을 향상시키는 데 도움이 됩니다.\n\n반면, 라쏘 회귀는 L1 규제를 사용하여 가중치를 0으로 만들 수 있습니다. 이는 희귀 학습에서 유용합니다. 희소성을 가진 데이터에서는 많은 특성 중 일부만 중요하다는 것을 알 수 있습니다. 따라서, 라쏘 회귀는 이러한 특성을 선택하고 다른 특성을 제거하여 모델을 희소하게 만들 수 있습니다.\n\nRefernece: [책_The Elements of Statistical Learning](https://hastie.su.domains/Papers/ESLII.pdf), [그림과 수식으로 배우는 통통 머신러닝](https://product.kyobobook.co.kr/detail/S000001875111), [2021데이터과학 최규빈교수님 lecture 노트](https://seoyeonc.github.io/md/posts/ml_basic/2021-03-31-Ridge%20Regression_note3_0331.html)\n\n\n기본 가정 : $\\bf{y= X\\beta + \\epsilon}$ 의 회귀식이 있을때, $\\bf{\\hat{\\beta} = (X'X)^{-1} X' y}$로 정의할 수 있음.\n\n- 의문 : $\\bf{(X'X)^{-1}}$을 구할 수 없을 때는?\n    - (1) $\\bf{(X'X)^{-1}}$가 full-rank 일 때\n    - (2) $\\bf{(X'X)^{-1}}$가 full rank 라면 rank = $p$\n        - $\\to$ $\\bf{(X'X)^{-1}}$가 존재하지 않는다는 의미는 $rank(\\bf{X'X})<p$\n        - 이유 1) 공선성 때문\n        - 이유 2) $n<p$, 데이터가 부족할 때 이러한 현상이 발생한다.\n        \n해결 방법\n\n1. 리지 Ridge\n\n- idea : $\\bf{(X'X+\\lambda I)^{-1}}$을 대신 계산한다.\n\n2. PCA를 이용하여 $\\bf{(X'X)} = \\Psi \\Lambda \\Psi$로 대신 구함\n\n- idea : $\\bf{(X'X)}$ $\\to$ 대칭이고 실수면 $\\to$ 고유 분해 가능\n- $\\bf{(X'X) = \\Psi \\lambda \\Psi^\\top}$는 $\\bf{(X'X)^{-1} = \\Psi \\lambda^{-1} \\Psi^\\top}$을 의미하니까\n    - $\\bf{(X'X)(X'X)^{-1}} = I$\n\n## 예제)\n\n$y = \\beta_1 x_{i1} +  \\beta_2 x_{i2} + \\epsilon, \\epsilon \\sim N(0,\\sigma^2)_{idd}$임 모형이 있다고 할 때,\n\n원래 모형은 $y = 5 x_{i1} +  600 x_{i2}$이다.\n\n이 때, $x_{i3}$이 $x_{i1}$과 거의 $1$의 상관관계를 가지고 있다면? $\\to$ 다중공선성을 가지고 있다면?\n\n$\\beta_1 + \\beta_3 = 5$만 된다면 원래 모형에 근사하다고 나올 터, 심지어 음수가 나올 때 조차도.\n\n- 다중공선성의 특징\n    - 추정하는 $\\beta$가 어떤 값인지 거의 예측이 안 된다.\n        - $\\beta$들의 분산이 크다. (다양한 값 나오고 그러니..)\n    - $\\beta$는 그래도 더하면 원래 값에 근사함.\n\n모두 참이라고 생각되는 모형 $\\to$ 합은 일단 원히던 5임\n\n- 조건) 다중공선성 없는 $x_{i2}$는 $y$쪽으로 옮긴 상황. 즉 신경 안 써도 됌\n\n1. $\\beta_1=2,\\beta_2=3$\n2. $\\beta_1=5,\\beta_2=0$\n3. $\\beta_1=10,\\beta_2=-5$\n4. $\\beta_1=10000,\\beta_2=-9995$\n\n$\\dots$\n\n-> 음수 있는 해석 불가한 이상한 모형들 다 가능하겠다. 해석 불가능.\n\n# Ridge와 Lasso\n\n$loss = \\sum(y-X_1\\beta_1 - X_2\\beta_2)^2$\n\n$loss\\tilde{} = (y-XB)^\\top (y-XB) + \\lambda B^\\top B$, $\\bf{\\hat{B} = X'X + \\lambda I} \\to \\hat{\\beta}$과 달리 **불편추정량**이 아님을 알 수 있음\n\n- 하지만, 분산이 더 작다!\n- 수정된 loss 값 구별 위해 $\\tilde{}$ 기호 추가\n\n- $(X'X)$가 양정치 행렬이라는 조건이 필요하지만, 저절로 만족되어 고려하지 않아도 됌.\n- $\\lambda$ 잘 찾으면 항상 ridge regression이 linear regression 보다 좋다고 주장할 수 있다.\n\n$\\beta^{QLS} = (X^\\top X)^{-1}X^\\top Y$^[Constrained Least Squares]\n\n$\\beta^{R} = (X^\\top X + \\lambda I)^{-1}X^\\top Y$\n\n- $(X^\\top X)$ 가 역행렬 구할 수 없다고 보고 $\\lambda I$를 더해줘서 역행렬 구할 수 있게\n    - 왜냐하면 역행렬 구할 수 없다는 말은 대각선에 0이 존재한다는 것이니까.\n\n벌점화를 주자. = 패널티를 주자 $loss + \\lambda(\\beta^2_1 + \\beta^2_2)$\n\n1. $\\lambda(2^2 + 3^2)$\n2. $\\lambda(5^2_)$\n3. $\\lambda(10^2+(-5)^2)$\n4. $\\lambda((10000)^2 + (-9995)^2)$\n\n$\\lambda$ = hyperparameter 하이퍼파라메터 = 조율 모수 tuning parameter = Regularization parameter 규제화항\n\n![image](https://miro.medium.com/v2/resize:fit:761/1*nrWncnoJ4V_BkzEf1pd4MA.png)\n\n`1`\n\n$loss = SSE + L_2-panalty:Ridge$\n\n- [Ridge](https://en.wikipedia.org/wiki/Ridge_regression) = $L_2$ norm = Regularization 정규화 = $L_2$ 패널티 = 벌점\n\n- $(\\lambda(\\beta_1^2 +\\beta^2_2)$이니까 원 모형이 나오지\n    - $(\\lambda(\\beta_1^2 +\\beta^2_2) = k$\n    - $\\beta_1^2 +\\beta^2_2 = \\frac{k}{\\lambda}$\n- $\\lambda$ 가 0이면 loss와 ridge 함수가 만나는 면,\n- $\\lambda$ 가 $\\infty$이면 중심(0,0)에 있을터.\n\n- 너무 큰 $\\beta_i$들을 구할 때 패널티를 부여하여 되도록 작은 $\\beta_i$들을 선택하게 제약을 거는 것\n\n- 가장 최적의 점 $\\beta_1 = \\beta_2$\n\n`2`\n\n$loss = SSE + L_1-panalty:Lasso$\n\n- [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) = $L_1$ norm\n\n- $(\\lambda(|\\beta_1| +|\\beta_2|)$이니까 마름모 모형이 나오지\n    - $(\\lambda(|\\beta_1| +|\\beta_2|) = k$\n    - $|\\beta_1| +|\\beta_2| = \\frac{k}{\\lambda}$\n- $\\lambda$ 가 0이면 loss와 lasso 함수가 만나는 면,\n- $\\lambda$ 가 $\\infty$이면 중심(0,0)에 있을터.\n\n- 가장 최적의 점 $(0,절편), (절편,0)$\n\n`3`\n\n$loss = SSE +  L_1-panalty + L_2-panalty : Elastic-net$\n\n- 섞어쓰기도 함\n\n$\\lambda \\beta^\\top\\beta$\n\n- 패널티항= 벌점항 = L2-패널티, 정규화항이라 부른다.\n- Ridge(최소제곱학습은 $L_2$norm노름에 대한 제약조건 사용)\n    - $\\lambda ||\\beta||^2_2$로 표현하기도 한다. $||\\dot||_2$는 벡터의 $L_2$-norm노름 이라고 한다.\n        - $x = (x_1,x_2) \\to ||x||^2_2 := x^2_1 + x^2_2. ||x||_2 := \\sqrt{x^2_1 + x^2_2}$\n        - $\\lambda \\beta^\\top \\beta = \\lambda(\\beta^2_1 + \\beta^2_2)$\n- Lasso(희소 학습에서는 $L_1$norm노름에 대한 제약조건 사용)\n    - 희소 학습이라 부르는 이유\n        - $|\\beta_1| +|\\beta_2|$해가 0이 되는 구간이 마름모라 4 꼭짓점 나오는데, 이게 희소한 해를 갖는다고 말함\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-03-29-Lasso and Ridge.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"Lasso and Ridge","author":"SEOYEON CHOI","date":"2023-03-29","categories":["Machine learning basic","벌점화모형","능형회귀"]},"extensions":{"book":{"multiFile":true}}}}}