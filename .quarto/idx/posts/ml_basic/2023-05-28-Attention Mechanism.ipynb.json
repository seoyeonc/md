{"title":"Attention Mechanism","markdown":{"yaml":{"title":"Attention Mechanism","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Attention Mechanism"]},"headingText":"Attention Function","containsRefs":false,"markdown":"\n\n> Attention Mechanism\n\nRef: [딥 러닝을 위한 자연어 처리 입문](https://wikidocs.net/22893)\n\n::: {.callout-note}\n\n**my own summary**\n\n현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태와 얼마나 유사한지를 판단하는 스코어 값인 어텐션 스코어를 이용하여 인코더의 문맥context를 포함한 값을 입력으로 넣어 정확도를 높이는 방법\n\n:::\n\nRNN에 기반한 seq2seq 모델의 문제점\n\n1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생\n2. RNN의 고질적인 문제인 기울기 소실 문제 존재\n\n$\\to$ 기계 번역 동안 입력 문장이 길면 품질이 떨어지는 현상 발생, 이의 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 들장한 어텐션.\n\n::: {.callout-note}\n\n- 그레디언트 기반의 학습에서 역전파할때 발생하는 문제로 0 근처의 숫자를 계속 곱하여 터지거나 0으로 가는 상황\n\n- 그레디언트 기반의 학습\n    - 소실함수의 기울기를 통하여 업데이트하는 방식\n- 역전파\n    - 손실함수를 여러 단계로 쪼갠다(여러 합성 함수로 표현한다).\n    - 각 단계의 미분값을 구한다,\n    - 그것들을 모두 곱하여 기울기를 계산한다.\n\n참고: [빅데이터 분석 12주차](https://seoyeonc.github.io/chch/big%20data%20analysis/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%86%8C%EB%A9%B8/%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C/2022/01/23/bd-12%EC%A3%BC%EC%B0%A8.html)\n\n:::\n\n\n```{mermaid}\nflowchart LR\n    subgraph \"Source\"\n        subgraph B2\n            direction BT\n            Key1 --> Value1\n            Key2 --> Value2\n            Key3 --> Value3\n        end\n    end\n  Query --> Source --> Attention\\nValue\n```\n\n$$Attention(Q, K, V) = \\text{Attention Value}$$\n\n1. 주어진 쿼리에 대해서 모든 키와의 유사도를 각각 구한다.\n2. 구한 유사도를 키와 맵핑되어 있는 각각의 값value에 반영한다.\n3. 유사도가 반영된 값을 모두 더해서 리턴한다. 여기서 값 = **어텐션 값 attention value**\n\n### 어텐션 모델의 요소\n\n- `Q`= Query \n    - t 시점의 디코더 셀에서의 은닉 상태\n- `K` = Keys\n    - 모든 시점의 인코더 셀의 은닉 상태들\n- `V` = Values\n    - 모든 시점의 인코더 셀의 은닉 상태들\n\n## Dot-Product Attention\n\n- 디코더의 마지막 LSTM 셀이 출력단어를 예측하기 위해 인코더의 모든 입력 단어들의 정보를 다시 참고하고자 함\n- 이 때, 소프트 맥스를 이용\n    - 인코더에 입력된 값들이 출력 단어를 예측할 때 얼마나 도움이 되었는지 수치화한 결과\n    - 다시 디코더로 전달\n    - 디코더가 출력 단어를 더 정확하게 예측하게 도와줌\n\n### 1. 어텐션 스코어를 구한다.\n\n- 인코더의 은닉 시점 time stamp = $1,2, \\dots, N$\n- 인코더의 은닉 상태 hidden state = $s_t$\n    - 단, 인코더와 디코더의 은닉 상태는 차원이 같음\n\n::: {.callout-important}\n\n디코더의 현재시점 t에서 필요한 입력값\n\nseq2seq\n\n- 이전 시점인 t-1의 은닉 상태\n- 이전 시점 t-1에서 나온 출력 단어\n\nattention\n\n- 이전 시점인 t-1의 은닉 상태\n- 이전 시점 t-1에서 나온 출력 단어\n- 어텐션 값 $a_t$\n\n:::\n\n\n**attention score 어텐션 값**\n\n- 현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지를 판단하는 스코어 값\n\n닷 프로덕트 어텐션은 은닉 상태 $s_t$를 전치(transpose)하고, 각 은닉 상태와 내적(dot product)를 수행하여 스칼라 결과 어텐션 스코어 score 추출\n\n- **닷 프러덕트 어텐션이라고 부르는 이유**\n\n$$score(s_t, h_i) = s_t^\\top h_i$$\n\n$$e^t = [s_t^\\top h_1, \\dots, s_t^\\top h_N]$$\n\n$e^t$는 $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값\n\n### 2. 소프트맥스 함수를 통해 어텐션 분포를 구한다.\n\n- $e^t$에 소프트맥스 함수 적용해서 합이 1이 되는 확률 분포 도출\n    - 각각 값은 어텐션 가중치 Attention weight\n    - 분포는 어텐션 분포 Attention Distribution $\\alpha^t$\n\n$$\\alpha^t = softmax(e^t)$$\n\n### 3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구한다.\n\n- 어텐션의 최종 결과값을 얻기 위해 각 인코더의 은닉상태 $s_t$와 어텐션 가중치 값attention weight를 곱하고 가중합weighted sum인 어텐션 값attention value $a_t$을 구한다.\n\n$$a_t = \\sum^N_{i=1} \\alpha_i^t h_i$$\n\n::: {.callout-tip}\n\n어텐션 값 attention value $a_t$는 인코더의 문맥을 포함하고 있다고 하여 컨텐스트 벡터 context vector라고도 부른다.\n\n단, seq2seq에서는 인코더의 마지막 은닉 상태를 컨텍스트 벡터context vector라고 불렀음.\n\n:::\n\n### 4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.\n\n- 어텐션 값$a_t$와 디코더의 은닉 상태 $s_t$를 결합concatenate하여 하나의 벡터로 만드는 작업을 수행한다. $\\to$ $v_t$\n- 아 $v_t$를 예측 연산의 입력으로 사용함으로써 인코더로부터 얻은 정보를 활용하여 $\\hat{y}$를 더 잘 예측할 수 있게 됨 $\\to$ 어텐션 메커니즘의 핵심\n\n### 5. 출력층 연산의 입력이 되는 $\\tilde{s}_t$를 계산한다.\n\n- 어텐션 값과 디코더의 t시점의 은닉 상태를 연결하고$a_t s_t$ tanh하이퍼볼릭탄젠트 함수를 통과하도록 해서 출력층 연산을 위한 새로운 벡터 $\\tilde{s}_t$를 얻는다.\n\n::: {.callout-note}\n\nseq2seq\n\n- 출력층의 입력이 t시점의 은닉 상태인 $s_t$\n\nattention\n\n- 출력층의 입력이 $\\tilde{s}_t$\n\n:::\n\n\n$$\\tilde{s}_t = tanh(W_c[a_t;s_t] + b_c$$\n\n$b_c$는 편향\n\n참고: [기계학습 특강 9주차 tanh](https://seoyeonc.github.io/chch/special%20topics%20in%20machine%20learning/%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D/embedding%20layer/2022/11/02/ml_(9%EC%A3%BC%EC%B0%A8)_10%EC%9B%9431%EC%9D%BC.html)\n\n### 6. $\\tilde{s}_t$를 출력층의 입력으로 사용한다.\n\n$$\\hat{y}_t = Softmax(W_y \\tilde{s}_t + b_y)$$\n\n# 다양한 종류의 어텐션\n\n$s_t$는 query, $h_i$는 keys, $W_a$, $W_b$는 학습 가능한 가중치 행렬\n\n| 이름 | 스코어 함수 | defined by | \n|---------|:-----|:------|\n| dot      | $score(s_t,h_i) = s_t^\\top h_i$   |    Luong et al. (2015) |  \n| scaled dot     | $score(s_t,h_i) = \\frac{s_t^\\top h_i}{\\sqrt{n}}$  |   Vaswani et al. (2017) |  \n| general       | $score(s_t,h_i) = s_t^\\top W_a h_i$ 단, $W_a$는 학습 가능한 가중치 행렬    | Luong et al. (2015)|  \n| concat| $score(s_t, h_i) = W_a^\\top tanh(W_b[s_t;h_i]),score(s_t,h_i) = W_a^\\top tanh(W_b s_t + W_c h_i)$| Bahdanau et al. (2015) |  \n| location-base       | $\\alpha_t = softmax(W_a s_t)$ $\\alpha_t$ 산출 시에 $s_t$만 사용하는 방법    |    Luong et al. (2015) |  \n","srcMarkdownNoYaml":"\n\n> Attention Mechanism\n\nRef: [딥 러닝을 위한 자연어 처리 입문](https://wikidocs.net/22893)\n\n::: {.callout-note}\n\n**my own summary**\n\n현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태와 얼마나 유사한지를 판단하는 스코어 값인 어텐션 스코어를 이용하여 인코더의 문맥context를 포함한 값을 입력으로 넣어 정확도를 높이는 방법\n\n:::\n\nRNN에 기반한 seq2seq 모델의 문제점\n\n1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생\n2. RNN의 고질적인 문제인 기울기 소실 문제 존재\n\n$\\to$ 기계 번역 동안 입력 문장이 길면 품질이 떨어지는 현상 발생, 이의 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 들장한 어텐션.\n\n::: {.callout-note}\n\n- 그레디언트 기반의 학습에서 역전파할때 발생하는 문제로 0 근처의 숫자를 계속 곱하여 터지거나 0으로 가는 상황\n\n- 그레디언트 기반의 학습\n    - 소실함수의 기울기를 통하여 업데이트하는 방식\n- 역전파\n    - 손실함수를 여러 단계로 쪼갠다(여러 합성 함수로 표현한다).\n    - 각 단계의 미분값을 구한다,\n    - 그것들을 모두 곱하여 기울기를 계산한다.\n\n참고: [빅데이터 분석 12주차](https://seoyeonc.github.io/chch/big%20data%20analysis/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%86%8C%EB%A9%B8/%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C/2022/01/23/bd-12%EC%A3%BC%EC%B0%A8.html)\n\n:::\n\n# Attention Function\n\n```{mermaid}\nflowchart LR\n    subgraph \"Source\"\n        subgraph B2\n            direction BT\n            Key1 --> Value1\n            Key2 --> Value2\n            Key3 --> Value3\n        end\n    end\n  Query --> Source --> Attention\\nValue\n```\n\n$$Attention(Q, K, V) = \\text{Attention Value}$$\n\n1. 주어진 쿼리에 대해서 모든 키와의 유사도를 각각 구한다.\n2. 구한 유사도를 키와 맵핑되어 있는 각각의 값value에 반영한다.\n3. 유사도가 반영된 값을 모두 더해서 리턴한다. 여기서 값 = **어텐션 값 attention value**\n\n### 어텐션 모델의 요소\n\n- `Q`= Query \n    - t 시점의 디코더 셀에서의 은닉 상태\n- `K` = Keys\n    - 모든 시점의 인코더 셀의 은닉 상태들\n- `V` = Values\n    - 모든 시점의 인코더 셀의 은닉 상태들\n\n## Dot-Product Attention\n\n- 디코더의 마지막 LSTM 셀이 출력단어를 예측하기 위해 인코더의 모든 입력 단어들의 정보를 다시 참고하고자 함\n- 이 때, 소프트 맥스를 이용\n    - 인코더에 입력된 값들이 출력 단어를 예측할 때 얼마나 도움이 되었는지 수치화한 결과\n    - 다시 디코더로 전달\n    - 디코더가 출력 단어를 더 정확하게 예측하게 도와줌\n\n### 1. 어텐션 스코어를 구한다.\n\n- 인코더의 은닉 시점 time stamp = $1,2, \\dots, N$\n- 인코더의 은닉 상태 hidden state = $s_t$\n    - 단, 인코더와 디코더의 은닉 상태는 차원이 같음\n\n::: {.callout-important}\n\n디코더의 현재시점 t에서 필요한 입력값\n\nseq2seq\n\n- 이전 시점인 t-1의 은닉 상태\n- 이전 시점 t-1에서 나온 출력 단어\n\nattention\n\n- 이전 시점인 t-1의 은닉 상태\n- 이전 시점 t-1에서 나온 출력 단어\n- 어텐션 값 $a_t$\n\n:::\n\n\n**attention score 어텐션 값**\n\n- 현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지를 판단하는 스코어 값\n\n닷 프로덕트 어텐션은 은닉 상태 $s_t$를 전치(transpose)하고, 각 은닉 상태와 내적(dot product)를 수행하여 스칼라 결과 어텐션 스코어 score 추출\n\n- **닷 프러덕트 어텐션이라고 부르는 이유**\n\n$$score(s_t, h_i) = s_t^\\top h_i$$\n\n$$e^t = [s_t^\\top h_1, \\dots, s_t^\\top h_N]$$\n\n$e^t$는 $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값\n\n### 2. 소프트맥스 함수를 통해 어텐션 분포를 구한다.\n\n- $e^t$에 소프트맥스 함수 적용해서 합이 1이 되는 확률 분포 도출\n    - 각각 값은 어텐션 가중치 Attention weight\n    - 분포는 어텐션 분포 Attention Distribution $\\alpha^t$\n\n$$\\alpha^t = softmax(e^t)$$\n\n### 3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구한다.\n\n- 어텐션의 최종 결과값을 얻기 위해 각 인코더의 은닉상태 $s_t$와 어텐션 가중치 값attention weight를 곱하고 가중합weighted sum인 어텐션 값attention value $a_t$을 구한다.\n\n$$a_t = \\sum^N_{i=1} \\alpha_i^t h_i$$\n\n::: {.callout-tip}\n\n어텐션 값 attention value $a_t$는 인코더의 문맥을 포함하고 있다고 하여 컨텐스트 벡터 context vector라고도 부른다.\n\n단, seq2seq에서는 인코더의 마지막 은닉 상태를 컨텍스트 벡터context vector라고 불렀음.\n\n:::\n\n### 4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.\n\n- 어텐션 값$a_t$와 디코더의 은닉 상태 $s_t$를 결합concatenate하여 하나의 벡터로 만드는 작업을 수행한다. $\\to$ $v_t$\n- 아 $v_t$를 예측 연산의 입력으로 사용함으로써 인코더로부터 얻은 정보를 활용하여 $\\hat{y}$를 더 잘 예측할 수 있게 됨 $\\to$ 어텐션 메커니즘의 핵심\n\n### 5. 출력층 연산의 입력이 되는 $\\tilde{s}_t$를 계산한다.\n\n- 어텐션 값과 디코더의 t시점의 은닉 상태를 연결하고$a_t s_t$ tanh하이퍼볼릭탄젠트 함수를 통과하도록 해서 출력층 연산을 위한 새로운 벡터 $\\tilde{s}_t$를 얻는다.\n\n::: {.callout-note}\n\nseq2seq\n\n- 출력층의 입력이 t시점의 은닉 상태인 $s_t$\n\nattention\n\n- 출력층의 입력이 $\\tilde{s}_t$\n\n:::\n\n\n$$\\tilde{s}_t = tanh(W_c[a_t;s_t] + b_c$$\n\n$b_c$는 편향\n\n참고: [기계학습 특강 9주차 tanh](https://seoyeonc.github.io/chch/special%20topics%20in%20machine%20learning/%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D/embedding%20layer/2022/11/02/ml_(9%EC%A3%BC%EC%B0%A8)_10%EC%9B%9431%EC%9D%BC.html)\n\n### 6. $\\tilde{s}_t$를 출력층의 입력으로 사용한다.\n\n$$\\hat{y}_t = Softmax(W_y \\tilde{s}_t + b_y)$$\n\n# 다양한 종류의 어텐션\n\n$s_t$는 query, $h_i$는 keys, $W_a$, $W_b$는 학습 가능한 가중치 행렬\n\n| 이름 | 스코어 함수 | defined by | \n|---------|:-----|:------|\n| dot      | $score(s_t,h_i) = s_t^\\top h_i$   |    Luong et al. (2015) |  \n| scaled dot     | $score(s_t,h_i) = \\frac{s_t^\\top h_i}{\\sqrt{n}}$  |   Vaswani et al. (2017) |  \n| general       | $score(s_t,h_i) = s_t^\\top W_a h_i$ 단, $W_a$는 학습 가능한 가중치 행렬    | Luong et al. (2015)|  \n| concat| $score(s_t, h_i) = W_a^\\top tanh(W_b[s_t;h_i]),score(s_t,h_i) = W_a^\\top tanh(W_b s_t + W_c h_i)$| Bahdanau et al. (2015) |  \n| location-base       | $\\alpha_t = softmax(W_a s_t)$ $\\alpha_t$ 산출 시에 $s_t$만 사용하는 방법    |    Luong et al. (2015) |  \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-05-28-Attention Mechanism.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"pulse","code-copy":false,"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Attention Mechanism","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Attention Mechanism"]},"extensions":{"book":{"multiFile":true}}},"ipynb":{"identifier":{"display-name":"Jupyter","target-format":"ipynb","base-format":"ipynb"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"2023-05-28-Attention Mechanism.ipynb"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"Attention Mechanism","author":"SEOYEON CHOI","date":"2023-05-28","categories":["Attention Mechanism"]}}},"projectFormats":["html"]}