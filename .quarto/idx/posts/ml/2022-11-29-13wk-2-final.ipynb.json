{"title":"Deep Learning final example","markdown":{"yaml":{"title":"Deep Learning final example","author":"SEOYEON CHOI","date":"11/29/2022"},"headingText":"`1`. hi?hello!! (90점)","containsRefs":false,"markdown":"\n\n> 기말고사\n\n\n아래와 같은 데이터가 있다고 하자. \n\n`txt_x`와 `txt_y`를 이용하여 아래와 같은 순서로 다음문자를 예측하고 싶은 신경망을 설계하고 싶다.\n\n`h` $\\to$ `i` $\\to$ `?` $\\to$ `h` $\\to$ `e` $\\to$ `l` $\\to$ `l` $\\to$ `o` $\\to$ `!` $\\to$ `!` $\\to$ `h` $\\to$ `i` $\\to$ `?` $\\to$ `h` $\\to$ `e` $\\to$ $\\dots$\n\n***(1)-(6)*** 의 풀이에 공통적으로 필요한 과정 정리 \n\n`(1)` `torch.nn.RNN()`을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라. \n\n(풀이)\n\n`(2)` `torch.nn.RNNCell()`을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라. \n\n`(3)` `torch.nn.Module`을 상속받은 클래스를 정의하고 (2)의 결과와 동일한 적합값이 나오는 신경망을 설계한 뒤 학습하라. (초기값을 적절하게 설정할 것) \n\n- class를 이용하지 않으면 점수없음. \n- torch.nn.RNN(), torch.nn.RNNCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.RNN(), torch.nn.RNNCell()을 코드에 포함시키는 것이 가능) \n\n`(4)` `torch.nn.LSTM()`을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라. \n\n(풀이)\n\n`(5)` `torch.nn.LSTMCell()`을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라. \n\n`(6)` (5)의 결과와 동일한 적합값을 출력하는 신경망을 직접설계한 뒤 학습시켜라. (초기값을 적절하게 설정할 것) \n\n- class를 이용하지 않아도 무방함. \n- torch.nn.LSTM(), torch.nn.LSTMCell() 을 이용한 네트워크를 학습시킬시 점수 없음. (초기값을 셋팅하는 용도로는 torch.nn.LSTM(), torch.nn.LSTMCell()을 코드에 포함시키는 것이 가능) \n\n## `2`. 다음을 읽고 참 거짓을 판단하여라. (10점) \n\n`(1)` RNN은 LSTM에 비하여 장기기억에 유리하다.\n\n참\n\n`(2)` `torch.nn.Embedding(num_embeddings=2,embedding_dim=1)`와 `torch.nn.Linear(in_features=1,out_features=1)`의 학습가능한 파라메터수는 같다.\n\n참\n\n`(3)`아래와 같은 네트워크를 고려하자.\n\n```python\nnet = torch.nn.Linear(1,1)\n```\n\n차원이 (n,1) 인 임의의 텐서에 대하여 net(x)와 net.forword(x)의 출력결과는 같다.\n\n참\n\n`(4)` 아래와 같이 a,b,c,d 가 반복되는 문자열이 반복되는 자료에서 다음문자열을 맞추는 과업을 수행하기 위해서는 반드시 순환신경망의 형태로 설계해야만 한다\n\na,b,c,d,a,b,c,d,...\n\n거짓\n\n`(5)` RNN 혹은 LSTM 으로 신경망을 설계할 시 손실함수는 항상 torch.nn.CrossEntropyLoss 를 사용해야 한다.\n\n거짓\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-11-29-13wk-2-final.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"Deep Learning final example","author":"SEOYEON CHOI","date":"11/29/2022"},"extensions":{"book":{"multiFile":true}}}}}