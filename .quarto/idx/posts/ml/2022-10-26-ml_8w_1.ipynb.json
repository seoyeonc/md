{"title":"CNN (8주차) 1","markdown":{"yaml":{"title":"CNN (8주차) 1","author":"SEOYEON CHOI","date":"2022-10-26","categories":["Special Topics in Machine Learning","이미지자료분석","CNN 다중클래스 분류","fastai metric"]},"headingText":"imports","containsRefs":false,"markdown":"\n\n기계학습 특강 (8주차) 10월26일--(1) [이미지자료분석 - CNN 다중클래스 분류, fastai metric 사용]\n\n\n## CNN 다중클래스 분류\n\n### 결론 (그냥 외우세요)\n\n`-` 2개의 class를 구분하는 문제가 아니라 $k$개의 class를 구분해야 한다면? \n\n***일반적인 개념*** \n\n- 손실함수: BCE loss $\\to$ Cross Entropy loss \n- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n- 마지막층의 활성화: sig $\\to$ softmax \n\n***파이토치 한정*** \n- **y의형태: (n,) vector + int형 // (n,k) one-hot encoded vector + float형**\n- 손실함수: torch.nn.BCEWithLogitsLoss,  $\\to$ torch.nn.CrossEntropyLoss\n- 마지막층의 선형변환: torch.nn.Linear(?,1) $\\to$ torch.nn.Linear(?,k) \n- 마지막층의 활성화: None $\\to$ None (손실함수에 이미 마지막층의 활성화가 포함) \n\n### 실습: 3개의 클래스를 구분 \n\ntraining set \n\n**다중일때 int가 아닌float으로서 y를 정의해준 모습**\n\ntest set \n\n(1) dls \n\n(2) lrnr \n\nadam기본인 learner\n\n(3) 학습\n\n지금은 epoch당 11번 도는 설정, 18623/1862 = 11.xx\n\n(4) 예측\n\n- 대체적으로 첫번째 칼럼의 숫자들이 다른칼럼보다 크다. \n\n- 대체적으로 두번째 칼럼의 숫자들이 다른칼럼보다 크다. \n\n- 대체적으로 세번째 칼럼의 숫자들이 다른칼럼보다 크다. \n\n`-` 예측하는방법? \n- 칼럼0의 숫자가 크다 -> y=0일 확률이 큼\n- 칼럼1의 숫자가 크다 -> y=1일 확률이 큼 \n- 칼럼2의 숫자가 크다 -> y=2일 확률이 큼\n\n### 공부: Softmax \n\n`-` 눈치: softmax를 쓰기 직전의 숫자들은 (n,k)꼴로 되어있음. 각 observation 마다 k개의 숫자가 있는데, 그중에서 유난히 큰 하나의 숫자가 있음. \n\n`-` torch.nn.Softmax() 손계산 \n\n(예시1) -- 잘못계산 \n\n(예시2) -- 이게 맞게 계산되는 것임 \n\n(예시3) -- 차원을 명시안하면 맞게 계산해주고 경고 줌 \n\n(예시4) -- 진짜 손계산 \n\n위에서 1은 축방향을 의미\n\n### 공부: CrossEntropyLoss\n\n#### `#` ***torch.nn.CrossEntropyLoss() 손계산: one-hot version***\n\n**위에서 꼭 1.0 곱해줌으로써 int가 아닌 float으로 만들어주기**\n\n`-` 계산결과 \n\n`-` 계산하는 방법도 중요한데 torch.nn.CrossEntropyLoss() 에는 **softmax 활성화함수가 이미 포함**되어 있다는 것을 확인하는 것이 더 중요함. \n\n`-` 따라서 torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다. \n\n#### `#` ***torch.nn.CrossEntropyLoss() 손계산: lenght $n$ vertor version***\n\n원핫인코딩 안하면 int로 만든 다음에 넣기, float은 또 계산되지 않음!\n\n### 실습: $k=2$로 두면 이진분류도 가능\n\n`-` download data \n\ntraining \n\n**float**만들어주기 원핫인코딩이기\n\ntest\n\n(1) dls \n\n(2) lrnr \n\n(3) 학습\n\n(4) 예측 및 시각화 \n\n`-` note: softmax(u1,u2)=[sig(u1-u2), sig(u2-u1)]=[1-sig(u2-u1),sig(u2-u1)]\n\n$\\frac{1}{e^{u_1}+e^{u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+e^{u_2-u_2}} \\to \\frac{e^{u_1-u_2}}{e^{u_1-u_2}+1} \\to sig(u_2-u_1)$\n\n### 공부: 이진분류에서 소프트맥스 vs 시그모이드 \n\n`-` 이진분류문제 = \"y=0 or y=1\" 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제 \n\n`-` softmax, sigmoid\n- softmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다. \n- sigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다. \n\n`-` 그런데 \"실패확률=1-성공확률\" 이므로 사실상 둘은 같은걸 추정하는 셈이다. (성공확률만 추정하면 실패확률은 저절로 추정되니까) \n\n`-` 아래는 사실상 같은 모형이다. \n\n`-` 둘은 사실상 같은 효과를 주는 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다. $\\to$ sigmoid를 사용하는 모형이 비용은 싸고(학습할 파라메터가 적음) 효과는 동일하다는 말 $\\to$ 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함. \n- softmax가 갑자기 너무 안좋아보이는데 sigmoid는 k개의 클래스로 확장이 불가능한 반면 softmax는 확장이 용이하다는 장점이 있음 \n\n### 소프트맥스 vs 시그모이드 정리 \n\n`-` 결론 \n1. 소프트맥스는 시그모이드의 확장이다. \n2. 클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다. \n\n\n`-` 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (흑백이미지를 칼라잉크로 출력하는 느낌) \n\n***참고***\n\n|$y$|분포가정|마지막층의 활성화함수|손실함수|\n|:--:|:--:|:--:|:--:|\n|3.45, 4.43, ... (연속형) |정규분포|None (or Identity)|MSE|\n|0 or 1|이항분포 with $n=1$ (=베르누이) |Sigmoid| BCE|\n|[0,0,1], [0,1,0], [1,0,0]| 다항분포 with $n=1$|Softmax| Cross Entropy |\n\n## fastai metric 사용 \n\n### 데이터준비\n\n`-` download data \n\n`-` training set \n\n`-` test set \n\n### 사용자정의 메트릭이용 \n\n(1) dls 만들기\n\n(2) lrnr 생성 \n\n(3) 학습\n\n(4) 예측\n\n- 생략\n\n### fastai지원 메트릭이용-- 잘못된사용 \n\n(1) dls 만들기\n\n(2) lrnr 생성 \n\n(3) 학습\n\n- 이상하다..? \n\n(4) 예측 \n\n- 맞추는건 잘 맞추는데? \n\n### fastai지원 메트릭이용-- 올바른 사용(1)\n\n`-` 가정\n- X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. \n- y의 형태는 (n,) 벡터이다. 즉 $n\\times 1$ 이 아니라 그냥 길이가 $n$인 벡터로 가정한다. \n- y의 각 원소는 0,1,2,3,... 와 같이 카테고리를 의미하는 숫자이어야 하며 이 숫자는 int형으로 저장되어야 한다. \n- loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\n(1) dls 만들기\n\n지원하는 함수로 바꿔주기\n\n(2) lrnr 생성 \n\n(3) 학습\n\n### fastai지원 메트릭이용-- 올바른 사용(2)\n\n`-` 가정 \n- X의 형태는 (n,채널,픽셀,픽셀)로 가정한다. \n- y의 형태는 (n,클래스의수)로 가정한다. 즉 y가 one_hot 인코딩된 형태로 가정한다. \n- y의 각 원소는 0 혹은 1이다. \n- loss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\n(1) dls 만들기\n\n(2) lrnr 생성 \n\n`accuracy_multi` \n\n(3) 학습\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-10-26-ml_8w_1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"CNN (8주차) 1","author":"SEOYEON CHOI","date":"2022-10-26","categories":["Special Topics in Machine Learning","이미지자료분석","CNN 다중클래스 분류","fastai metric"]},"extensions":{"book":{"multiFile":true}}}}}