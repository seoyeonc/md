{"title":"RNN (11주차)","markdown":{"yaml":{"title":"RNN (11주차)","author":"SEOYEON CHOI","date":"2022-11-21","categories":["Special Topics in Machine Learning","순환신경망"]},"headingText":"import","containsRefs":false,"markdown":"\n\n기계학습 특강 (11주차) 11월16일 [순환신경망– abc예제, abdc예제, abcde예제, AbAcAd예제]\n\n\n## Define some funtions\n\n## Exam4: AbAcAd (2)\n\n### data\n\n`-` 기존의 정리방식\n\n### 순환신경망 구현1 (손으로 직접구현) -- 리뷰\n\n`(1)` 숙성담당 네트워크\n\n`(2)` 조리담당 네트워크\n\n`(3)` 손실함수, 옵티마이저 설계 \n\n`(4)` 학습 (6분정도 걸림)\n\n`(5)` 시각화 \n\n### 순환신경망 구현2 (with RNNCell, hidden node 2)\n\nref: <https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html>\n\n#### 구현1과 같은 초기값 (확인용) \n\n`(1)` 숙성네트워크\n\n- input_size = 4\n- hindden_size = 2\n\n`rNNCell()` 는 사실 `torch.nn.RNNCell()`와 같은 동작을 하도록 설계를 하였음. \n\n같은동작을 하는지 확인하기 위해서 동일한 초기상태에서 `rNNCell()`에 의하여 학습된 결과와 `torch.nn.RNNCell()`에 의하여 학습된 결과를 비교해보자.\n\n_rnncell 초기값을 rnncell에 넣어주기\n\n`(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저 \n\n`(4)` 학습\n\n`(5)` 시각화 \n\n뒷부분갈수록 잘 맞음\n\n#### 새로운 초기값\n\n`(1)` 숙성네트워크\n\n앞 부분은 잘 맞지 않고 뒷부분은 잘 맞을 것!\n\n`(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저 \n\n`(4)` 학습\n\n`(5)` 시각화 \n\nx -> h -> o -> yhat(softmax(o))\n\n네트워크만 학습된 상태, 따라서 hiddenlayer를 재구성해줘야 한다.\n\n### 순환신경망 구현3 (with RNN, hidden node 2) -- 성공 \n\n(예비학습)\n\n`-` 네트워크학습이후 yhat을 구하려면 번거로웠음 \n\n```Python\nhidden = torch.zeros(T,2) \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\nyhat = soft(cook(hidden))\n```\n\n`-` 이렇게 하면 쉽게(?) 구할 수 있음 \n\n`-` rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다 \n\n***(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)***\n\n---\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자. \n\n`(1)` 숙성네트워크\n\n선언\n\n가중치초기화\n\n`(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저\n\n`(4)` 학습\n\n결과는 같을 것\n\n`(5)` 시각화1: yhat\n\n- 처음은 좀 틀렸음 ㅎㅎ\n\n- 뒤에는 잘맞음 \n\n**실전팁: `_water` 대신에 `hT`를 대입 (사실 큰 차이는 없음)**\n\nhT는 이미 값이 저장되어 있잖아\n\n`(6)` 시각화2: hidden, yhat\n\n- 히든노드의 해석이 어려움. \n\nhidden layer->layer 더 있음 좋겠다\n\n### 순환신경망 구현4 (with RNN, hidden node 3) -- 성공 \n\n`(1)` 숙성네트워크~ `(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저\n\n`(4)` 학습\n\n`(5)` 시각화1: yhat\n\n`(6)` 시각화2: hidden, yhat\n\n- 세번째 히든노드 = 대소문자(a/A)를 구분\n- 1,2 히든노드 = bcd를 구분 \n\n## GPU 실험\n\n### 20000 len + 20 hidden nodes\n\n***cpu***\n\n왕 느려\n\n***gpu***\n\n- 왜 빠른지? \n\n### 20000 len + 20 hidden nodes + 역전파주석처리 \n\n***cpu***\n\nloss 미분할 때 시간 많이 잡음ㅁ\n\n***gpu***\n\n### 2000 len + 20 hidden nodes \n\n***cpu***\n\n***gpu***\n\n### 2000 len + 20 hidden nodes + 역전파주석처리\n\n***cpu***\n\n***gpu***\n\n### 2000 len + 5000 hidden nodes \n\n***cpu***\n\n***gpu***\n\n### 2000 len + 5000 hidden nodes + 역전파주석처리\n\n***cpu***\n\n***gpu***\n\n### 실험결과 요약 \n\n|len |# of hidden nodes | backward |cpu | gpu| ratio| \n|:-:|:-:|:-:|:-:|:-:|:-:|\n|20000 | 20| O | 93.02 | 3.26 | 28.53 |\n|20000 | 20| X | 18.85 | 1.29 | 14.61 | \n|2000 | 20| O | 6.53 | 0.75 | 8.70 | \n|2000 | 20| X | 1.25 | 0.14 | 8.93 |\n|2000 | 1000| O | 58.99 | 4.75 | 12.41 | \n|2000 | 1000| X | 13.16 | 2.29 | 5.74 |\n\n## Exam5: abcabC\n\n### data \n\n### RNN \n\n- bc\n- bC\n- b의 수준이 2개 \n- abc\n- amC\n- 문맥 고려해서 $\\to$ hiddenlayer = 3\n\n`-` 3000 epochs\n\n- 어차피 시각화하려면 cpu에 있어야해\n- 나중 기억!\n\n`-` 6000 epochs\n\n- 3: a일 확률\n- 4: b일 확률\n- 5: c일 확률\n- 6: C일 확률\n\n`-` 9000 epochs\n\n`-` 12000 epochs\n\n`-` 15000 epochs\n\n- 15,000번 정도 하니 c와 C를 구분하는 모습\n- hidden layer(0,1,2)의 색 순서에 따라 문맥상 다른 것을 알 수 있고 학습도 되는 모습을 볼 수 있다.\n\n### LSTM \n\n`-` LSTM \n\n`-` 3000 epochs\n\n- 하얀부분이 0 파란 부분이 -1 빨간 부분이 +1\n\n`-` 6000 epochs\n\n- rnn에 비해 lstm은 조금 돌려도 어느정도 비교 잘 해낸다\n\n### RNN vs LSTM 성능비교실험\n\n`-` RNN \n\n`-` LSTM\n\n- lstm이 rnn보다 이런 상황에서는 더 잘 학습해낸다.\n- linear 의 hiddenlayer로 구분되어 있다.\n\n## Exam6: abcdabcD\n\n### data\n\n### RNN vs LSTM 성능비교실험\n\n`-` RNN \n\n`-` LSTM\n\n`-` 관찰1: LSTM이 확실히 장기기억에 강하다. \n\n`-` 관찰2: LSTM은 hidden에 0이 잘 나온다. \n\n- 사실 확실히 구분되는 특징을 판별할때는 -1,1 로 히든레이어 값들이 설정되면 명확하다. \n- 히든레이어에 -1~1사이의 값이 나온다면 애매한 판단이 내려지게 된다. \n- 그런데 이 애매한 판단이 어떻게 보면 문맥의 뉘앙스를 이해하는데 더 잘 맞다.\n- 그런데 RNN은 -1,1로 셋팅된 상황에서 -1~1로의 변화가 더디다는 것이 문제임. \n\n## LSTM의 계산과정\n\n### data: abaB\n\n- ab\n- aB\n- 로서 a의 수준이 2개로 나뉨 $\\to$ hidden node = 2\n\n### 1 epoch ver1 (with torch.nn.LSTMCell)\n\n- 데이터 적으니까 cpu로 할 것임\n\n- hidden node가 많고 len 이 클수록 GPU가 효율이 좋다\n\n### 1 epoch ver2 (완전 손으로 구현) \n\n#### ***t=0 $\\to$ t=1***\n\n`-` lstm_cell 을 이용한 계산 (결과비교용)\n\n- 이런결과를 어떻게 만드는걸까? \n- <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n\n$i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})$\n\n$f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})$\n\n$g_t = \\tanh (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg})$\n\n$o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{hg})$\n\n$o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})$\n\n$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$\n\n$h_t = o_t \\odot \\tanh (c_t)$\n\n$\\sigma = \\text{ Sigmoid }$\n\n$x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ  \\text{ }\\underrightarrow{sig} \\text{ }i_t$\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t$\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ }\\star  \\text{ } \\underrightarrow{tanh} \\text{ }g_t$\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ } \\triangleleft  \\text{ }\\underrightarrow{sig} \\text{ }o_t$ \n\n$x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t$\n\n- 위에 나온 W가 어떻게 계산되나\n\nweight_ih_l[k] – the learnable input-hidden weights of the $\\text{k}^{th}$ layer $(W_ii|W_if|W_ig|W_io)$, of shape (4*hidden_size, input_size) for $k = 0$. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size). If proj_size > 0 was specified, the shape will be (4*hidden_size, num_directions * proj_size) for $k > 0$\n\nweight_hh_l[k] – the learnable hidden-hidden weights of the $\\text{k}^{th}$layer $(W_hi|W_hf|W_hg|W_ho)$, of shape (4*hidden_size, hidden_size). If proj_size > 0 was specified, the shape will be (4*hidden_size, proj_size).\n\n`-` 직접계산 \n\n- $o_t$ = output_gate\n- $f_t$ = forget_gate\n- $i_t$ = input_gate\n\n$\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft$각 두 개씩\n\n#### ***t=0 $\\to$ t=T***\n\n### 1 epoch ver3 (with torch.nn.LSTM)\n\n- 초기화된 가중치값들로 덮어씌우기\n\n## LSTM은 왜 강한가? \n\n### data: abaB\n\n### 1000 epoch \n\n### 시각화 \n\n- 변수를 담을 빈 셋 설정\n\n- 상단그림은 게이트의 값들만 시각화, 하단그림은 게이트 이외의 값들을 시각화 \n\n### 시각화의 해석I \n\n`-` input_gate, forget_gate, output_gate는 모두 0~1 사이의 값을 가진다. \n\n`-` 이 값들은 각각 모두 ${\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}, \\tanh({\\boldsymbol c}_t)$에 곱해진다. 따라서 input_gate, forget_gate, output_gate 는 gate의 역할로 비유가능하다. (1이면 통과, 0이면 차단)\n\n- input_gate: ${\\boldsymbol g}_t$의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정 \n- forget_gate: ${\\boldsymbol c}_{t-1}$의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정 \n- output_gate: $\\tanh({\\boldsymbol c}_t)$의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정 \n\n(서연 필기)\n\n- 값들이 0과 1사이의 값을 가진다\n- 파 -1 흰 0 빨 1\n- 0 곱하면 어떤 값이든 0이 되니까 차단한다 표현\n\n### 시각화의 해석II\n\n`-` 결국 ${\\boldsymbol g}_t\\to {\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\to \\hat{\\boldsymbol y}$ 의 느낌이다. (${\\boldsymbol h}_t$를 계산하기 위해서는 ${\\boldsymbol c}_t$가 필요했고 ${\\boldsymbol c}_t$를 계산하기 위해서는 ${\\boldsymbol c}_{t-1}$과 ${\\boldsymbol g}_t$가 필요했음) \n\n- ${\\boldsymbol h}_t= \\tanh({\\boldsymbol c}_t) \\odot {\\boldsymbol o}_t$\n- ${\\boldsymbol c}_t ={\\boldsymbol c}_{t-1} \\odot {\\boldsymbol f}_t + {\\boldsymbol g}_{t} \\odot {\\boldsymbol i}_t$\n\n`-` ${\\boldsymbol g}_t,{\\boldsymbol c}_t,{\\boldsymbol h}_t$ 모두 ${\\boldsymbol x}_t$의 정보를 숙성시켜 가지고 있는 느낌이 든다. \n\n`-` ${\\boldsymbol g}_t$ 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어 있다. (마치 RNN의 hidden node처럼!) \n\n- $\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})$\n\n`-` ${\\boldsymbol c}_t$ 특징: ${\\boldsymbol g}_t$와 매우 비슷하지만 약간 다른값을 가진다. 그래서 ${\\boldsymbol g}_t$와는 달리 -1,1 이외의 값도 종종 등장. \n\n`-` ${\\boldsymbol h}_t$ 특징: (1) ${\\boldsymbol c}_t$의 느낌이 있음 하지만 약간의 변형이 있음. (2) -1~1 사이에의 값을 훨씬 다양하게 가진다. (tanh때문)\n\n(서연 필기)\n\n- comparison of g,c part \n    - 보니까 빨간 색은 1에 가까운 값, 파란색은 -1에 가까운 값들을 띄었다.\n    - 그리고 연한 빨간색인 부분은 0.3592로 낮았고, g부분과 c부분이 열별로 보았을 때 달랐다\n\n(서연 필기)\n\n- comparison of c,h part \n    - h는 c와 무관해보이지 않는다.\n    - 단지 어떤 변형이 있는 것 같다.\n\n`-` 예전의문 해결\n\n- 실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n- 그 이유: RRN은 ${\\boldsymbol h}_t$의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 ${\\boldsymbol h}_t$이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음. \n- 왜 LSTM의 ${\\boldsymbol h}_t$은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문 \n\n### LSTM의 알고리즘 리뷰 I (수식위주)\n\n**(step1)** calculate ${\\tt ifgo}$\n\n${\\tt ifgo} = {\\boldsymbol x}_t  \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1}  \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias$\n\n$=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg}  ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias$\n\n참고: 위의 수식은 아래코드에 해당하는 부분\n\n```Python\nifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n```\n\n**(step2)** decompose ${\\tt ifgo}$ and get ${\\boldsymbol i}_t$, ${\\boldsymbol f}_t$, ${\\boldsymbol g}_t$, ${\\boldsymbol o}_t$\n\n${\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )$ \n\n${\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )$ \n\n${\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )$ \n\n${\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )$ \n\n**(step3)** calculate ${\\boldsymbol c}_t$ and ${\\boldsymbol h}_t$\n\n${\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}$\n\n${\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)$\n\n### LSTM의 알고리즘 리뷰 II (느낌위주)\n\n\n\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ } \\triangleleft  \\text{ }\\underrightarrow{sig} \\text{ }o_t$ \n\n$x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ \\text{ , } \\square \\text{, } \\star \\text{, } \\triangleleft \\text{ } \\to \\sigma(circ) \\text{, }\\sigma(\\square) \\text{ , }\\tanh(\\star)\\text{ , } \\sigma(\\triangleleft) \\sim i_t, f_t, g_t, o_t$\n\n- 이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n\n`-` 느낌1: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은 콩물에서 간장을 3차로 나누어 숙성하는 느낌이다. \n\n- 콩물: ${\\boldsymbol x}_t$\n- 1차숙성: ${\\boldsymbol g}_t$\n- 2차숙성: ${\\boldsymbol c}_t$ \n- 3차숙성: ${\\boldsymbol h}_t$ \n\n`-` 느낌2: ${\\boldsymbol g}_t$에 대하여\n\n- 계산방법: ${\\boldsymbol x}_t$와 ${\\boldsymbol h}_{t-1}$를 ${\\bf W}_{ig}, {\\bf W}_{hg}$를 이용해 선형결합하고 $\\tanh$를 취한 결과\n- RNN에서 간장을 만들던 그 수식에서 $h_t$를 $g_t$로 바꾼것 \n- 크게 2가지의 의미를 가진다 (1) 과거와 현재의 결합 (2) 활성화함수 $\\tanh$를 적용 \n\n(서연 필기)\n\n$x_t, h_{t-1} \\underrightarrow{lin} \\text{ }\\circ  \\text{ }\\underrightarrow{sig} \\text{ }i_t$\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ }\\square \\text{ } \\underrightarrow{sig} \\text{ }f_t$\n\n$x_t, h_{t-1} \\underrightarrow{lin}  \\text{ }\\star  \\text{ } \\underrightarrow{tanh} \\text{ }g_t$\n\n를 풀어서 쓰면\n\n$\\tanh(x_t W_{ig} + h_{t-1} W_{hg} + bias)$\n\nRNN: $h_t = \\tanh(x_t W + h_{t-1} W + bias)$\n\nLSTM: $g_t = \\tanh(x_t W + h_{t-1} W + bias)$\n- 과거$h_{t-1}$와 현재$x_t$의 결합\n\n`-` 느낌3: ${\\boldsymbol c}_t$에 대하여 (1)\n\n- 계산방법: ${\\boldsymbol g}_{t}$와 ${\\boldsymbol c}_{t-1}$를 요소별로 선택하고 더하는 과정 \n- $g_t$는 (1) 과거와 현재의 결합 (2) 활성화함수 tanh를 적용으로 나누어지는데 이중에서 (1) 과거와 현재의 정보를 결합하는 과정만 해당한다. 차이점은 요소별 선택 후 덧셈\n    - $g_t$는 선형 결합\n- 이러한 결합을 쓰는 이유? 게이트를 이용하여 과거와 현재의 정보를 제어 (일반적인 설명, 솔직히 내가 좋아하는 설명은 아님)\n\n(서연 필기)\n\n$c_t = g_t \\odot Input + c_{t-1} \\odot Forget$\n\n`-` 느낌4: ${\\boldsymbol c}_t$에 대하여 (2) // ${\\boldsymbol c}_t$는 왜 과거와 현재의 정보를 제어한다고 볼 수 있는가? \n\n$t=1$ 시점 계산과정관찰\n\n$[0.9,1.0] \\odot {\\boldsymbol g}_t + [1.0,0.0] \\odot {\\boldsymbol c}_{t-1}$\n\n(서연 필기)\n\n여기서 곱은 element별 곱\n- $[0.9,1.0] \\odot g_t = [0.9,1.0]\\odot [g_1,g_2] = [0.9g_1(현재)_,1.0g_2(과거)]$\n- 여기서 0이 현재에 곱해지면 현재를 기억하지 않고 과거에 0이 곱해지면 과거를 기억하지 않도록 조정할 수 있음\n\n$\\star$ gate없으면 조정 못 하나??$\\to$ no, weigjht로도 조정할 수 있지 않을까?\n\n- forget_gate는 $c_{t-1}$의 첫번째 원소는 기억하고, 두번째 원소는 잊으라고 말하고 있음 // forget_gate는 과거($c_{t-1}$)의 정보를 얼마나 잊을지 (= 얼마나 기억할지) 를 결정한다고 해석할 수 있다. \n- input_gate는 $g_{t}$의 첫번째 원소와 두번째 원소를 모두 기억하되 두번째 원소를 좀 더 중요하게 기억하라고 말하고 있음 // input_gate는 현재($g_{t}$)의 정보를 얼만큼 강하게 반영할지 결정한다. \n- 이 둘을 조합하면 ${\\boldsymbol c}_t$가 현재와 과거의 정보중 어떠한 정보를 더 중시하면서 기억할지 결정한다고 볼 수 있다. \n\n> 이 설명은 제가 좀 싫어해요, 싫어하는 이유는 (1) \"기억의 정도를 조절한다\"와 \"망각의 정도를 조절한다\"는 사실 같은말임. 그래서 forget_gate의 용어가 모호함. (2) 기억과 망각을 조정하는 방식으로 꼭 gate의 개념을 사용해야 하는건 아님\n\n`-` 느낌5: ${\\boldsymbol c}_t$에 대하여 (3) \n\n- 사실상 LSTM 알고리즘의 꽃이라 할 수 있음. \n- LSTM은 long short term memory의 약자임. 기존의 RNN은 장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘 활용함. \n- LSTM이 장기기억을 잘 활용하는 비법은 바로 ${\\boldsymbol c}_t$에 있다.\n\n(서연필기) $c_t$로 과거, 현재 기억 조절할 수 있기 때문에\n\n`-` 느낌6: ${\\boldsymbol h}_t$에 대하여\n- 계산방법: $\\tanh({\\boldsymbol c}_t)$를 요소별로 선택 \n\n데이터 다 가져와서 선택하는 방식\n\n`-` RNN, LSTM의 변수들 비교 테이블 \n\n||과거정보|현재정보|과거와 현재의 결합방식|활성화|느낌|비고|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|RNN-${\\boldsymbol h}_t$|${\\boldsymbol h}_{t-1}$|${\\boldsymbol x}_t$|$\\times$ W $\\to$ $+$|$\\tanh$|간장|\n|||||||\n|LSTM-${\\boldsymbol g}_t$|${\\boldsymbol h}_{t-1}$|${\\boldsymbol x}_t$|$\\times$W $\\to$ $+$|$\\tanh$|1차간장|\n|LSTM-${\\boldsymbol c}_t$|${\\boldsymbol c}_{t-1}$|${\\boldsymbol g}_t$|$\\odot$ W$\\to$ $+$ |None|2차간장|gate를 열림정도를 판단할때 ${\\boldsymbol x}_t$와 ${\\boldsymbol h}_{t-1}$을 이용|\n|LSTM-${\\boldsymbol h}_t$|None|${\\boldsymbol c}_t$|None|$\\tanh$, $\\odot$|3차간장|gate를 열림정도를 판단할때 ${\\boldsymbol x}_t$와 ${\\boldsymbol h}_{t-1}$을 이용|\n\n\n- RNN은 기억할 과거정보가 ${\\boldsymbol h}_{t-1}$ 하나이지만 LSTM은 ${\\boldsymbol c}_{t-1}$, ${\\boldsymbol h}_{t-1}$ 2개이다. \n\n`-` 알고리즘리뷰 : \n\n- 콩물$x_t$,과거3차간장$h_{t-1}$ $\\overset{\\times,+,\\tanh}{\\longrightarrow}$ 현재1차간장$g_t$\n- 현재1차간장$c_{t-1}$, 과거2차간장 $\\overset{\\odot,+,\\tanh}{\\longrightarrow}$ 현재2차간장\n- 현재2차간장$c_t$ $\\overset{\\tanh,\\odot}{\\longrightarrow}$ 현재3차간장$h_t$\n\n### LSTM이 강한이유 \n\n`-` LSTM이 장기기억에 유리함. 그 이유는 input, forget, output gate 들이 과거기억을 위한 역할을 하기 때문. \n\n- 비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭 LSTM같은 구조일 필요는 없음. (왜 3차간장을 만들때 tanh를 써야하는지? 게이트는 꼭3개이어야 하는지?)\n\n`-` 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다. \n\n- 실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n- 그 이유: RRN은 ${\\boldsymbol h}_t$의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 ${\\boldsymbol h}_t$이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음. \n- 왜 LSTM의 ${\\boldsymbol h}_t$은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문 \n\n문잭적으로 이해 ->유리하다 칭함\n\n## 참고자료들 \n\n- <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n- <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n- <https://arxiv.org/abs/1402.1128>\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-11-21-ml-11w.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"RNN (11주차)","author":"SEOYEON CHOI","date":"2022-11-21","categories":["Special Topics in Machine Learning","순환신경망"]},"extensions":{"book":{"multiFile":true}}}}}