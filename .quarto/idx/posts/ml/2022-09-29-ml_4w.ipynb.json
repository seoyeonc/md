{"title":"DNN (4주차)","markdown":{"yaml":{"title":"DNN (4주차)","author":"SEOYEON CHOI","date":"2022-09-29","categories":["Special Topics in Machine Learning","회귀분석","로지스틱"]},"headingText":"imports","containsRefs":false,"markdown":"\n\n기계학습 특강 (4주차) 9월28일 [회귀분석(2)--step1~4, step1의 다른표현, step4의 다른표현, 로지스틱 intro]\n\n\n## numpy, torch (선택학습)\n\n### numpy, torch는 엄청 비슷해요 \n\n`-` torch.tensor() = np.array() 처럼 생각해도 무방 \n\n`-` 소수점의 정밀도에서 차이가 있음 (torch가 좀 더 쪼잔함) \n\n(서연필기)tensor는 gpu에 저장하기 때문에 메모리 아끼기 위해 정밀도가 낮은 경향이 있다.\n\n`-` 기본적인 numpy 문법은 np 대신에 torch를 써도 무방 // 완전 같지는 않음 \n\n### length $n$ vector, $n \\times 1$ col-vector, $1 \\times n$ row-vector \n\n브로드캐스팅 길이가 3인 벡터와 1인벡터를 더하면 오류 뜨지 않고 더해줌\n\n`-` 길이가 3인 벡터 선언방법 \n\n`-` 3x1 col-vec 선언방법 \n\n(방법1)\n\n(방법2)\n\n`-` 1x3 row-vec 선언방법 \n\n(방법1)\n\n(방법2)\n\n`-` 3x1 col-vec 선언방법, 1x3 row-vec 선언방법에서 `[[1],[2],[3]]` 혹은 `[[1,2,3]]` 와 같은 표현이 이해안되면 아래링크로 가셔서 \n\nhttps://guebin.github.io/STBDA2022/2022/03/14/(2주차)-3월14일.html\n\n첫번째 동영상 12:15 - 22:45 에 해당하는 분량을 학습하시길 바랍니다. \n\n### torch의 dtype\n\n`-` 기본적으로 torch는 소수점으로 저장되면 dtype=torch.float32 가 된다. (이걸로 맞추는게 편리함) \n\n`-` 정수로 선언하더라도 dtype를 torch.float32로 바꾸는게 유리함 \n\n(안 좋은 선언예시) \n\n(좋은 선언예시1) \n\n(좋은 선언예시2) \n\n(사실 int로 선언해도 나중에 float으로 바꾸면 큰 문제없음) \n\n`-` 왜 정수만으로 torch.tensor를 만들때에도 torch.float32로 바꾸는게 유리할까? $\\to$ torch.tensor끼리의 연산에서 문제가 될 수 있음 \n\n별 문제 없을수도 있지만 \n\n아래와 같이 에러가 날수도 있다 \n\n(에러1)\n\n(에러2)\n\n(해결1) 둘다 정수로 통일 \n\n(해결2) 둘다 소수로 통일 <-- 더 좋은 방법임\n\n### shape of vector\n\n`-` 행렬곱셈에 대한 shape 조심\n\n`-` A@b1: 계산불가, b1@A: 계산가능\n\n`-` A@b2: 계산가능, b2@A: 계산불가 \n\n`-` A@b3: 계산가능, b3@A: 계산가능 \n\n- 뒤에 놓으면 b3를 컬럼벡터로 인식\n- 앞에 놓으면 b3를 로우벡터로 인식\n\n`-` 브로드캐스팅 \n\n계산이 되지 않아야 맞지 않나\n\n잘못 계싼할 수 있으니 dimension 명시해주자\n\n## Review: step1~4 \n\n(서연필기)\n\nfloat64 숫자 정밀 저장\n\nfloat32이면 `dtype=torch.float64)`꼬리표가 붙지 않음\n```python\n_trt = torch.tensor(df.x).float()\n_trt = torch.tensor(df.x,dtype=float30)\n```\n같은 역할, 메모리 적게 쓰기 위해 타입 바꿔주자\n\n```python\nx= torch.tensor(df.x,dtype=torch.float32).reshape(100,1)\n```\n컬럼형식으로 받아주기 위해 변경\n\n```pyhon\ntorch.ones([100,1])\ntorch.tensor([[1]*100,x]).T\n```\n같은 셋\n\n```python\nrequires_grad=True \n```\nreshape 미분 가능 옵션 주기 전에 shape 정해주자\n\n### ver1: loss = sum of squares error\n\n- note: 왜 What = What - alpha*What.grad 는 안되는지?\n\nWhat과 What.data는 달라요, requires_grad=True 미분 가능 꼬리표가 붙지 않기 때문!\n\n### ver2: loss = mean squared error = MSE \n\n(서연필기)mean 정의\n- 데이터를 더 효율적으로 학습 가능, 데이터 수만큼 안 해도 돼, 계산 덜 해도 돼\n\n## step1의 다른버전 -- net 설계만\n\n### ver1: net = torch.nn.Linear(1,1,bias=True) \n\ninput 잡는 법\n- x의 컬럼 부분을 input이라고 생각하자\n\noutput 잡는 법\n- y의 컬럼 부분을 output이라고 생각하자\n\n출력결과 같음을 확인\n\n`-` net에서 $\\hat{w}_0, \\hat{w}_1$ 의 값은? \n\n`-` 수식표현: $\\hat{y}_i = \\hat{w}_0 + \\hat{w}_1 x_i = \\hat{b} + \\hat{w}x_i =  -0.8470 + -0.3467 x_i$ for all $i=1,2,\\dots,100$. \n\n### ver2: net = torch.nn.Linear(2,1,bias=False) \n\n`-` 입력이 x가 아닌 X를 넣고 싶다면? (보통 잘 안하긴 해요, 왜? bias=False로 주는게 귀찮거든요)\n- X는 바이어스가 고려된 상황 \n\n위에 $w_0,w_1$ 순\n\nbias 없음을 확인\n\n`-` 수식표현: $\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix}  \\begin{bmatrix} -0.2451 \\\\ -0.5989  \\end{bmatrix}$\n\n### 잘못된사용1\n\n```python\nnet(_x.reshape(100,1))\n```\n과 같이 정의\n\n### 잘못된사용2\n\n- 수식표현: $\\hat{\\bf y} = {\\bf X} {\\bf \\hat W} + \\hat{b}= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix}  \\begin{bmatrix} -0.2451 \\\\ -0.5989  \\end{bmatrix} + 0.2549$\n\n## step1의 다른버전 -- 끝까지 \n\n### ver1: net = torch.nn.Linear(1,1,bias=True) \n\n`-` 준비\n\n`-` step1 \n\n`-` step2\n\n`-` step3\n\n(미분전)\n\n(미분)\n\n(미분후)\n\n`-` step4 \n\n(업데이트전) \n\n(업데이트) \n\n(업데이트후) \n\n`-` 반복 \n\n### ver2: net = torch.nn.Linear(2,1,bias=False) \n\n`-` 준비\n\n`-` step1\n\n`-` step2\n\n`-` step3\n\n(미분전) \n\n(미분)\n\n(미분후)\n\n`-` step4\n\n(업데이트전) \n\n(업데이트)\n\n(업데이트후)\n\n`-` 반복\n\n## step4의 다른버전: 옵티마이저!\n\n### ver1: net = torch.nn.Linear(1,1,bias=True) \n\n`-` 준비 \n\nStocastic Gradiant Decscent\n\n`-` step1~3\n\n`-` step4 \n\n(update 전)\n\n(update) \n\n(update 후) \n\n`-` 반복 \n\n### ver2: net = torch.nn.Linear(2,1,bias=False) \n\n`-` 바로 반복하겠습니다.. \n\n--- \n\n## Appendix: `net.parameters()`의 의미? (선택학습)\n\n`-` iterator, generator의 개념필요 \n- https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고 \n\n\n`-` 탐구시작: 네트워크 생성 \n\n`-` torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음 \n\n```\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n```\n\n`-` 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 `__iter__`를 가지고 있는 오브젝트를 의미)\n\n`-` 무슨의미? \n\n`-` 이건 이런느낌인데? \n\n`-` 즉 아래는 같은코드이다. \n\n```python\n### 코드1\n_generator = net.parameters() \ntorch.optim.SGD(_generator,lr=1/10) \n### 코드2\n_generator = iter([net.weight,net.bias])\ntorch.optim.SGD(_generator,lr=1/10) \n### 코드3 (이렇게 써도 코드2가 실행된다고 이해할 수 있음)\n_iterator = [net.weight,net.bias]\ntorch.optim.SGD(_iterator,lr=1/10) \n```\n\n결론: `net.parameters()`는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트(iterable object)로 만드는 함수라 이해할 수 있다. \n\n`-` 응용예제1\n\n`-` 응용예제2\n\n## Logistic regression \n\n### motive \n\n`-` 현실에서 이런 경우가 많음 \n- $x$가 커질수록 (혹은 작아질수록) 성공확률이 증가함. \n\n`-` (X,y)는 어떤모양? \n\n`-` (예비학습) 시그모이드라는 함수가 있음 \n\n베르누이 특정 확률로 0 또는 1 뽑기\n\n### model\n\n`-` $x$가 커질수록 $y=1$이 잘나오는 모형은 아래와 같이 설계할 수 있음 <--- 외우세요!!!\n\n- $y_i \\sim Ber(\\pi_i),\\quad$ where $\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}$\n\n- $\\hat{y}_i= \\hat{pi}_\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+\\exp(-\\hat{w}_0-\\hat{w}_1x_i)}$ \n\n- $loss= - \\sum_{i=1}^{n} \\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)$ <--- 외우세요!!\n\n$y_i=1$ $\\hat{y_i} = 1$loss가 0 근처 $\\hat{y_i} = 0$ loss가- 무한대\n\n$y_i = 0$ $\\hat{y_i} = 0$loss가 0근처 $\\hat{y_i} = 1$ loss가 1\n\n### toy example\n\n`-` 예제시작 \n\n- 우리의 목적: $x$가 들어가면 빨간선 $\\hat{y}$의 값을 만들어주는 mapping을 학습해보자. \n\n---\n\n## 숙제 \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-09-29-ml_4w.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"DNN (4주차)","author":"SEOYEON CHOI","date":"2022-09-29","categories":["Special Topics in Machine Learning","회귀분석","로지스틱"]},"extensions":{"book":{"multiFile":true}}}}}