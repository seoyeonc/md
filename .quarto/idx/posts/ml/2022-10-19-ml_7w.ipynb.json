{"title":"CNN (7주차)","markdown":{"yaml":{"title":"CNN (7주차)","author":"SEOYEON CHOI","date":"2022-10-19","categories":["Special Topics in Machine Learning","드랍아웃","이미지자료분석"]},"headingText":"imports","containsRefs":false,"markdown":"\n\n기계학습 특강 (7주차) 10월19일 [딥러닝의 기초 - 드랍아웃, 이미지자료분석]\n\n\n## 깊은신경망-- 오버피팅 \n\n### 데이터\n\n`-` model: $y_i = (0\\times x_i) + \\epsilon_i$\n\n### 모든 데이터를 사용하여 적합 (512, relu, 1000 epochs) \n\n### 전체데이터를 8:2로 나누어서 8만을 학습\n\n`-` 데이터를 8:2로 나눈다\n\n`-` (xtr,ytr) 만 가지고 net를 학습시킨다. \n\n(서연 필기) 오차항이 너무 잘 따라가면 영향을 미칠 수 있다.\n\n데이터에 비해 노드 수가 많으면 오버피팅의 가능성\n- 한 변수로 모든 변수 맞추는 우연을 마주한다면?\n- 모델에 비해 feature가 너무 클때?\n    - 위를 예로 들면 input은 1이었는데 output은 512렸다\n\n**차원의 저주**\n\n## 깊은신경망-- 드랍아웃 \n\n### 오버피팅의 해결 \n\n`-` 오버피팅의 해결책: 드랍아웃 \n\n동등한 초기값에서 시작한다고 설명 \n- manual_seed 정해준거\n\n계속 바뀌는 plot\n\n`-` 올바른 사용법 \n\nevaliation method 사용\n\n### 드랍아웃 레이어 \n\n- 90%의 드랍아웃: 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 0으로 만든다. + 그리고 0이 되지않고 살아남은 값들은 10배 만큼 값이 커진다. \n\n`-` 드랍아웃레이어 정리 \n- 구조: 입력 -> 드랍아웃레이어 -> 출력 \n- 역할: (1) 입력의 일부를 임의로 0으로 만드는 역할 (2) 0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정 \n- 효과: 오버피팅을 억제하는 효과가 있음 (왜??) \n    - 추측일뿐!\n- 의미: each iteration (each epoch x) 마다 학습에 참여하는 노드가 로테이션으로 랜덤으로 결정됨. \n- 느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전반적으로 개선됨 \n\n(서연 필기) 지배적인 예측 값들보다 비지배적인 예측값을 건들려고 하면 의미가 없음.\n\n## 이미지자료분석-- data \n\n`-` download data \n\n`-` training set \n\n`-` test set \n\n## 이미지자료분석-- CNN 예비학습\n\n### 기존의 MLP 모형 \n\n`-` 교재의 모형 \n\n`-` 왜 28 by 28 이미지를 784개의 벡터로 만든 다음에 모형을 돌려야 하는가? \n\n`-` 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어서 이미지자료를 분석하는 느낌 \n\n`-` observation의 차원은 $784$가 아니라 $1\\times (28\\times 28)$이 되어야 맞다. \n\n### 새로운 아키텍처의 제시\n\n`-` 예전\n\n$\\underset{(n,784)}{\\bf X} \\overset{l_1}{\\to} \\underset{(n,30)}{\\boldsymbol u^{(1)}} \\overset{relu}{\\to} \\underset{(n,30)}{\\boldsymbol v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\boldsymbol u^{(2)}} \\overset{sig}{\\to} \\underset{(n,1)}{\\boldsymbol v^{(2)}}=\\underset{(n,1)}{\\hat{\\boldsymbol y}}$  \n\n- $l_1$: 선형변환, feature를 뻥튀기하는 역할 \n    - $\\sim$ 꺾인 선이 많아진다\n- $relu$: 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화 \n- $l_2$: 선형변환, 뻥튀기된 feature를 요약 하는 역할 (=데이터를 요약하는 역할) \n\n`-` 새로운 아키텍처 \n- $conv$: feature를 뻥튀기하는 역할 (2d ver $l_1$ 느낌) \n- $relu$: \n- $pooling$: 데이터를 요약하는 역할 \n\n### CONV 레이어 (선형변환의 2D 버전) \n\n`-` 우선 연산하는 방법만 살펴보자. \n\n**(예시1)**\n\n**(예시2) 잘하면 평균도 계산하겠다?**\n\n**(예시3) 이동평균?**\n\n**(예시4) window size가 증가한다면? (2d의 이동평균느낌)**\n\n(3,3)이나~ 3이나~\n\n**(예시5) 피처뻥튀기**\n\n결국 아래를 계산한다는 의미\n\n***(잔소리) axis 사용 익숙하지 않으면 아래 꼭 들으세요..***\n\n- https://guebin.github.io/IP2022/2022/04/11/(6주차)-4월11일.html , numpy공부 4단계: 축\n\n### ReLU (2d)\n\n### Maxpooling 레이어\n\n가장 중요한 특징만 남게 될 것이다.\n\n버려지는 데이터\n\n## 이미지자료분석-- CNN 구현 (CPU)\n\n### (1) Conv2d\n\n### (2) ReLU\n\n### (3) MaxPool2D\n\n### (4) 적당히 마무리하고 시그모이드 태우자 \n\n`-` 펼치자. \n\n(방법1)\n\n(방법2)\n\n`-` 2304 $\\to$ 1 로 차원축소하는 선형레이어를 설계\n\n`-` 시그모이드\n\n`-` 네트워크 설계\n\n## 이미지자료분석-- CNN 구현 (GPU)\n\n### 1. dls\n\n### 2. lrnr 생성: 아키텍처, 손실함수, 옵티마이저 \n\n### 3. 학습\n\n### 4. 예측 및 시각화 \n\n`-` 결과를 시각화하면 아래와 같다. \n\n1/10만 사용했는데 잘 training된 것 같다\n\n`-` 빠르고 적합결과도 좋음 \n\n### Lrnr 오브젝트\n\n같은 결과\n\n20221026 수업\n\n### BCEWithLogitsLoss\n\n`-` BCEWithLogitsLoss = Sigmoid + BCELoss\n- 왜 써요? 수치적으로 더 안정 \n\ntorch.nn.BCEWithLogitsLoss\n- This loss combines a `Sigmoid` layer and the `BCELoss` in one single\nclass. This version is more numerically stable than using a plain `Sigmoid`\nfollowed by a `BCELoss` as, by combining the operations into one layer,\nwe take advantage of the log-sum-exp trick for **numerical stability**.\n\n\n`-` 사용방법\n\n(1) dls 만들기 \n\n(2) lrnr생성 \n\n(3) 학습\n\n(4) 예측 및 시각화\n\n시각화 위해서 cpu로 옮겨주기\n\nsigmoid 취하기 전이지 우리는 bcewithlogiticsLoss 썼잖아, 그래서 0~1사이 아님\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-10-19-ml_7w.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"CNN (7주차)","author":"SEOYEON CHOI","date":"2022-10-19","categories":["Special Topics in Machine Learning","드랍아웃","이미지자료분석"]},"extensions":{"book":{"multiFile":true}}}}}