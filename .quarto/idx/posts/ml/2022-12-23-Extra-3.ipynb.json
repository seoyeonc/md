{"title":"Extra-3: 딥러닝의 기초 (5)","markdown":{"yaml":{"title":"Extra-3: 딥러닝의 기초 (5)","author":"SEOYEON CHOI","date":"2022-12-21","categories":["딥러닝의 기초"]},"headingText":"import","containsRefs":false,"markdown":"\n\n> 벡터미분, 역전파와 기울기 소멸 \n\n기울기 소멸: loss 를 W로 미분했더니 그 값이 거의 0ㅇ이 나오는 현상 $\\to$ update가 거의 이루어지지 않음\n\n이유\n- W -> loss인 함수는 W에 어떤한 선형변환 $\\to$ 비선형변환 $\\to$ 선형변환 $\\to$ 비선형변환 $\\to \\dots$ $\\to$ loss 의 과정으로 해석 가능\n- 즉 loss는 W의 합성의 합섭ㅇ의... 합성함수로 해석가능\n- loss 를 W로 미분한 값은 각 변환단ㅇ계에서 정의되는 함수의 도함수를 모두 곱한 것과 같음(체인룰)\n- 체인 중에서도 하나라도 0이 나오면 곱한 값은 0 이다. \n    - 체인이 길수록 하나라도 0이 나오는 경우가 많음\n    \n왜 깊은 신경망일수록 기울기 소멸이 빈번한가?\n- 체인이 길기 때문에.\n\n왜 순환신경망일수록 기울기 소멸이 빈번할까>?\n- 체인이 길기 때문에.\n\n\n# 벡터미분 \n\n`-` 벡터미분에 대한 강의노트: \n\n- <https://github.com/guebin/STML2022/blob/main/posts/II.%20DNN/supp.pdf>\n\n`-` 요약: 회귀분석에서 손실함수에 대한 미분은 아래와 같은 과정으로 계산할 수 있다. \n\n- $loss = ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\bf W} - {\\bf W}^\\top {\\bf X}^\\top {\\bf y} + {\\bf W}^\\top {\\bf X}^\\top {\\bf X} {\\bf W}$\n\n- $\\frac{\\partial }{\\partial {\\bf W}}loss = -2{\\bf X}^\\top {\\bf y} +2 {\\bf X}^\\top {\\bf X} {\\bf W}$\n\n---\n\n`-` 모형의 매트릭스화 \n\n우리의 모형은 아래와 같다. \n\n$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10$ \n\n풀어서 쓰면 \n\n$\\begin{cases}\ny_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ \ny_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \n\\dots \\\\ \ny_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \n\\end{cases}$\n\n아래와 같이 쓸 수 있다.\n\n$\\begin{bmatrix} \ny_1 \\\\ \ny_2 \\\\ \n\\dots \\\\\ny_{10} \n\\end{bmatrix} \n= \\begin{bmatrix} \n1 & x_1 \\\\ \n1 & x_2 \\\\ \n\\dots & \\dots \\\\\n1 & x_{10} \n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \n\\epsilon_1 \\\\ \n\\epsilon_2 \\\\ \n\\dots \\\\\n\\epsilon_{10} \n\\end{bmatrix}$\n\n벡터와 매트릭스 형태로 정리하면 \n\n${\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}$ \n\n`-` 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$\n\n이것을 벡터표현으로 하면 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})$\n\n풀어보면 \n\n$loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$\n\n`-` 미분하는 과정의 매트릭스화 \n\nloss를 최소화하는 ${\\boldsymbol \\beta}$를 구해야하므로 loss를 ${\\boldsymbol \\beta}$로 미분한 식을 0이라고 놓고 풀면 된다. \n\n$\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$ \n\n$= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta}$\n\n따라서 $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0$을 풀면 아래와 같다. \n\n$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}$\n\n`-` 공식도 매트릭스로 표현하면: $\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}$ <-- 외우세요 \n\n---\n\n**벡터미분 / 매트릭스 미분**\n\n(1) 정의 1: 벡터로 미분\n\n$\\frac{\\partial}{\\partial y} = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix}$ , $y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n\n(2) 정의 2: 매트릭스로 미분\n\n$\\frac{\\partial}{\\partial \\bf{X} } := \\begin{bmatrix} \\frac{\\partial}{\\partial x_{11} } & \\dots & \\frac{\\partial}{\\partial x_{1p} } \\\\ \\vdots &  & \\vdots \\\\ \\frac{\\partial}{\\partial x_{n1} }  & \\dots & \\frac{\\partial}{\\partial x_{np} } \\end{bmatrix}, \\bf{X} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np}  \\end{bmatrix}$\n\n`1` \n\n$\\frac{\\partial}{\\partial x}(x^\\top y) = y$, 단 $x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n\npf. $\\bf{x}^\\top \\bf{y} = \\begin{bmatrix} x_1 & \\dots x_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1 y_1 + x_2 y_2 + \\dots  + x_n y_n$\n\n$\\frac{\\partial}{\\partial x} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix}$ 이므로\n\n$\\big( \\frac{\\partial}{\\partial x} \\big) x^\\top y = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} \\end{bmatrix} (x_1 y_1 + x_2 y_2 + \\dots + x_n y_n)$\n\n$= \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}(x_1y_1 + \\dots + x_n y_n) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}(x_1y_1 + \\dots + x_n y_n) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 y_1 + \\frac{\\partial}{\\partial x_1}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_1}x_n y_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n} x_1 y_1 + \\frac{\\partial}{\\partial x_n}x_2 y_2 + \\dots + \\frac{\\partial}{\\partial x_n}x_n y_n  \\end{bmatrix}$\n\n$= \\begin{bmatrix} y_1 + 0 + \\dots + 0 \\\\ \\vdots \\\\ 0+0+ \\dots + y_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{y}$\n\n`2`  $\\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$ 임을 보이는 다른 풀이\n\npf. $\\frac{\\partial}{\\partial x} \\big(\\bf{x}^\\top \\bf{y} \\big) = \\big( \\frac{\\partial}{\\partial x} x^\\top \\big) y = \\bf{I} \\bf{y} = \\bf{y}$\n\n$\\frac{\\partial}{\\partial x} x^\\top = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_n}\\end{bmatrix} \\begin{bmatrix} x_1 & \\dots & x_n \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} x_1 & \\dots & \\frac{\\partial}{\\partial x_n} x_n \\\\ \\vdots & & \\\\ \\frac{\\partial}{\\partial x_n} x_1 & \\dots &\\frac{\\partial}{\\partial x_n} x_n \\end{bmatrix} = \\bf{I}$\n\n`2` $\\frac{\\partial}{\\partial x}(\\bf{y}^\\top \\bf{x} ) = \\bf{y}$\n\npf. $\\frac{\\partial}{\\partial x}(\\bf{y}^\\top\\bf{x}) = \\frac{\\partial}{\\partial x}(\\bf{x}^\\top \\bf{y}) = \\bf{y}$\n\n`3` $\\frac{\\partial}{\\partial B}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{x}^\\top \\bf{y}$\n\n단, $\\bf{B} := p \\times 1$ vector, $\\bf{X} := n \\times p$ matrix, $\\bf{y} := n \\times 1$ vector\n\npf. $\\bf{y}^\\top \\bf{X} \\bf{B}$는 스칼라($1 \\times 1$)이므로 $\\bf{y}^\\top\\bf{X}\\bf{B} = (\\bf{y}^\\top \\bf{X} \\bf{B})^\\top = \\bf{B}^\\top \\bf{X}^\\top \\bf{y}$\n\n$\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\frac{\\partial}{\\partial \\bf{B}}(\\bf{B}^\\top \\bf{X}^\\top \\bf{y}) = \\big( \\frac{\\partial}{\\partial \\bf{B}} \\bf{B}^\\top \\big) \\bf{X}^\\top \\bf{y} = \\bf{X}^\\top \\bf{y}$\n\n`(다른 풀이)` $\\frac{\\partial}{\\partial \\bf{B}}(\\bf{y}^\\top \\bf{X} \\bf{B}) = \\bf{X}^\\top \\bf{y}$임을 보이는 다른 풀이\n\n$\\bf{XB} = \\begin{bmatrix} x_{11} & \\dots & x_{1p} \\\\ \\vdots & & \\vdots \\\\ x_{n1} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} = \\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n\n$\\bf{y}^\\top \\bf{XB} = \\begin{bmatrix} y_1 & \\dots y_n \\end{bmatrix}\\begin{bmatrix} x_{11}B_1 + \\dots + x_{1p} B_p \\\\ x_{21}B_1 + \\dots + x_{2p}B_p \\\\ \\vdots \\\\ x_{n1} B_1 + \\dots + x_{np} B_p \\end{bmatrix}$\n\n$= y_1(x_{11}B_1 + \\dots + x_{1p}B_p) + y_2(x_{21}B_1 + \\dots + x_{2p}B_p ) + \\dots + y_n(x_{n1}B_1 + \\dots + x_{np} B_p) = A_1 + A_2 + \\dots A_n$\n\n* note: $A_1, A_2, \\dots$은 모두 스칼라($1 \\times 1$)\n\n$\\big( \\frac{\\partial}{\\partial B} \\big)(\\bf{y}^\\top \\bf{XB}) = \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} \\\\ \\vdots \\\\\\frac{\\partial}{\\partial B_p} \\end{bmatrix}(A_1 + A_2 + \\dots A_n)$\n\n$= \\begin{bmatrix}\\frac{\\partial}{\\partial B_1} A_1 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_1 \\end{bmatrix} + \\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_2 \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_2 \\end{bmatrix}+ \\dots +\\begin{bmatrix} \\frac{\\partial}{\\partial B_1} A_n \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial B_p} A_n \\end{bmatrix}$\n\n* $\\frac{\\partial}{\\partial B_1}A_1 = \\frac{\\partial}{\\partial B_1}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = y_1 x_{11} + 0 + \\dots + 0$\n\n* $\\frac{\\partial}{\\partial B_2}A_2 = \\frac{\\partial}{\\partial B_2}y_1(x_{11}B_1 + \\dots + x_{1p}B_p) = 0 + y_1 x_{12} + 0 + \\dots + 0$\n\n$= \\begin{bmatrix} y_1 x_{11} \\\\ y_1 x_{12} \\\\ \\vdots \\\\ y_1 x_{1p}\\end{bmatrix} + \\begin{bmatrix} y_2 x_{21} \\\\ y_2 x_{22} \\\\ \\vdots \\\\ y_2 x_{2p} \\end{bmatrix}  + \\dots + \\begin{bmatrix} y_n x_{n1} \\\\ y_n x_{n2} \\\\ \\vdots \\\\ y_n x_{np} \\end{bmatrix}$\n\n$= \\begin{bmatrix} y_1 x_{11} + y_2 x_{21} + \\dots + y_n x_{n1} \\\\ y_1 x_{12} + y_2 x_{22} + \\dots + y_n x_{n2} \\\\ \\vdots \\\\ y_1 x_{1p} + y_2 x_{2p} + \\dots + y_n x_{np} \\end{bmatrix} = \\begin{bmatrix} x_{11} & x_{21} & \\dots & x_{n1} \\\\ x_{12} & x_{22} & \\dots & x_{n2} \\\\ \\vdots & \\vdots & & \\vdots \\\\ x_{1p} & x_{2p} & \\dots & x_{np} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\bf{X}^\\top \\bf{y}$\n\n`4` $\\frac{\\partial}{\\partial y} \\bf{y}^\\top \\bf{y} = 2\\bf{y}$\n \npf. $\\bf{y}^\\top \\bf{y} = \\begin{bmatrix} y_1 & \\dots & y_n \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = y_{1}^{2} + \\dots + y_{n}^{2}$\n\n$\\therefore \\frac{\\partial}{\\partial y}(\\bf{y}^\\top y) = \\frac{\\partial}{\\partial y}(y_{1}^{2} + \\dots + y_{n}^{2}) = \\begin{bmatrix} \\frac{\\partial}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n} \\end{bmatrix} (y_{1}^{2} + \\dots + y_{n}^{2})$\n\n$= \\begin{bmatrix} \\frac{\\partial}{\\partial y_1}(y_{1}^{2} + \\dots + y_{n}^{2}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial y_n}(y_{1}^{2} + \\dots + y_{n}^{2}) \\end{bmatrix} = \\begin{bmatrix}2y_1 + 0 + \\dots + 0 \\\\ 0 + 2y_2 + \\dots + 0 \\\\ \\vdots \\\\ 0 + 0 + \\dots + 2y_n \\end{bmatrix} = 2\\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = 2\\bf{y}$\n\n(틀린 풀이) $\\frac{\\partial}{\\partial y}(\\bf{y}^\\top \\bf{y}) {\\color{orange}=} \\big( \\frac{\\partial}{\\partial y} \\bf{y}^\\top \\big) \\bf{y}$ 미분하는 값에 대해 상수일때만 가능\n\n그런데 $\\big(\\frac{\\partial}{\\partial y}\\bf{y}^\\top \\big) = \\bf{I}$이므로, $\\frac{\\partial}{\\partial y} (\\bf{y}^\\top \\bf{y}) = \\bf{I} \\bf{y} = \\bf{y}$\n\n(틀린 풀이의 스칼라 버전) $\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} y \\big) y = 1 \\times y = \\bf{y}$\n\n(올바른 풀이) : $\\frac{d}{dy} y^2 = \\big( \\frac{d}{dy} \\big) {\\color{red}y} {\\color{blue}y} = \\big( \\frac{d}{dy} {\\color{red}y} \\big) {\\color{blue}y} + {\\color{red}y} \\big( \\frac{d}{dy} {\\color{blue}y} \\big)$\n\nNote: 곱의 미분 : 함수 $f(x), g(x)$ 가 $x$에 대하여 미분 가능하면 $\\{ {\\color{red}f(x)} \\times {\\color{blue}g(x)} \\}' = {\\color{red}f(x)}'g{\\color{blue}(x)} + {\\color{red}f(x)}{\\color{blue}g(x)}'$\n\n* 여기서는 ${\\color{red}y}$가 $f(x)$, ${\\color{blue}y}$가 $g(x)$\n\n다시 벡터로 돌아오자\n\n(올바른 풀이) $\\frac{\\partial}{\\partial y}({\\color{red}y}^\\top {\\color{blue}y}) = A + B = \\bf{I}{\\color{red}y} + \\bf{I}{\\color{blue}y} = 2\\bf{y}$\n\n* $A :=$ 빨간 $y$만 변수로 보고 미분\n* $B :=$ 파란 $y$만 변수로 보고 미분\n\n$A = \\big( \\frac{\\partial}{\\partial y} {\\color{red}y}^\\top \\big) {\\color{blue}y} = \\bf{I} {\\color{blue}y}$\n\n$B = \\big( \\frac{\\partial}{\\partial y} \\big) ( {\\color{red}y}^\\top {\\color{blue}y} )$ 스칼라라 순서 변경 가능$:= \\big( \\frac{\\partial}{\\partial y} \\big) ({\\color{blue}y}^\\top {\\color{red}y} \\big) = ( \\frac{\\partial}{\\partial y} {\\color{blue}y}^\\top \\big) {\\color{red}y} = \\bf{I} {\\color{red}y}$\n\n`5` $\\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{X} \\bf{B} = 2\\bf{X}^\\top \\bf{XB}$\n\npf. $\\frac{\\partial}{\\partial B}({\\color{red}B}^\\top{\\color{red}X}^\\top {\\color{blue}X} {\\color{blue}B}) = A + B = \\bf{I}\\bf{X}^\\top \\bf{XB} + \\bf{I}\\bf{X}^\\top\\bf{XB} = 2\\bf{X}^\\top \\bf{XB}$\n\n$A = \\big( \\frac{\\partial}{\\partial B} {\\color{red}B}^\\top\\big) \\bf{X}^\\top \\bf{X} {\\color{blue}B} = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$\n\n$B = \\big( \\frac{\\partial}{\\partial B} \\big) ( {\\color{red}B}^\\top \\bf{X}^\\top \\bf{X} {\\color{blue}B}) = \\big(\\frac{\\partial}{\\partial B} \\big) ({\\color{blue}B}^\\top \\bf{X}^\\top \\bf{X}{\\color{red}B}) = \\bf{I} \\bf{X}^\\top \\bf{X} {\\color{red}B}$\n\n---\n\n$loss = (\\bf{y} - \\bf{XB})^\\top(\\bf{y} - \\bf{XB}) = \\bf{y}\\bf{y}^\\top - \\bf{y}^\\top\\bf{XB} - \\bf{B}^\\top\\bf{X}^\\top \\bf{y} + \\bf{B}^\\top \\bf{X}^\\top \\bf{XB}$\n\n$\\frac{\\partial}{\\partial B} loss = 0 - \\frac{\\partial}{\\partial B} \\bf{y}^\\top \\bf{XB} - \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{y} + \\frac{\\partial}{\\partial B} \\bf{B}^\\top \\bf{X}^\\top \\bf{XB} = 0 - \\bf{X}^\\top \\bf{y} - \\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB} = - 2\\bf{X}^\\top \\bf{y} + 2\\bf{X}^\\top \\bf{XB}$\n\n$\\therefore \\frac{\\partial}{\\partial B} loss = 0 \\leftrightarrow 2\\bf{X}^\\top \\bf{y} = 2\\bf{X}^\\top \\bf{XB}$\n\n$\\therefore \\hat{\\bf{B}} = (\\bf{X}^\\top \\bf{X})^{-1}\\bf{X}^\\top \\bf{y}$\n\n---\n\n# 체인룰 \n\n`-` 체인룰: 어려운 하나의 미분을 손쉬운 여러개의 미분으로 나누는 기법\n\n`-` 손실함수가 사실 아래와 같은 변환을 거쳐서 계산되었다고 볼 수 있다. \n\n- ${\\bf X} \\to {\\bf X}{\\bf W} \\to {\\bf y} -{\\bf X}{\\bf W} \\to ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n\n`-` 위의 과정을 수식으로 정리해보면 아래와 같다. \n\n- ${\\bf u}={\\bf X}{\\bf W}$, $\\quad {\\bf u}: n \\times 1$ \n\n- ${\\bf v} = {\\bf y}- {\\bf u},$  $\\quad {\\bf v}: n \\times 1$\n\n- $loss={\\bf v}^\\top {\\bf v},$ $\\quad loss: 1 \\times 1$\n\n`-` 손실함수에 대한 미분은 아래와 같다. \n\n$$\\frac{\\partial }{\\partial {\\bf W}} loss = \\frac{\\partial }{\\partial {\\bf W}} {\\bf v}^\\top {\\bf v}$$ \n\n(그런데 이걸 어떻게 계산함?)\n\n`-` 계산할 수 있는것들의 모음.. \n\n- $\\frac{\\partial}{\\partial {\\bf v}} loss = 2{\\bf v}$ $\\quad \\to$ (n,1) 벡터 \n\n- $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top = -{\\bf I}$ $\\quad \\to$ (n,n) 매트릭스 \n\n- $\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top = {\\bf X}^\\top$ $\\quad \\to$ (p,n) 매트릭스 \n\n`-` 혹시.. 아래와 같이 쓸 수 있을까?  \n\n$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) = \n\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n\\frac{\\partial loss}{\\partial \\bf v}$$ \n\n- 가능할것 같다. 뭐 기호야 정의하기 나름이니까!\n\n`-` 그렇다면 혹시 아래와 같이 쓸 수 있을까? \n\n$$\\frac{\\partial {\\bf u}^\\top}{\\partial \\bf W}\n\\frac{\\partial {\\bf v}^\\top}{\\partial \\bf u}\n\\frac{\\partial loss}{\\partial \\bf v} = \\frac{\\partial loss }{\\partial\\bf W}=\\frac{\\partial }{\\partial \\bf W} loss$$\n\n- 이건 선을 넘는 것임.\n- 그런데 어떠한 공식에 의해서 가능함. 그 공식 이름이 체인룰이다. \n\n`-` 결국 정리하면 아래의 꼴이 되었다. \n\n$$\\left(\\frac{\\partial }{\\partial \\bf W}{\\bf u}^\\top \\right) \n\\left(\\frac{\\partial }{\\partial \\bf u}{\\bf v}^\\top \\right) \n\\left(\\frac{\\partial }{\\partial \\bf v}loss \\right) \n=\n\\frac{\\partial }{\\partial \\bf W}loss$$ \n\n`-` 그렇다면? \n\n$$\\left({\\bf X}^\\top  \\right) \n\\left(-{\\bf I} \\right) \n\\left(2{\\bf v}\\right) \n= \n\\frac{\\partial }{\\partial \\bf W}loss$$ \n\n그런데, ${\\bf v}={\\bf y}-{\\bf u}={\\bf y} -{\\bf X}{\\bf W}$ 이므로 \n\n$$-2{\\bf X}^\\top\\left({\\bf y}-{\\bf X}{\\bf W}\\right) \n= \n\\frac{\\partial }{\\partial \\bf W}loss$$ \n\n정리하면 \n\n$$\\frac{\\partial }{\\partial \\bf W}loss = -2{\\bf X}^\\top{\\bf y}+2{\\bf X}^\\top {\\bf X}{\\bf W}$$ \n\n## 예시: 2021 빅데이터분석 중간고사 문제 2-(b)\n\n`-` 미분계수를 계산하는 문제였음.. \n\n- <https://guebin.github.io/BDA2021/2021/11/09/mid.html>\n\n`-` 체인룰을 이용하여 미분계수를 계산하여 보자.\n\n`-` $\\frac{\\partial}{\\partial\\bf W}loss$ 의 계산 \n\n$\\frac{\\partial }{\\partial \\bf W}loss = \\left({\\bf X}^\\top  \\right) \n\\left(-{\\bf I} \\right) \n\\left(2{\\bf v}\\right)$\n\n`-` 참고로 중간고사 답은 \n\n입니다. \n\n`-` 확인 \n\n`-` $\\frac{\\partial}{\\partial \\bf v} loss= 2{\\bf v}$ 임을 확인하라. \n\n`-` $\\frac{\\partial }{\\partial {\\bf u}}{\\bf v}^\\top$ 의 계산\n\n- 사실 토치에서는 스칼라아웃풋에 대해서만 미분을 계산할 수 있음 \n\n그런데 $\\frac{\\partial}{\\partial {\\bf u}}{\\bf v}^\\top=\\frac{\\partial}{\\partial {\\bf u}}(v_1,v_2,v_3,v_4,v_5)=\\big(\\frac{\\partial}{\\partial {\\bf u}}v_1,\\frac{\\partial}{\\partial {\\bf u}}v_2,\\frac{\\partial}{\\partial {\\bf u}}v_3,\\frac{\\partial}{\\partial {\\bf u}}v_4,\\frac{\\partial}{\\partial {\\bf u}}v_5\\big)$ 이므로\n\n조금 귀찮은 과정을 거친다면 아래와 같은 알고리즘으로 계산할 수 있다. \n\n(0) $\\frac{\\partial }{\\partial {\\bf u}} {\\bf v}^\\top$의 결과를 저장할 매트릭스를 만든다. 적당히 `A`라고 만들자. \n\n(1) `_u` 하나를 임시로 만든다. 그리고 $v_1$을 `_u`로 미분하고 그 결과를 `A`의 첫번째 칼럼에 기록한다. \n\n(2) `_u`를 또하나 임시로 만들고 $v_2$를 `_u`로 미분한뒤 그 결과를 `A`의 두번째 칼럼에 기록한다. \n\n(3) (1)-(2)와 같은 작업을 $v_5$까지 반복한다. \n\n***(0)을 수행***\n\n***(1)을 수행***\n\n- 이때 $v_1=g(f({\\bf u}))$와 같이 표현할 수 있다. 여기에서 $f((u_1,\\dots,u_5)^\\top)=(y_1-u_1,\\dots,y_5-u_5)^\\top$, 그리고 $g((v_1,\\dots,v_n)^\\top)=v_1$ 라고 생각한다. 즉 $f$는 벡터 뺄셈을 수행하는 함수이고, $g$는 프로젝션 함수이다. 즉 $f:\\mathbb{R}^5 \\to \\mathbb{R}^5$인 함수이고, $g:\\mathbb{R}^5 \\to \\mathbb{R}$인 함수이다. \n\n여기서 $v_1$은 꼬리표로서 selection 작성되어 있음\n\nA의 첫번째 칼럼에 이것을 넣어주세요\n\n***(2)를 수행***\n\n***(3)을 수행*** // 그냥 (1)~(2)도 새로 수행하자. \n\n- 이론적인 결과인 $-{\\bf I}$와 일치한다. \n\n`-` $\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top$의 계산 \n\n$\\frac{\\partial }{\\partial {\\bf W}}{\\bf u}^\\top = \\frac{\\partial }{\\partial {\\bf W}}(u_1,\\dots,u_5)=\\big(\\frac{\\partial }{\\partial {\\bf W}}u_1,\\dots,\\frac{\\partial }{\\partial {\\bf W}}u_5 \\big)$\n\n- 이론적인 결과와 일치\n\n## 잠깐 생각해보자.. \n\n`-` 결국 위의 예제에 한정하여 임의의 ${\\bf \\hat{W}}$에 대한 $\\frac{\\partial}{\\partial {\\bf \\hat W}}loss$는 아래와 같이 계산할 수 있다. \n\n- (단계1) $2{\\bf v}$를 계산하고 \n- (단계2) (단계1)의 결과 앞에 $-{\\bf I}$를 곱하고 \n- (단계3) (단계2)의 결과 앞에 ${\\bf X}^\\top$를 곱한다. \n\n`-` step1에서 ${\\bf v}$는 어떻게 알지? \n\n- X $\\to$ u=X@W $\\to$ v = y-u \n\n- 그런데 이것은 우리가 loss를 구하기 위해서 이미 계산해야 하는것 아니었나? \n- step1: yhat, step2: loss, step3: derivate, step4: update \n\n`-` **(중요)** step2에서 loss만 구해서 저장할 생각 하지말고 중간과정을 다 저장해라. (그중에 v와 같이 필요한것이 있을테니까) 그리고 그걸 적당한 방법을 통하여 이용하여 보자. \n\n### backprogation 알고리즘 모티브\n\n`-` 아래와 같이 함수의 변환을 아키텍처로 이해하자. (함수의입력=레이어의입력, 함수의출력=레이어의출력) \n\n- ${\\bf X} \\overset{l1}{\\to} {\\bf X}{\\bf W} \\overset{l2}{\\to} {\\bf y} -{\\bf X}{\\bf W} \\overset{l3}{\\to} ({\\bf y}-{\\bf X}{\\bf W})^\\top ({\\bf y}-{\\bf X}{\\bf W})$\n\n`-` 그런데 위의 계산과정을 아래와 같이 요약할 수도 있다. (${\\bf X} \\to {\\bf \\hat y} \\to loss$가 아니라 ${\\bf W} \\to loss({\\bf W})$로 생각해보세요)\n\n- ${\\bf W} \\overset{l1}{\\to} {\\bf u} \\overset{l2}{\\to} {\\bf v} \\overset{l3}{\\to} loss$\n\n`-` 그렇다면 아래와 같은 사실을 관찰할 수 있다. \n\n- (단계1) $2{\\bf v}$는 function of ${\\bf v}$이고, ${\\bf v}$는 l3의 입력 (혹은 l2의 출력) \n- (단계2) $-{\\bf I}$는 function of ${\\bf u}$이고, ${\\bf u}$는 l2의 입력 (혹은 l1의 출력) \n- (단계3) 마찬가지의 논리로 ${\\bf X}^\\top$는 function of ${\\bf W}$로 해석할 수 있다. \n\n`-` 요약: $2{\\bf v},-{\\bf I}, {\\bf X}^\\top$와 같은 핵심적인 값들이 사실 각 층의 입/출력 값들의 함수꼴로 표현가능하다. $\\to$ 각 층의 입/출력 값들을 모두 기록하면 미분계산을 유리하게 할 수 있다. \n\n- 문득의문: 각 층의 입출력값 ${\\bf v}, {\\bf u}, {\\bf W}$로 부터 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 만들어내는 방법을 모른다면 헛수고 아닌가? \n- 의문해결: 어차피 우리가 쓰는 층은 선형+(렐루, 시그모이드, ...) 정도가 전부임. 따라서 변환규칙은 미리 계산할 수 있음.\n\n`-` 결국 \n\n`(1)` 순전파를 하면서 입출력값을 모두 저장하고 \n\n`(2)` 그에 대응하는 층별 미분계수값 $2{\\bf v}, -{\\bf I}, {\\bf X}^\\top$ 를 구하고 \n\n`(3)` 층별미분계수값을 다시 곱하면 (그러니까 ${\\bf X}^\\top (-{\\bf I}) 2{\\bf v}$ 를 계산) 된다. \n\n### backpropagation\n\n`(1)` 순전파를 계산하고 각 층별 입출력 값을 기록 \n\n- yhat = net(X) \n- loss = loss_fn(yhat,y) \n\n`(2)` 역전파를 수행하여 손실함수의 미분값을 계산 \n\n- loss.backward()\n\n\n`-` 참고로 (1)에서 층별 입출력값은 GPU의 메모리에 기록된다.. 무려 GPU 메모리.. \n\n\n`-` 작동원리를 GPU의 관점에서 요약 (슬기로운 GPU 활용) \n\n***gpu특징: 큰 차원의 매트릭스 곱셈 전문가 (원리? 어마어마한 코어숫자)***\n\n- 아키텍처 설정: 모형의 파라메터값을 GPU 메모리에 올림 // `net.to(\"cuda:0\")`\n- 순전파 계산: ***중간 계산결과를 모두 GPU메모리에 저장*** (순전파 계산을 위해서라면 굳이 GPU에 있을 필요는 없으나 후에 역전파를 계산하기 위한 대비) // `net(X)` \n- 오차 및 손실함수 계산: `loss = loss_fn(yhat,y)`\n- 역전파 계산: ***순전파단계에서 저장된 계산결과를 활용***하여 손실함수의 미분값을 계산 // `loss.backward()`\n- 다음 순전파 계산: ***이전값은 삭제하고 새로운 중간계산결과를 GPU메모리에 올림*** \n- 반복. \n\n## some comments \n\n`-` 역전파기법은 체인룰 + $\\alpha$ 이다. \n- 미분 계산을 하기 위함인데 여기서 파라메터 업데이트 필요하지 \n\n`-` 오차역전파기법이라는 용어를 쓰는 사람도 있다. \n\n`-` 이미 훈련한 네트워크에 입력 $X$를 넣어 결과값만 확인하고 싶을 경우 순전파만 사용하면 되고, 이 상황에서는 좋은 GPU가 필요 없다. \n- 예) 개/고양이 확인 등\n\n# 기울기소멸 \n\n## 고요속의 외침 \n\n`-` <https://www.youtube.com/watch?v=ouitOnaDtFY>\n\n`-` 중간에 한명이라도 잘못 말한다면.. \n\n## 정의 \n\n`-` In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. \n\n## 이해\n\n`-` 당연한것 아닌가? \n\n- 그레디언트 기반의 학습 (그레디언트 기반의 옵티마이저): 손실함수의 기울기를 통하여 업데이트 하는 방식 \n- 역전파: 손실함수의 기울기를 구하는 테크닉 (체인룰 + $\\alpha$). 구체적으로는 (1) 손실함수를 여러단계로 쪼개고 (2) 각 단계의 미분값을 각각 구하고 (3) 그것들을 모두 곱하여 기울기를 계산한다. \n- 0 근처의 숫자를 계속 곱하면 터지거나 0으로 간다. (사실 안정적인 기울기가 나올 것이라고 생각하는것 자체가 이상함) \n\n0 전달되면 업데이트 안 되잖아\n\n- 기울기가 소멸함 \n\n- 기울기가 폭발함. \n\n`-` 도깨비: 기울기가 소멸하기도 하고 터지기도 한다. \n\n## [해결책](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) (기울기 소멸에 대한 해결책) \n\n`-` Multi-level hierarchy\n\n- 여러층을 쪼개서 학습하자 $\\to$ 어떻게? 사전학습, 층벼학습 \n- 기울기소실문제를 해결하여 딥러닝을 유행시킨 태초의(?) 방법임. \n- 결국 입력자료를 바꾼뒤에 학습하는 형태 \n\n`-` Gradient clipping\n\n- 너무 큰 값의 기울기는 사용하지 말자. (기울기 폭발에 대한 대비책)\n\n`-` Faster hardware\n\n- GPU를 중심으로 한 테크닉 \n- 근본적인 문제해결책은 아니라는 힌튼의 비판 \n- CPU를 쓸때보다 GPU를 쓰면 약간 더 깊은 모형을 학습할 수 있다 정도? \n\n`-` Residual Networks, LSTM \n\n- 아키텍처를 변경하는 방법\n\n`-` Other activation functions\n\n- 렐루의 개발\n\n- 렐루가 음수는 아예 0, 양수는 기울기 확실하니까\n\n`-` 배치정규화 \n\n- 어쩌다보니 되는것. \n- 배치정규화는 원래 공변량 쉬프트를 잡기 위한 방법임. 그런데 기울기 소멸에도 효과가 있음. 현재는 기울기소멸문제에 대한 해결책으로 빠짐없이 언급되고 있음. 2015년의 원래 논문에는 기울기소멸에 대한 언급은 없었음. (https://arxiv.org/pdf/1502.03167.pdf)\n- 심지어 배치정규화는 오버피팅을 잡는효과도 있음 (이것은 논문에 언급했음) \n\n`-` **기울기를 안구하면 안되나?**\n\n- 베이지안 최적화기법: (https://arxiv.org/pdf/1807.02811.pdf) $\\to$ GPU를 어떻게 쓰지? $\\to$ 느리다 \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-12-23-Extra-3.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"Extra-3: 딥러닝의 기초 (5)","author":"SEOYEON CHOI","date":"2022-12-21","categories":["딥러닝의 기초"]},"extensions":{"book":{"multiFile":true}}}}}