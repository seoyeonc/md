{"title":"RNN (10주차)","markdown":{"yaml":{"title":"RNN (10주차)","author":"SEOYEON CHOI","date":"2022-11-09","categories":["Special Topics in Machine Learning","순환신경망"]},"headingText":"import","containsRefs":false,"markdown":"\n\n기계학습 특강 (10주차) 11월9일 [순환신경망-- abc예제, abdc예제, abcde예제, AbAcAd예제]\n\n\n---\n\n선행학습\n\n`iterable object?`\n\n이게 없어\n\n이제 있음\n\n`StopIteration` iter 끝내는 옵션\n\n---\n\n## 예비학습: `net.parameters()`의 의미 \n\n9월27일 강의노트 중 \"`net.parameters()`의 의미?\"를 설명한다. \n\n`-` iterator, generator의 개념필요 \n- https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고 \n\n\n`-` 탐구시작: 네트워크 생성 \n\n`-` torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음 \n\n```\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n```\n\n`-` 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 `__iter__`를 가지고 있는 오브젝트를 의미)\n\n`-` 무슨의미? \n\n`-` 그냥 이건 이런느낌인데? \n\n결론: `net.parameters()`는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트같은 iterable object로 만드는 함수라 이해할 수 있다. \n\n```python\nyhat = net(x)\n```\n꼭 이런 식으로 정의할 필요는 없다\n\n`-` 응용예제1\n\n`-` 응용예제2\n\n### 스스로 학습 (중간고사 대체과제)\n\n아래와 같은 자료가 있다고 가정하자. \n\n아래의 모형을 가정하고 $\\alpha_0,\\alpha_1,\\beta_0,\\beta_1$을 파이토치를 이용하여 추정하고자한다.\n\n- $y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)$ \n\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라. \n\n- 3.14 근처\n\n- 6.28 근처\n\n## Define some funtions\n\n## Exam2: abc\n\n### data\n\n### 하나의 은닉노드를 이용한 풀이 -- 억지로 성공\n\n`-` 데이터정리 \n\n`-` 학습\n\n```python\nnet[0] = embedding\nnet[1] = tanh\nnet[2] = linear\n```\n\n`-` 결과해석 \n\n- a blue\n- b orange\n- c green\n\n- 억지로 맞추고있긴한데 파라메터가 부족해보인다. \n\n`-` 결과시각화1\n\n- 학습이 제대로 되었다면 0의 결과가 나오지 않지\n- net진행될때 거의 w만 곱한 결과가 나오는 것인데 0이 hidden에서 나오면 w만 곱해도 변화가 없잖아? 그래서 주황색 선이 나온 거고\n\n`-` 첫번째 그림 $\\to$ 두번빼 그림\n\n- (파랑,주황,초록) 순서로 그려짐 \n- 파랑 = hidden * (-4.6804) + (-1.5440) \n- 주황 = hidden * (0.3071) + (0.9143) \n- 초록 = hidden * (5.2894) + (-1.3970) \n\n`-` 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. \n- weight: 파랑과 초록을 구분하는 역할을 함 \n- weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 'b'라고 나올 확률을 학습시킨다. $\\to$ 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0) \n\n--- \n\n**[참고](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): torch.nn.Linear()의 비밀?**\n- 사실 ${\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}$ 꼴에서의 ${\\bf W}$와 ${\\boldsymbol b}$가 저장되는게 아니다. \n- ${\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}$ 꼴에서의 ${\\bf A}$와 ${\\boldsymbol b}$가 저장된다. \n- ${\\bf W} = {\\bf A}^T$ 인 관계에 있으므로 l1.weight 가 우리가 생각하는 ${\\bf W}$ 로 해석하려면 사실 transpose를 취해줘야 한다. \n\n**왜 이렇게..?**\n- 계산의 효율성 때문 (numpy의 구조를 알아야함)\n- ${\\boldsymbol x}$, ${\\boldsymbol y}$ 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음) \n\n***잠깐 딴소리!!***\n\n(예시1)\n\n- 아래로 한칸 = 16칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시2)\n\n- 아래로 한칸 = 16칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시3)\n\n- 아래로 한칸 = 24칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n- 아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n- 오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n**진짜 참고..**\n\n- 1바이트 = 8비트 \n- 1바이트는 2^8=256 의 정보 표현\n- np.int8은 8비트로 정수를 저장한다는 의미\n\n***딴소리 끝!!***\n\nweight의 transfose가 저장되는 이유 끝!\n\n혼자 크다고 인식하면 바꾸는...\n\n--- \n\n`-` 결과시각화2\n\n## Exam3: abcd\n\n### data\n\n### 하나의 은닉노드를 이용한 풀이 -- 억지로 성공 \n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n맞춘게 아니야. 0 근처인게 있잖아 hidden에서\n\n### 두개의 은닉노드를 이용한 풀이 -- 깔끔한 성공\n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n- hidden layer 2장만으로 4가지 state 표현\n\n## Exam4: abcde (스스로 공부)\n\n### data\n\n주어진 자료가 다음과 같다고 하자. \n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\n\n```Python\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)\n```\n\n### 3개의 은닉노드를 이용한 풀이 \n\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. \n- 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. ($2^1$)\n- 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. ($2^2$)\n- 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음. ($2^3$)\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n## Exam5: AbAcAd\n\n### data\n\n### 두개의 은닉노드를 이용한 풀이 -- 실패\n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n- A 뒤는 a,b,c 3개 중 하나, 따라서 1/3 의 확률이겠네 하고 학습해버렸다.\n- observation 마다 봐야하는 x가 달라(수가 )\n\n- 실패\n\n\n`-` 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다. \n\n|X|y|\n|:-:|:-:|\n|A,b|A|\n|b,A|c|\n|A,c|A|\n|c,A|d|\n|A,d|A|\n|d,A|b|\n|A,b|A|\n|b,A|c|\n|...|...|\n\n`-` 순진한 접근방식의 비판: \n- 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조\n- 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. \n- 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름. \n\n***이것에 대한 해결책은 순환신경망이다.***\n\n### 순환망을 위하여 data 다시정리\n\n`-` 기존의 정리방식\n\n`-` 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n\n숫자형태의 벡터형태라고 가정하고 학습하는 순환신경망\n\n### 실패했던 풀이의 재구현1\n\n`-` 방금 실패한 풀이 \n\n```Python\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n```\n\n`-` Tanh까지만 클래스로 바꾸어서 구현 \n- 클래스를 이용하는 방법: https://guebin.github.io/DL2022/2022/11/01/(9주차)-11월1일.html#로지스틱-모형을-이용한-풀이\n\n`-` for문돌릴준비\n\n`-` for문: 20회반복\n\n`-` linr(hnet(x)) 적합결과 <-- 숫자체크\n\n### 실패했던 풀이의 재구현2\n\n`-` Tanh까지 구현한 클래스\n\n`-` for문돌릴준비 \n\n`-` for문: 20회 반복\n\n`-` linr(hnet(x)) 적합결과 <-- 숫자체크\n\n- 4개가 필요한 원핫인코딩과 달리 hidden layer는 2개만으로 4state 표현했다\n\n### 순환신경망의 아이디어\n\n#### ***모티브***\n\n***(예비생각1) ${\\boldsymbol h}$에 대한 이해***\n\n${\\boldsymbol h}$는 사실 문자열 'abcd'들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) \n- (why1) h는 \"학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태\"라고 이해가능\n- (why2) 실제로 예시를 살펴보면 그러했다. \n\n결론: 사실 ${\\boldsymbol h}$는 잘 숙성되어있는 입력정보 ${\\bf X}$ 그 자체로 해석 할 수 있다. \n\n***(예비생각2) [수백년전통을 이어가는 방법](https://www.joongang.co.kr/article/24087690#home)***\n\n```\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n```\n\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬 \n\n`*` 기존방식\n- $\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}$\n\n`*` 수백년 전통의 간장맛을 유지하는 방식\n\n- $\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3$\n\n`*` 수백년 전통의 간장맛을 유지하면서 조리를 한다면? \n\n- $\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3$\n\n점점 맛있는 간장계란밥이 탄생함 \n\n`*` 알고리즘의 편의상 아래와 같이 생각해도 무방 \n\n- $\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1$, $\\text{간장}_0=\\text{맹물}$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3$\n\n***아이디어***\n\n`*` 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로? \n\n- $\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1$\n- $\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2$\n- $\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3$\n\n이제 우리가 배울것은 (1) \"$\\text{콩물}_{t}$\"와 \"$\\text{간장}_{t-1}$\"로 \"$\\text{간장}_t$\"를 `숙성`하는 방법 (2) \"$\\text{간장}_t$\"로 \"$\\text{간장계란밥}_t$를 `조리`하는 방법이다 \n\n즉 `숙성`담당 네트워크와 `조리`담당 네트워크를 각각 만들어 학습하면 된다. \n\n#### ***알고리즘***\n\n**세부적인 알고리즘 ($t=0,1,2,\\dots$에 대하여 한줄 한줄 쓴 알고리즘)**\n\n(1) $t=0$\n\n${\\boldsymbol h}_0=[[0,0]]$ <-- $\\text{간장}_0$은 맹물로 초기화\n\n(2) $t=1$ \n\n${\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})$\n- ${\\boldsymbol x}_1$: (1,4) \n- ${\\bf W}_{ih}$: (4,2) \n- ${\\boldsymbol h}_0$: (1,2) \n- ${\\bf W}_{hh}$: (2,2) \n- ${\\boldsymbol b}_{ih}$: (1,2)\n- ${\\boldsymbol b}_{hh}$: (1,2)\n\n${\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}$\n\n$\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)$\n\n(3) $t=2$ <-- 여기서부터는 $t=2$와 비슷\n\n---\n\n**좀 더 일반화된 알고리즘**\n\n***(ver1)***\n\ninit $\\boldsymbol{h}_0$ \n\nfor $t$ in $1:T$ \n\n- ${\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})$\n- ${\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}$\n- $\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)$\n    \n\n***(ver2)***\n\n```Python \ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden)) # $H_{t-1}$\n    output = linr(hidden)\n    yt_hat = soft(output)\n```    \n\n- 코드상으로는 $h_t$와 $h_{t-1}$의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n---\n\n***전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음***\n\n```Python \n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)\n\n```    \n\n### 순환신경망 구현1 -- 성공 \n\n`(1)` 숙성담당 네트워크\n\n`(2)` 조리담당 네트워크\n\n`(3)` 손실함수, 옵티마이저 설계 \n\n`(4)` 학습 (6분정도 걸림)\n\n`(5)` 시각화 \n\n- 아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n- 왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까..\n\n### 순환신경망 구현2 (with RNNCell) -- 성공 \n\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n`(1)` 숙성네트워크\n\n선언\n\n가중치초기화 (순환신경망 구현1과 동일하도록)\n\n`(2)` 조리담당 네트워크\n\n`(3)` 손실함수, 옵티마이저 설계 \n\n`(4)` 학습 (6분정도 걸림)\n\n`(5)` 시각화 \n\n### 순환신경망 구현3 (with RNN) -- 성공 \n\n(예비학습)\n\n`-` 아무리 생각해도 yhat구하려면 좀 귀찮음\n\n`-` 이렇게 하면 쉽게(?) 구할 수 있음 \n\n- 똑같음!\n\n`-` rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다 \n\n***(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)***\n\n---\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자. \n\n`(1)` 숙성네트워크\n\n선언\n\n`(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저\n\n`(4)` 학습\n\n`(5)` 시각화\n","srcMarkdownNoYaml":"\n\n기계학습 특강 (10주차) 11월9일 [순환신경망-- abc예제, abdc예제, abcde예제, AbAcAd예제]\n\n## import\n\n---\n\n선행학습\n\n`iterable object?`\n\n이게 없어\n\n이제 있음\n\n`StopIteration` iter 끝내는 옵션\n\n---\n\n## 예비학습: `net.parameters()`의 의미 \n\n9월27일 강의노트 중 \"`net.parameters()`의 의미?\"를 설명한다. \n\n`-` iterator, generator의 개념필요 \n- https://guebin.github.io/IP2022/2022/06/06/(14주차)-6월6일.html, 클래스공부 8단계 참고 \n\n\n`-` 탐구시작: 네트워크 생성 \n\n`-` torch.optim.SGD? 를 확인하면 params에 대한설명에 아래와 같이 되어있음 \n\n```\nparams (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n```\n\n`-` 설명을 읽어보면 params에 iterable object를 넣으라고 되어있음 (iterable object는 숨겨진 명령어로 `__iter__`를 가지고 있는 오브젝트를 의미)\n\n`-` 무슨의미? \n\n`-` 그냥 이건 이런느낌인데? \n\n결론: `net.parameters()`는 net오브젝트에서 학습할 파라메터를 모두 모아 리스트같은 iterable object로 만드는 함수라 이해할 수 있다. \n\n```python\nyhat = net(x)\n```\n꼭 이런 식으로 정의할 필요는 없다\n\n`-` 응용예제1\n\n`-` 응용예제2\n\n### 스스로 학습 (중간고사 대체과제)\n\n아래와 같은 자료가 있다고 가정하자. \n\n아래의 모형을 가정하고 $\\alpha_0,\\alpha_1,\\beta_0,\\beta_1$을 파이토치를 이용하여 추정하고자한다.\n\n- $y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)$ \n\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라. \n\n- 3.14 근처\n\n- 6.28 근처\n\n## Define some funtions\n\n## Exam2: abc\n\n### data\n\n### 하나의 은닉노드를 이용한 풀이 -- 억지로 성공\n\n`-` 데이터정리 \n\n`-` 학습\n\n```python\nnet[0] = embedding\nnet[1] = tanh\nnet[2] = linear\n```\n\n`-` 결과해석 \n\n- a blue\n- b orange\n- c green\n\n- 억지로 맞추고있긴한데 파라메터가 부족해보인다. \n\n`-` 결과시각화1\n\n- 학습이 제대로 되었다면 0의 결과가 나오지 않지\n- net진행될때 거의 w만 곱한 결과가 나오는 것인데 0이 hidden에서 나오면 w만 곱해도 변화가 없잖아? 그래서 주황색 선이 나온 거고\n\n`-` 첫번째 그림 $\\to$ 두번빼 그림\n\n- (파랑,주황,초록) 순서로 그려짐 \n- 파랑 = hidden * (-4.6804) + (-1.5440) \n- 주황 = hidden * (0.3071) + (0.9143) \n- 초록 = hidden * (5.2894) + (-1.3970) \n\n`-` 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. \n- weight: 파랑과 초록을 구분하는 역할을 함 \n- weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 'b'라고 나올 확률을 학습시킨다. $\\to$ 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0) \n\n--- \n\n**[참고](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): torch.nn.Linear()의 비밀?**\n- 사실 ${\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}$ 꼴에서의 ${\\bf W}$와 ${\\boldsymbol b}$가 저장되는게 아니다. \n- ${\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}$ 꼴에서의 ${\\bf A}$와 ${\\boldsymbol b}$가 저장된다. \n- ${\\bf W} = {\\bf A}^T$ 인 관계에 있으므로 l1.weight 가 우리가 생각하는 ${\\bf W}$ 로 해석하려면 사실 transpose를 취해줘야 한다. \n\n**왜 이렇게..?**\n- 계산의 효율성 때문 (numpy의 구조를 알아야함)\n- ${\\boldsymbol x}$, ${\\boldsymbol y}$ 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음) \n\n***잠깐 딴소리!!***\n\n(예시1)\n\n- 아래로 한칸 = 16칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시2)\n\n- 아래로 한칸 = 16칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시3)\n\n- 아래로 한칸 = 24칸 jump\n- 오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n- 아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n- 오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n**진짜 참고..**\n\n- 1바이트 = 8비트 \n- 1바이트는 2^8=256 의 정보 표현\n- np.int8은 8비트로 정수를 저장한다는 의미\n\n***딴소리 끝!!***\n\nweight의 transfose가 저장되는 이유 끝!\n\n혼자 크다고 인식하면 바꾸는...\n\n--- \n\n`-` 결과시각화2\n\n## Exam3: abcd\n\n### data\n\n### 하나의 은닉노드를 이용한 풀이 -- 억지로 성공 \n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n맞춘게 아니야. 0 근처인게 있잖아 hidden에서\n\n### 두개의 은닉노드를 이용한 풀이 -- 깔끔한 성공\n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n- hidden layer 2장만으로 4가지 state 표현\n\n## Exam4: abcde (스스로 공부)\n\n### data\n\n주어진 자료가 다음과 같다고 하자. \n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\n\n```Python\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)\n```\n\n### 3개의 은닉노드를 이용한 풀이 \n\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. \n- 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. ($2^1$)\n- 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. ($2^2$)\n- 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음. ($2^3$)\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n## Exam5: AbAcAd\n\n### data\n\n### 두개의 은닉노드를 이용한 풀이 -- 실패\n\n`-` 데이터정리 \n\n`-` 학습\n\n`-` 결과시각화1\n\n`-` 결과시각화2\n\n- A 뒤는 a,b,c 3개 중 하나, 따라서 1/3 의 확률이겠네 하고 학습해버렸다.\n- observation 마다 봐야하는 x가 달라(수가 )\n\n- 실패\n\n\n`-` 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다. \n\n|X|y|\n|:-:|:-:|\n|A,b|A|\n|b,A|c|\n|A,c|A|\n|c,A|d|\n|A,d|A|\n|d,A|b|\n|A,b|A|\n|b,A|c|\n|...|...|\n\n`-` 순진한 접근방식의 비판: \n- 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조\n- 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. \n- 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름. \n\n***이것에 대한 해결책은 순환신경망이다.***\n\n### 순환망을 위하여 data 다시정리\n\n`-` 기존의 정리방식\n\n`-` 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n\n숫자형태의 벡터형태라고 가정하고 학습하는 순환신경망\n\n### 실패했던 풀이의 재구현1\n\n`-` 방금 실패한 풀이 \n\n```Python\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n```\n\n`-` Tanh까지만 클래스로 바꾸어서 구현 \n- 클래스를 이용하는 방법: https://guebin.github.io/DL2022/2022/11/01/(9주차)-11월1일.html#로지스틱-모형을-이용한-풀이\n\n`-` for문돌릴준비\n\n`-` for문: 20회반복\n\n`-` linr(hnet(x)) 적합결과 <-- 숫자체크\n\n### 실패했던 풀이의 재구현2\n\n`-` Tanh까지 구현한 클래스\n\n`-` for문돌릴준비 \n\n`-` for문: 20회 반복\n\n`-` linr(hnet(x)) 적합결과 <-- 숫자체크\n\n- 4개가 필요한 원핫인코딩과 달리 hidden layer는 2개만으로 4state 표현했다\n\n### 순환신경망의 아이디어\n\n#### ***모티브***\n\n***(예비생각1) ${\\boldsymbol h}$에 대한 이해***\n\n${\\boldsymbol h}$는 사실 문자열 'abcd'들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) \n- (why1) h는 \"학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태\"라고 이해가능\n- (why2) 실제로 예시를 살펴보면 그러했다. \n\n결론: 사실 ${\\boldsymbol h}$는 잘 숙성되어있는 입력정보 ${\\bf X}$ 그 자체로 해석 할 수 있다. \n\n***(예비생각2) [수백년전통을 이어가는 방법](https://www.joongang.co.kr/article/24087690#home)***\n\n```\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n```\n\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬 \n\n`*` 기존방식\n- $\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}$\n\n`*` 수백년 전통의 간장맛을 유지하는 방식\n\n- $\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3$\n\n`*` 수백년 전통의 간장맛을 유지하면서 조리를 한다면? \n\n- $\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3$\n\n점점 맛있는 간장계란밥이 탄생함 \n\n`*` 알고리즘의 편의상 아래와 같이 생각해도 무방 \n\n- $\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1$, $\\text{간장}_0=\\text{맹물}$\n- $\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2$\n- $\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3$\n\n***아이디어***\n\n`*` 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로? \n\n- $\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1$\n- $\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2$\n- $\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3$\n\n이제 우리가 배울것은 (1) \"$\\text{콩물}_{t}$\"와 \"$\\text{간장}_{t-1}$\"로 \"$\\text{간장}_t$\"를 `숙성`하는 방법 (2) \"$\\text{간장}_t$\"로 \"$\\text{간장계란밥}_t$를 `조리`하는 방법이다 \n\n즉 `숙성`담당 네트워크와 `조리`담당 네트워크를 각각 만들어 학습하면 된다. \n\n#### ***알고리즘***\n\n**세부적인 알고리즘 ($t=0,1,2,\\dots$에 대하여 한줄 한줄 쓴 알고리즘)**\n\n(1) $t=0$\n\n${\\boldsymbol h}_0=[[0,0]]$ <-- $\\text{간장}_0$은 맹물로 초기화\n\n(2) $t=1$ \n\n${\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})$\n- ${\\boldsymbol x}_1$: (1,4) \n- ${\\bf W}_{ih}$: (4,2) \n- ${\\boldsymbol h}_0$: (1,2) \n- ${\\bf W}_{hh}$: (2,2) \n- ${\\boldsymbol b}_{ih}$: (1,2)\n- ${\\boldsymbol b}_{hh}$: (1,2)\n\n${\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}$\n\n$\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)$\n\n(3) $t=2$ <-- 여기서부터는 $t=2$와 비슷\n\n---\n\n**좀 더 일반화된 알고리즘**\n\n***(ver1)***\n\ninit $\\boldsymbol{h}_0$ \n\nfor $t$ in $1:T$ \n\n- ${\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})$\n- ${\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}$\n- $\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)$\n    \n\n***(ver2)***\n\n```Python \ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden)) # $H_{t-1}$\n    output = linr(hidden)\n    yt_hat = soft(output)\n```    \n\n- 코드상으로는 $h_t$와 $h_{t-1}$의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n---\n\n***전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음***\n\n```Python \n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)\n\n```    \n\n### 순환신경망 구현1 -- 성공 \n\n`(1)` 숙성담당 네트워크\n\n`(2)` 조리담당 네트워크\n\n`(3)` 손실함수, 옵티마이저 설계 \n\n`(4)` 학습 (6분정도 걸림)\n\n`(5)` 시각화 \n\n- 아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n- 왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까..\n\n### 순환신경망 구현2 (with RNNCell) -- 성공 \n\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n`(1)` 숙성네트워크\n\n선언\n\n가중치초기화 (순환신경망 구현1과 동일하도록)\n\n`(2)` 조리담당 네트워크\n\n`(3)` 손실함수, 옵티마이저 설계 \n\n`(4)` 학습 (6분정도 걸림)\n\n`(5)` 시각화 \n\n### 순환신경망 구현3 (with RNN) -- 성공 \n\n(예비학습)\n\n`-` 아무리 생각해도 yhat구하려면 좀 귀찮음\n\n`-` 이렇게 하면 쉽게(?) 구할 수 있음 \n\n- 똑같음!\n\n`-` rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다 \n\n***(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)***\n\n---\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자. \n\n`(1)` 숙성네트워크\n\n선언\n\n`(2)` 조리네트워크\n\n`(3)` 손실함수와 옵티마이저\n\n`(4)` 학습\n\n`(5)` 시각화\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-11-09-ml-10w.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"pulse","code-copy":false,"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"RNN (10주차)","author":"SEOYEON CHOI","date":"2022-11-09","categories":["Special Topics in Machine Learning","순환신경망"]},"extensions":{"book":{"multiFile":true}}},"ipynb":{"identifier":{"display-name":"Jupyter","target-format":"ipynb","base-format":"ipynb"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"2022-11-09-ml-10w.ipynb"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title-block-banner":true,"comments":{"utterances":{"repo":"seoyeonc/md"}},"title":"RNN (10주차)","author":"SEOYEON CHOI","date":"2022-11-09","categories":["Special Topics in Machine Learning","순환신경망"]}}},"projectFormats":["html"]}