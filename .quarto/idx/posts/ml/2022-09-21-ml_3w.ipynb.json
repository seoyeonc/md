{"title":"DNN (3주차)","markdown":{"yaml":{"title":"DNN (3주차)","author":"SEOYEON CHOI","date":"2022-09-21","categories":["Special Topics in Machine Learning","회귀분석","선형모형","손실함수","경사하강법"]},"headingText":"imports","containsRefs":false,"markdown":"\n\n기계학습 특강 (3주차) 9월21일 [회귀분석, 선형모형, 손실함수, 경사하강법]\n\n\n## 로드맵\n\n\n`-` 회귀분석 $\\to$ 로지스틱 $\\to$ 심층신경망(DNN) $\\to$ 합성곱신경망(CNN) \n\n`-` [강의계획서](https://github.com/guebin/DL2022/blob/master/_notebooks/DL2022.pdf)\n\n## ref \n\n`-` 넘파이 문법이 약하다면? (reshape, concatenate, stack)\n\n(1) reshape: 아래 링크의 넘파이공부 2단계 reshape 참고\n\nhttps://guebin.github.io/IP2022/2022/04/06/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%946%EC%9D%BC.html\n\n(2) concatenate, stack: 아래 링크의 넘파이공부 4단계 참고\n\nhttps://guebin.github.io/IP2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html\n\n## 회귀모형 소개\n\n`-` model: $y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n$ \n\n`-` model: ${\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}$\n\n- ${\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}$\n\n- $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix} \\quad = \\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}  + \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}$\n\n## 회귀모형에서 데이터 생성 \n\n---\n\n곱하지지 않았어야하지만 곱해짐..!\n\n---\n\nblue를 observe한 상태에서 orange를 measure함\n\n학습이 된 상태: prediction을 제시할 수 있는 상태\n\nunderline function을 아는 상태는 w0와 w1을 아는 상태라고 할 수 있다.\n\n$x_{new}$가 주어졌을때 underline function과 얼마나 떨어져 있나 보면 되니까\n\n## 회귀모형에서 학습이란?\n\n`-` 파란점만 주어졌을때, 주황색 점선을 추정하는것. 좀 더 정확하게 말하면 given data로 $\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}$를 최대한 $\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}$와 비슷하게 찾는것. \n\n- given data : $\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}$\n\n- parameter: ${\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$\n\n- estimated parameter: ${\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}$\n\n$\\hat{y} = x \\hat{W}$\n\n`-` 더 쉽게 말하면 아래의 그림을 보고 `적당한` 추세선을 찾는것이다. \n\n적당한?\n\n`-` 시도: $(\\hat{w}_0,\\hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. \n\n- $\\hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 \n\n`-` 벡터표현으로 주황색점선을 계산 \n\n**data를 보고 architecture를 설계하는 modeling과정**\n\n## 파라메터를 학습하는 방법 (적당한 선으로 업데이트 하는 방법) \n\n`-` 이론적으로 추론 <- 회귀분석시간에 배운것\n\n`-` **컴퓨터의 반복계산을 이용하여 추론 (손실함수도입 + 경사하강법)** <- 우리가 오늘 파이토치로 실습해볼 내용. \n\n`-` 전략: 아래와 같은 3단계 전략을 취한다. \n\n- stage1: 아무 점선이나 그어본다.. \n- stage2: stage1에서 그은 점선보다 더 좋은 점선으로 바꾼다. \n- stage3: stage1 - 2 를 반복한다. \n\n### ***Stage1: 첫번째 점선 -- 임의의 선을 일단 그어보자***\n\n`-` $\\hat{w}_0=-5, \\hat{w}_1 = 10$ 으로 설정하고 (왜? 그냥) 임의의 선을 그어보자.\n\n- 처음에는 ${\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}=\\begin{bmatrix} -5 \\\\ 10 \\end{bmatrix}$ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 \n\n- 끝에 requires_grad=True는 나중에 미분을 위한 것 \n\ntensor에서 tf.variable로 출력할떄롸 같은 결과임\n\n꼬리표가 생겼다.\n\n꼬리표가 사라졌다.\n\n꼬리표 있어도 계산은 되지만, matplot에서는 오류..\n\n그려보자!\n\n### ***Stage2: 첫번째 수정 -- 최초의 점선에 대한 '적당한 정도'를 판단하고 더 '적당한' 점선으로 업데이트 한다.***\n\n`-` '적당한 정도'를 판단하기 위한 장치: loss function 도입!\n\n$loss=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2$\n\n$=({\\bf y}-{\\bf\\hat{y}})^\\top({\\bf y}-{\\bf\\hat{y}})=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})$\n\n`-` loss 함수의 특징 \n- $y_i \\approx \\hat{y}_i$ 일수록 loss값이 작다. \n- $y_i \\approx \\hat{y}_i$ 이 되도록 $(\\hat{w}_0,\\hat{w}_1)$을 잘 찍으면 loss값이 작다. \n- (중요) 주황색 점선이 '적당할 수록' loss값이 작다. \n\n`-` 우리의 목표: 이 loss(=8587.6875)을 더 줄이자. \n- 궁극적으로는 아예 모든 조합 $(\\hat{w}_0,\\hat{w}_1)$에 대하여 가장 작은 loss를 찾으면 좋겠다. (stage2에서 할일은 아님)\n\n`-` 문제의 치환: 생각해보니까 우리의 문제는 아래와 같이 수학적으로 단순화 되었다. \n- 적당해보이는 주황색 선을 찾자 $\\to$ $loss(w_0,w_1)$를 최소로하는 $(w_0,w_1)$의 값(정의역 set)을 찾자. \n\n`-` 수정된 목표: $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$을 구하라. \n- 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음.\n- 즉 \"적당한 선으로 업데이트 하라 = 파라메터($W$)를 학습 하라 = 손실함수를 최소화 하라\" \n\n`-` 우리의 무기: 경사하강법, 벡터미분 \n\n---\n\n##### ***Stage2를 위한 경사하강법 복습*** \n\n**경사하강법 아이디어 (1차원)**\n\n(step 1) 임의의 점을 찍는다. \n\n(step 2) 그 점에서 순간기울기를 구한다. (접선) <-- 미분\n\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. \n\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절한다. \n\n(서연필기) 접선 음수면 오른쪽으로 가고 접선 양수면 왼쪽으로 가쟈~\n\n**경사하강법 아이디어 (2차원)**\n\n(step 1) 임의의 점을 찍는다. \n\n(step 2) 그 점에서 순간기울기를 구한다. (접평면) <-- 편미분\n\n(step 3) 순간기울기(=미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다.\n\n(팁) 기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절한다. \n\n(서연필기) x,y 다르게 정의하는~편미분\n\n**loss를 줄이도록 ${\\bf W}$를 개선하는 방법**\n\n`-` $수정값 \\leftarrow 원래값 - 기울어진크기(=미분계수) \\times \\alpha$\n\n- 여기에서 $\\alpha$는 전체적인 보폭의 크기를 결정한다. 즉 $\\alpha$값이 클수록 한번의 update에 움직이는 양이 크다. \n\n`-` ${\\bf W} \\leftarrow {\\bf W} - \\alpha \\times \\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)$\n\n(서연필기) 미분계수 반대로 움직이기 위해 마이너스(-) 취해주자\n\n(서연필기) 알파자체가 음수면 방향이 바뀌니까 양수!\n\n- 마이너스의 의미: 기울기의 부호를 보고 반대방향으로 움직여라. \n\n- $\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. \n\n- $\\alpha$의 의미: 전체적인 보폭의 속도를 조절, $\\alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. \n\n---\n\n`-` 우리의 목표: loss=8587.6875 인데, 이걸 줄이는 것이 목표라고 했었음. 이것을 줄이는 방법이 경사하강법이다. \n\n`-` 경사하강법으로 loss를 줄이기 위해서는 $\\frac{\\partial}{\\partial {\\bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. (`loss.backward()`로 하면된다)\n\n(서연필기) What.grad의 결과 값이 생겼다!\n\n(서연필기) loss 계산할때 What에있는 꼬리표가 따라와서 loss에도 꼬리표가 붙었다.\n\n- loss.backward()의 의미: loss를 미분해라! 뭘로? `requires_grad=True`를 가진 텐서로!!\n\n- \n```python\nloss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2)\n# 이었고 \nWhat=torch.tensor([-5.0,10.0],requires_grad=True)\n# 이므로 결국 What으로 미분하라는 의미. \n# 미분한 식이 나오는 것이 아니고, \n# 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. \n```\n\n`-` 위에서 `loss.backward()`의 과정은 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. \n\n`-` (-5,10)에서 loss의 순간기울기 값은 `What.grad`로 확인가능하다. \n\n- 이것이 의미하는건 $(-5,10)$에서의 $loss(w_0,w_1)$의 순간기울기가 $(-1342.2523, 1188.9307)$ 이라는 의미\n\n`-` (확인1) `loss.backward()`가 미분을 잘 계산해 주는 것이 맞는가? 손계산으로 검증하여 보자. \n\n- $loss(w_0,w_1)=({\\bf y}-\\hat{\\bf y})^\\top ({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf XW})^\\top ({\\bf y}-{\\bf XW})$\n\n- $\\frac{\\partial}{\\partial {\\bf W} }loss(w_0,w_1)=-2{\\bf X}^\\top {\\bf y}+2{\\bf X}^\\top {\\bf X W}$\n\n`-` (확인2) `loss.backward()`가 미분을 잘 계산해 주는 것이 맞는가? 편미분을 간단히 구현하여 검증하여 보자. \n\n- $\\frac{\\partial}{\\partial {\\bf W} } loss(w_0,w_1)=\\begin{bmatrix}\\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\end{bmatrix}loss(w_0,w_1) =\\begin{bmatrix}\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\end{bmatrix}$\n\n- $\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}$\n\n- $\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}$\n\n스칼라일때\n```python\nh = 0.01\n(loss(w+h) - loss(w)) / h\n```\n\n-5,10에서의 편미분한 순간기울기\n\n- 약간 오차가 있지만 얼추비슷 $\\to$ 잘 계산했다는 소리임 \n\n(서연필기) 꼭 정확하진 않지!\n\n`-` 수정전, 수정하는폭, 수정후의 값은 차례로 아래와 같다. \n\n`-` Wbefore, Wafter 계산 \n\ndata쓰는지 grad 쓰는지 명확히\n\n`-` Wbefore, Wafter의 시각화 \n\n### ***Stage3: Learn (=estimate $\\bf\\hat{W})$*** \n\n`-` 이 과정은 Stage1,2를 반복하면 된다. \n\n(서연필기) **What.grad=None 해주는 이유는 grad가 미분을 누적하기 때문에 막아주기 위해서**\n\n- 원래 철자는 epoch이 맞아요 \n\n`-` 반복결과는?! (최종적으로 구해지는 What의 값은?!)\n- 참고로 true\n\n`-` 반복결과를 시각화하면? \n\n## 파라메터의 학습과정 음미 (학습과정 모니터링) \n\n### 학습과정의 기록\n\n`-` 기록을 해보자. \n\n(서연필기) list 그대로 받으니까 꼬리표 삭제\n\n`-` $\\hat{y}$ 관찰 (epoch=3, epoch=10, epoch=15)\n\n`-` $\\hat{\\bf W}$ 관찰\n\n`-` loss 관찰 \n\n### 학습과정을 animation으로 시각화\n\n`-` 왼쪽에는 $(x_i,y_i)$ and $(x_i,\\hat{y}_i)$ 을 그리고 오른쪽에는 $loss(w_0,w_1)$ 을 그릴것임\n\n`-` 왼쪽그림! \n\n`-` 오른쪽 그림1: $loss(w_0,w_1)$\n\n`-` 오른쪽 그림2: $(w_0,w_1)=(2.5,4)$ 와 $loss(2.5,4)$ 값 <- loss 함수가 최소가 되는 값 (이거 진짜야? ㅋㅋ)\n\n`-` 오른쪽 그림3: $(w_0,w_1)=(-3.66, 8.81)$ 와 $loss(-3.66,8.81)$ 값\n\n`-` 애니메이션\n\n`-` 함수로 만들자.. \n\n(서연필기) 알파의 정도에 따라 학습 속도가 달라져..\n\n## $\\alpha$에 대하여 ($\\alpha$는 학습률) \n\n### (1) $\\alpha=0.0001$: $\\alpha$ 가 너무 작다면? $\\to$ 비효율적이다. \n\n### (2) $\\alpha=0.0083$: $\\alpha$가 너무 크다면? $\\to$ 다른의미에서 비효율적이다 + 위험하다.. \n\n### (3) $\\alpha=0.0085$\n\n(서연필기) 최솟값보다 오히려 커지는 경향이 나와버림\n\n### (4) $\\alpha=0.01$\n\n## [숙제](https://ieilms.jbnu.ac.kr/)\n\n`-` 학습률($\\alpha$)를 조정하며 실습해보고 스크린샷 제출 \n\n### (1) $\\alpha=0.0015$\n\n### (2) $\\alpha=0.0038$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-09-21-ml_3w.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"pulse","code-copy":false,"title-block-banner":true,"title":"DNN (3주차)","author":"SEOYEON CHOI","date":"2022-09-21","categories":["Special Topics in Machine Learning","회귀분석","선형모형","손실함수","경사하강법"]},"extensions":{"book":{"multiFile":true}}}}}