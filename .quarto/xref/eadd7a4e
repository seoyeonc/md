{"entries":[],"headings":["seq2seq-의-한계","트랜스포머의-주요-하이퍼파라미터","transformer","positional-encoding","attention","encoder","self-attention-of-encoder","self-attention","q-k-v","scaled-dot-product-attention","행렬-연산으로-일괄-처리","scaled-dot-product-attention-구현","multi-head-attention","multi-head-attention-구현","padding-mask","position-wise-feed-forward-neural-network","residual-connection-and-layer-normalization","잔차-연결","층-정규화","encoder-구현","encoder-쌓기","encoder에서-decoder로","decoder-self-attention-and-look-ahead-mask","nd-decoder-sublayer-encoder-decoder-attention","decoder-구현","decoder-쌓기","transformer-구현","transformer-hyperparameter-정하기","loss-function-정의","학습률"]}